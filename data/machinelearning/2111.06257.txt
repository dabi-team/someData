1
2
0
2

v
o
N
5

]

G
L
.
s
c
[

1
v
7
5
2
6
0
.
1
1
1
2
:
v
i
X
r
a

Branch and Bound in Mixed Integer Linear Programming
Problems: A Survey of Techniques and Trends

Lingying Huang a 1 , Xiaomeng Chen a 1 , Wei Huo a 1 , Jiazheng Wang b,
Fan Zhang b, Bo Bai b, Ling Shi a

aDepartment of Electronic Engineering, HKUST, Clear Water Bay, Kowloon, Hong Kong

bTheory Lab, Huawei Hong Kong Research Centre, Hong Kong SAR, China

Abstract

In this paper, we surveyed the existing literature studying diﬀerent approaches and algorithms for the four critical components
in the general branch and bound (B&B) algorithm, namely, branching variable selection, node selection, node pruning, and
cutting-plane selection. However, the complexity of the B&B algorithm always grows exponentially with respect to the increase
of the decision variable dimensions. In order to improve the speed of B&B algorithms, learning techniques have been introduced
in this algorithm recently. We further surveyed how machine learning can be used to improve the four critical components in
B&B algorithms. In general, a supervised learning method helps to generate a policy that mimics an expert but signiﬁcantly
improves the speed. An unsupervised learning method helps choose diﬀerent methods based on the features. In addition,
models trained with reinforcement learning can beat the expert policy, given enough training and a supervised initialization.
Detailed comparisons between diﬀerent algorithms have been summarized in our survey. Finally, we discussed some future
research directions to accelerate and improve the algorithms further in the literature.

Key words: Branch and bound, Mixed integer linear programming problems, Machine learning

1 Introduction

Branch and bound (B&B) algorithm is a widely-used method to produce exact solutions to non-convex and combi-
natorial problems which cannot be solved in polynomial time. It was initially proposed by Land and Doig [1] to solve
discrete programming problems. This method implicitly enumerates all possible solutions by iteratively dividing the
original problem into a series of sub-problems, organized in a tree structure, and discarding the sub-problems where
a global optimum cannot be found. The approaches adopted to generate the sub-problems of the unexplored nodes
in the tree represent the “branching” step, while the “bounding” phase consists of rules used to prune oﬀ regions of
sub-optimal search space. Once the entire tree has been explored, the exact solution can be achieved.

Many decisions aﬀect the performance of B&B by guiding the search to promising space and enhancing the chance of
quickly ﬁnding an exact solution. These decisions are the variable selection (i.e., which of the fractional variables
to branch on), the node selection (i.e., which of the current nodes to explore next), the pruning rules (i.e., rules
that prevent exploration of the sub-optimal space) and the cutting rules (i.e., rules that add constraints to ﬁnd
cutting planes). A well-designed decision strategy can reduce the search space and thus speed up the search progress
of the B&B algorithm signiﬁcantly.

1 These authors contributed equally to this work.

Email addresses: lhuangaq@connect.ust.hk (Lingying Huang), xiaomeng.chen@connect.ust.hk (Xiaomeng Chen),
whuoaa@connect.ust.hk (Wei Huo), wang.jiazheng@huawei.com (Jiazheng Wang), zhang.fan2@huawei.com (Fan Zhang),
baibo8@huawei.com (Bo Bai), eesling@ust.hk (Ling Shi).

Preprint submitted to Discrete Optimization

12 November 2021

 
 
 
 
 
 
Substantial research has been done to study the eﬃcient and manually-designed strategies to deal with the four
decisions [2], [3]. Unfortunately, the traditional strategies are only designed for each problem type, and are not
adopted to a family of problems. In order to speed up B&B that are adapted to a family of problems, machine
learning components are integrated. The aim of this paper is to provide a survey of learning-based B&B, particularly
regarding the above four decisions.

1.1 Motivation

For a B&B algorithm, machine learning improves its performance on a family of problems in two ways. On the
one hand, expert knowledge is assumed about the decision strategy, while some heavy computation is preferred
to be substituted by an approximation method. Under this circumstance, the learning method can generate such
approximation without the need to devise new algorithms. On the other hand, the expert knowledge may be insuf-
ﬁcient, which cannot satisfy some decisions. Hence, in this case, the learning approach aims to explore the space of
all decisions and learn the best performing policy from this experience, improving the algorithm performance. For
a machine learning method, B&B is able to decompose the original problem into smaller learning tasks. The tree
structure of B&B acts as a relevant prior for the learning model.

1.2 Setting

A common setting where B&B is adopted is the Mixed-Integer Linear Programming (MILP) problem, which is
deﬁned as follows.

Deﬁnition 1 (MILP). Given a matrix A ∈ Rm×n, vectors b ∈ Rm and c ∈ Rn, and a subset I ⊆ {1, . . . , n}, the
mixed-integer linear program M ILP = (A, b, c, I) is:

z(cid:63) = min{cT x|Ax ≤ b, x ∈ Rn, xj ∈ Z, ∀j ∈ I}

The vectors in the XM ILP = {x ∈ Rn|Ax ≤ b, x ∈ Rn, xj ∈ Z, ∀j ∈ I} are called feasible solution of MILP. A feasible
solution x(cid:63) ∈ XM ILP of MILP is optimal if its objective value satisﬁes cT x(cid:63) = z(cid:63).

Owing to the integrality requirement, MILP is an NP-hard problem. Most modern MILP solvers such as CPLEX [4],
LINDO [5] and SIP [6], iteratively split the problem into smaller subproblems, building a search tree (see Section
2.1). To solve MILP, B&B implements a divide-and-conquer algorithm, where a linear programming (LP) relaxation
of the problem is computed by removing the integrality conditions.

Deﬁnition 2 (LP relaxation of a MILP). The LP relaxation of a MILP is:

ˇz = min{cT x|Ax ≤ b, x ∈ Rn}

A lower bound for the whole subtree is provided by solving the LP relaxation, and if the objective value ˇz of the LP
relaxation is larger than or equal to the value ˆz = cT ˆx of the current best solution ˆx, the corresponding node can
be discarded.

The most essential components of an MILP solver implementation are a branching rule, a node selection strategy, a
fast and stable LP solver and cutting plane separators, which will be introduced in the next section.

1.3 Outline

We have introduced the context and motivations for building the B&B algorithms together with machine learning.
The remainder of this survey is organized as follows. Section 2 overviews the implementation of the general B&B
algorithm and summarizes the existing literature studying diﬀerent approaches and algorithms for the four key
components, namely, branching variable selection, node selection, node pruning, and cutting-plane selection. Section
3 provides prerequisites in machine learning to make the reader familiar with some essential concepts required to
understand the learning-based B&B algorithms. Section 4 provides a survey of the learning techniques to deal with
the four critical components in B&B algorithms for MILP. Further elaboration on the contributions and limitations
of diﬀerent studies are provided. Section 5 presents some future work directions, and Section 6 summarizes the
contributions of this survey.

2

Fig. 1. Four core components in B&B algorithms and their related work.

2 Branch and Bound Algorithms

In this section, we give an overview of the general B&B algorithm, along with a detailed description of the four core
components in B&B. The main survey results are summarized in Figure 1.

2.1 Algorithm Overview

Deﬁne an optimization problem as P = (D, f ), where D (search space) is denoted as a set of valid solutions
to the problem and f : D → R is denoted as the objective function. The problem P aims to ﬁnd an optimal
solution x(cid:63) ∈ arg minx∈D f (x). A search tree T of subproblems is built by Branch and-Bound in order to solve
problem P. Moreover, a feasible solution ˆx ∈ D is globally stored. At each iteration, B&B selects a new subset of the
search space S ⊂ D for exploration from a queue L of unexplored subsets. Then, if a solution ˆx(cid:48) ∈ S (candidate

3

incumbent) has a better objective value than ˆx, i.e., f (ˆx(cid:48)) < f (ˆx), the incumbent solution is updated. On the
other side, the subset is pruned or fathomed if there is no solution in S with better objective solution than ˆx,
i.e., f (ˆx) ≥ f (ˆx), ∀x ∈ S. Otherwise, the subset S is branched into child subproblems S1, S2, . . . , Sr, which are then
pushed onto L. Once there is no unexplored subsets in queue L, the best incumbent solution is returned and the
algorithm terminates. Pseudocode for the generic B&B is given in Algorithm 1.

Select a subproblem S from L to explore
if a solution ˆx(cid:48) ∈ {x ∈ S|f (x) < f (ˆx)} can be found then

Set ˆx = ˆx(cid:48)

Algorithm 1 Branch and Bound (D, f )
1: Set L = D and initialize ˆx
2: while L (cid:54)= ∅ do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12: end while
13: return ˆx

Partition S into S1, S2, . . . , Sr
Insert S1, S2, . . . , Sr into L

end if
if S cannot be pruned then

end if
Remove S from L

In terms of the above pseudocode, the variable selection strategy (branching rules) aﬀects how the subproblem is
partitioned in Line 7 of Algorithm 1; the node selection strategy aﬀects the order of which nodes is selected to
explore (Line 3 of Algorithm 1), and the pruning rule in Line 6 of Algorithm 1 determines if S is fathomed.

Fig. 2. Adopting B&B to solve a minimization integer linear programming.

Figure 2 illustrates a concrete example of B&B algorithm for a minimization mixed linear integer programming. The
optimization problem is shown in the upper right corner of the Figure 2. The original upper bound is 0, which is
computed at x1 = 0, x2 = 0. At each node, a local lower bound is computed by the LP relaxation problem by the
LP solver. A local upper bound is updated when an integer solution is found. At each iteration, we can compare the
solution of the current LP relaxation with the minimum upper bound so far, which is called the incumbent solution
value. If the latter is smaller than the former for a certain subproblem, no better mixed-integer solution can be found
and the node can be fathomed. In Figure 2, the fathomed nodes are shown in red rectangles.

4

Algorithm 2 Branching Variable Selection
Input: Subproblem of the current node S with its optimal LP solution ˆx /∈ XM ILP
Output: A subscript i ∈ I of an integer variable with fractional value ˆxi /∈ Z
1: Deﬁne branching candidates set C = {i ∈ I | ˆxi /∈ Z}
2: For each candidate i ∈ C, calculate its score value si ∈ R
3: return i = arg mini∈C si

In order to enhance the above enumeration framework, mixed-integer linear programming solvers tend to adopt the
cutting planes (linear inequalities) to the original formulation, especially at the root node. By adding cuts to the
original linear programming, some parts of the feasible region is eliminated which strengthens the LP relaxation.

2.2 Branching Variable Selection

As a critical task in Brand-and-Bound, branching variable selection decides how to partition a current node into
two-child nodes recursively. Speciﬁcally, it decides which fractional variables (also known as candidates) to branch
on. Branching on an ineﬃcient variable that does not simplify subproblems doubles the size of the B&B tree, thus
making no sense. The ultimate objective of an eﬀective branching strategy is to minimize the number of explored
nodes before the algorithm terminates. To indicate the quality of a candidate variable, score of this variable is used
to measure its eﬀectiveness, and the candidate with the highest score is picked to branch on. Pseudocode for the
generic variable selection is presented in Algorithm 2.

The diﬀerence among various branching policies is how the score is computed, and we will introduce diﬀerent variable
selection methods in the following. Making high-quality branching decisions is usually nontrivial and time-consuming.
Although a good branching method should produce trees as small as possible, the primary goal of solving large-scale
optimizations is to spend as little time as possible. Hence, great branching strategies should compromise between the
quality of decisions taken and the time spent taking each decision. This trade-oﬀ is the focus of the entire branching
study.

An early intuitive strategy is known as most infeasible branching (MIB), choosing the most fractional variable
to branch on, i.e., branching on a variable whose fractional part is closest to 0.5. Although this method can be
evaluated easily, numerical results in Achterberg et al. [7,8] have indicated that the performance of this rule is worse
than that of selecting the variable randomly. Later, pseudocost branching (PC) was proposed (see Falk [9]). This
method maintaining a history of variables’ branchings and corresponding dual bound increases averages previous
improvements to obtain the expected gain for each candidate. This expected gain is the score for each candidate
variable; thus, the variable that can yield a signiﬁcant change in the objective value is chosen to branch on. While
PC works well in saving computation time, it is ineﬃcient at the beginning of the B&B tree since there is no
reliable history at the root. Hence, the pseudocosts for each variable should be initialized, which requires signiﬁcant
manual tuning. An extreme strategy is strong branching (SB), which was ﬁrst proposed in the context of the traveling
salesman problem, shown in Bixby and Cook [10]. This policy chooses the candidate that yields the best improvement
in the dual bound. Its full version, full strong branching (FSB), solves the resulting LPs optimally for each candidate.
This method makes an excellent decision at each step, producing the smallest B&B tree, but the computation time
is extremely high, causing it intractable in practice. Hence, SB tries to accelerate the FSB by only focusing on a
smaller candidate set of variables instead of all candidates, and operating fewer simplex pivots rather than solving
subproblems optimally. To circumvent the drawbacks of PC and SB, hybrid strong and pseudocost (HSB) applies
accurate SB in the upper level of the tree until a certain depth and then employs PC based on the initialization given
by SB. In this way, the diﬃculty of PC initialization and expensive computation of SB are both overcome. This idea
was ﬁrst proposed by Falk [9] and further developed by Linderoth and Savelsbergh [11]. More recently, reliability
branching (RB) further improves the combination of pseudocost and strong branchings, presented in [8]. It switches
between PC and SB according to the “reliability” of a candidate based on a pre-deﬁned reliability threshold. For
a variable, if the number of times it is selected is less than the threshold, it will be considered unreliable, and its
score will be calculated via SB. Otherwise, if this candidate has been selected several times, it will be regarded as
very reliable. Therefore, we can use the less accurate but very time-eﬃcient pseudocost method to get its score.
The last rule worth mentioned here is hybrid branching (HB) from Achterberg and Berthold [12], which integrates
ﬁve kinds of scores from diﬀerent selection criteria. These scores are normalized and merged into a single value
through a weighted sum. In addition to what we have introduced above, there are some other branching strategies,
such as backdoor branching developed by Fischetti and Monaci [13] selecting a subset of top-ranked candidates,
information-theoretic entropy branching in Gilpin and Sandholm [14] regarding fractional variables as probabilities

5

and trying to remove as much uncertainty as possible from subproblems by branching, and nonchimerical branching
shown in Fischetti and Monaci [15].

2.3 Node Selection

After a sub-problem has been produced by constraining some integer variables in MILP, the solving process can
continue with any sub-problem that is a leaf of the current search tree. We refer to the sub-problems as nodes, and
node selection designs which node to process in the next step. The existing literature always selects the next node
based on the following two usually opposing goals:

(1) Finding good feasible MILP solutions to improve the primal (upper) bound, which helps to prune the search

tree by bounding;

(2) Improving the global dual (lower) bound.

Therefore, in the following, we ﬁrst take a brief overview of diﬀerent search methods in the literature and then
introduce diﬀerent methods of primal heuristics to ﬁnd feasible solutions.

2.3.1 Diﬀerent Search Methods

Dakin [16] proposed depth-ﬁrst search for MILP. This node selection rule always chooses a node from the leaf
queue with the maximal depth in its search tree. Depth-ﬁrst search is the preferred strategy for pure feasibility
problems. The sub-problem management is reduced to the minimum due to the similarity between two subsequent
sub-problems. Compared with other search methods, it has small memory consumption. However, depth-ﬁrst search
completely disregards the second goal to improve the global dual bound. To improve the global dual bound as fast
as possible, the best-ﬁrst search, which selects a leaf with the currently smallest dual objective value, was introduced
by Hart [17]. It was shown by Achterberg [7] that there exists a node selection strategy of the best-ﬁrst search type,
which solves the instance in a minimal number of nodes. None of the above methods, however, tries to improve
the quality of integer feasible solution. In order to ﬁnd a good feasible solution as soon as possible, the best search
estimate was proposed. There are mainly two kinds of estimate schemes, i.e., the best projection criterion by B´enichou
et al. [18], and the best estimate rule by Forrest et al. [19], which diﬀer in how they determine an estimate of the
best solution obtainable from a node. The best projection criterion calculates the objective value increase per unit
decrease in infeasibility, while the best estimate rule employs the pseudocost values [18] to estimate the increase in
the objective value. Linderoth and Savelsbergh [20] further indicated that the best estimate rule outperforms the
best projection rule in computing time and shows the backtracking methods to improve the node selection method
by using estimates to avoid superﬂuous nodes.

2.3.2 Primal Heuristics

Heuristics are procedures that try to ﬁnd good solutions in a short time by orientating themselves on some information
which is helpful to lead to the desired result. Since it is a costly method and has a worst-case runtime to solve the
MILP problems, primal heuristics are crucial to ﬁnding quality feasible solutions quickly [21]. In addition, primal
heuristics which help ﬁnd a feasible solution have the following advantages [7]

(1) It proves the model is feasible, which is an indication that there is no error in the model.
(2) The quality of the heuristic solution can be adjusted to the user’s requirement; thus, the process can be

terminated at an early stage.

(3) Feasible solutions help to prune the search tree by bounding which reduces the work of the B&B algorithm.
(4) The better the current incumbent is, the more reduced cost ﬁxing and other dual reductions can be applied to

tighten the problem formulation.

Berthold [21] summarized ﬁve classic primal heuristic methods, including OCTANE by Balas et al. [22], feasi-
bility pump by Fischetti et al. [23], local branching by Fischetti and Lodi [24], relaxation induced neighborhood
search (RINS) by Danna et al. [25] and Mutation by Rothberg [26]. They then developed two new methods based
on the above classic methods, namely, relaxation enforced neighborhood search (RENS) and Crossover. The above
primal heuristics are grouped into two categories based on whether the algorithm needs a previous feasible point:

(1) Start heuristics: ﬁnd a feasible solution early, such as OCTANE [22], feasibility pump [23] and RENS [21].

6

(2) Improve heuristics: start with a primal feasible point and improve either the feasibility condition or the opti-

mality quality iteratively, such as local branching [24], RINS [25] and Mutation [26].

Berthold also summarized the advantages and disadvantages of diﬀerent primal heuristics in [21]. More overviews
comparing diﬀerent primal heuristics about their computational costs and their impact on problem-solving can be
seen in [27, 28].

2.3.3 Evaluation Criteria

To evaluate the quality of a node search criteria, we divide the evaluation criteria into two groups, one related to
the feasibility quality, while the other is related to the quality of the feasible solution found so far.

The feasibility quality is evaluated by the pseudocost values of the variables in the best estimate rule of Forrest et
al. [19], i.e.,

(cid:88)

eF =

min{P −

j f −

j , P +

j f +

j },

(1)

j = xj − (cid:98)xj(cid:99) and f +

with f −
function, respectively, and the diﬀerent pseudocoast values initialization can be seen from [20].

j = (cid:100)xj(cid:101) − xj, where xj is the current LP solution, (cid:98)·(cid:99) and (cid:100)·(cid:101) are the ﬂoor and ceil

j∈I

In order to evaluate the quality of a feasible solution concerning the objective function, three relative gaps are
introduced.

Deﬁnition 3 Let x(t) be a feasible solution of a given MILP, x∗ be an optimal or best known solution of this MILP
at time t, and c(cid:62)y(t) be the current dual bound during a MILP solving process.

The primal gap is deﬁned as

γp(t) =






1

if c(cid:62)x∗ · c(cid:62)x(t) < 0,

c(cid:62)x(t) − c(cid:62)x∗
max{|c(cid:62)x(t)|, |c(cid:62)x∗|, ε}

otherwise.

where use ε = 10−12 to avoid the division by 0.

The dual gap is deﬁned as

γd(t) =






1

if c(cid:62)x∗ · c(cid:62)y(t) < 0,

c(cid:62)x∗ − c(cid:62)y(t)
max{|c(cid:62)x∗|, |c(cid:62)y(t)|, ε}

otherwise.

The primal-dual gap is

γpd(t) =






1

if c(cid:62)x(t) · c(cid:62)y(t) < 0,

c(cid:62)x(t) − c(cid:62)y(t)
max{|c(cid:62)x(t)|, |c(cid:62)y(t)|, ε}

otherwise.

(2)

(3)

(4)

To take into account the whole solution process, Berthold [29] introduced a new performance measure, in particular
for benchmarking primal heuristics. The progress of the primal bound’s convergence towards the optimal solution
over the entire solving time is revealed in this measurement. This method makes use of the primal gap and computes
the integral of the function over time. Therefore, we call this measurement the primal integral.

Deﬁnition 4 (Primal Integral) Let tmax ∈ R≥0 be a limit on the solution time of the B&B MILP solver. The
primal gap function p : [0, tmax] (cid:55)−→ [0, 1] is deﬁned as

p(t) =

(cid:40)

1

if no incumbent is found until time t,

γp(t) otherwise.

(5)

7

The primal integral P (T ), T ∈ [0, tmax] of a B&B run is deﬁned as

(cid:90) T

P (T ) =

p(t)dt.

t=0

(6)

2.4 Node Pruning

Pruning suboptimal branches is an important part of B&B algorithms since it keeps the B&B tree and the computing
steps small, which reduces the solving time and the required memory. In a standard B&B algorithm, the pruning
policy prunes a node only if one of the following conditions is met:

(1) Prune by bound: compute a lower bound on the objective function value at each node. If the lower bound of

the node is larger than the optimal objective value obtained, the node will be pruned, i.e., ˇz > ˆz.

(2) Prune by infeasibility: if the relaxed problem of this node is infeasible, which means the lower bound of this

node is ∞. This can be viewed as a special case of prune by bound.

(3) Prune by integrality: if the obtained solution for the relaxed problem satisﬁes the integer constraints, it is

unnecessary to search the children of this node.

We call the nodes satisfying one of the above conditions as fathomed nodes.

2.4.1 Lower Bounds

The most common way to prune is pruning by bound. If the lower bound of the objective function value ˆz is smaller,
more subproblems can be pruned since the chance for ˇz > ˆz is larger. In general, many diﬀerent lower bounds can
be computed as necessary; some lower bound computations may be easy to perform, whereas others may be more
computationally intensive. Vil`a and Pereira [30], and Morrison et al. [31] attempted to prune using the manageable
lower bounds ﬁrst and then move on to more complex, but tighter, lower bounds. To improve lower bounds, Arora
et al. [32] derived a new formulation with a tighter integrality gap.

Another method for deriving lower bounds is through duality. Integer programming duality methods is adopted not
limited in Phan [33], Vil`a and Pereira [30], and Gendron et al. [34].

2.4.2 Dominance Relations

Kohler and Steiglitz [35] ﬁrst studied the dominance relations. If a node S1 dominates the other node S2, this means
that for any solution that is a descendant of S2, there exists a complete solution descending from S1 which is no worse
than that. Therefore, dominance relations allow nodes to be pruned if they are dominated by some other nodes.
There are two primary types of dominance relations, the memory-based dominance rules such as Sewell et al. [36]
and the non-memory-based dominance rules such as Fischetti and Salvagnin [37]. The memory-based dominance
rules require the entire search tree to be stored for the duration of the algorithm. In contrast, non-memory-based
dominance relations do not require the dominating state to have been previously generated in the search process. For
example, by solving an auxiliary problem, the method in Fischetti and Salvagnin [37] is able to identify whether a
node in the tree is dominated by some other node regardless of whether explored before. Note that Demeulemeester
et al. [38] showed that at least one node cannot be pruned if there is a dominance cycle. Ibaraki [39] showed that
dominance relations do not always improve the pruning quality; however, dominance relations would prune a lot of
redundant nodes for problems with a high degree of symmetry. Margot [40, 41] introduced isomorphic pruning to
recognize symmetry.

2.5 Cutting-Plane Selection

Cutting planes are additional linear constraints violated by the current LP solution but do not cut oﬀ integer feasible
solutions. Speciﬁcally, cutting plane (sometimes called valid inequalities) methods repeatedly add cuts to the LPs,
excluding some part of the feasible region while conserving the integral optimal solution so that the LP relaxation
can be tightened. The diﬀerence between tightening LP relaxation by branching and cutting planes is illustrated in
Figure 3.

8

(a) Feasible region of original LP re-
laxation

(b) branch and bound

(c) Cutting-plane

Fig. 3. Tighting LP relaxation by branching and cutting planes.

Depending solely on cutting plane methods is intractable for solving MILPs, and thus, they are always combined
with B&B to tighten the bound further for pruning the tree. According to where cutting planes are generated at
the root or subproblems in the B&B trees, there are two algorithms called cut-and-branch and branch-and-cut,
respectively. The former only generates cutting planes at the root of the B&B trees, while the latter also produces
cutting planes at subproblems. The branch-and-cut is the core of state-of-the-art commercial IP solvers and has
been applied in Mitchell [42], Padberg and Rinaldi [43], and Balas et al. [44]. Moreover, in branch-and-cut, globally
valid cuts and locally valid cuts should be distinguished since cuts locally generated at a particular node may be
invalid for other nodes, while valid global inequalities can be used for all subproblems. Next, we will brieﬂy introduce
cutting planes useful in solving MILPs, including valid inequalities for general integer programs and problems with
speciﬁc structures.

2.5.1 General-purpose Cuts

One of the most fundamental cutting plane methods for MILPs is Gomory’s method. Gomory’s fractional cut was
proposed by Gomory [45] to solve pure integer linear programmings. This method is based on the simplex tableau
and has been proved to converge in ﬁnite time. From the geometric perspective, Chv´atal et al. [46] illustrated that a
convex hull of the integer feasible solutions could be obtained by adding a ﬁnite number of Gomory’s cuts, known as
Chv´atal-Gomory cutting-planes. However, Marchand et al. [47] have shown that this approach still fails if a problem
involves continuous variables. Then, Gomory’s extended the fractional cuts to deal with MILPs, known as Gomory’s
mixed integer cuts (GMI) in Gomory [48]. It has been the ﬁrst general-purpose cutting planes successfully applied
with a branch and cut framework to solve MILPs. Bonam et al. [49] have shown that for 41 MILPLIB instances, GMI
cuts can help to reduce the integrality gap by 24% on average. In general, Gomory’s mixed integer cut dominates
Gomory’s fractional cut. Nonetheless, numerical issues in Gomory’s approach prevent pure cutting plane methods
from being eﬀective in practice.

Mixed integer rounding (MIR) cuts were introduced by Nemhauser and Wolsey [50], which are more general than
GMI cuts, and gave a complete description for any mixed 0-1 polyhedron. Marchand and Wolsey [51, 52] showed
that MIR inequalities could produce strong cutting planes for structured mixed integer programs and solve them
eﬀectively. Furthermore, it was proved in Nemhauser and Wolsey [50] that MIR cuts are equivalent to split cuts
proposed by Cook et al. [53]. Lift-and-project cuts shown in Lov´asz and Schrijver [54] and Balas et al. [55] lift the
LP relaxation into a higher-dimensional space and ﬁnd valid cuts in this space, and then project valid cuts back into
the original space.

2.5.2 Problem-speciﬁc Cuts

Apart from the above “general-purpose” valid inequalities, there are also some methods considering the structure
of speciﬁc problems and obtaining stronger inequalities. For example, Flow cover cuts were introduced by Padberg
et al. [56] and generalized in Van Roy and Wolsey [57], and Aardal et al. [58] are based on node ﬂow problems,
while clique cuts proposed by Savelsbergh [59] are usually applied on conﬂict graphs, shown in Atamt¨urk et al. [60].
Knapsack cover cuts presented in Atamtuerk [61] regard constraints as separation knapsack problems, and they are
one of the ﬁrst cutting planes incorporated into commercial MILP solvers [62]. Lifted cover inequalities in van de

9

ˆx<latexit sha1_base64="F45QG9VbQ+5p/kUmgxE+LExzOus=">AAADcXiclVLLSgMxFL3t+H5XV+JmaBFEocxUQZcFNy4VrBW0SGYa2+C8yGREKX6EW/0y/0DX/oAnMQUfWDUhMzfnnnNzc3ODLBK58rynUtkZG5+YnJqemZ2bX1hcqiyf5GkhQ94K0yiVpwHLeSQS3lJCRfw0k5zFQcTbwdW+9revucxFmhyr24x3YtZLxKUImQLUPu8zNbi5u1iqeXXPDPe74VujRnYcppXSJp1Tl1IKqaCYOCWkYEfEKMc8I588yoB1aABMwhLGz+mOZqAtwOJgMKBX+PawO7Nogr2OmRt1iFMiLAmlS+vQpOBJ2Po01/gLE1mjP8UemJg6t1v8AxsrBqqoD/Q33ZD5d92QI+G9xPr53gPkE/8j8seKjMpbYjLcd3TFFbLbM5UWqHxmEP0Goc2oMG+n6+t+qL1ChAyYtrvwS9ihUQ67wTWa3LwQM5lo/7NhalTvQ8st6EXfGH3of+2678ZJo+5v1xtHO7Xmju3IKVqjKm2g63apSQd0SC1zy3t6oMfyq7PquE71nVouWc0KfRrO1huKz7GP</latexit>ˆx<latexit sha1_base64="F45QG9VbQ+5p/kUmgxE+LExzOus=">AAADcXiclVLLSgMxFL3t+H5XV+JmaBFEocxUQZcFNy4VrBW0SGYa2+C8yGREKX6EW/0y/0DX/oAnMQUfWDUhMzfnnnNzc3ODLBK58rynUtkZG5+YnJqemZ2bX1hcqiyf5GkhQ94K0yiVpwHLeSQS3lJCRfw0k5zFQcTbwdW+9revucxFmhyr24x3YtZLxKUImQLUPu8zNbi5u1iqeXXPDPe74VujRnYcppXSJp1Tl1IKqaCYOCWkYEfEKMc8I588yoB1aABMwhLGz+mOZqAtwOJgMKBX+PawO7Nogr2OmRt1iFMiLAmlS+vQpOBJ2Po01/gLE1mjP8UemJg6t1v8AxsrBqqoD/Q33ZD5d92QI+G9xPr53gPkE/8j8seKjMpbYjLcd3TFFbLbM5UWqHxmEP0Goc2oMG+n6+t+qL1ChAyYtrvwS9ihUQ67wTWa3LwQM5lo/7NhalTvQ8st6EXfGH3of+2678ZJo+5v1xtHO7Xmju3IKVqjKm2g63apSQd0SC1zy3t6oMfyq7PquE71nVouWc0KfRrO1huKz7GP</latexit>ˆx<latexit sha1_base64="F45QG9VbQ+5p/kUmgxE+LExzOus=">AAADcXiclVLLSgMxFL3t+H5XV+JmaBFEocxUQZcFNy4VrBW0SGYa2+C8yGREKX6EW/0y/0DX/oAnMQUfWDUhMzfnnnNzc3ODLBK58rynUtkZG5+YnJqemZ2bX1hcqiyf5GkhQ94K0yiVpwHLeSQS3lJCRfw0k5zFQcTbwdW+9revucxFmhyr24x3YtZLxKUImQLUPu8zNbi5u1iqeXXPDPe74VujRnYcppXSJp1Tl1IKqaCYOCWkYEfEKMc8I588yoB1aABMwhLGz+mOZqAtwOJgMKBX+PawO7Nogr2OmRt1iFMiLAmlS+vQpOBJ2Po01/gLE1mjP8UemJg6t1v8AxsrBqqoD/Q33ZD5d92QI+G9xPr53gPkE/8j8seKjMpbYjLcd3TFFbLbM5UWqHxmEP0Goc2oMG+n6+t+qL1ChAyYtrvwS9ihUQ67wTWa3LwQM5lo/7NhalTvQ8st6EXfGH3of+2678ZJo+5v1xtHO7Xmju3IKVqjKm2g63apSQd0SC1zy3t6oMfyq7PquE71nVouWc0KfRrO1huKz7GP</latexit>Leensel et al. [63] use lifting to strength cover inequalities and yield a large class of facet-deﬁning inequalities, which
have been used successfully in general-purpose branch-and-cut algorithms [55].

Selecting which cutting plane to add is a non-trivial task. Modern solvers decompose cutting plane selection into
two phases, maintaining valid inequalities in cut-pool ﬁrstly, then ranking these cuts and selecting some of them to
tighten the feasible region. Table 7 in Achterberg and Wunderling [64] details the contribution of diﬀerent cutting-
plane methods in CPLEX 12.5, indicating that MIR cuts are the most useful, followed by Gomory cuts and knapsack
cover cuts, signiﬁcantly outperforming other cuts. However, general-purpose cutting planes are mostly seen from
theoretical interest and analysis. Dey and Molinaro [65] illustrated several questions that need to be considered in
the cutting-plane selection and analyzed existing theoretical challenges in understanding and addressing these issues.

3 Machine Learning

In this section, we provide an introduction of the traditional machine learning framework, with the aim to make the
reader familiar with some essential concepts which are required to understand the remainder of the survey.

3.1 Supervised Learning

In supervised learning, a set of pairs Dn = {(x1, y1), (x2, y2), . . . , (xn, yn)} consisting of input (features) xi and
output (target) yi is provided with the aim to ﬁnd a function f that for every input has a prediction as close as
possible to the provided output. The search for a proper function is based on an optimization problem over a certain
(parametrized) family of functions {fθ | θ ∈ Rp} and this procedure is called learning. Together with the family
functions, a loss function l : (fθ, Dn) → R is to compute the discrepancy between the prediction and the target,
which is task-dependent. In supervised learning problems, dependent on the target type, two main predictive tasks
can be identiﬁed as follows:

(1) Classiﬁcation: the output is a qualitative label which can diﬀerentiate between m ∈ N categories. By encoding
the label y ∈ {1, . . . , m} into a vector y ∈ Rm, the accuracy of a classiﬁer f can be measured by the loss
function l(f, (x, y)) = I{f (x)(cid:54)=y}, where I{·} represents the indicator function.

(2) Regression: the output is a quantitative value y ∈ Rm and the regressor f outputs the expected value of y given

x. A commonly used loss function in this setting is the quadratic error l(f, (x, y)) = ||f (x) − y||2.

Generally, the problem has a statistical nature, i.e., the data (xi, yi) is a realization of random variables (X, Y ) which
follows a joint probability distribution P . With the goal to ﬁnd the optimal function as a minimizer of the expected
value of l(fθ(X), Y ) under the probability distribution P , the supervised learning problem can be formulated as

EX,Y ∼P l(fθ(X), Y ).

min
θ∈Rp

(7)

For example, θ could be the weights of a linear function and in this case fθ is a linear model. Moreover, since the
probability distribution is unknown, instead of minimizing the expected risk, one aims at minimizing the empirical
risk by using the examples of ﬁnite data Dn and the optimization problem solved is

min
θ∈Rp

1
n

n
(cid:88)

i=1

l(fθ(xi), yi).

(8)

It is easy for a model to achieve a good performance on the given data. However, one usually hopes the learned
model achieve a good performance on unseen data. This is known as generalization, which is a fundamental property
in a learned predictor. Owing to the ﬁnite number of observations, the learning problem of an optimal function is a
delicate task with many traps, one of them is overﬁtting.

Generally speaking, overﬁtting represents the phenomenon of doing well on the given data but not generalizing to
the unseen data. This occurs because the optimal function in (8) is obtained by ﬁtting the speciﬁc dataset Dn,
making the training loss underestimates the expected loss. In particular, the more overﬁtting occurs, the larger the

10

generalization error (the diﬀerence between training loss and the expected loss). Since the expected loss remains
inaccessible, we can estimate the generalization error through evaluating the learned function on a separate test set
Dtest with

1
|Dtest|

(cid:88)

(x,y)∈Dtest

l(fθ(x), y).

(9)

The aim to ﬁnd a function which does not overﬁt is at odds with the goal of searching a complex function to capture
the characteristics of the data. This is known as the bias-variance trade-oﬀ.

For the purpose of selecting the best among various trained models, a proper model selection procedure divides the
dataset into three parts, to be used in training, validation and test phases. The validation set is used to estimate
the generalization error. Based on this estimate, the model selection can be done and the ﬁnal generalization error
of the selected value is obtained by the test set.

3.2 Unsupervised Learning

In unsupervised learning, there are no targets for the task one wants to solve, i.e., the prediction is performed
without a supervisor who knows the correct answers. The aim of the problem is to learn a function f capturing
certain characteristic of the distribution of the observation. Several tasks in this setting are:

(1) Clustering: similarities within the input space is discovered in the clustering problem. Data could be grouped
within a partition of the whole space and for a new point, it is able to predict its membership to one or several
groups.

(2) Density estimation: the function f is an estimator for the distribution of the input data. Since there is no
labeled target, one can maximize the (log-)likelihood of the observation, i.e., their probability with regard to
the underlying distribution P .

(3) Dimensionality reduction: the transformation of input data, usually from a high-dimensional space into a lower-
dimensional space. The purpose of this problem is to identify some essential characteristics of the input data
via extraction or selection.

Since unsupervised learning has received only a little concern on mixed-integer linear programming and its immediate
application seems diﬃcult, in this survey, we are not going to discuss it further. The reader is referred to the classical
textbooks (e.g., [66], [67], [68]) on machine learning.

3.3 Reinforcement Learning

In reinforcement learning(RL), an agent interacts with an environment for the purpose of maximizing its cumulative
reward through trail an error using feedback from its own action. The setting of a Markov decision process(MDP)
provides a theoretical framework for reinforcement learning, as illustrated in Fig. 4. At each time iteration, the agent
is in a given state st ∈ S of the environment, and subsequently takes an action at ∈ A(st) according to its stochastic
policy π(at|st). As a result, the agents enters a new state st+1 with probability P at
st,st+1 and it receives an immediate
reward Rat
st,st+1 from the environment. The aim of reinforcement learning is to learn an optimal policy π : S → A
that maximizes the expected discount sum of future rewards (discounted return):

G(s0, π) = E[Ra0

s0,s1 + γRa1

s1,s2 + · · · + γtRat

st,st+1 + · · · ],

(10)

where γ ∈ [0, 1] denotes the discount rate, which is used to model the fact that the future reward is less worth than
an immediate one. Given a policy π and state st (resp. state and action pair (st, at)), the expected return is referred
to the value function (resp. state action value function). The value function follows the Bellman equation, thus the
learning problem can be formulated as a dynamic programming, and solved approximately.

A major concern in reinforcement learning is the exploration vs exploitation dilemma: choosing between exploring
new states by trying new actions in order to reﬁne the knowledge of the environment for possible improvements in
the long term, or exploiting experienced actions which yield a high reward. Moreover, deﬁning a reward is sometimes
a diﬃcult task. Reinforcement learning is able to credit the states/actions, bringing about future rewards due to
its dynamic programming process. However, it is still challenging since no learning opportunity is oﬀered until the
agent solves the problem. In addition, it is not guaranteed that the learning converges to the global optimum when
the policy is approximated. For more details, the interested reader can refer to [69], which is an extensive textbook
on reinforcement learning.

11

Fig. 4. The Markov decision process associated with reinforcement learning.

4 Learning-Based Branch and Bound Algorithms

In this section, we survey the learning techniques to deal with the four key components in B&B algorithms for MILP.
The contributions and limitations of diﬀerent studies are summarized in this section. In addition, we make some
comparisons of diﬀerent learning methods in these four key components.

4.1 Learn to Branch

With the development of learning, alleviating the computational burden in some traditional branching strategies
with learning has been a hot topic. Some works have used imitation learning to approximate decisions by observing
the demonstrations shown by an expert, while others capitalize on reinforcement learning to explore better branching
policies.

4.1.1 Supervised Learning in Branching

Since strong branching is the most eﬃcient expert in terms of the number of expanded nodes till now, and its main
disadvantage is prohibitive computational cost, most of the works learn to mimic strong branching and use learning
to estimate scores quickly. This method is called imitation learning, which can be regarded as supervised learning for
decision-making. Some early attempts making use of this idea utilize traditional machine learning methods. Alvarez
et al. [70–72] trained a regression model to approximate strong branching scores eﬃciently. Alvarez et al. [70, 71]
learned the SB score function by ExtraTree shown in Geurts et al. [73], while [72] learned by online linear regression.
The method proposed in [70,71] consists of two phases to learn and solve MILPs. In the ﬁrst phase, the SB decisions
are recorded by solving a set of randomly generated problems optimally, and a regressor is learned to predict strong
branching scores. Then in the second phase, the learned function is employed for instances from MIPLIB, which
is the conventional evaluation benchmark for MILP methods. On the other hand, they presented some signiﬁcant
features to describe the current problem, which are not only complete and precise but also eﬃcient to compute.
These features are divided into three types. Speciﬁcally, static problem features are computed solely from parameters
of the original problem, c, A and b, thus calculated once and for all. They gave an overall description of the problem
and represent the static state of the problem. Dynamic problem features describe the state of a speciﬁc candidate
concerning the LP solution at the current node. Besides, dynamic optimization features extract the overall statistical
eﬀect of the candidate about the optimization process. These features mainly take into account the variables’ roles
at the current nodes. Zarpellon et al. [74] generalized features in the space of B&B search trees, leading to a more
ﬂexible and adaptive variable selection. Therefore, feature design is a crucial work in learning-based methods. Alvarez
et al. [71] revealed three desirable properties of branching features, followed by many later works. First, features
should be size-independence, which is intuitive and fundamental to solve large-scale MILPs. Second, features should
be invariant to some irrelevant changes, like permutating rows or columns of matrix A. Experiment results show
that the learned branching strategy compares favorably with SB, but its performance is slightly below that of RB.
When solving MILPs, the learned strategy performs badly only on a small number of instances, but it is faster
than RB in 11/30 cases and faster than FSB in 21/30 instances. Inspired by the idea behind RB, Alvarez et al. [72]
made use of online learning to imitate SB. For a candidate, if its SB score has been computed a certain number of
times, it would be deemed reliable, and its SB score could be approximated by learning. Otherwise, feature vectors
and scores of unreliable candidates can be put into the training set. Hence, in contrast to [70, 71], the training
data was generated during the B&B process on-the-ﬂy, and thus, no preliminary phase in needed. Khalil et al. [75]

12

learned string branching scores in an on-the-ﬂy fashion, without an upfront oﬄine training phase on a large set of
instances, so no preliminary phase is required to record expert behavior, and computing time is saved. On the other
hand, this method is instance-speciﬁc since it learns from the expert at the beginning of the tree, and the learned
ranking function is then used for branching seamlessly. Ranking formulation with binary labels in this work is natural
for variable selection, as the score itself is not important and what we care about is “whether” this candidate is
eﬀective. Moreover, binary labels relax the deﬁnition of “best” branching variables, allowing us to consider many
good variables that also have high scores in the learning. They avoid learning from correctly rank candidates with low
scores, which further saves time. Experiment results show that imitating SB with machine learning outperforms SB
and PC in terms of the solved instances and the number of nodes, and even solves more instances than the CPLEX-
D solver. Although the running time of the learned policy is more than pseudocost branching for easy instances,
machine learning runs fastest when solving medium and hard MILPs. These early works reveal the potential of
taking advantage of machine learning to speed up large-scale MILPs.

Moreover, to tackle tedious parameters tuning, Balcan et al. [76] learned the optimal parameter setting for the
instance distribution. A certain application domain can be modeled as a distribution over problem instances, and
samples can be accessed by the algorithm designer so that a nearly optimal branching strategy can be learned. Here,
the optimal branching policy means the optimal convex combination of diﬀerent branching rules. This work theo-
retically proved that using a data-independent discretization of the parameters to ﬁnd an empirically optimal B&B
conﬁguration is impossible, indicating its intrinsic adaptiveness. In other words, the eﬀect of branching parameters
on the average number of expanded nodes varies signiﬁcantly with the applications.

Fig. 5. Variable selection in B&B integer programming algorithm as an MDP. In the left ﬁgure, a state st includes the B&B
tree with a leaf node chosen by the solver to be expanded next (in pink). In the right ﬁgure, a new state st+1 is obtained by
branching over the variable at = x1 [77].

Nowadays, deep learning has achieved huge success in various ﬁelds, with the power to process huge amounts of
data. Many works have made use of powerful neural networks to speed up variable selections. Due to the bipartite
graph representing of mixed integer linear programming (Fig. 6), it is natural to encode the branching policies to a
graph convolutional neural network (GCNN), which speeds up the MILP solver by reducing the amount of manual
feature engineering. Moreover, the previous statistical learning of branching strategies is only able to generalize to
similar instances while the GNN model has a better generalization ability since it can model problems of arbitrary
size. Gasse et al. [77] ﬁrst adopted imitation learning and a dedicated graph convolutional neural network model
to address the B&B variable selection problem. In this work, the problem was formulated by the task of searching
the optimal policy of an MDP, as illustrated in Fig. 5. Since the graph structure is the same for all LP relaxation
in the B&B tree represented as a bipartite graph with node and edge feature, the cost of extraction is reduced to
a great extent. By adopting imitation learning, a GCNN model was trained to approximate the strong branching
policy, a very good but computationally expensive expert policy. The overview of their architecture is depicted in
Fig. 7. The bipartite representation is taken as input of the model and a single graph revolution is performed in the
form of two half convolutions. A probability distribution over the candidate branching strategies is ﬁnally obtained
by discarding the constraints node and adopting a multi-layer preceptron to the variable nodes, combined with a
softmax activation. Their resulting branching policy is shown to perform better than previously proposed methods
for branching on several MILP problem benchmarks and generalize to larger instances than trained on. In order to
make the GNN model more competitive on CPU-only machines, Gupta et al. [78] devised a hybrid branching model
that uses a GNN model only at the initial decision point and a weak but fast predictor, such as multi-layer perceptron,
for subsequent steps. The proposed hybrid architecture improves the weak model by extracting high-level structural
information at the initial point by the GNN model and preserves the original GNN model’s ability to generalize

13

to harder problems than trained on. Nair et al. [79] adopted imitation learning to obtain a MILP brancher, where
the GNN approach was expanded by implementing a large amount of parallel GPU computation. By mimicking an
ADMM-based expert and combining the branching rule and primal heuristic, their work is advantageous over the
SCIP [80] in terms of solving time on ﬁve benchmarks in real life.

Fig. 6. Bipartite graph presentation of MILP [77]. The bipartite graph consists of two sets of nodes, the variable sets
{x1, . . . , xn} and the constraint nodes {δ1, . . . , δm}. The edge connecting the variable node and the constraint node denotes
the coeﬃcients of the MILP.

Fig. 7. The architecture of the GCNN model in [77].

However, most of the above works specialize the branching rule to distinct classes of problems, obtaining generaliza-
tion ability to larger but similar MILP instances [70–72,75]. Besides, these works lack a mathematical understanding
of branching due to its inherent exponential nature. With the hypothesis that parameterizing the underlying space of
B&B search trees can aid generalization to heterogeneous MILPs, Zarpellon et al. [74] introduced new features and
architectures to represent branching, making the branching criteria adapt to the tree evolution and generalizing across
problems not belonging to the same combinatorial class. Speciﬁcally, to capture the dynamics of the B&B process
linked to branching decisions, some features were proposed to describe the roles of candidates in the B&B process.
At a certain branching step, besides some features capturing statistics and multiple roles of a variable throughout
the search, denoted as Ct, there are some features, T reet, describing the state of the search tree, such as the growth
rate, the composition of the tree, and the bound quality of the current node. All these features, {Ct, T reet}, are
not explicitly dependent on the speciﬁc problem parameters and evolve with the search, diﬀerent from static and
parameters-dependent features utilized by the above works. This work ﬁrst used a baseline DNN (NoTree) that
only uses Ct as inputs to demonstrate the signiﬁcance of search-based features. The output layer equipped with the
softmax produces a probability distribution over the candidate set, indicating the probability for each variable to be
selected. Then, T reet was input to the NoTree by gating layers, called as TreeGate models. Considering that people
rarely use SB in practice, this work chose the SCIP default branching scheme, relpscost [81], a reliability version of
HB, as a more realistic expert. In the experiment, 27 heterogeneous MILP problems were partitioned into 19 train
and 8 test problems. Both NoTree and TreeGate outperform GCNN, RB, and PC in terms of the total number of
nodes explored. Since GCNN struggles to generalize across heterogeneous instances, it expands around three times
as many nodes as this method does. On the other hand, the test accuracy of TreeGate test accuracy is 83.70%,

14

Table 1
Machine Learning-Based Variable Selection Comparison

Alvarez et al., 2016 [71]

Alvarez et al., 2016 [72]

Khalil et al., 2016 [75]

Learning
approach

Oﬄine extraTrees for regression

Online linear regression

SVM with rank formulation

Static problem features

Features

Dynamic problem features

Features from Alvarez et al. [71]

Dynamic optimization features

72 atomic features computed on
the node LP and candidate vari-
able;

computed
Interation features
from a product of two atomic
features

Expert

SB

SB

SB

Compared
algorithm(s)

Measure(s)

Advantage(s)

Random branching

MIB

FSB

NCB (nonchimerical branching)

RB

FSB

RB

closed gap

# solved problems

# nodes

time

Reduce time required to take a
decision, and thus close the gap
by exploring more nodes;

Run fastest when cuts and
heuristics are used by CPLEX.

Perform worse than RB under
the setting of node limit and the
setting of no cuts and heuristics;

Oﬄine learned branching

#nodes

time

CPLEX 12.6.1 default

SB

PC

SB + PC

# unsloved problems

# nodes

total time

Save time in an online fashion.

Allow one to take into account
many good candidates in the
learning;

Perform best in medium and
hard problems.

Limitation(s)

Cannot generalize to very large
or heterogenous problems;

Cannot generalize to
heterogenous problems.

Require
phase.

expensive

training

Be limited by the expert.

Focus on static and
parameters-dependent
properties of problems.

improving 19% over the NoTree model, while the accuracy of GCNN is only 15.28%. The comparison between the
above papers are shown in Table 1 and 2.

4.1.2 Reinforcement Learning in Branching

One of the main limitations of imitation learning is that the expert bounds the performance of the learned strategy.
However, expert knowledge is not satisfactory sometimes. Sun et al. [83] analyzed why imitating SB for learning
branching strategy is not a wise choice: SB yielding small B&B trees is a consequence of reductions from solving
branch LP instead of its decision quality, and these reductions cannot be learned by imitating a branching policy.
They designed experiments that eliminate the side-eﬀect of reduction obtained in solving branch LP and found that
SB has poor performance. Thus, researchers hope to propose better methods of selecting variables.

Sun et al. [83] designed a novel primal-dual policy network over reduced LP relaxation and a novel set representation
for the branching strategy. To encourage the agent to solve problems with as few steps as possible, the reward is
set to rt = 1 at each time step. Although the primal-dual policy is similar to GCN, it uses a dynamic reduced
graph by removing redundant ﬁxed variables and trivial constraints during the process, which gives a more precise

15

Table 2
Deep Learning-Based Variable Selection Comparison

Learning
approach

Features

Gasse et al., 2019 [77]

Gupta et al., 2020 [78]

Nair et al., 2020 [79]

Zarpellon et al., 2020 [74]

GCNN

HyperGNN-FiLM

GCNN

NN with feature gating

5 features for the con-
straint;

13 features for the vari-
able;

1 feature for the edge.

Root node: 19 features
from Gasse et al. [77]

Remaining node: 72 fea-
tures from Khalil et al.
[75]

Features from Gasse et
al. [77]

25 features representing
set of candidate

variables

61 features encoding dy-
namic state of tree

search

Expert

SB

SB

ADMM-based FSB

SCIP default

RPB (reliability pseudo-
cost)

RPB

Compared
algorithm(s)

FSB

Oﬄine learned branching

SB + ML

LMART [82]

# nodes

time

Reduce feature calcula-
tion cost;

Faster solving time than
the solver default proce-
dure;

Generalize to larger in-
stances than trained on.

Uncompetitive on CPU-
only machines;

Tailored to the type of
MILP problems
is
trained on.

it

Measure(s)

Advantage(s)

Limitation(s)

PB(heuristic Pesudocost)

SB + ML

ExtraTree

GCNN

# nodes

time

Random branching

FSB

PC

SCIP 7.0.1 default

SCIP default

GCNN

time

# nodes

Show improvements on
CPU-only hardware.

Improve SCIP in solving
time over real-life
benchmarks and
MIPLIB.

Generalize across
heterogenous MILPs;
Handle candidates of
varying size.

perfor-
Only
mance
trained
model on small instances;

evaluate
the
of

A poor generalization ca-
pability for Maximum In-
dependent Set problem;

Focus on speciﬁc problem
types.

Sometimes fail to ﬁnd an
optimal or near-optimal
solution;

Only generalize to unseen
instances from the same
problem distribution.

Be limited by the experts.

and accurate description of the state. On the other hand, instead of edge embedding in GCN, primal-dual policy
relies on simple matrix multiplication, which further saves computational time. Here, set representation and optimal
transport distance (or Wasserstein distance) are used to deﬁne the novelty score. By regarding every subproblem Q
generated from an instance by a branch policy π as a polytope, the collection of its subproblems can be deﬁned as
b(π, Q) = {R1, . . . , RH }. For each subproblem, Ri, the weight function w(·) represents the number of feasible point
in the associated polytope. The distance between two subproblems Ri and Rj are deﬁned as d(Ri, Rj) := (cid:107)gi − gj(cid:107)1,
where gi and gj are their mass centers. Then the optimal transport distance between two policy representations is
computed as

D(b1, b2) = min

Γ

(cid:88)

i,j

ΓijWij(b1, b2),

s.t. Γ1 = p(b1), ΓT 1 = p(b2),

(11)

where p(b) ∈ ∆H−1 is a simplex mapped from b by normalizing the weights. Therefore, given a collection of older

16

policies M and an instance Q, novelty score is deﬁned as

N (θ, Q, M ) =

1
k

(cid:88)

πj ∈kNN(M,θ)

D(b(πθ, Q), b(πj, Q)),

(12)

where kNN is the k nearest neighbor of πθ in M . Equipped with this novelty score, Novelty Search Evolution Strategy
was proposed to encourage exploration in reinforcement learning. Their experiments compared the RL agent with
SVM and GCN, two competitive learning methods in imitation learning, in addition to three SCIP’s branching
rules. The RL agent performs best in terms of the running time, the number of expanded nodes, and primal bounds.
However, about obtaining a higher dual value, the agent performs worst initially, but it ﬁnally obtains the best result,
indicating its non-myopic policy. Diﬀerent from [84], this work can transfer to larger instances, and the performance
of the RL agent is signiﬁcantly superior to FSB, RPB (reliability pseudocost branch), SVM, and GCN.

Although reinforcement learning can potentially produce a better branching strategy than the expert, the agent
performance is limited by the episode length, and it learns ineﬃciently at the beginning due to little experience.

4.1.3 Dynamic Approach for Switching Branching Heuristics

Based on the observation of the highly dynamic and sequential nature of B&B, Di Liberto et al. [85] believed that
there is no single branching heuristics given in Section 2.2 that would perform the best on all problems, even on
diﬀerent subproblems induced from the same MILP. Thus, the eﬃciency of the search can be much improved if we
adopt the correct branching method at the right time during the B&B search. Motivated by portfolio algorithms
that attempt to predict the best heuristic for a given instance, this work proposed an algorithm named Dynamic
Approach for Switching Heuristics (DASH). Based on the deﬁned features, a clustering of problems was learned with
the g-means algorithm at the ﬁrst step. Then the correct assignment of branching methods to clusters is learned
during an oﬄine training phase with a similar method shown in Kadioglu et al. [86]. With the search depth increasing,
the instance tends to shift to a diﬀerent cluster. When such a change occurs, the heuristic would be switched to
a new one that best ﬁts the current cluster. Numerical results show that DASH outperforms static and randomly
switching heuristics methods in terms of the running time, indicating the beneﬁt of dynamics and adaptiveness of
the switching methods. Nevertheless, Lodi and Zarpellon [87] observed that the upfront oﬄine clustering conﬂicts
with the ever-changing characteristic of the tree evolution and somehow aﬀects time eﬃciency.

4.2 Learn to Select

As shown in [20], the eﬀectiveness of diﬀerent selection methods depends on the problem type. It is preferable to
seek a selection strategy that adapts to diﬀerent problem types. We divide the existing learning-based literature into
the following two categories, i.e., one is adaption in evaluation criteria, and the other is learning in heuristics.

4.2.1 Adaption in Evaluation Criteria

As summarized in Section 2.3.3, the evaluation criteria of a node during a B&B run are from the following two
sides, the feasibility side and the optimality side. If the selection criteria pay more attention to the feasibility side,
the selection strategy will perform more like the depth-ﬁrst search strategy, while on the other side, the selection
strategy would perform more like the best-ﬁrst strategy.

Borrowing a RL vocabulary, some grade of adaption could be pursued in the combination of best-ﬁrst and depth-ﬁrst
strategies to balance exploration and exploitation in the B&B run. Sabharwal et al. [88] exploited RL framework.
The score of a node N is a weighted sum of two terms

score(N ) = estimate(N ) + Γ

visits(P )
100visits(N )

,

(13)

where estimate(N ) is some measure of the quality of node N , P is the parent node of N and visits(·) counts
the number of times the search algorithm has visited a node. The parameter Γ balances the tradeoﬀ between
exploitation(ﬁrst term) and exploration(second term), and nodes with a higher estimate or have been visited less
time than their siblings will be pursued ﬁrst. This geometric means of runtime, the number of searched nodes, and the

17

simplex iterations are improved compared with three other selection strategies: best-ﬁrst, breath-ﬁrst, and CPLEX
default heuristic. The improvement is gained due to a balanced usage of best-ﬁrst and breath-search-like schemes.
However, this work treats every node of a B&B tree equivalently, ignoring the uniqueness of each node’s feature
during the B&B run. An interesting question arises about employing the node feature into the scoring system to
improve the learning quality.

4.2.2 Learning in Heuristics in Selecting

The main idea of learning in heuristics is rating each node based on a weighted sum of criteria and choosing the
node with the highest rating. The ﬁrst proposed use of learning methods within a heuristic search procedure comes
from Glover, and Greenberg [89,90] which adjusts the wights oﬄine using a learning procedure. Ans´otegui et al. [91]
induced the hyper conﬁgurable reactive search recently to learn the parameters of a metaheuristic online with a
linear regression, where the regression weights are tuned oﬄine with the GGA++ algorithm conﬁguration [92].

Daum´e et al. [93] and Chang et al. [94] converted the solving problem into a sequential decision-making problem, for
which a policy is then learned or improved. A fundamental limitation of their work is that there is no correction due
to the greedy search at each test time; thus, the obtained solution sequence will have deviations from the optimal
one.

He et al. [95] instead proposed a method to learn the node selection strategy in a B&B run by imitation learning.
They categorized their features into three groups:

(1) Node features include bounds, objective function estimation at a given node, indications about the current

depth, and the (parental) relationship for the past proceeded node.

(2) Branching features describe the variable whose branching led to a given node, including pseudocosts, variable’s

value modiﬁcations, and bound improvement.

(3) Tree features consider measures such as the number of solutions found, global bounds, and gaps (see in Deﬁnition

3).

They assumed that a small set of solved problems are given at training time and the problems to be solved at the
test time are of the same type. The node selection policy is learned to repeatedly pick a node from the queue of all
unexplored nodes that mimics a simple oracle that knows the optimal solution in advance and only expands nodes
containing the optimal solution. This learning-based selection method ﬁnds solutions with a better objective and
establishes a smaller gap, using less time demonstrated on multiple datasets compared to SCIP. However, the author
leaves the seek of certiﬁed optimality for speed as future work. In addition, this work is problem-dependent which
requires the test data to have the same type of training data. Yilmaz and Smith [96] used a similar method in [95]
to select the direct children at non-leaf nodes.

Hottung et al. [97] developed a new method that integrates deep neural networks (DNNs) into a heuristic tree search
to decide which branch to choose next, namely, the Deep Learning Heuristic Tree Search (DLTS). DLTS is able
to achieve a high level of performance with no problem-speciﬁc information. The problem-speciﬁc information is
almost exclusively provided as input to the DNN, where the DNNs are trained oﬄine via supervised learning on
existing (near-) optimal solutions. It is shown in [97] that DLTS signiﬁcantly ﬁnds smaller gaps to optimality on
real-world-sized instances compared to that for state-of-the-art metaheuristics proposed by Karapetyan et al. [98].
In addition, it does not require extra training data since a learned model to fully control decisions is learned during
the search. However, as a coin has two sides, the system performance relies on the quality of the provided solutions.
In addition, the policy is not adjusted in terms of the runtime and solution quality.

The comparison between the above papers is shown in Table 3.

A relatively large amount of literature focuses on embedding learning methods into primal heuristics, including
but not limited to [79, 99–105]. Khalil et al. [99] used binary classiﬁcation to predict whether a primal heuristic
would succeed at a given node. It is the ﬁrst systematic attempt at optimizing the use of heuristics in tree search.
This work boosts the primal performance of SCIP, even on instances for which experts already ﬁnetune the solver.
However, the learning rules can be reﬁned, taking into account the running time left for the solver. Hendel [100]
formulated a multi-armed bandit approach to learn to switch nine primal heuristic strategies online. Hottung and
Tierney [101], Addanki et al. [102] and Song et al. [103] adopted learning methods to improve the neighborhood
search to improve the primal performance. The learning methods were adopted to either select the variables to be

18

Table 3
Learning-Based Node Selection Comparison

Learning approach

Sabharwal et al.,
2012 [88]

Reinforcement
learning

He et al., 2014 [95]

Yilmaz and Smith,
2021 [96]

Hottung et al.,
2020 [97]

Imitation Learning

Imitation Learning

Supervised Learning

Input(s)

Expert

Output(s)

Compared
algorithm(s)

Measure(s)

Normalized LP objec-
tive value of a leaf node

# times that a node
has been visited

Node features

Node features

Node features

Branching features

Branching features

Branching features

Tree features

Tree features

Tree features

Oracle

Best-estimate with
plunging (SCIP
default)

Existing solutions
through search

A parameter Γ to
balance the tradeoﬀ
between exploitation
and exploration

The next node to be ex-
plored picked from the
queue of unexplored
node

Top-best pruning ac-
tion

The direct children at
non-leaf nodes to be se-
lected

k-best pruning action

Trained deep neural
networks

Probability to select
this branch

Estimated lower bound
of each node

Best-ﬁrst search

Breath-ﬁrst search

Depth-ﬁrst
(DFS)

search

Selection along with
pruning

SCIP default

Pruning only

DFS

SCIP with a node limit

RestartDFS

CPLEX default heuris-
tic

Gurobi with a time
limit

He et al., 2014 [95]

Metaheuristics
(Karapetyan et al.,
2017 [98])

Geometric means
runtime

of

Speedup w.r.t. SCIP
default

# searched nodes

Optimality gap

# simplex iterations

Integrality gap

Geometric means
runtime

of

Geometric means
runtime

of

Optimality gap

Optimality gap

Advantage(s)

Improve all the above
measures.

Limitation(s)

Treat every node of a
B&B tree equivalently,
ignoring the
uniqueness of each
node’s feature during
B&B run.

Find solutions with a
better objective and
establish a smaller
gap, using less time.

No children selection
policy;

Problem dependent,
training data should
be of the same type;

Cannot obtain a certi-
ﬁed optimality within
the time or node limi-
tation.

Is eﬀective when the
is able to
ML model
classify
meaningfully
optimal
child nodes
correctly;

Achieve better opti-
mality gap.

Only
the
consider
optimality gap as the
rank
criterion,ignore
the depth;

k-best solutions do not
have a optimality gap
bound;

Assume the optimum
was known.

Find better solutions
on real-world sized in-
stances;

A learned policy does
not require extra train-
ing data.

Rely on the provided
solutions;

No adjusting in terms
of runtime and solution
quality.

modiﬁed or assign new values to an already selected subset of variables. However, those methods require at least a
feasible point as input. Xavier et al. [104] proposed three diﬀerent learning models to eﬀectively extract information
from previously solved instances in order to signiﬁcantly improve the computational performance of MILP solvers
when solving similar instances. The ﬁrst model was designed to predict which constraints should be initially added
to the relaxation and which constraints can be safely omitted. The second model proposed using k-nearest neighbors
to set the values only for the variables with high conﬁdence and to let the MILP solver determine the values for

19

Fig. 8. Main studies on learning-based primal heuristics and their main contributions

the remaining variables as a good warm start point. The third model develops an ML model for ﬁnding constraints
to restrict the solution without aﬀecting the optimal solution set. These three models boost the speed to ﬁnd a
solution with optimality guarantees. The main limitation of this work is that a large number of solved instances
must be available for the third model. In addition, it requires the problem to be solved in the future suﬃciently
similar to the past samples. Unlike the above learning primal heuristic works [99–104], Ding et al. [105] and Nair et
al. [79] did not limit to problems of certain types and built a tripartite graph representation to extract correlations
among variables, constraints and objective function without human intervention. Nair et al. [79] posed the problem
of predicting variable assignments as a generative modeling problem, which provides a principled way to learn on
all available feasible assignments and also generates partial assignments at test time. However, this training method
requires GPU. Fig. 8 demonstrates the main contributions of the above literature.

4.3 Learn to Prune

When using the B&B algorithm to solve large-scale systems, the number of non-pruning nodes by traditional pruning
methods is still quite massive. As a result, it either costs huge computational consumption to solve the problem,
or a possible good solution cannot be obtained within the time limit. Therefore, the question arises of whether
we could use some learning methods to eﬀectively prune a large set of unexplored nodes as early as possible and
achieve a solution with an optimality gap guarantee. There are mainly three ways to accelerate the pruning policy
by learning. One is learning whether to discard or expand the selected node given the current progress of the solver
once the popped node is not fathomed, such as [95]. The second is using learning methods to achieve better bounds
to accelerate the pruning process, such as [97]. The last one is trimming the neural network by cutting oﬀ redundant
connections between weights or neurons of adjacent layers and attaching ﬁne-tuning subsequently to the pruned
model for improving the performance, such as Han et al. [106]. Note that the third method is not a particular
method for the B&B algorithm since it can be applied to simplify the problem model if it implements the neural
network. In this survey, we will add more references related to the ﬁrst two ways as follows. We call the ﬁrst method
as learning-based node pruning and the second one learning-based bounds.

4.3.1 Learning-based Node Pruning

Traditional nodes pruning methods only prune nodes if one of the three cases is met as summarized in 2.4. Nowadays,
learning methods are adopted to prune the non-optimal nodes to accelerate the B&B algorithm. He et al. [95] treated
the node pruning policy as a binary classiﬁer that predicts an action in {prune, expand}. The classiﬁer takes a feature-
vector description of the current state and attempts to predict the current action to mimic the expert action, which
prunes a node when it does not belong to the optimal branch. This method learns when to leave an unpromising
area and when to stop for a good enough solution; thus, it obtains a smaller gap solution using less time than SCIP.
However, when the network scale is large, the computational complexity of [95] is still high since the state space
is quite ample. Shen et al. [107] made great eﬀorts to improve the computational eﬃciency over [95] by exploiting

20

the structure of the B&B algorithm and problem data. The authors also proposed self-learning–one kind of transfer
learning method without labeling, to address the task mismatch issue by relying on only a few additional unlabeled
training samples. As a result, this method requires few training samples and shows eﬀectiveness on acceleration
compared to the algorithm in [95]. Unfortunately, the above two methods are redundant to learn to prune the
fathomed nodes. To cope with this diﬃculty, Lee et al. [108] proposed to simplify the learning task by keeping the
traditional prune policy and learning an auxiliary prune policy to reinforce it. Diﬀerent from only keeping the top
solution and pruning the other non-optimal nodes in the above literature, Yilmaz and Smith [96] kept the best k
solutions. Empirical results on ﬁve MIP datasets indicate that this policy leads to solutions signiﬁcantly more quickly
than the state-of-the-art precedent in the literature. In addition, with the same time limitation requirement, this
algorithm can achieve a better optimality gap.

4.3.2 Learning-based Bounds

Since the MILP is hard to solve, relaxed convex subproblems are solved as the lower bounds. More nodes can be
pruned if the lower bound can be determined much closer to the original subproblem. Hottung et al. [97] used a
DNN to heuristically determine the lower bounds. The authors then proposed three pruning functions to determine
whether to prune certain branches or nodes based on the output. On the other hand, if the upper bound is loose,
which is often much higher than the actual objective, then few branches of the search tree can be pruned. In the
traditional B&B algorithm, one usually updates the upper bound along with diving the tree. However, if a tighter
upper bound can be obtained faster, more subproblems can be pruned. The primal heuristics [79, 99–105] which aim
to ﬁnd a possible solution as fast as possible give a possible solution with a high quality upper bound; thus they
will also have a signiﬁcant improvement to accelerate the pruning speed. Some special learning-based upper bounds
are derived for some particular MILP problems, such as decision trees [109] by Again et al. and informative path
planning [110] by Binney and Sukhatme.

4.4 Learn to Cut

Existing traditional theoretical analysis limit us to understand and address cutting-plane selections, and they have so
far failed to help in practical cutting-plane selection [65] directly. Applying learning to cut generation and selection
has the potential to improve the solving MILPs and oﬀer help in understanding and tackling related issues.

4.4.1 Reinforcement Learning in Cut Selection

i x ≤ di}. Then x∗

LP (t), D(t)}, where C(t) = {aT

i x ≤ di from D(t), the new feasible region becomes C(t) = C(t) ∪ {eT

Tang et al. [111] used reinforcement learning to enhance the performance of heuristics. It formulated the process
of sequentially selecting cutting planes as a MDP. At iteration t, the numerical representation of the state is st =
i ≤ bi}Nt
{C(t), c, x∗
i=1 is the feasible region with Nt constraints of the current LP, c is
the parameter of objective function. Solving this new LP yields an optimal solution x∗
LP (t) and the set of candidate
Gomory’s cuts D(t). Thus, the action space at iteration t is D(t). After taking an action, which is adding one
inequality eT
LP (t + 1) and
LP (t + 1), D(t+1)} is determined. The gap between
D(t+1) can be computed, and the new state st+1 = {C(t+1), c, x∗
objective values of these LP solutions rt = cT x∗
LP (t + 1) − cT x∗
LP (t) is the reward for the RL agent at iteration t,
encouraging the agent to approach the optimal integer solution as fast as possible. The trained policy speciﬁes a
distribution over the action space D(t+1) at a given state st. To make the policy agnostic to the ordering among
the constraints, an attention network was adopted. Besides, to enable the network to handle IP instances with
various sizes, LSTM with hidden states was utilized, which encoded all information in the original inequalities.
Concerning four classes of IP instances: Packing, Production Planning, Binary Packing, and Max-Cut, experiment
results illustrate the performance of the RL agent from ﬁve perspectives: eﬃciency of cuts, integrality gap closed,
generalization properties, impact on the eﬃciency of branch-and-cut, and interpretability of cuts. Compared to some
commonly used human-designed heuristics: Random, Max Violation, Max Normalized Violation, and Lexicographical
Rule, the RL agent needs the least number of cuts. For some larger-scale instances with sizes close to 5000, the agent
can close the highest fraction of integrality. Although trained on smaller instances, the RL agent can still have
extremely competitive performance on 10 times larger test problems. Moreover, even if the agent is trained on small
packing problems, it can be applied to 10 times larger graph-based maximum-cut problems, indicating its great
generation properties. Compared to the pure B&B method, this learning-based cutting-plane selection reduces the
number of expanded nodes signiﬁcantly. However, the running time of the RL policy is not improved substantially
compared with other methods since running the policy is somehow costly.

21

Fig. 9. Illustration that depth-ﬁrst would cost redundant eﬀort to solve nonoptimal nodes. (If the optimal node belongs to
the right side tree, it would cost a lot of eﬀort but is meaningless to search the whole left side tree.)

4.4.2 Ranking-based Cut Selection

With the aim to reduce the total running time, Huang et al. [112] proposed a new metric, named Problem Solvability
Improvement, to measure the quality of the selected cut subset. Since obtaining the problem solvability is infeasible
in practice, this metric was substituted with the reduction ratio of solution time of selecting a cut subset. A score
function that measures this quality was learned as a rank formulation which labels the top-ranked cuts as positive and
the remaining cuts as negative because the main goal is to diﬀerentiate good cuts from poor cuts, rather than to rank
all candidate cuts. Diﬀerent from Khalil et al. [75], labels for individual cuts are not easy to obtain. Hence, Multiple
Instance Learning (MIL) was capitalized on to tackle this issue. Instead of requiring instances labeled individually,
in Babenko [113], MIL receives a set of labeled bags, each including several instances. In the binary classiﬁcation, a
bag would be labeled negative when all instances in it are negative. Otherwise, a bag would be labeled positive as
long as there is at least one positive instance in it. This technique rather ﬁts the scenario of cut selection since the
label assignment is determined by more than one instance, and the eﬀect of an individual cut is very imperceptible
for large-scale problems. To better generalize the mode, 14 problem-speciﬁc atomic features adapted from [75] were
designed, including statistics of cut and objective coeﬃcients, support, normalized violation, distance, parallelism,
and expected improvement. Experimental results demonstrate the power of this ranking-based cut selection policy in
terms of the quality, generalization ability, and performance on large-scale real-world tasks. The cut ranking policy
has reduced the solving time and the number of nodes more signiﬁcantly than human-designed heuristics, and shown
higher stability. Besides, this learned policy has a certain generalization ability on instances with diﬀerent sizes,
structures. However, it may have inferior performance when encountering a large range of coeﬃcients. It should be
noted that this work ﬁrstly applied the learning-based cut selection strategy for large-scale MILPs with more than
107 variables and constraints. For real-world production planning problems, this method improves the eﬃciency of
Huawei’s industrial MILP solver and solves problems without loss of accuracy, speeding up the ratio of 14.98% and
12.42% in the oﬄine and online setting, respectively.

Besides, several works have applied learning to cut generation in nonlinear programming, such as Dey et al. [114]
and Baltean-Lugojan et al. [115]. This topic is beyond our scope, and we will not talk about related works in detail.

5 Future Work Direction and Methodology

In this section, we review some of the algorithmic concepts previously introduced by taking into account their
limitations. Future work directions to cope with their limitations and possible methodologies are included in this
section.

5.1 Online Tuning Learning Methods

In Section 4.2.1, we reviewed a work using learning methods to obtain a suitable tuning parameter, which balances
the best-ﬁrst and depth-ﬁrst in the B&B run. However, this work ignores the uniqueness of each node during the
B&B run. For example, it should count more on the quality of the achieved solution for the root node since achieving

22

bbbbbbFig. 10. Demonstration of 3-layer multi-layer perceptron

a solution with high quality would signiﬁcantly reduce the searching space. Otherwise, it should make a redundant
eﬀort to solve nonoptimal nodes, as illustrated in Fig. 9. Therefore, it would be preferable for the root node to
assign more weight to the best-ﬁrst policy. On the other hand, satisfying the feasibility constraints would be more
critical for the near leaf nodes since the quality can be adjusted with just a few backward exploitations. From the
above analysis, it would be interesting to consider the nodes’ features (such as depth) and propose an online tuning
learning method to adjust the tradeoﬀ along with the B&B run.

Moreover, we can improve the estimate in [88] using the evaluation criteria summarized in Section 2.3.3. Instead of
using the normalized relaxed LP objective value as the estimate of this node in [88], we can take into account the
feasibility quality along with the optimality quality. We can further introduce a parameter to tune the importance
of the feasibility quality versus the optimality quality and learn this parameter online.

5.2 Extension to Nonbinary Classiﬁcations and Adapt Learning Strategies at a Given Time

Most literature in this survey does not consider the time required to adjust the optimal learning strategy. For
example, in Section 4.2.2, existing literature [99] adopts binary classiﬁcation to learn whether primal heuristics will
succeed at a given node. However, whether to conduct primal heuristics might also be inﬂuenced by the quality
constraints and time limitation for implementation, and all those can not be included in the binary classiﬁcation.

Inspired by works [79,107] and so on, we could expand the output to indicate the probability of whether to conduct a
primal heuristics (or other problems which has binary classes) with problem features (A, b, c) or the extracted feature
by GCNN [77–79, 105] as shown in Fig. 6 and 7. The learning task is to approximate the probability distribution.
We employ an L-layer multi-layer perceptron, a type of neural network to learn the mapping from the input feature
vector to the output indicating the probability of each class, as shown in Fig. 10. We then adopt the supervised
learning, or Data Aggregation [116] to learn the model parameters by minimizing the weighted loss function. The time
limitation could be considered to tune the threshold. For example, in the standard classiﬁcation, if the probability
is larger than 0.5 of belonging to the ﬁrst class, we cluster this input to the ﬁrst class; otherwise, it belongs to the
second class. Suppose the ﬁrst-class means conducting primal heuristics, decreasing the threshold leads to a larger
exploration space and obtains a feasible solution with higher quality. We could iteratively decrease the threshold to
satisfy the quality constraints and time limitations and obtain the learned parameter adaptively.

5.3 Reduce Training Samples

A common limitation of learning-based algorithms in this survey is that in order to have good predictors, a large
number of solved instances must be available, and the solved instances in most literature require to be of the same
type as the problem we expect to solve in the future. However, when the problem scale becomes quite large, generating
the training data is costly, let alone training the network by the data.

In order to address this limitation, there are two possible directions. One is generating the training data along
with the problem-solving progress. Dataset Aggregation (DAGGER), which was proposed by Ross et al. [116], is an

23

InputlayerHiddenlayer1Hiddenlayer2OutputlayerFig. 11. Demonstration of the diﬀerence between transfer learning and traditional ML.

iterative algorithm that trains a deterministic policy that achieves good performance guarantees under its induced
distribution of states. Pseudocode for DAGGER is shown in Algorithm 3, where π∗ is the presence of the expert
in DAGGER and βi ∈ [0, 1] represents the trust of the expert decisions. The other direction is using transfer
learning to exploit the pre-trained neural network and train a new model with only a few additional samples. Fig. 11
demonstrates the transfer learning method. The essential advantage is that with transfer learning, the new task can
be trained with fewer additional training samples than the traditional one since some information can be achieved
by transferring the knowledge from the old task into the new task.

Algorithm 3 DAGGER Algorithm
1: Initialization: D ← ∅, and pruning policy ˆπ(0).
2: for i = 0 : N do
3:
4:
5:
6:
7:
8: end for
9: return best ˆπ(i) on validation.

Let π(i) ← βiπ∗ + (1 − βi)ˆπ(i)
Sample T −step trajectories using π(i).
Collect dataset Di = {(s, π∗(s))} of visited states by π(i).
Aggregate datasets D ← D ∪ Di.
Train the policy π(i+1) based on D.

5.4 Extension to Speciﬁc Joint Bilinear Problems and Discrete Network Design Problems

A wide range of resource management problems in networks can be formulated as jointly bilinear problems. For
example, a concave piecewise linear network ﬂow problem (CPLNFP) is equivalent to a jointly constrained bilinear
program [117]. Let G(N, V ) represent a network where N and V are the sets of nodes and arcs, respectively. The
general form of the CPLNFP is

min
x

(cid:88)

a∈A

fa(xa),

(14)

s.t. x ∈ X, xa ∈ [λ0

a, λna

a ], ∀a ∈ V,

where X is a convex set representing the constraints on x, and fa(xa) are piecewise linear concave functions, i.e.,

fa(xa) =






c1
axa + s1
axa + s2
c2

a(= f 1
a(= f 2

a (xa)), xa ∈ [λ0
a (xa)), xa ∈ [λ1

a, λ1
a, λ2

a),
a),

:
a xa + sna
cna

a (= f na

a (xa)), xa ∈ [λna−1

a

(15)

, λna

a ],

24

a > c2

with c1
of the following jointly bilinear problem [117]

a > · · · > cna

a . The solution of the above problem (14) can be easily constructed from a global solution

min
x,y

(cid:88)

na(cid:88)

a∈A

i=1

a(xa)yi
f i
a,

s.t. x ∈ X,

na(cid:88)

i=1

a yi
λi−1

a ≤ xa ≤

na(cid:88)

i=1

ayi
λi
a,

na(cid:88)

i=1

a = 1, xa ≥ 0, yi
yi

a ≥ 0.

A general jointly bilinear constrained program is

f (x) + x(cid:62)Ay + g(y),

min
x,y
s.t. (x, y) ∈ S ∩ Ω,

(16)

(17)

where x ∈ Rp and y ∈ Rq are decision variables; A is a given p × q matrix; f and g are given functions which are
convex over S ∩ Ω; S is a closed, convex set; and Ω = {(x, y) : l ≤ x ≤ L, m ≤ y ≤ M }. It is shown in [118] that the
optimal solution of a general jointly constrained bilinear program belongs to the boundary of the feasible region.

Diﬀerent from MILP iteratively constructing a binary search tree, the B&B algorithm for jointly bilinear constrained
program generates a four-branch search tree iteratively by splitting a selected rectangle into four subrectangles. In
addition, since the variables are changed from discrete integers to continuous intervals, the stopping criterion is also
changed. From the above analysis, the complexity of this problem increases dramatically. One possible future work
is to use learning methods to accelerate the branch, select, pruning, and cutting procedure in joint bilinear problems,
which can be used to accelerate solving the large-scale discrete network design problems.

6 Conclusions

In this paper, we have surveyed the existing literature studying diﬀerent approaches and algorithms for the four
critical components in the general B&B algorithm, namely, branching variable selection, node selection, node pruning,
and cutting-plane selection. However, the complexity of the B&B algorithm always grows exponentially with respect
to the increase of the decision variable dimensions.

As the problem dimension size increases in real applications, the traditional B&B algorithm is not able to obtain a
certiﬁed solution within the time limit. In order to improve the speed of B&B algorithms, learning techniques have
been introduced in this algorithm recently (mainly in the past decade). We further surveyed how machine learning can
be used to improve the four critical components in B&B algorithms. In general, a supervised learning method helps
to generate a policy that mimics an expert but signiﬁcantly improves the speed. An unsupervised learning method
helps choose diﬀerent methods based on the features. In addition, models trained with reinforcement learning can
beat the expert policy, given enough training and a supervised initialization. Detailed comparisons between diﬀerent
algorithms have been summarized in our survey.

Although most of the approaches we discussed in this paper speed the run time and achieved better gaps with
respect to the optimal solutions. Most learning approach does not consider the features generated from the B&B
run; that is to say, they view all the nodes equally and then derive a deterministic approach based on the problem-
dependent features. To further improve the speed of the B&B algorithm, one possible direction is ﬁnding an online
tuning method based on the features generated from the B&B run. Meanwhile, another major limitation is that
most literature in this survey does not take into account the time requirement and the quality bound. The learning
algorithm, which is adaptive with respect to the time and quality requirement, is worth studying. Apart from that,
for large-scale problems, generating training samples would also cost signiﬁcant computational eﬀort. Reducing the
training samples is another big challenge. Last, the discrete network design problems do not fall into the MILP
problems but can be viewed as a joint bilinear problem. How to extend the above learning methods to this new type
of problem is also a possible topic. Finally, we discuss some possible methodologies on these future directions.

25

References

[1] A. H. Land and A. G. Doig, “An automatic method for solving discrete programming problems,” in Econometrica, 1960, vol. 28,

no. 3, pp. 497–520.

[2]

J. Clausen, “Branch and bound algorithms-principles and examples,” Department of Computer Science, University of Copenhagen,
pp. 1–30, 1999.

[3] D. R. Morrison, S. H. Jacobson, J. J. Sauppe, and E. C. Sewell, “Branch-and-bound algorithms: A survey of recent advances in

searching, branching, and pruning,” Discrete Optimization, vol. 19, pp. 79–102, 2016.

[4] Cplex. http://www.ilog.com/products/cplex.

[5]

Lindo. http://www.lindo.com.

[6] A. Martin, “Integer programs with block structure,” 1999.

[7] T. Achterberg, “Constraint integer programming,” 2007.

[8] T. Achterberg, T. Koch, and A. Martin, “Branching rules revisited,” Operations Research Letters, vol. 33, no. 1, pp. 42–54, 2005.

[Online]. Available: https://www.sciencedirect.com/science/article/pii/S0167637704000501

[9] P. G. Falk, “Experiments in mixed integer linear programming in a manufacturing system,” Omega, vol. 8, no. 4, pp. 473–484,

1980.

[10] R. Bixby and W. Cook, “Finding cuts in the tsp (a preliminary report),” Surgery, vol. 89, no. 12, 1995.

[11] J. T. Linderoth and M. W. P. Savelsbergh, “A computational study of search strategies for mixed integer programming,”
INFORMS Journal on Computing, vol. 11, no. 2, pp. 173–187, 1999. [Online]. Available: https://doi.org/10.1287/ijoc.11.2.173

[12] T. Achterberg and T. Berthold, “Hybrid branching,” in Integration of AI and OR Techniques in Constraint Programming for
Combinatorial Optimization Problems, W.-J. van Hoeve and J. N. Hooker, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg,
2009, pp. 309–311.

[13] M. Fischetti and M. Monaci, “Backdoor branching,” in Integer Programming and Combinatoral Optimization, O. G¨unl¨uk and G. J.

Woeginger, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, pp. 183–191.

[14] A. Gilpin and T. Sandholm, “Information-theoretic approaches to branching in search,” Discrete Optimization, vol. 8, no. 2, pp.

147–159, 2011. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S1572528610000423

[15] M. Fischetti and M. Monaci, “Branching on nonchimerical fractionalities,” Operations Research Letters, vol. 40, no. 3, pp.

159–164, 2012. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S016763771200017X

[16] R. J. Dakin, “A tree-search algorithm for mixed integer programming problems,” The computer journal, vol. 8, no. 3, pp. 250–255,

1965.

[17] P. E. Hart, N. J. Nilsson, and B. Raphael, “A formal basis for the heuristic determination of minimum cost paths,” IEEE transactions

on Systems Science and Cybernetics, vol. 4, no. 2, pp. 100–107, 1968.

[18] M. B´enichou, J.-M. Gauthier, P. Girodet, G. Hentges, G. Ribi`ere, and O. Vincent, “Experiments in mixed-integer linear

programming,” Mathematical Programming, vol. 1, no. 1, pp. 76–94, 1971.

[19] J. Forrest, J. Hirst, and J. A. Tomlin, “Practical solution of large mixed integer programming problems with umpire,” Management

Science, vol. 20, no. 5, pp. 736–773, 1974.

[20] J. T. Linderoth and M. W. Savelsbergh, “A computational study of search strategies for mixed integer programming,” INFORMS

Journal on Computing, vol. 11, no. 2, pp. 173–187, 1999.

[21] T. Berthold, “Primal heuristics for mixed integer programs,” 2006.

[22] E. Balas, S. Ceria, M. Dawande, F. Margot, and G. Pataki, “Octane: A new heuristic for pure 0–1 programs,” Operations Research,

vol. 49, no. 2, pp. 207–225, 2001.

[23] M. Fischetti, F. Glover, and A. Lodi, “The feasibility pump,” Mathematical Programming, vol. 104, no. 1, pp. 91–104, 2005.

[24] M. Fischetti and A. Lodi, “Local branching,” Mathematical programming, vol. 98, no. 1, pp. 23–47, 2003.

[25] E. Danna, E. Rothberg, and C. Le Pape, “Exploring relaxation induced neighborhoods to improve mip solutions,” Mathematical

Programming, vol. 102, no. 1, pp. 71–90, 2005.

[26] E. Rothberg, “An evolutionary algorithm for polishing mixed integer programming solutions,” INFORMS Journal on Computing,

vol. 19, no. 4, pp. 534–541, 2007.

[27] T. Berthold, “Primal minlp heuristics in a nutshell,” in Operations Research Proceedings 2013. Springer, 2014, pp. 23–28.

[28] ——, “A computational study of primal heuristics inside an mi (nl) p solver,” Journal of Global Optimization, vol. 70, no. 1, pp.

189–206, 2018.

[29] ——, “Measuring the impact of primal heuristics,” Operations Research Letters, vol. 41, no. 6, pp. 611–614, 2013.

[30] M. Vila and J. Pereira, “A branch-and-bound algorithm for assembly line worker assignment and balancing problems,” Computers

& Operations Research, vol. 44, pp. 105–114, 2014.

[31] D. R. Morrison, E. C. Sewell, and S. H. Jacobson, “An application of the branch, bound, and remember algorithm to a new simple

assembly line balancing dataset,” European Journal of Operational Research, vol. 236, no. 2, pp. 403–409, 2014.

[32] S. Arora, B. Bollob´as, and L. Lov´asz, “Proving integrality gaps without knowing the linear program,” in The 43rd Annual IEEE

Symposium on Foundations of Computer Science, 2002. Proceedings.

IEEE, 2002, pp. 313–322.

26

[33] D. T. Phan, “Lagrangian duality and branch-and-bound algorithms for optimal power ﬂow,” Operations Research, vol. 60, no. 2,

pp. 275–285, 2012.

[34] B. Gendron, P.-V. Khuong, and F. Semet, “A lagrangian-based branch-and-bound algorithm for the two-level uncapacitated facility

location problem with single-assignment constraints,” Transportation Science, vol. 50, no. 4, pp. 1286–1299, 2016.

[35] W. H. Kohler and K. Steiglitz, “Characterization and theoretical comparison of branch-and-bound algorithms for permutation

problems,” Journal of the ACM (JACM), vol. 21, no. 1, pp. 140–156, 1974.

[36] E. C. Sewell, J. J. Sauppe, D. R. Morrison, S. H. Jacobson, and G. K. Kao, “A bb&r algorithm for minimizing total tardiness on
a single machine with sequence dependent setup times,” Journal of Global Optimization, vol. 54, no. 4, pp. 791–812, 2012.

[37] M. Fischetti and D. Salvagnin, “Pruning moves,” INFORMS Journal on Computing, vol. 22, no. 1, pp. 108–119, 2010.

[38] E. Demeulemeester, W. Herroelen et al., “The discrete time/resource trade-oﬀ problem in project networks: a branch-and-bound

approach,” IIE transactions, vol. 32, no. 11, pp. 1059–1069, 2000.

[39] T. Ibaraki, “The power of dominance relations in branch-and-bound algorithms,” Journal of the ACM (JACM), vol. 24, no. 2, pp.

264–279, 1977.

[40] F. Margot, “Pruning by isomorphism in branch-and-cut,” Mathematical Programming, vol. 94, no. 1, pp. 71–90, 2002.

[41] ——, “Exploiting orbits in symmetric ilp,” Mathematical Programming, vol. 98, no. 1, pp. 3–21, 2003.

[42] J. E. Mitchell, Branch and Cut. American Cancer Society, 2011. [Online]. Available: https://onlinelibrary.wiley.com/doi/abs/10.

1002/9780470400531.eorms0117

[43] M. Padberg and G. Rinaldi, “A branch-and-cut algorithm for the resolution of large-scale symmetric traveling salesman problems,”

SIAM Review, vol. 33, no. 1, pp. 60–100, 1991. [Online]. Available: https://doi.org/10.1137/1033004

[44] E. Balas, S. Ceria, and G. Cornu´ejols, “Mixed 0-1 programming by lift-and-project in a branch-and-cut framework,” Management

Science, vol. 42, no. 9, pp. 1229–1246, 1996. [Online]. Available: https://doi.org/10.1287/mnsc.42.9.1229

[45] R. E. Gomory, “Outline of an algorithm for integer solutions to linear programs and an algorithm for the mixed integer problem,”

in 50 Years of Integer Programming 1958-2008. Springer, 2010, pp. 77–103.

[46] V. Chv´atal, “Edmonds polytopes and a hierarchy of combinatorial problems,” Discrete Mathematics, vol. 4, no. 4, pp. 305–337,

1973.

[47] H. Marchand, A. Martin, R. Weismantel, and L. Wolsey, “Cutting planes in integer and mixed integer programming,” Discrete

Applied Mathematics, vol. 123, no. 1-3, pp. 397–446, 2002.

[48] R. Gomory, “An algorithm for the mixed integer problem,” Rand Corporation, 1960.

[49] P. Bonami, G. Cornu´ejols, S. Dash, M. Fischetti, and A. Lodi, “Projected chv´atal–gomory cuts for mixed integer linear programs,”

Mathematical Programming, vol. 113, no. 2, pp. 241–257, 2008.

[50] G. L. Nemhauser and L. A. Wolsey, “A recursive procedure to generate all cuts for 0–1 mixed integer programs,” Mathematical

Programming, vol. 46, no. 1-3, pp. 379–390, 1990.

[51] H. Marchand, “A polyhedral study of the mixed knapsack set and its use to solve mixed integer programs,” Ph.D. dissertation,

UCL-Universit´e Catholique de Louvain, 1998.

[52] H. Marchand and L. A. Wolsey, “Aggregation and mixed integer rounding to solve mips,” Operations research, vol. 49, no. 3, pp.

363–371, 2001.

[53] W. Cook, R. Kannan, and A. Schrijver, “Chv´atal closures for mixed integer programming problems,” Mathematical Programming,

vol. 47, no. 1-3, pp. 155–174, 1990.

[54] L. Lov´asz and A. Schrijver, “Cones of matrices and set-functions and 0–1 optimization,” Siam J Opt, vol. 1, no. 2, pp. 166–190,

1991.

[55] E. Balas, S. Ceria, and G. Cornu´ejols, “A lift-and-project cutting plane algorithm for mixed 0–1 programs,” Mathematical

Programming, vol. 58, no. 1-3, pp. 295–324, 1993.

[56] M. W. Padberg, T. J. Van Roy, and L. A. Wolsey, “Valid linear inequalities for ﬁxed charge problems,” Operations Research,

vol. 33, no. 4, pp. 842–861, 1985. [Online]. Available: https://doi.org/10.1287/opre.33.4.842

[57] T. J. Van Roy and L. A. Wolsey, “Valid inequalities for mixed 0-1 programs,” Discrete Applied Mathematics, vol. 14, no. 2, pp.

199–213, 1986. [Online]. Available: https://www.sciencedirect.com/science/article/pii/0166218X86900612

[58] K. Aardal, Y. Pochet, and L. A. Wolsey, “Capacitated facility location: valid inequalities and facets,” Mathematics of Operations

Research, vol. 20, no. 3, pp. 562–582, 1995.

[59] M. W. Savelsbergh, “Preprocessing and probing techniques for mixed integer programming problems,” ORSA Journal on

Computing, vol. 6, no. 4, pp. 445–454, 1994.

[60] A. Atamt¨urk, G. L. Nemhauser, and M. W. Savelsbergh, “Conﬂict graphs in solving integer programming problems,” European

Journal of Operational Research, vol. 121, no. 1, pp. 40–55, 2000.

[61] A. Atamtuerk, “Cover and pack inequalities for (mixed) integer programming,” Annals of Operations Research, vol. 139, no. 1, pp.

21–38, 2005.

[62] H. Crowder, E. L. Johnson, and M. Padberg, “Solving large-scale zero-one linear programming problems,” Operations Research,

vol. 31, no. 5, pp. 803–834, 1983.

[63] R. L. van de Leensel, C. Van Hoesel, and J. Van de Klundert, “Lifting valid inequalities for the precedence constrained knapsack

problem,” Mathematical programming, vol. 86, no. 1, pp. 161–185, 1999.

27

[64] T. Achterberg and R. Wunderling, Mixed Integer Programming: Analyzing 12 Years of Progress. Berlin, Heidelberg: Springer

Berlin Heidelberg, 2013, pp. 449–481. [Online]. Available: https://doi.org/10.1007/978-3-642-38189-8 18

[65] S. S. Dey and M. Molinaro, “Theoretical challenges towards cutting-plane selection,” Mathematical Programming, vol. 170, no. 1,

pp. 1–30, 2018.

[66] C. M. Bishop, Pattern recognition and machine learning.

springer, 2006.

[67] K. P. Murphy, Machine learning: a probabilistic perspective. MIT press, 2012.

[68]

I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, Deep learning. MIT press Cambridge, 2016, vol. 1, no. 2.

[69] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018.

[70] A. M. Alvarez, Q. Louveaux, and L. Wehenkel, “A supervised machine learning approach to variable branching in branch-and-

bound,” in In ecml. Citeseer, 2014.

[71] ——, “A machine learning-based approximation of strong branching,” INFORMS Journal on Computing, vol. 29, no. 1, pp. 185–195,

2017.

[72] A. Marcos Alvarez, L. Wehenkel, and Q. Louveaux, “Online learning for strong branching approximation in branch-and-bound,”

2016.

[73] P. Geurts, D. Ernst, and L. Wehenkel, “Extremely randomized trees,” Machine learning, vol. 63, no. 1, pp. 3–42, 2006.

[74] G. Zarpellon, J. Jo, A. Lodi, and Y. Bengio, “Parameterizing branch-and-bound search trees to learn branching policies,” 2020.

[75] E. Khalil, P. Le Bodic, L. Song, G. Nemhauser, and B. Dilkina, “Learning to branch in mixed integer programming,”
[Online]. Available:

Intelligence, vol. 30, no. 1, Feb. 2016.

the AAAI Conference on Artiﬁcial

Proceedings of
https://ojs.aaai.org/index.php/AAAI/article/view/10080

[76] M.-F. Balcan, T. Dick, T. Sandholm, and E. Vitercik, “Learning to branch,” in Proceedings of the 35th International Conference
on Machine Learning, ser. Proceedings of Machine Learning Research, J. Dy and A. Krause, Eds., vol. 80. PMLR, 10–15 Jul
2018, pp. 344–353. [Online]. Available: http://proceedings.mlr.press/v80/balcan18a.html

[77] M. Gasse, D. Ch´etelat, N. Ferroni, L. Charlin, and A. Lodi, “Exact combinatorial optimization with graph convolutional neural

networks,” arXiv preprint arXiv:1906.01629, 2019.

[78] P. Gupta, M. Gasse, E. B. Khalil, M. P. Kumar, A. Lodi, and Y. Bengio, “Hybrid models for learning to branch,” arXiv preprint

arXiv:2006.15212, 2020.

[79] V. Nair, S. Bartunov, F. Gimeno, I. von Glehn, P. Lichocki, I. Lobov, B. O’Donoghue, N. Sonnerat, C. Tjandraatmadja, P. Wang

et al., “Solving mixed integer programs using neural networks,” arXiv preprint arXiv:2012.13349, 2020.

[80] G. Gamrath, D. Anderson, K. Bestuzheva, W.-K. Chen, L. Eiﬂer, M. Gasse, P. Gemander, A. Gleixner, L. Gottwald, K. Halbig

et al., “The scip optimization suite 7.0,” 2020.

[81] A. Gleixner, M. Bastubbe, L. Eiﬂer, T. Gally, G. Gamrath, R. L. Gottwald, G. Hendel, C. Hojny, T. Koch, M. L¨ubbecke, S. J.
Maher, M. Miltenberger, B. M¨uller, M. Pfetsch, C. Puchert, D. Rehfeldt, F. Schl¨osser, C. Schubert, F. Serrano, Y. Shinano, J. M.
Viernickel, M. Walter, F. Wegscheider, J. T. Witt, and J. Witzig, “The scip optimization suite 6.0,” ZIB, Takustr. 7, 14195 Berlin,
Tech. Rep. 18-26, 2018.

[82] C. Hansknecht, I. Joormann, and S. Stiller, “Cuts, primal heuristics, and learning to branch for the time-dependent traveling

salesman problem,” arXiv preprint arXiv:1805.01415, 2018.

[83] H. Sun, W. Chen, H. Li, and L. Song, “Improving learning to branch via reinforcement learning,” in Learning Meets Combinatorial

Algorithms at NeurIPS2020, 2020. [Online]. Available: https://openreview.net/forum?id=z4D7-PTxTb

[84] M. Etheve, Z. Al`es, C. Bissuel, O. Juan, and S. Kedad-Sidhoum, “Reinforcement learning for variable selection in a branch and
bound algorithm,” in Integration of Constraint Programming, Artiﬁcial Intelligence, and Operations Research, E. Hebrard and
N. Musliu, Eds. Cham: Springer International Publishing, 2020, pp. 176–185.

[85] G. Di Liberto, S. Kadioglu, K. Leo, and Y. Malitsky, “Dash: Dynamic approach for switching heuristics,” European Journal of

Operational Research, vol. 248, no. 3, pp. 943–953, 2016.

[86] S. Kadioglu, Y. Malitsky, and M. Sellmann, “Non-model-based search guidance for set partitioning problems,” in Proceedings of

the AAAI Conference on Artiﬁcial Intelligence, vol. 26, no. 1, 2012.

[87] A. Lodi and G. Zarpellon, “On learning and branching: a survey,” Top, vol. 25, no. 2, pp. 207–236, 2017.

[88] A. Sabharwal, H. Samulowitz, and C. Reddy, “Guiding combinatorial optimization with uct,” in International Conference on
Integration of Artiﬁcial Intelligence (AI) and Operations Research (OR) Techniques in Constraint Programming. Springer, 2012,
pp. 356–361.

[89] F. Glover, “Future paths for integer programming and links to artiﬁcial intelligence,” Computers & operations research, vol. 13,

no. 5, pp. 533–549, 1986.

[90] F. Glover and H. J. Greenberg, “New approaches for heuristic search: A bilateral linkage with artiﬁcial intelligence,” European

Journal of Operational Research, vol. 39, no. 2, pp. 119–130, 1989.

[91] C. Ans´otegui, J. Pon, M. Sellmann, and K. Tierney, “Reactive dialectic search portfolios for maxsat,” in Proceedings of the AAAI

Conference on Artiﬁcial Intelligence, vol. 31, no. 1, 2017.

[92] C. Ans´otegui, Y. Malitsky, H. Samulowitz, M. Sellmann, and K. Tierney, “Model-based genetic algorithms for algorithm

conﬁguration.” in IJCAI, 2015, pp. 733–739.

[93] H. Daum´e, J. Langford, and D. Marcu, “Search-based structured prediction,” Machine learning, vol. 75, no. 3, pp. 297–325, 2009.

28

[94] K.-W. Chang, A. Krishnamurthy, A. Agarwal, H. Daume, and J. Langford, “Learning to search better than your teacher,” in

International Conference on Machine Learning. PMLR, 2015, pp. 2058–2066.

[95] H. He, H. Daume III, and J. M. Eisner, “Learning to search in branch and bound algorithms,” Advances in neural information

processing systems, vol. 27, pp. 3293–3301, 2014.

[96] K. Yilmaz and N. Yorke-Smith, “A study of learning search approximation in mixed integer branch and bound: Node selection in

scip,” AI, vol. 2, no. 2, pp. 150–178, 2021.

[97] A. Hottung, S. Tanaka, and K. Tierney, “Deep learning assisted heuristic tree search for the container pre-marshalling problem,”

Computers & Operations Research, vol. 113, p. 104781, 2020.

[98] D. Karapetyan, A. P. Punnen, and A. J. Parkes, “Markov chain methods for the bipartite boolean quadratic programming problem,”

European Journal of Operational Research, vol. 260, no. 2, pp. 494–506, 2017.

[99] E. B. Khalil, B. Dilkina, G. L. Nemhauser, S. Ahmed, and Y. Shao, “Learning to run heuristics in tree search.” in IJCAI, 2017,

pp. 659–666.

[100] G. Hendel, “Adaptive large neighborhood search for mixed integer programming,” 2018.

[101] A. Hottung and K. Tierney, “Neural large neighborhood search for the capacitated vehicle routing problem,” arXiv preprint

arXiv:1911.09539, 2019.

[102] R. Addanki, V. Nair, and M. Alizadeh, “Neural large neighborhood search,” in Learning Meets Combinatorial Algorithms NeurIPS

Workshop, 2020.

[103] J. Song, R. Lanka, Y. Yue, and B. Dilkina, “A general large neighborhood search framework for solving integer programs,” arXiv

preprint arXiv:2004.00422, 2020.

[104] ´A. S. Xavier, F. Qiu, and S. Ahmed, “Learning to solve large-scale security-constrained unit commitment problems,” INFORMS

Journal on Computing, 2020.

[105] J.-Y. Ding, C. Zhang, L. Shen, S. Li, B. Wang, Y. Xu, and L. Song, “Accelerating primal solution ﬁndings for mixed integer
programs based on solution prediction,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 34, no. 02, 2020,
pp. 1452–1459.

[106] S. Han, H. Mao, and W. J. Dally, “Deep compression: Compressing deep neural networks with pruning, trained quantization and

huﬀman coding,” arXiv preprint arXiv:1510.00149, 2015.

[107] Y. Shen, Y. Shi, J. Zhang, and K. B. Letaief, “Lorm: Learning to optimize for resource management in wireless networks with few

training samples,” IEEE Transactions on Wireless Communications, vol. 19, no. 1, pp. 665–679, 2019.

[108] M. Lee, G. Yu, and G. Y. Li, “Learning to branch: Accelerating resource allocation in wireless networks,” IEEE Transactions on

Vehicular Technology, vol. 69, no. 1, pp. 958–970, 2019.

[109] G. Aglin, S. Nijssen, and P. Schaus, “Learning optimal decision trees using caching branch-and-bound search,” in Proceedings of

the AAAI Conference on Artiﬁcial Intelligence, vol. 34, no. 04, 2020, pp. 3146–3153.

[110] J. Binney and G. S. Sukhatme, “Branch and bound for informative path planning,” in 2012 IEEE International Conference on

Robotics and Automation.

IEEE, 2012, pp. 2147–2154.

[111] Y. Tang, S. Agrawal, and Y. Faenza, “Reinforcement learning for integer programming: Learning to cut,” in Proceedings of the
37th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, H. D. III and A. Singh,
Eds., vol. 119. PMLR, 13–18 Jul 2020, pp. 9367–9376. [Online]. Available: http://proceedings.mlr.press/v119/tang20a.html

[112] Z. Huang, K. Wang, F. Liu, H.-l. Zhen, W. Zhang, M. Yuan, J. Hao, Y. Yu, and J. Wang, “Learning to select cuts for eﬃcient

mixed-integer programming,” arXiv preprint arXiv:2105.13645, 2021.

[113] B. Babenko, “Multiple instance learning: algorithms and applications,” View Article PubMed/NCBI Google Scholar, pp. 1–19,

2008.

[114] S. S. Dey, A. M. Kazachkov, A. Lodi, and G. Munoz, “Cutting plane generation through sparse principal component analysis,”

URL http://www. optimization-online. org/DB HTML/2021/02/8259. html, 2021.

[115] R. Baltean-Lugojan, P. Bonami, R. Misener, and A. Tramontani, “Selecting cutting planes for quadratic semideﬁnite outer-

approximation via trained neural networks,” Technical Report, CPLEX Optimization, IBM, Tech. Rep., 2018.

[116] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning and structured prediction to no-regret online learning,” in
JMLR Workshop and Conference

Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics.
Proceedings, 2011, pp. 627–635.

[117] A. Nahapetyan and P. M. Pardalos, “A bilinear relaxation based algorithm for concave piecewise linear network ﬂow problems,”

Journal of Industrial & Management Optimization, vol. 3, no. 1, p. 71, 2007.

[118] F. A. Al-Khayyal and J. E. Falk, “Jointly constrained biconvex programming,” Mathematics of Operations Research, vol. 8, no. 2,

pp. 273–286, 1983.

29

