1
2
0
2

t
c
O
3
2

]

G
L
.
s
c
[

2
v
8
8
1
4
0
.
6
0
1
2
:
v
i
X
r
a

Stability and Generalization of Bilevel Programming
in Hyperparameter Optimization

Fan Bao∗, Guoqiang Wu∗†, Chongxuan Li∗†, Jun Zhu‡, Bo Zhang
Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua-Huawei Joint Center for AI
BNRist Center, State Key Lab for Intell. Tech. & Sys., Tsinghua University, Beijing, China
bf19@mails.tsinghua.edu.cn,{guoqiangwu90, chongxuanli1991}@gmail.com,
{dcszj, dcszb}@tsinghua.edu.cn

Abstract

The (gradient-based) bilevel programming framework is widely used in hyperpa-
rameter optimization and has achieved excellent performance empirically. Previous
theoretical work mainly focuses on its optimization properties, while leaving the
analysis on generalization largely open. This paper attempts to address the issue by
presenting an expectation bound w.r.t. the validation set based on uniform stability.
Our results can explain some mysterious behaviours of the bilevel programming
in practice, for instance, overﬁtting to the validation set. We also present an ex-
pectation bound for the classical cross-validation algorithm. Our results suggest
that gradient-based algorithms can be better than cross-validation under certain
conditions in a theoretical perspective. Furthermore, we prove that regularization
terms in both the outer and inner levels can relieve the overﬁtting problem in
gradient-based algorithms. In experiments on feature learning and data reweighting
for noisy labels, we corroborate our theoretical ﬁndings.

1

Introduction

Hyperparameter optimization (HO) is a common problem arising from various ﬁelds including
neural architecture search [25, 9], feature learning [11], data reweighting for imbalanced or noisy
samples [34, 39, 10, 36], and semi-supervised learning [15]. Formally, HO seeks the hyperparameter-
hypothesis pair that achieves the lowest expected risk on testing samples from an unknown distribution.
Before testing, however, only a set of training samples and a set of validation ones are given. The
validation and testing samples are assumed to be from the same distribution. In contrast, depending
on the task, the underlying distributions of the training and testing samples can be the same [25, 11]
or different [34].

Though various methods [18] have been developed to solve HO (See Section 5 for a comprehensive
review), the bilevel programming (BP) [7, 33, 11] framework is a natural solution and has achieved
excellent performance in practice [25]. BP consists of two nested search problems: in the inner
level, it seeks the best hypothesis (e.g., a prediction model) on the training set given a speciﬁc
conﬁguration of the hyperparameter (e.g., the model architectures), while in the outer level, it seeks
the hyperparameter (and its associated hypothesis) that results in the best hypothesis in terms of the
error on the validation set.

∗Equal contribution
†G. Wu is now at School of Software, Shandong University and C. Li is now at Gaoling School of AI, Renmin

University of China. The work was done when they were at Tsinghua University.

‡Corresponding author.

35th Conference on Neural Information Processing Systems (NeurIPS 2021).

 
 
 
 
 
 
Generally, it is hard to solve the BP problem exactly, and several approximate algorithms have been
developed in previous work. As a classical approach, Cross-validation (CV)4 can be viewed as an
approximation method. It obtains a ﬁnite set of hyperparameters via grid search [32] or random
search [3] as well as a set of the corresponding hypothesises trained in the inner level, and selects
the best hyperparameter-hypothesis pair according to the validation error. CV is well understood in
theory [32, 37] but suffers from the issue of scalability [11]. Recently, alternative algorithms [10–
12, 30, 36] based on unrolled differentiation (UD) have shown promise on tuning up to millions of
hyperparameters. In contrast to CV, UD exploits the validation data more aggressively: it optimizes
the validation error directly via (stochastic) gradient descent in the space of the hyperparameters, and
the gradient is obtained by ﬁnite steps of unrolling in the inner level.

Though promising, previous theoretical work [11, 41, 36] of UD mainly focuses on the optimization,
while leaving its generalization analysis largely open. This paper takes a ﬁrst step towards solving it
and aiming to answer the following questions rigorously:

• Can we obtain certain learning guarantees for UD and insights to improve it?
• When should we prefer UD over classical approaches like CV in a theoretical perspective?

Our ﬁrst main contribution is to present a notion of uniformly stable on validation (in expectation)
and an expectation bound of UD algorithms (with stochastic gradient descent in the outer level) on
the validation data as follows:

|Expected risk of UD−Empirical risk of UD on validation| (cid:46) ˜O

(cid:19)

(cid:18) T κ
m

and ˜O

(cid:18) (1 + ηγϕ)2K
m

(cid:19)

,

(1)

where T and K are the numbers of steps in the outer level and the inner level respectively, κ ∈ (0, 1)
is a constant, η is the learning rate in the inner level, γϕ is a smoothness coefﬁcient of the inner loss,
m is the size of the validation set and (cid:46) means the inequality holds in expectation. As detailed in
Section 4.1, our results not only present the order of the important factors in the generalization gap
but also explain some mysterious behaviours of UD in practice, for instance, the trade-off on the
values of T and K [11].

Our second main contribution is to systematically compare UD and CV from the perspective of
generalization in Section 4.2. Instead of using the existing high probability bounds of CV (see
Theorem 4.4 in [32]), we present an expectation bound of CV for a direct comparison to Eq. (1) as
follows:5

|Expected risk of CV − Empirical risk of CV on validation| (cid:46) O

(cid:32)(cid:114)

(cid:33)

.

log T
m

(2)

On the one hand, Eq. (1) and Eq. (2) suggest that with a large T ,6 CV has a much lower risk of
overﬁtting than UD. On the other hand, the dependence of Eq. (2) on m is O
, which is worse
than that of UD. Furthermore, as discussed before, probably UD has a much lower validation risk than
CV. Indeed, we show that CV with random search suffers from the curse of dimensionality. These
analyses may explain the superior performance of UD [25], especially when we have a reasonable
choice of T , a sufﬁciently large m and a sufﬁciently high-dimensional hyperparameter space.

(cid:16)(cid:113) 1
m

(cid:17)

Our third main contribution is to present a regularized UD algorithm in Section 4.3 and prove that
both a weight decay term of the parameter in the inner level and that of the hyperparameter in the
outer level can increase the stability of the UD algorithms. Thus, the generalization performance will
probably improve if the terms do not hurt the validation risk too much.

Finally, experiments presented in Section 6 validate our theory ﬁndings. In particular, we reproduce
the mysterious behaviours of UD (e.g., overﬁtting to the validation data) observed in [11], which can
be explained via Eq. (1). Besides, we empirically compare UD and CV and analyze their performance
via Eq. (1) and Eq. (2). Further, we show the promise of the regularization terms in both levels.

4While CV can refer to a general class of approaches by splitting a training set and a validation set, we focus

on grid search and random search and denote the two methods as CV collectively.

5We do not ﬁnd a strong dependency of CV on K in both the theory and practice.
6Every hyperparameter considered corresponds to a loop for the inner level optimization, which is the
computational bottleneck in HO. Therefore, for fairness, we assume UD and CV share the same T by default.

2

2 Problem Formulation

Let Z, Θ and Λ denote the data space, the hypothesis space and the hyperparameter space respectively.
(cid:96) : Λ × Θ × Z → [a, b] is a bounded loss function7 and its range is s((cid:96)) = b − a.

Given a hyperparameter λ ∈ Λ, a hypothesis θ ∈ Θ and a distribution D on the data space Z,
R(λ, θ, D) denotes the expected risk of λ, θ on D, i.e., R(λ, θ, D) = Ez∼D [(cid:96)(λ, θ, z)]. Hyperparam-
eter optimization (HO) seeks the hyperparameter-hypothesis pair that achieves the lowest expected
risk on testing samples from an unknown distribution.
Before testing, however, only a training set Str of size n and a validation set Sval of size m are
accessible. As mentioned in Section 1, we consider a general HO problem, where the distribution of
the testing samples Dte is assumed to be the same as that of the validation ones Dval but can differ
from that of the training ones Dtr. In short, we assume Dval = Dte, but we do not require Dtr =
Dte. Given a hyperparameter λ ∈ Λ, a hypothesis θ ∈ Θ and the validation set Sval, ˆRval(λ, θ, Sval)
denotes the empirical risk of λ, θ on Sval, i.e., ˆRval(λ, θ, Sval) = 1
m

). The empirical

(cid:96)(λ, θ, zval

i

m
(cid:80)
i=1

risk on training is deﬁned as ˆRtr(λ, θ, Str) = 1
n

ϕi(λ, θ, ztr

i ), where ϕi can be a slightly modiﬁed

n
(cid:80)
i=1

(e.g., reweighted) version of (cid:96) for i-th training sample8.
Technically, an HO algorithm A is a function mapping Str and Sval to a hyperparameter-hypothesis
pair, i.e., A : Z n × Z m → Λ × Θ. In contrast, a randomized HO algorithm does not necessarily
return a deterministic hyperparameter-hypothesis pair but more generally a random variable with the
outcome space Λ × Θ.

Though extensive methods [18] have been developed to solve HO (See Section 5 for a comprehen-
sive review), the bilevel programming [7, 33, 11] is a natural solution and has achieved excellent
performance in practice recently [25]. It consists of two nested search problems as follows:

λ∗(Str, Sval) = arg min

ˆRval(λ, θ∗(λ, Str), Sval)

, where θ∗(λ, Str) = arg min

ˆRtr(λ, θ, Str)

.

(cid:124)

λ∈Λ

(cid:123)(cid:122)
Outer level optimization

(cid:125)

(cid:124)

θ∈Θ
(cid:123)(cid:122)
Inner level optimization

(cid:125)

(3)

In the inner level, it seeks the best hypothesis on Str given a speciﬁc conﬁguration of the hyperpa-
rameter. In the outer level, it seeks the hyperparameter λ∗(Str, Sval) (and its associated hypothesis
θ∗(λ∗(Str, Sval), Str)) that results in the best hypothesis in terms of the error on Sval. Eq. (3) is
sufﬁciently general to include a large portion of the HO problems we are aware of, such as:

• Differential Architecture Search [25] Let λ be the coefﬁcients of a set of network archi-
tecture, let θ be the parameters in the neural network deﬁned by the coefﬁcients, and let l
be the cross-entropy loss associated with θ and λ. Namely, l(λ, θ, z) = CE(hλ,θ(x), y),
where hλ,θ(·) is a neural network with architecture λ and parameter θ.

• Feature Learning [11] Let λ be a feature extractor, let θ be the parameters in a classiﬁer
that takes features as input and let l be the cross-entropy loss associated with θ and λ.
Namely, l(λ, θ, z) = CE(gθ(hλ(x)), y), where hλ(·) is the feature extractor and gθ(·) is
the classiﬁer.

• Data Reweighting for Imbalanced or Noisy Samples [36] Let λ be the coefﬁcients of the
training data, let θ be the parameters of a classiﬁer that takes the data as input, let l be the
cross-entropy loss associated with θ. Namely, l(θ, z) = CE(hθ(x), y), where hθ(·) is the
classiﬁer and l is irrelevant to λ. The empirical risk on the training set is deﬁned through
a reweighted version of the loss ϕi(λ, θ, zi) = σ(λi)l(θ, zi), where σ(·) is the sigmoid
function and λi is the coefﬁcient corresponding to zi.

7This formulation also includes the case where the loss function is irrelevant to the hyperparameter. Also see

Appendix F for a discussion of the boundedness assumption of the loss function.

8While in many tasks such as differential architecture search [25] and feature learning [11], ϕi is just the same
as (cid:96), we distinguish between them to include tasks where ϕi and (cid:96) are different, such as data reweighting [36].

3

Algorithm 1 Unrolled differentiation for hyperparameter
optimization
1: Input: Number of steps T and K; initialization ˆθ0 and

ˆλ0; learning rate scheme α and η

2: Output: The hyperparameter ˆλud and hypothesis ˆθud
3: for t = 0 to T − 1 do
4:
5:
6:

ˆθt
0 ← ˆθ0
for k = 0 to K − 1 do

k − ηk+1∇θ ˆRtr(ˆλt, θ, Str)|θ=ˆθt

ˆθt
k+1 ← ˆθt
end for
ˆλt+1 ← ˆλt − αt+1∇λ ˆRval(λ, ˆθt

K(λ), Sval)|λ=ˆλt

k

7:
8:
9: end for
10: return ˆλT and ˆθT
K

Algorithm 2 Cross-validation for hyper-
parameter optimization

1: Input: Number of steps T and K;
t=1;

initialization {ˆλt}T
learning rate scheme η

t=1 and {ˆθt

0}T

2: Output: The hyperparameter ˆλcv

and hypothesis ˆθcv
3: for k = 0 to K − 1 do
4:

ˆθt
ˆθt
←
k
k+1
ηk+1∇θ ˆRtr(ˆλt, θ, Str)|θ=ˆθt

k

−

5: end for
6: t∗ ← arg min
1≤t≤T

7: return ˆλt∗ and ˆθt∗
K

ˆRval(ˆλt, ˆθt

K, Sval)

3 Approximate the Bilevel Programming Problem

In most of the situations (e.g., neural network as the hypothesis class [25]), the global optima of
both the inner and outer level problems in Eq. (3) are nontrivial to achieve. It is often the case to
approximate them in a certain way (e.g., using (stochastic) gradient descent) as follows:

ˆλ(Str, Sval) ≈ arg min

ˆRval(λ, ˆθ(λ, Str), Sval)

, where ˆθ(λ, Str) ≈ arg min

ˆRtr(λ, θ, Str)

.

(cid:124)

(cid:123)(cid:122)
Approximate outer level optimization

(cid:125)

(cid:124)

λ∈Λ

θ∈Θ
(cid:123)(cid:122)
Approximate inner level optimization

(cid:125)

(4)

Here ˆθ(λ, Str) can be deterministic or random.
differentiation (UD) and cross-validation (CV) algorithms as two implementation of Eq. (4).

In this perspective, we can view the unrolled

Unrolled differentiation. The UD-based algorithms [10–12, 30, 36] solve Eq. (4) via performing
ﬁnite steps of gradient descent in both levels. Given a hyperparameter, the inner level performs
K updates, and keeps the whole computation graph. The computation graph is a composite of K
parameter updating functions, which are differentiable with respect to λ, and thereby the memory
complexity is O(K). As a result, the corresponding hypothesis is a function of the hyperparameter.
The outer level updates the hyperparameter a single step by differentiating through inner updates,
which has a O(K) time complexity, and the inner level optimization repeats given the updated
hyperparameter. Totally, the outer level updates T times. Formally, it is given by Algorithm 1, where
we omit the dependency of λ and θ on Sval and Strfor simplicity.

In some applications [25, 11], n and m can be very large and we cannot efﬁciently calculate the
gradients in Algorithm 1. In this case, stochastic gradient descent (SGD) can be used to update the
hyperparameter (corresponding to line 8 in Algorithm 1) as follows:

ˆλt+1 ← ˆλt − αt+1∇λ ˆRval(λ, ˆθt

K(λ), {zj})|λ=ˆλt

,

(5)

where zj is randomly selected from Sval. Similarly, we can also adopt SGD when updating the
hypothesis and then all intermediate hypothesises are random functions of λ and Str.

Cross-validation. CV is a classical approach for HO. It ﬁrst obtains a ﬁnite set of hyperparameters,
which is often a subset of Λ, via grid search [32] or random search [3] 9. Then, it separately trains the
inner level to obtain the corresponding hypothesis given a hyperparameter. Finally, it selects the best
hyperparameter-hypothesis pair according to the validation error. It is formally given by Algorithm 2,
where we use gradient descent to approximate the inner level (i.e., line 4). We can also adopt SGD to
update the hypothesis.

9In our experiments, the hyperparameter is too high-dimensional to perform grid search, and thus random

search is preferable. Nevertheless, they will be shown to have similar theoretical properties.

4

In terms of optimization, CV searches over a preﬁxed subset of Λ in a discrete manner, while UD
leverages the local information of the optimization landscape (i.e., gradient). Therefore, UD is more
likely to achieve a lower empirical risk on the validation data with the same T . In the following, we
will discuss the two algorithms from the perspective of generalization.

4 Main Results

We present the main results below for clarity, and the readers can refer to Appendix A for all proofs.

4.1 Stability and Generalization of UD

In most of the recent HO applications [11, 25, 15, 34] that we are aware of, stochastic gradient
descent (SGD) is adopted for its scalability and efﬁciency. Therefore, we present the main results on
UD with SGD in the outer level here.10

Recall that a randomized HO algorithm returns a random variable with the outcome space Λ × Θ in
general. To establish the generalization bound, we deﬁne the following notion of uniform stability on
validation in expectation.
Deﬁnition 1. A randomized HO algorithm A is β-uniformly stable on validation in expectation if for
(cid:48)val differ in at most one sample, we have
all validation datasets Sval, S

(cid:48)val ∈ Z m such that Sval, S

∀Str ∈ Z n, ∀z ∈ Z, EA

(cid:104)

(cid:96)(A(Str, Sval), z) − (cid:96)(A(Str, S

(cid:105)

(cid:48)val), z)

≤ β.

Compared to existing work [16], Deﬁnition 1 considers the expected inﬂuence of changing one
validation sample on a randomized HO algorithm. The reasons are two-folded. On the one hand, in
HO, the distribution of the testing samples is assumed to be the same as that of the validation ones
but can differ from that of the training ones [34], making it necessary to consider the bounds on the
validation set. On the other hand, classical results in CV suggest that such bounds are usually tighter
than the ones on the training set [32].

If a randomized HO algorithm is β-uniformly stable on validation in expectation, then we have the
following generalization bound.
Theorem 1 (Generalization bound of a uniformly stable algorithm). Suppose a randomized HO
algorithm A is β-uniformly stable on validation in expectation, then
(cid:104)

(cid:105)

R(A(Str, Sval), Dval) − ˆRval(A(Str, Sval), Sval)

| ≤ β.

|EA,Str∼(Dtr)n,Sval∼(Dval)m

Note that this is an expectation bound for any randomized HO algorithm with uniform stability. We
now analyze the stability, namely bounding β, for UD with SGD in the outer level. Indeed, we
consider a general family of algorithms that solve Eq. (4) via SGD in the outer level without any
restriction on the inner level optimization in the following Theorem 2.
Theorem 2 (Uniform stability of algorithms with SGD in the outer level). Suppose ˆθ is a random
function in a function space Gˆθ and ∀Str ∈ Z n, ∀z ∈ Z, ∀g ∈ Gˆθ, (cid:96)(λ, g(λ, Str), z) as a function
of λ is L-Lipschitz continuous and γ-Lipschitz smooth, let c ≤ s((cid:96))
2L2 and κ = c((1−1/m)γ)
c((1−1/m)γ)+1 . Then,
solving Eq. (4) with T steps SGD and learning rate αt ≤ c
t in the outer level is β-uniformly stable
on validation in expectation with

β =

2cL2
m

(cid:18) 1
κ

(cid:18)(cid:18) T s((cid:96))
2cL2

(cid:19)κ

(cid:19)

(cid:19)

− 1

+ 1

,

which is increasing w.r.t. L and γ. (Recall that s((cid:96)) = b − a is the range of the loss.)

Theorem 2 doesn’t assume a speciﬁc form of ˆθ. Indeed, UD instantiates ˆθ as the output of SGD or
GD in the inner level. For SGD, the corresponding Gˆθ is formed by iterating over all possible random
indexes and initializations in ˆθ. For GD, the corresponding Gˆθ is only formed by iterating over all
10See Appendix D for the results on UD with GD in the outer level. The generalization gap of UD with GD in
the outer level has an exponential dependence on T and K, and has the same 1/m dependence on m as SGD.

5

possible initializations in ˆθ, since GD uses full batches. We analyze the constants L and γ appearing
in β of UD, which solves the inner level problem by either SGD or GD, given the following mild
assumptions on the outer loss (cid:96) and the inner loss ϕi.
Assumption 1. Λ and Θ are compact and convex with non-empty interiors, and Z is compact.
Assumption 2. (cid:96)(λ, θ, z) ∈ C 2(Ω), where Ω is an open set including Λ × Θ × Z (i.e., (cid:96) is second
order continuously differentiable on Ω).
Assumption 3. ϕi(λ, θ, z) ∈ C 3(Ω), where Ω is an open set including Λ × Θ × Z (i.e., ϕi is third
order continuously differentiable on Ω).
Assumption 4. ϕi(λ, θ, z) is γϕ-Lipschitz smooth as a function of θ for all 1 ≤ i ≤ n, z ∈ Z and
λ ∈ Λ (Assumption 1 and Assumption 3 imply such a constant γϕ exists).
Theorem 3. Suppose Assumption 1,2,3,4 hold and the inner level problem is solved with K steps
SGD or GD with learning rate η, then ∀Str ∈ Z n, ∀z ∈ Z, ∀g ∈ Gˆθ, (cid:96)(λ, g(λ, Str), z) as a function
of λ is L = O((1 + ηγϕ)K) Lipschitz continuous and γ = O((1 + ηγϕ)2K) Lipschitz smooth.

Remark: Generally, a neural network composed of smooth operators satisfy all assumptions. We
notice that the continuously differentiable assumption in Assumption 2 and Assumption 3 does not
hold for ReLU. However, we argue that there are many smooth approximations of ReLU including
Softplus, Gelu [17], and Lipswish [6], which satisfy the assumption and achieve promising results in
classiﬁcation and deep generative modeling.

m ) or ˜O( (1+ηγϕ)2K

Combining the results in Theorem 1, Theorem 2 and Theorem 3, we obtain an expectation bound
of UD that depends on the number of steps in the outer level T , the number of steps in the inner
level K and the validation sample size m. Roughly speaking, its generalization gap has an order of
˜O( T κ
). In Appendix B, we construct a worst case where the Lipschitz constant L
in Theorem 3 increases at least exponentially w.r.t. K. According to Theorem 2, the stability bound
also increases exponentially w.r.t. K in the worst case. Besides, if we further assume the inner loss
ϕi is convex or strongly convex, we can derive tighter generalization gaps. Indeed, the dependence
on K of the generalization gap is O(K 2) in the convex case and O(1) in the strongly convex case.
Please see Appendix C for a complete proof.

m

Our results can explain some mysterious behaviours of the UD algorithms in practice [11]. According
to Theorem 2 and Theorem 3, very large values of K and T will signiﬁcantly decrease the stability
of UD (i.e., increasing β), which suggests a high risk of overﬁtting. On the other hand, if we use
very small T and K, the empirical risk on the validation data might be insufﬁciently optimized,
probably leading to underﬁtting. This trade-off on the values of K and T has been observed in
previous theoretical work [11], which mainly focuses on optimization and does not provide a formal
explanation. We also conﬁrmed this phenomenon in two different experiments (See results in
Section 6.2).
As for the number of validation data m, the generalization gap has an order of O( 1
satisfactory compared to that of CV as presented in Section 4.2.

m ), which is

4.2 Comparison with CV

CV is a classical approach for HO with theoretical guarantees, which serves as a natural baseline of
our results on UD in Theorem 1, Theorem 2 and Theorem 3. However, existing results on CV (see
Theorem 4.4 in [32]) are in the form of high probability bounds, which are not directly comparable
to ours. To compare under the same theoretical framework to obtain meaningful conclusions, we
present an expectation bound for CV as follows.
Theorem 4 (Expectation bound of CV). Suppose Str ∼ (Dtr)n, Sval ∼ (Dval)m and Str and Sval
are independent, and let Acv(Str, Sval) denote the results of CV as shown in Algorithm 2, then

(cid:104)
R(Acv(Str, Sval), Dval) − ˆRval(Acv(Str, Sval), Sval)

(cid:105)

|E

(cid:114)

| ≤ s((cid:96))

log T
2m

.

Technically, the expectation bound of CV is proved via the property of the maximum of a set of
subgaussian random variables (see Theorem 1.14 in [35]), which is distinct from the union bound
used in the high probability bound of CV (see Theorem 4.4 in [32]). In Appendix G.2, we also verify

(cid:113) 1

the O(

m ) dependence of the expectation bound empirically.

6

On one hand, we note that the growth of the generalization gap w.r.t. T is logarithmic in Theorem 4,
which is much slower than that of our results on UD. Besides, it does not explicitly depend on K.
Therefore, sharing the same large values of K and T , CV has a much lower risk of overﬁtting than
UD. On the other hand, the dependence of Theorem 4 on m is O(
m ), which is worse than that of
UD. Furthermore, as discussed in Section 3, probably UD has a much lower validation risk than CV
via exploiting the gradient information of the optimization landscape. Indeed, we show that CV with
random search suffers from the curse of dimensionality (See Theorem 7 in Appendix E). Namely,
CV requires exponentially large T w.r.t. the dimensionality of the λ to achieve a reasonably low
empirical risk. The above analysis may explain the superior performance of UD [25], especially when
we have a reasonable choice of T and K, a sufﬁciently large m and a sufﬁciently high-dimensional
hyperparameter space.

(cid:113) 1

4.3 The Regularized UD Algorithm

Building upon the above theoretical results and analysis, we further investigate how to improve the
stability of the UD algorithm via adding regularization. Besides the commonly used weight decay
term on the parameter in the inner level, we also employ a similar one on the hyperparameter in the
outer level. Formally, the regularized bilevel programming problem is given by:

λ∗(Str, Sval) = arg min

ˆRval(λ, θ∗(λ, Str), Sval) +

(cid:124)

λ∈Λ

(cid:123)(cid:122)
Regularized outer level optimization

µ
2

||λ||2
2

,

(cid:125)

where θ∗(λ, Str) = arg min

ˆRtr(λ, θ, Str) +

θ∈Θ

(cid:124)

(cid:123)(cid:122)
Regularized inner level optimization

ν
2

||θ||2
2

,

(cid:125)

(6)

where µ and ν are coefﬁcients of the regularization terms. Similar to Algorithm 1, we use UD to
approximate Eq. (6). We formally analyze the effect of the regularization terms in both levels (See
Theorem 2 and Theorem 3 in Appendix A). In summary, both regularization terms can increase the
stability of the UD algorithm, namely, decreasing β in a certain way. In particular, the regularization
in the outer level decreases κ in Theorem 2 while the regularization in the inner level decreases L
and γ in Theorem 3. Therefore, we can probably obtain a better generalization guarantee by adding
regularization in both levels, assuming that the terms do not hurt the validation risk too much.

5 Related work

Hyperparamter optimization. In addition to UD [10–12, 30, 36] and CV [3, 32] analyzed in
this work, there are alternative approaches that can solve the bilevel programming problem in
HO approximately, including implicit gradient [2, 33, 27, 28], Bayesian optimization [40, 22] and
hypernetworks [26, 29]. Implicit gradient methods directly estimate the gradient of the outer level
problem in Eq. (3), which generally involves an iterative procedure such as conjugate gradient [33]
and Neumann approximation [27] to estimate the inverse of a Hessian matrix. [28] also shows that
the identity matrix can be a good approximation of the Hessian matrix. Bayesian optimization [40,
22] views the outer level problem of Eq. (4) as a black-box function sampled from a Gaussian
process (GP) and updates the posterior of the GP as Eq. (4) is evaluated for new hyperparameters.
Hypernetworks [26, 29] learn a proxy function that outputs an approximately optimal hypothesis
given a hyperparameter. Besides, UD [10–12, 30, 36] also has variants which are more time and
memory efﬁcient [36, 12]. [36] proposes a faster version of UD by truncating the differentiation w.r.t.
the hyperparameter, while [12] proposes a memory-saving version of UD by approximating the trace
of inner level optimization in a linear interpolation scheme.

To our knowledge, the analysis of HO in previous work mainly focuses on convergence [11, 36] and
unrolling bias [41] from the optimization perspective. The generalization analysis is largely open,
which can be dealt with our stability framework in principle. In fact, in addition to UD [10, 11]
analyzed in this work, Theorem 1 is also a potential tool for HO algorithms discussed above [2, 33,
27, 28, 40, 22, 26, 29, 12, 30, 36], upon which future work can be built.

Bilevel Programming. The bilevel programming is extensively studied from the perspective of
optimization. [13, 14, 21] analyze the convergence rate of different approximation methods (e.g., UD

7

and implicit gradient) when the inner level problem is strongly convex. [20] analyze the complexity
lower bounds of bilevel programming when the lower level problem is strongly convex. [19] analyze
the convergence rate of bilevel programming under the setting of meta-learning.

Stability. The traditional stability theory [4] builds generalization bound of a learning algorithm
by studying its stability w.r.t. changing one data point in the training set. It has various extensions.
[8, 16] extend the stability for randomized algorithms. [8] focuses on the random perturbations of
the cost function. [16] focuses on the randomness in SGD and provides generalization bounds in
expectation. Our bound for UD solved by SGD is also in expectation. In such cases, as claimed
by [16], “high probability bounds are hard to obtain by the fact that SGD is itself randomized, and
thus a concentration inequality must be devised to account for both the randomness in the data and
in the training algorithm”. The stability notion is also extended to meta-learning [31, 5], where the
stability is analyzed w.r.t. changing one meta-dataset to bound the transfer risk on unseen tasks. This
work differs from previous extensions in that it considers the stability w.r.t. changing one data point
in the validation set, which leads to a generalization bound w.r.t. empirical risk on the validation set.

6 Experiments

We conduct experiments to validate our theoretical ﬁndings, which are three-folded as follows:

1. We reproduce the mysterious behaviours of UD (e.g., overﬁtting to the validation data)

observed in [11] and attempt to explain them via Theorem 2 and Theorem 3.

2. We empirically compare UD and CV and analyze their performance from the perspective of

the expectation bounds in Theorem 2 and Theorem 4.

3. We show the promise of the regularization terms in both levels to validate Theorem 2 and

Theorem 3 in Appendix A.

6.1 Experimental settings

In our experiments, we consider two widely used tasks, feature learning [11] and data reweighting
for noisy labels [36]. Please refer to Section 2 for the formulation of the two tasks.

In feature learning, we evaluate all algorithms on the Omniglot dataset [23] following [11]. Omniglot
consists of grayscale images, and we resize them to 28 × 28. The images are symbols from different
alphabets. We randomly select 100 classes and obtain a training, validation and testing set of size
500, 100, and 1000 respectively. λ represents the parameters in a linear layer of size 784 → 256
following the input x. θ represents the parameters in a MLP of size 256 → 128 → 100 to predict the
label y. We employ a mini-batch version of SGD in both levels of UD with a learning rate 0.1 and
batch size 50.

In data reweighting, we evaluate all algorithms on the MNIST dataset [24] following [36]. MNIST
consists of grayscale hand-written digits of size 28 × 28. We randomly select 2000, 200, and 1000
images for training, validation and testing respectively. The label of a training sample is replaced by
a uniformly sampled wrong label with probability 0.5. λ represents the logits of the weights of the
training data, and θ represents the parameters in an MLP of size 784 → 256 → 10. We employ a
mini-batch version of SGD in both levels of UD with a batch size 100. The learning rate is 10 in the
outer level and 0.3 in the inner level.

By default, CV shares the same inner loop as UD for a fair comparison in both settings. We tune
learning rates and coefﬁcients of the weight decay on another randomly selected set of the same size
as the validation set. To eliminate randomness, we average over 5 different runs and report the mean
results with the standard deviations in all experiments. Each experiment takes at most 10 hours in
one GeForce GTX 1080 Ti GPU.

6.2 Trade-off on the values of T and K in UD

Figure 1 presents the results of UD in the feature learning (FL) and data reweighting (DR) tasks, with
different values of K and T . On the one hand, in both tasks, UD with a large K (e.g., 256 in FL)
and a large T overﬁts the validation data. Namely, the validation loss continues decreasing while
the testing loss increases. On the other hand, a small value of K (e.g., 1 in FL) and T will result in

8

(a) Validation loss (FL)

(b) Testing loss (FL)

(c) Validation loss (DR)

(d) Testing loss (DR)

Figure 1: Results of UD in feature learning (FL) and data reweighting (DR). In both settings,
the performance of UD is sensitive to the values of K and T . We plot the generalization gap in
Appendix G.1.

underﬁtting to the validation data. The trade-off on the values of T and K agree with our analysis in
Theorem 2 and Theorem 3.

We also try a smaller learning rate in the inner level and get a similar overﬁtting phenomenon of
UD (see results in Appendix G.3). In such a case, we use a larger K. For instance, a learning rate
of η = 0.1 requires K = 1024 inner iterations to overﬁt. This can be explained by our Theorem 3,
which implies that a smaller η requires a larger K to make the generalization gap unchanged.

6.3 Comparison between CV and UD

(a) Validation loss (FL)

(b) Testing loss (FL)

(c) Validation loss (DR)

(d) Testing loss (DR)

Figure 2: Results of CV in feature learning (FL) and data reweighting (DR). We do not observe the
overﬁtting phenomenon of CV in all settings. We plot the generalization gap in Appendix G.1.

Figure 2 presents the results of CV in the feature learning (FL) and data reweighting (DR) tasks.
First, unlike UD, we do not observe the overﬁtting phenomenon when using a large T and K. This
corroborates our analysis in Theorem 4, which claims that the generalization gap of CV grows
logarithmically w.r.t. T and does not explicitly depends on K. Second, we note that the validation
loss of CV is clearly higher than that of UD in Figure 1, which may explain the relatively worse
testing loss of CV. Lastly, in FL, the validation loss of CV does not decrease clearly using up to
10,000 hyperparameters. This is because the dimensionality of the hyperparameter is around 200,000,
which is too large for CV to optimize, as suggested in Theorem 7 in Appendix E.

We also compare between CV and UD with a smaller number of hyperparameters (see results in
Appendix G.4). In this case, the validation losses of UD and CV are comparable and both algorithms
ﬁt well on the validation data. However, UD overﬁts much severely, leading to a worse testing loss
than CV. The results agree with our theory.

6.4 Effects of Regularization in Both Levels of UD

Figure 3 presents the results of the regularized UD algorithms where the weight decay term is added
in the outer (referred to as Reg-outer) or the inner level (referred to as Reg-inner). We observe that in
most of the cases, both Reg-outer and Reg-inner can individually relieve the overﬁtting problems
of UD. Further, within a range, the larger the coefﬁcients of the weight decay terms, the better the
results. Such behaviours conﬁrm our Theorem 2 and Theorem 3 in Appendix A. We note that if the
regularization is too heavy, for instance, ν = 0.1 in Panel (b) Figure 3, the optimization might be
unstable. This suggests another trade-off on determining the values of µ and ν.

9

(a) Reg-outer (FL)

(b) Reg-inner (FL)

(c) Reg-outer (DR)

(d) Reg-inner (DR)

Figure 3: Testing loss of the regularized UD algorithms in feature learning (FL) and data reweighting
(DR). Reg-outer and Reg-inner refer to adding regularization individually in the outer and inner levels
of UD respectively. We set K = 256 in FL and K = 512 in DR. In most of the cases, both Reg-outer
and Reg-inner can relieve overﬁtting in UD but overall there is no clear winner.

(a) Reg-both (FL)

(b) Reg-both (FL)

(c) Reg-both (DR)

(d) Reg-both (DR)

Figure 4: Testing loss of the regularized UD algorithms in feature learning (FL) and data reweighting
(DR). Reg-both refers to adding regularization in both the outer and inner levels of UD. We set
K = 256 in FL and K = 512 in DR. In most of the cases, Reg-both is slightly worse than the winner
of Reg-outer and Reg-inner.

According to Figure 3, considering the two tasks together, there is no clear winner of Reg-inner and
Reg-outer overall. In fact, our Theorem 2 and Theorem 3 in Appendix A show that they inﬂuence β
in incomparable ways. We further apply the two weight decay terms at the same time (referred to as
Reg-both), and the results are demonstrated in Figure 4. We ﬁnd that Reg-both is slightly worse than
the winner of Reg-outer and Reg-inner. We hypothesize that if one of the weight decay terms can
successfully relieve the overﬁtting problem, then adding another may hurt the ﬁnal generalization
performance because of a higher validation loss. A deeper analysis of the issue is left as future work.

7 Conclusion

The paper attempts to understand the generalization behaviour of approximate algorithms to solve
the bilevel programming problem in hyperparameter optimization. In particular, we establish an
expectation bound for the unrolled differentiation algorithm based on a notion of uniform stability
on validation. Our results can explain some mysterious behaviours of the bilevel programming in
practice, for instance, overﬁtting to the validation set. We also present an expectation bound of the
classical cross-validation algorithm. Our results suggest that unrolled differentiation algorithms can
be better than cross-validation in a theoretical perspective under certain conditions. Furthermore, we
prove that regularization terms in both the outer and inner levels can relieve the overﬁtting problem
in the unrolled differentiation algorithm. In experiments on feature learning and data reweighting for
noisy labels, we corroborate our theoretical ﬁndings.

As an early theoretical work in this area, we ﬁnd some interesting problems unsolved in this paper,
which may inspire future work. First, we do not consider the implicit gradient algorithm, which is
an alternative approach to unrolled differentiation and can be analyzed in the stability framework in
principle. Second, the comparison between the weight decay terms in different levels is not clear yet.
Third, in Theorem 2, we assume the learning rate in the outer level is O( 1
t ) as in [16], which is a gap
between our analysis and the practice.

10

Acknowledgements

We thank Yuhao Zhou for valuable feedback on our work. This work was supported by NSFC
Projects (Nos. 61620106010, 62061136001, 61621136008, U1811461, 62076145), Beijing NSF
Project (No. JQ19016), Tsinghua-Bosch Joint Center for Machine Learning, Beijing Academy of
Artiﬁcial Intelligence (BAAI), a grant from Tsinghua Institute for Guo Qiang, Tiangong Institute for
Intelligent Computing, and the NVIDIA NVAIL Program with GPU/DGX Acceleration.

References

[1] Ramy E Ali, Jinhyun So, and A Salman Avestimehr. On polynomial approximations for
privacy-preserving and veriﬁable relu networks. arXiv preprint arXiv:2011.05530, 2020.

[2] Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural computation,

12(8):1889–1900, 2000.

[3] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal

of machine learning research, 13(2), 2012.

[4] Olivier Bousquet and André Elisseeff. Stability and generalization. The Journal of Machine

Learning Research, 2:499–526, 2002.

[5] Jiaxin Chen, Xiao-Ming Wu, Yanke Li, Qimai Li, Li-Ming Zhan, and Fu-lai Chung. A
closer look at the training strategy for modern meta-learning. Advances in Neural Information
Processing Systems, 33, 2020.

[6] Ricky TQ Chen, Jens Behrmann, David Duvenaud, and Jörn-Henrik Jacobsen. Residual ﬂows

for invertible generative modeling. arXiv preprint arXiv:1906.02735, 2019.

[7] Benoît Colson, Patrice Marcotte, and Gilles Savard. An overview of bilevel optimization.

Annals of operations research, 153(1):235–256, 2007.

[8] Andre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil, and Leslie Pack Kaelbing. Stability
of randomized learning algorithms. Journal of Machine Learning Research, 6(1), 2005.

[9] Thomas Elsken, Jan Hendrik Metzen, Frank Hutter, et al. Neural architecture search: A survey.

J. Mach. Learn. Res., 20(55):1–21, 2019.

[10] Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and
reverse gradient-based hyperparameter optimization. In International Conference on Machine
Learning, pages 1165–1173. PMLR, 2017.

[11] Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil.
Bilevel programming for hyperparameter optimization and meta-learning. In International
Conference on Machine Learning, pages 1568–1577. PMLR, 2018.

[12] Jie Fu, Hongyin Luo, Jiashi Feng, Kian Hsiang Low, and Tat-Seng Chua. Drmad: distilling
reverse-mode automatic differentiation for optimizing hyperparameters of deep neural networks.
arXiv preprint arXiv:1601.00917, 2016.

[13] Saeed Ghadimi and Mengdi Wang. Approximation methods for bilevel programming. arXiv

preprint arXiv:1802.02246, 2018.

[14] Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo. On the iteration
complexity of hypergradient computation. In International Conference on Machine Learning,
pages 3748–3758. PMLR, 2020.

[15] Lan-Zhe Guo, Zhen-Yu Zhang, Yuan Jiang, Yu-Feng Li, and Zhi-Hua Zhou. Safe deep semi-
supervised learning for unseen-class unlabeled data. In International Conference on Machine
Learning, pages 3897–3906. PMLR, 2020.

[16] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of
stochastic gradient descent. In International Conference on Machine Learning, pages 1225–
1234. PMLR, 2016.

11

[17] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint

arXiv:1606.08415, 2016.

[18] Frank Hutter, Jörg Lücke, and Lars Schmidt-Thieme. Beyond manual tuning of hyperparameters.

KI-Künstliche Intelligenz, 29(4):329–337, 2015.

[19] Kaiyi Ji, Jason D Lee, Yingbin Liang, and H Vincent Poor. Convergence of meta-learning with
task-speciﬁc adaptation over partial parameters. arXiv preprint arXiv:2006.09486, 2020.

[20] Kaiyi Ji and Yingbin Liang. Lower bounds and accelerated algorithms for bilevel optimization.

arXiv preprint arXiv:2102.03926, 2021.

[21] Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and
enhanced design. In International Conference on Machine Learning, pages 4882–4892. PMLR,
2021.

[22] Kirthevasan Kandasamy, Karun Raju Vysyaraju, Willie Neiswanger, Biswajit Paria, Christo-
pher R Collins, Jeff Schneider, Barnabas Poczos, and Eric P Xing. Tuning hyperparameters
without grad students: Scalable and robust bayesian optimisation with dragonﬂy. arXiv preprint
arXiv:1903.06694, 2019.

[23] Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept
learning through probabilistic program induction. Science, 350(6266):1332–1338, 2015.

[24] Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs

[Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.

[25] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search.

arXiv preprint arXiv:1806.09055, 2018.

[26] Jonathan Lorraine and David Duvenaud. Stochastic hyperparameter optimization through

hypernetworks. arXiv preprint arXiv:1802.09419, 2018.

[27] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters
by implicit differentiation. In International Conference on Artiﬁcial Intelligence and Statistics,
pages 1540–1552. PMLR, 2020.

[28] Jelena Luketina, Mathias Berglund, Klaus Greff, and Tapani Raiko. Scalable gradient-based
tuning of continuous regularization hyperparameters. In International conference on machine
learning, pages 2952–2960. PMLR, 2016.

[29] Matthew MacKay, Paul Vicol, Jon Lorraine, David Duvenaud, and Roger Grosse. Self-tuning
networks: Bilevel optimization of hyperparameters using structured best-response functions.
arXiv preprint arXiv:1903.03088, 2019.

[30] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter opti-
mization through reversible learning. In International conference on machine learning, pages
2113–2122. PMLR, 2015.

[31] Andreas Maurer and Tommi Jaakkola. Algorithmic stability and meta-learning. Journal of

Machine Learning Research, 6(6), 2005.

[32] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.

MIT press, 2018.

[33] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In International

conference on machine learning, pages 737–746. PMLR, 2016.

[34] Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples
for robust deep learning. In International Conference on Machine Learning, pages 4334–4343.
PMLR, 2018.

[35] Phillippe Rigollet and Jan-Christian Hütter. High dimensional statistics. Lecture notes for

course 18S997, 813:814, 2015.

12

[36] Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-
In The 22nd International Conference on Artiﬁcial

propagation for bilevel optimization.
Intelligence and Statistics, pages 1723–1732. PMLR, 2019.

[37] Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to

algorithms. Cambridge university press, 2014.

[38] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability,
stability and uniform convergence. The Journal of Machine Learning Research, 11:2635–2670,
2010.

[39] Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng.
arXiv preprint

Meta-weight-net: Learning an explicit mapping for sample weighting.
arXiv:1902.07379, 2019.

[40] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine

learning algorithms. arXiv preprint arXiv:1206.2944, 2012.

[41] Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. Understanding short-horizon bias in

stochastic meta-optimization. arXiv preprint arXiv:1803.02021, 2018.

A Proofs of Main Theoretical Results

A.1 Proof of Theorem 1

Theorem 1 (Generalization bound of a uniformly stable algorithm). Suppose a randomized HO
algorithm A is β-uniformly stable on validation in expectation, then
(cid:104)

(cid:105)

R(A(Str, Sval), Dval) − ˆRval(A(Str, Sval), Sval)

| ≤ β.

|EA,Str∼(Dtr)n,Sval∼(Dval)m

Proof.

|EA,Str,Sval[R(A(Str, Sval), Dval) − ˆRval(A(Str, Sval), Sval)]|
)(cid:3) |
) − (cid:96)(A(Str, Sval), zval
) − (cid:96)(A(Str, Sval), zval

(cid:2)(cid:96)(A(Str, Sval), z) − (cid:96)(A(Str, Sval), zval
(cid:2)(cid:96)(A(Str, z, zval
, · · · , zval
(cid:2)(cid:96)(A(Str, z, zval
, · · · , zval

=|EA,Str,Sval,z∼Dval
=|EA,Str,Sval,z∼Dval
≤EStr,Sval,z∼Dval|EA

m ), zval
1
m ), zval

2

2

1

1

1

1

)(cid:3) |
)(cid:3) | ≤ β,

where the last inequality is due to the deﬁnition of stability.

A.2 Proof of Theorem 2

Here we prove a more general version of Theorem 2 in the full paper by considering SGD with weight
decay in the outer level, i.e.,

λt+1 = (1 − αt+1µ)λt − αt+1∇λt(cid:96)(λt, ˆθ(λt, Str), zval
where αt is the learning rate, µ is the weight decay, j is randomly selected from {1, · · · , m} and ˆθ is
a random function. Theorem 2 in the full paper can be simply derived by letting µ = 0.
Theorem 2 (Uniform stability of algorithms with SGD in the outer level). Suppose ˆθ is a random
function in a function space Gˆθ and ∀Str ∈ Z n, ∀z ∈ Z, ∀g ∈ Gˆθ, (cid:96)(λ, g(λ, Str), z) as a function
of λ is L-Lipschitz continuous and γ-Lipschitz smooth, let c ≤ s((cid:96))
c , (1 − 1/m)γ) and
κ = c((1−1/m)γ−µ)
c((1−1/m)γ−µ)+1 . Then, solving Eq. (4) in the full paper with T steps SGD, learning rate
αt ≤ c
t and weight decay µ in the outer level is β-uniformly stable on validation in expectation with

2L2 , µ ≤ min( 1

(1)

),

j

β =

2cL2
m

(cid:18) 1
κ

(cid:18)(cid:18) T s((cid:96))
2cL2

(cid:19)κ

(cid:19)

(cid:19)

− 1

+ 1

,

which is increasing w.r.t. L, γ and decreasing w.r.t. µ.

13

Proof. Suppose Str ∈ Z n and z ∈ Z, let f (λ, g) = (cid:96)(λ, g(λ, Str), z), where we omit the depen-
dency on Str and z for simplicity, then f (λ, g) is as a function of λ is L-Lipschitz continuous and
(cid:48)val differ in at most one point, let {λt}t≥0 and {λ(cid:48)
γ-Lipschitz smooth. Suppose Sval and S
t}t≥0 be
(cid:48)val respectively. Then the output of the HO algorithm A with t
the trace of Eq. (1) with Sval and S
steps SGD in the outer level is

A(Str, Sval) = (λt, ˆθ(λt, Str)), A(Str, S

and

(cid:48)val) = (λ(cid:48)

t, ˆθ(λ(cid:48)

t, Str)),

(cid:96)(A(Str, Sval), z) = (cid:96)(λt, ˆθ(λt, Str), z) = f (λt, ˆθ),
t, ˆθ).
(cid:96)(A(Str, S

t, Str), z) = f (λ(cid:48)

(cid:48)val), z) = (cid:96)(λ(cid:48)

t, ˆθ(λ(cid:48)

Let δt = ||λt − λ(cid:48)

t||. Suppose 0 ≤ t0 ≤ t, we have

(cid:104)

E

|f (λt, ˆθ) − f (λ(cid:48)

(cid:105)
t, ˆθ)|

(cid:104)

=E

|f (λt, ˆθ) − f (λ(cid:48)

t, ˆθ)| · 1δt0 =0

(cid:105)

+ E

(cid:104)
|f (λt, ˆθ) − f (λ(cid:48)

(cid:105)

t, ˆθ)| · 1δt0 >0
(cid:3) + P (δt0 > 0)s((cid:96)).

≤LE (cid:2)δt · 1δt0 =0

Without loss of generality, we assume Sval and S
doesn’t selects the ﬁrst point for the ﬁrst t0 iterations, then δt0 = 0. As a result,

(cid:48)val at most differ in at the ﬁrst point. If SGD

P (δt0 = 0) ≥ (1 −

1
m

)t0 ≥ 1 −

t0
m

.

Therefore, P (δt0 > 0) ≤ t0

m and we have

(cid:104)

E

|f (λt, ˆθ) − f (λ(cid:48)

t, ˆθ)|

(cid:105)

≤ LE (cid:2)δt · 1δt0 =0

(cid:3) +

t0
m

s((cid:96)).

(2)

(cid:3). Let γ(cid:48) = (1 − 1/m)γ − µ and let j be the index selected by SGD at

Now we bound E (cid:2)δt · 1δt0 =0
the t + 1 iteration, then we have
E (cid:2)δt+1 · 1δt0 =0

(cid:3) + E (cid:2)δt+1 · 1j>1 · 1δt0 =0

(cid:3)

(cid:3) ≤E (cid:2)δt+1 · 1j=1 · 1δt0 =0
≤

1
m

+

(|1 − αt+1µ| · E[δt · 1δt0 =0] + 2αt+1L)
m − 1
m

(|1 − αt+1µ| + αt+1γ)E[δt · 1δt0 =0]

=(1 + αt+1γ(cid:48))E[δt · 1δt0 =0] +

≤ exp(αt+1γ(cid:48))E[δt · 1δt0 =0] +

≤ exp(

c
t + 1

γ(cid:48))E[δt · 1δt0 =0] +

2αt+1L
m
2αt+1L
m
2cL
(t + 1)m

.

As a result,

E[δt · 1δt0 =0] ≤

t
(cid:88)

j=t0+1

t
(cid:88)

j=t0+1
2cLtcγ(cid:48)
m

≤

=

2cL
jm

2cL
jm

t
(cid:89)

k=j+1

exp(

cγ(cid:48)
k

) =

t
(cid:88)

j=t0+1

2cL
jm

exp(cγ(cid:48)

t
(cid:88)

k=j+1

1
k

)

exp(cγ(cid:48) ln

t
j

) =

t
(cid:88)

2cL
jm

(cid:19)cγ(cid:48)

(cid:18) t
j

t
(cid:88)

(cid:19)1+cγ(cid:48)

≤

(cid:18) 1
j

j=t0+1
2cLtcγ(cid:48)
m

t−cγ(cid:48)

− t−cγ(cid:48)
0
−cγ(cid:48)

j=t0+1
(cid:19)cγ(cid:48)

=

2L
mγ(cid:48)

(cid:32)(cid:18) t
t0

(cid:33)

− 1

.

14

Combining with Eq. (2), we have

(cid:104)
|f (λT , ˆθ) − f (λ(cid:48)

(cid:105)
T , ˆθ)|

E

≤ inf

0≤t0≤T

2L2
mγ(cid:48)

(cid:32)(cid:18) T
t0

(cid:19)cγ(cid:48)

(cid:33)

− 1

+

t0
m

s((cid:96)).

(3)

The right hand side is approximately minimized when

t0 = (

2cL2
s((cid:96))

1
cγ(cid:48)+1 T

)

cγ(cid:48)
cγ(cid:48)+1 ≤ T,

which gives

(cid:104)

E

|f (λT , ˆθ) − f (λ(cid:48)

T , ˆθ)|

(cid:105)

≤

1 + 1/cγ(cid:48)
m

(2cL2)

1
cγ(cid:48)+1 T

cγ(cid:48)
cγ(cid:48)+1 (s((cid:96)))

cγ(cid:48)
cγ(cid:48)+1 −

2L2
mγ(cid:48) =: β.

Let κ = cγ(cid:48)

cγ(cid:48)+1 = c((1−1/m)γ−µ)

c((1−1/m)γ−µ)+1 , then β can be written as

β =

2cL2
m

(cid:18) 1
κ

(cid:18)(cid:18) T s((cid:96))
2cL2

(cid:19)κ

(cid:19)

(cid:19)

− 1

+ 1

.

Since the r.h.s. of Eq. (3) is increasing w.r.t. L and γ(cid:48), where γ(cid:48) is further increasing w.r.t. γ and
decreasing w.r.t. µ, we can conclude β is increasing w.r.t. L, γ and decreasing w.r.t. µ.

A.3 Proof of Theorem 3

Deﬁnition 1. (Lipschitz continuous) Suppose (X, dX ), (Y, dY ) are two metric spaces and f : X →
Y . We deﬁne f is L Lipschitz continuous iff ∀a, b ∈ X, dY (f (a), f (b)) ≤ LdX (a, b).
Deﬁnition 2. (Lipschitz smooth) Suppose X, Y are subsets of two real normed vector spaces and
f : X → Y is differentiable. We deﬁne f is γ Lipschitz smooth iff f (cid:48) is γ Lipschitz continuous.
Deﬁnition 3. (Lipschitz norm) Suppose (X, dX ), (Y, dY ) are two metric spaces, f : X → Y , we
deﬁne ||f ||Lip (cid:44) inf{L ∈ [0, ∞] : ∀a, b ∈ X, dY (f (a), f (b)) ≤ LdX (a, b)}, i.e., the minimum L
such that f is L Lipschitz continuous.
Deﬁnition 4. Given a function f (λ, θ), we use ||f (λ, θ)||λ∈Λ,Lip and ||f (λ, θ)||θ∈Θ,Lip to explicitly
denote the Lipschitz norm of f w.r.t. λ ∈ Λ and θ ∈ Θ respectively.
Deﬁnition 5. (Vector norm) Suppose a ∈ Rm, we use ||a|| to denote the l2 norm of a.
Deﬁnition 6. (Matrix norm) Suppose A ∈ Rm×n, we deﬁne ||A|| (cid:44) sup

||Aa||
||a|| , i.e., the norm

0(cid:54)=a∈Rm

of the linear operator induced by A.
Lemma 1. Suppose X, Y are two real normed vector spaces, Ω is an open set of X, f : Ω → Y
is continuously differentiable, S ⊂ Ω is convex and has non-empty interior, then ||f |S||Lip =
sup
c∈S

||f (cid:48)(c)||.

Proof. Suppose a, b ∈ S, according to the mean value theorem, there is a c lies in the segment
determined by a and b, s.t., ||f (b) − f (a)|| ≤ ||f (cid:48)(c)(b − a)||. Furthermore, we have

||f (cid:48)(c)(b − a)|| ≤ ||f (cid:48)(c)|| · ||b − a|| ≤ sup
c∈S

||f (cid:48)(c)|| · ||b − a||.

Thereby, f |S is sup
c∈S

||f (cid:48)(c)|| Lipschitz continuous and ||f |S||Lip ≤ sup
c∈S

||f (cid:48)(c)||.

Suppose c ∈ S◦, where S◦ is the interior of S and u ∈ X with ||u|| = 1, then

Thereby,

lim
(cid:15)→0

f (c + (cid:15)u) − f (c)
(cid:15)

= f (cid:48)(c)u.

||f |S||Lip ≥ lim
(cid:15)→0

||

f (c + (cid:15)u) − f (c)
(cid:15)

|| = ||f (cid:48)(c)u||.

15

Since u is arbitrary, we have ||f (cid:48)(c)|| =

sup
u∈X,||u||=1

||f (cid:48)(c)u|| ≤ ||f |S||Lip.

Since S has non-empty interior, we have S ⊂ S◦ by the property of convex sets. Suppose c ∈ S, then
c ∈ S◦ and there is a sequence cn ∈ S◦, s.t., cn → c. Since cn ∈ S◦, we have ||f (cid:48)(cn)|| ≤ ||f |S||Lip.
Let n → ∞, by the continuity of f (cid:48), we have ||f (cid:48)(c)|| ≤ ||f |S||Lip. Since c ∈ S is arbitrary, we have
sup
c∈S

||f (cid:48)(c)|| ≤ ||f |S||Lip. Finally, we have sup
c∈S

||f (cid:48)(c)|| = ||f |S||Lip.

Lemma 2. Suppose Λ and Θ are convex and compact with non-empty interiors, Z is compact,
Λ × Θ × Z is included in an open set Ω and f (λ, θ, z) ∈ C k(Ω), then for all i ≤ k − 1 or-
||h(λ, θ, z)||λ∈Λ,Lip < ∞ and
der partial differential h(λ, θ, z) of f (λ, θ, z), we have

sup
θ∈Θ,z∈Z

sup
λ∈Λ,z∈Z

||h(λ, θ, z)||θ∈Θ,Lip < ∞.

Proof. Suppose h(λ, θ, z) is a i ≤ k − 1 order partial differential of f (λ, θ, z), then h(λ, θ, z) ∈
C 1(Ω) and ∇λh(λ, θ, z) ∈ C(Ω). Since Λ×Θ×Z is compact, ∇λh(λ, θ, z) is bounded in Λ×Θ×Z.
According to Lemma 1, we have

sup
θ∈Θ,z∈Z

||h(λ, θ, z)||λ∈Λ,Lip = sup

θ∈Θ,z∈Z

||∇λh(λ, θ, z)|| < ∞.

sup
λ∈Λ

Similarly, we can derive

sup
λ∈Λ,z∈Z

||h(λ, θ, z)||θ∈Θ,Lip < ∞.

Lemma 3. Suppose (1) ∀1 ≤ k ≤ K, ∀λ ∈ Λ, Gλ,k(θ) is a mapping from Θ to Θ, i.e., Gλ,k :
Θ → Θ, (2) ∀1 ≤ k ≤ K, ∀θ ∈ Θ, Gλ,k(θ) as a function of λ is LG
1 < ∞ Lipschitz continuous,
(3) ∀1 ≤ k ≤ K, ∀λ ∈ Λ, Gλ,k(θ) as a function of θ is LG
2 < ∞ Lipschitz continuous. Let
ˆθ(λ) = Gλ,K(Gλ,K−1(· · · (Gλ,1(θ0)))), then ˆθ(λ) is Lˆθ Lipschitz continuous with

(cid:40)

ˆθ =

L

2 )K −1
(LG
LG
2 −1

LG
1
KLG
1

LG
LG

2 (cid:54)= 1
2 = 1

.

Proof. We use θK(λ) to denote Gλ,K(Gλ,K−1(· · · (Gλ,1(θ0)))). Suppose λ, λ(cid:48) ∈ Λ and K ≥ 1, we
have

||θK(λ) − θK(λ(cid:48))|| = ||Gλ,K(θK−1(λ)) − Gλ(cid:48),K(θK−1(λ(cid:48))||

≤||Gλ,K(θK−1(λ)) − Gλ(cid:48),K(θK−1(λ))|| + ||Gλ(cid:48),K(θK−1(λ)) − Gλ(cid:48),K(θK−1(λ(cid:48)))||
≤LG

2 ||θK−1(λ) − θK−1(λ(cid:48))||.

1 ||λ − λ(cid:48)|| + LG

If LG

If LG

2 (cid:54)= 1, we have ||θK(λ) − θK(λ(cid:48))|| ≤ (LG
2 )K −1
2 −1 LG
LG
1 ||λ − λ(cid:48)||.
2 = 1, we have ||θK(λ) − θK(λ(cid:48))|| ≤ KLG

1 ||λ − λ(cid:48)||.

Lemma 4. Suppose (1) ∀1 ≤ k ≤ K, ∀λ ∈ Λ, Gλ,k(θ) is a mapping from Θ to Θ, i.e., Gλ,k : Θ →
Θ, (2) ∀1 ≤ k ≤ K, ∀θ ∈ Θ, Gλ,k(θ) and ∂
1 Lipschitz
continuous respectively, (3) ∀1 ≤ k ≤ K, ∀λ ∈ Λ, Gλ,k(θ) and ∂
∂θ Gλ,k(θ) as a function of θ is LG
2
and γG
∂θ Gλ,k(θ) as a function of λ
is γG
4 ≥ 0
Lipschitz continuous. Let ˆθ(λ) = Gλ,K(Gλ,K−1(· · · (Gλ,1(θ0)))), then ˆθ(λ) is γ ˆθ Lipschitz smooth
with

2 Lipschitz continuous respectively, (4) ∀1 ≤ k ≤ K, ∀θ ∈ Θ, ∂

3 ≥ 0 Lipschitz continuous, (5) ∀1 ≤ k ≤ K, ∀λ ∈ Λ, ∂

∂λ Gλ,k(θ) as a function of λ is LG

∂λ Gλ,k(θ) as a function of θ is γG

1 and γG

ˆθ =

γ






O((LG
O(K 3)
O(K)
O(1)

2 )2K) LG
LG
LG
LG

2 > 1
2 = 1, LG
2 = 1, LG
2 < 1

1 > 0
1 = 0

,

and γ ˆθ is determined by LG

1 , LG

2 , γG

1 , γG

2 , γG

3 , γG

4 , K.

16

Proof. Suppose 1 ≤ k ≤ K, we use θk(λ) to denote Gλ,k(Gλ,k−1(· · · (Gλ,1(θ0)))). According
LG
LG

Lipschitz continuous. Taking gradient to

to Lemma 3, θk(λ) is Lˆθ,k =

2 )k−1
(LG
LG
2 −1

(cid:40)

2 (cid:54)= 1
2 = 1

LG
1
kLG
1

θk(λ) w.r.t. λ, we have

∂
∂λ

θk(λ) =

∂
∂λ

Gλ,k(θk−1(λ)) =

(cid:20) ∂
∂λ

Gλ,k(θ)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=θk−1(λ)

+

(cid:20) ∂
∂θ

Gλ,k(θ)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=θk−1(λ)

(cid:20) ∂
∂λ

(cid:21)

θk−1(λ)

.

Taking the Lipschitz constant w.r.t. λ, we have

(cid:20) ∂
∂λ

||

(cid:20) ∂
∂θ

||

Gλ,k(θ)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=θk−1(λ)

Gλ,k(θ)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=θk−1(λ)

||λ,Lip ≤ γG

1 + γG

4 L

ˆθ,k−1,

||λ,Lip ≤ γG

3 + γG

2 L

ˆθ,k−1,

||

∂
∂λ

θk(λ)||λ,Lip ≤||

Gλ,k(θ)

(cid:20) ∂
∂λ
(cid:20) ∂
∂θ

+ ||

Gλ,k(θ)

||λ,Lip

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=θk−1(λ)
(cid:21) (cid:12)
(cid:12)
(cid:12)θ=θk−1(λ)
∂
∂λ

Gλ,k(θ)|| ||

∂
∂θ

+ sup

||

λ∈Λ,θ∈Θ

||λ,Lip sup
λ∈Λ

||

∂
∂λ

θk−1(λ)||

θk−1(λ)||λ,Lip

≤γG

1 + γG

4 L

ˆθ,k−1 + (γG

3 + γG

2 L

ˆθ,k−1)L

=γG

2 (L

ˆθ,k−1)2 + (γG

3 + γG

4 )L

ˆθ,k−1 + γG

ˆθ,k−1 + LG
2 ||
∂
∂λ

1 + LG
2 ||

∂
∂λ

θk−1(λ)||λ,Lip

θk−1(λ)||λ,Lip.

As for θ0, we have

||

∂
∂λ

θ0(λ)||λ,Lip = 0.

Let γ ˆθ be the Kth term of the sequence deﬁned by

ak = γG

ˆθ,k−1 + γG

1 + LG

ˆθ,k−1)2 + (γG

3 + γG

2 (L

4 )L

2 ak−1,

a0 = 0,
∂λ θK(λ)||λ,Lip ≤ γ ˆθ and ˆθ(λ) = θK(λ)
2 )K)
. If

2 > 1, then Lˆθ,K = O((LG
1 = 0
1 > 0

LG
O(K 3) LG

(cid:26) O(K)

which is determined by LG
2 , γG
is γ ˆθ Lipschitz smooth. Finally, we analyze the order of γ ˆθ. If LG
and γ ˆθ = O((LG

2 = 1, then Lˆθ,K = KLG

4 , then || ∂

1 , LG

2 , γG

3 , γG

1 , γG

1 + Lθ0 and γ ˆθ =

2 )2K). If LG
2 < 1, then Lˆθ,K = O(1) and γ ˆθ = O(1).
LG
Assumption 1. Λ and Θ are compact and convex with non-empty interiors, and Z is compact.
Assumption 2. (cid:96)(λ, θ, z) ∈ C 2(Ω), where Ω is an open set including Λ × Θ × Z (i.e., (cid:96) is second
order continuously differentiable on Ω).
Assumption 3. ϕi(λ, θ, z) ∈ C 3(Ω), where Ω is an open set including Λ × Θ × Z (i.e., ϕi is third
order continuously differentiable on Ω).
Assumption 4. ϕi(λ, θ, z) is γϕ-Lipschitz smooth as a function of θ for all 1 ≤ i ≤ n, z ∈ Z and
λ ∈ Λ (Assumption 3 implies such a constant γϕ exists).

Here we prove a more general version of Theorem 3 in the full paper by considering SGD or GD with
weight decay ν in the inner level. Theorem 3 in the full paper can be simply derived by letting ν = 0.
Theorem 3. Suppose Assumption 1,2,3,4 hold and the inner level problem is solved with K steps SGD
or GD with learning rate η and weight decay ν, then ∀Str ∈ Z n, ∀z ∈ Z, ∀g ∈ Gˆθ, (cid:96)(λ, g(λ, Str), z)
as a function of λ is L = O((1 + η(γϕ − ν))K) Lipschitz continuous and γ = O((1 + η(γϕ − ν))2K)
Lipschitz smooth.

17

Proof. The kth updating step of SGD can be written as

Gλ,k(θ) = (1 − ην)θ − η∇θϕjk (λ, θ, ztr
jk

) = ∇θ

(cid:18) (1 − ην)
2

||θ||2 − ηϕjk (λ, θ, ztr
jk

(cid:19)
)

,

The output of K steps SGD is
where jk is randomly selected from {1, 2, · · · , n}.
ˆθ(λ, Str) = Gλ,K(Gλ,K−1(· · · (Gλ,1(θ0)))) and Gˆθ is formed by iterates over (j1, j2, · · · , jK) ∈
{1, 2, · · · , n}K.

According to Lemma 2 and Assumption 3, we have

LG
1

(cid:44) sup

k,jk,Str,θ

||Gλ,k(θ)||λ∈Λ,Lip = sup
i,z,θ

||∇θ

(cid:18) (1 − ην)
2

||θ||2 − ηϕi(λ, θ, z)

(cid:19)

||λ∈Λ,Lip < ∞.

Similarly, we have

γG
1

γG
3

(cid:44) sup

k,jk,Str,θ

(cid:44) sup

k,jk,Str,θ

||

∂
∂λ

||

∂
∂θ

Gλ,k(θ)||λ∈Λ,Lip < ∞, γG
2

Gλ,k(θ)||λ∈Λ,Lip < ∞, γG
4

(cid:44) sup

k,jk,Str,λ

(cid:44) sup

k,jk,Str,λ

||

∂
∂θ

||

∂
∂λ

Gλ,k(θ)||θ∈Θ,Lip < ∞,

Gλ,k(θ)||θ∈Θ,Lip < ∞.

According to Assumption 4, we have

sup
k,jk,Str,λ

||Gλ,k(θ)||θ∈Θ,Lip ≤ 1 − ην + ηγϕ = 1 + η(γϕ − ν) (cid:44) LG

2 < ∞.

2 )K −1
(LG
According to Lemma 3 and Lemma 4, ˆθ(λ, Str) is Lˆθ = LG
2 −1 Lipschitz continuous and
1
LG
2 )2K) Lipschitz smooth as a function of λ. By deﬁnition, Lˆθ and γ ˆθ are independent of
γ ˆθ = O((LG
the training dataset Str and the random indices (j1, j2, · · · , jK) and thereby the randomness of ˆθ.
According to Lemma 2 and Assumption 2, we have
||(cid:96)(λ, θ, z)||λ∈Λ,Lip < ∞, L(cid:96)

||(cid:96)(λ, θ, z)||θ∈Θ,Lip < ∞.

L(cid:96)

2 = sup

1 = sup

θ∈Θ,z∈Z

λ∈Λ,z∈Z

Similarly, we have

γ(cid:96)
1

(cid:44) sup
θ,z

||

γ(cid:96)
3

(cid:44) sup
θ,z

||

(cid:21)

(cid:96)(λ, θ, z)

(cid:21)

(cid:96)(λ, θ, z)

(cid:20) ∂
∂λ

(cid:20) ∂
∂θ

||λ∈Λ,Lip < ∞, γ(cid:96)
2

(cid:44) sup
λ,z

||

||λ∈Λ,Lip < ∞, γ(cid:96)
4

(cid:44) sup
λ,z

||

(cid:20) ∂
∂θ

(cid:20) ∂
∂λ

(cid:21)

(cid:96)(λ, θ, z)

||θ∈Θ,Lip < ∞,

(cid:21)

(cid:96)(λ, θ, z)

||θ∈Θ,Lip < ∞.

Suppose z ∈ Z, ﬁrstly we consider the Lipschitz continuity of (cid:96)(λ, ˆθ(λ, Str), z):

||(cid:96)(λ, ˆθ(λ, Str), z)||λ∈Λ,Lip

||(cid:96)(λ, θ, z)||λ∈Λ,Lip + sup

λ∈Λ,z∈Z

||(cid:96)(λ, θ, z)||θ∈Θ,Lip · ||ˆθ(λ, Str)||λ∈Λ,Lip

≤ sup

θ∈Θ,z∈Z
1 + L(cid:96)

≤L(cid:96)

2L

ˆθ (cid:44) L.

Then we consider the Lipschitz continuity of ∂

∂
∂λ

(cid:96)(λ, ˆθ(λ, Str), z) =

(cid:20) ∂
∂λ

(cid:96)(λ, θ, z)

∂λ (cid:96)(λ, ˆθ(λ, Str), z), which can be expanded as
(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)

(cid:20) ∂
∂λ

(cid:20) ∂
∂θ

(cid:96)(λ, θ, z)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)

+

ˆθ(λ, Str)

(4)

(cid:21)

.

Taking the Lipschitz norm w.r.t. λ, we have

(cid:20) ∂
∂λ

||

(cid:96)(λ, θ, z)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)

18

||λ∈Λ,Lip ≤ γ(cid:96)

1 + γ(cid:96)

4L

ˆθ,

(cid:20) ∂
∂θ

||

(cid:96)(λ, θ, z)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)

which yields

||λ∈Λ,Lip ≤ γ(cid:96)

3 + γ(cid:96)

2L

ˆθ,

≤||

||

(cid:96)(λ, θ, z)

∂
∂λ
(cid:20) ∂
∂λ
(cid:20) ∂
∂θ
1 + γ(cid:96)

(cid:96)(λ, ˆθ(λ, Str), z)||λ∈Λ,Lip
(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)
(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)
ˆθ)L

ˆθ + (γ(cid:96)

(cid:96)(λ, θ, z)

3 + γ(cid:96)

4L

2L

+ ||

≤γ(cid:96)

||λ∈Λ,Lip

||λ∈Λ,LipL

ˆθ + L(cid:96)
2||

∂
∂λ

ˆθ(λ, Str)||λ∈Λ,Lip

ˆθ + L(cid:96)
2γ

ˆθ (cid:44) γ.

(5)

With Eq. (4) and Eq. (5), we can conclude (cid:96)(λ, ˆθ(λ, Str), z) as a function of λ is L = O((1 + η(γϕ −
ν))K) Lipschitz continuous and γ = O((1 + η(γϕ − ν))2K) Lipschitz smooth. By deﬁnition, L and
γ are independent of the training dataset Str, z, the random indices (j1, j2, · · · , jK) and thereby the
randomness of ˆθ. Thereby, we have ∀Str ∈ Z n, ∀z ∈ Z, ∀g ∈ Gˆθ, (cid:96)(λ, g(λ, Str), z) as a function
of λ is L = O((1 + η(γϕ − ν))K) Lipschitz continuous and γ = O((1 + η(γϕ − ν))2K) Lipschitz
smooth. Similarly, the result also holds for GD.

A.4 Proof of Theorem 4

Theorem 4 (Expectation bound of CV). Suppose Str ∼ (Dtr)n, Sval ∼ (Dval)m and Str and Sval
are independent, and let Acv(Str, Sval) denote the results of CV as shown in Algorithm 2, then

(cid:104)
R(Acv(Str, Sval), Dval) − ˆRval(Acv(Str, Sval), Sval)

(cid:105)

|E

(cid:114)

| ≤ s((cid:96))

log T
2m

.

Proof. Let λt ∈ Λ be the tth hyperparameter, which is a random vector taking value on Λ, ˆθt be
the random function corresponding to the tth optimization in the inner level, then ˆθt(λt, Str) is the
output hypothesis given hyperparameter λt and training dataset Str. Let t∗ be the index of the best
hyperparameter, i.e.,

t∗ = arg min
1≤t≤T

ˆRval(λt, ˆθt(λt, Str), Sval),

then the output of CV is Acv(Str, Sval) = (λt∗ , ˆθt∗
Let Xt = R(λt, ˆθt(λt, Str), Dval) − ˆRval(λt, ˆθt(λt, Str), Sval), then we have

(λt∗ , Str)).

R(Acv(Str, Sval), Dval) − ˆRval(Acv(Str, Sval), Sval)
(λt∗ , Str), Dval) − ˆRval(λt∗ , ˆθt∗

=R(λt∗ , ˆθt∗

(λt∗ , Str), Sval) = Xt∗ .

By Hoeffding’s lemma, we have for any s > 0

EesXt =Eλt,ˆθt,Str ESval exp

(cid:32)

s
m

m
(cid:88)

R(λt, ˆθt(λt, Str), Dval) − (cid:96)(λt, ˆθt(λt, Str), zval
k )

(cid:33)

(cid:16)

R(λt, ˆθt(λt, Str), Dval) − (cid:96)(λt, ˆθt(λt, Str), zval
k )

(cid:17)(cid:17)

k=1
(cid:16) s
m

=Eλt,ˆθt,Str

m
(cid:89)

Ezval

k

exp

≤

m
(cid:89)

k=1

exp(

k=1
s2
m2

s((cid:96))2
8

) = exp(

s2
m

s((cid:96))2
8

).

19

Then we have

EXt∗ ≤E max
1≤t≤T

Xt =

1
s

=

1
s

log E max
1≤t≤T

E log exp(s max
1≤t≤T
1
s

exp(sXt) ≤

log

(cid:88)

1≤t≤T

E exp(sXt)

Xt) ≤

1
s

log E exp(s max
1≤t≤T

Xt)

≤

1
s
(cid:113) 8m log T
s((cid:96))2

log

(cid:18)

T exp(

s2
m

s((cid:96))2
8

(cid:19)
)

, we have EXt∗ ≤ s((cid:96))
(cid:113) log T
2m .

Taking s =

Finally, |EXt∗ | ≤ s((cid:96))

+

s · s((cid:96))2
8m

.

=

log T
s
(cid:113) log T

2m . Similarly, we have −EXt∗ ≤ s((cid:96))

(cid:113) log T
2m .

B Construct a Worst Case for Theorem 3

We construct a worst case where the Lipschitz constant L in Theorem 3 increases at least exponentially
w.r.t. K. It is a feature learning example with a small neural network. The model has one parameter
and one hyperparameter and uses squared activation function [1]. We use the squared loss. The
data distribution is any distribution in the support Z = {(x, y) : 1
2 ≤ x ≤ 1, 1 ≤ y ≤ 2}. The
parameter space and hyperparameter space are Θ = [0, 1] and Λ = [0, 1
4 ] respectively. Formally, the
loss function is (cid:96)(λ, θ, z) = (y − λ(θx)2)2. The inner loop is solved by SGD with a learning rate η.
We formalize the result in Proposition 1.
Proposition 1. Suppose (cid:96)(λ, θ, z) = (y − λ(θx)2)2, Λ = [0, 1
2 ≤ x ≤
1, 1 ≤ y ≤ 2} and the inner level problem is solved with K steps SGD with learning rate η, then
∀Str ∈ Z n, ∀z ∈ Z, ∀g ∈ Gˆθ, (cid:96)(λ, g(λ, Str), z) as a function of λ is at least L = Ω((1 + 3
16 η)K)
Lipschitz continuous.

4 ], Θ = [0, 1], Z = {(x, y) : 1

Proof. We use z = (x, y) ∈ Z to denote the data point used in one step of SGD, where we omit
the index of the data point for simplicity. Firstly, the gradient of the loss function is ∇θ(cid:96)(λ, θ, z) =
2(y − λ(θx)2)(−λx22θ) = −4(yλx2θ − λ2θ3x4) and one step SGD satisﬁes

θ − η∇θ(cid:96)(λ, θ, z) = θ + 4η(yλx2θ − λ2θ3x4) = (1 + 4ηyλx2)θ − 4ηλ2θ3x4

≥(1 + 4ηyλx2)θ − 4ηλ2θx4 = (1 + 4ηyλx2 − 4ηλ2x4)θ ≥ (1 + 3ηλx2)θ ≥ (1 +

3
4

ηλ)θ.

Let {ˆθk(λ)}k≥0 be the trajectory of SGD, then we have ˆθk(λ) ≥ (1 + 3
Taking gradient of ˆθk(λ) w.r.t. λ, we have
∇λ

ˆθk+1(λ) =4ηyx2 ˆθk(λ) + (1 + 4ηyλx2)∇λ
=4ηyx2 ˆθk(λ) + (1 + 4ηyλx2)∇λ
=4ηx2 ˆθk(λ)(y − 2x2λˆθk(λ)2) + (1 + 4ηyλx2 − 12ηx4λ2 ˆθk(λ)2)∇λ

ˆθk(λ) − 4ηx4(2λˆθk(λ)3 + λ23ˆθk(λ)2∇λ
ˆθk(λ) − 8ηx4λˆθk(λ)3 − 12ηx4λ2 ˆθk(λ)2∇λ

ˆθk(λ)

4 ηλ)kθ0.

ˆθk(λ))

ˆθk(λ)

As for the ﬁrst term, we have 4ηx2 ˆθk(λ)(y − 2x2λˆθk(λ)2) ≥ 2ηx2 ˆθk(λ) ≥ 0. As for the coefﬁcient
of the second term, we have 1 + 4ηyλx2 − 12ηx4λ2 ˆθk(λ)2 ≥ 1 + ηλx2 ≥ 1 + ηλ/4 ≥ 0. Besides,
∇λ

ˆθ1(λ) = 4ηyx2θ0 − 8ηλx4θ3

ˆθk(λ) ≥ 0 and furthermore

0 ≥ 2x2θ0η ≥ 1

2 θ0η. Thereby, ∇λ

∇λ

ˆθk+1(λ) ≥ (1 + ηλ/4)∇λ

ˆθk(λ) ≥ (1 + ηλ/4)k∇λ

ˆθ1(λ) ≥

1
2

(1 + ηλ/4)kθ0η.

Then, we consider (cid:96)(λ, ˆθK(λ), z) = (y − λ(ˆθK(λ)x)2)2. Its gradient w.r.t. λ is

∇λ(cid:96)(λ, ˆθK(λ), z) = 2(y − λ(ˆθK(λ)x)2)(−(ˆθK(λ)x)2 − 2λx2 ˆθK(λ)∇λ

ˆθK(λ)).

Thereby,

|∇λ(cid:96)(λ, ˆθK(λ), z)| =2|y − λ(ˆθK(λ)x)2| · |(ˆθK(λ)x)2 + 2λx2 ˆθK(λ)∇λ

ˆθK(λ)|
ˆθK(λ)| · ˆθK(λ) · x2.

=2|y − λ(ˆθK(λ)x)2| · |ˆθK(λ) + 2λ∇λ

20

Since |y − λ(ˆθK(λ)x)2| ≥ (1 − 1
ˆθK(λ) ≥ (1 + 3

4 ηλ)Kθ0, we have

4 ) = 3

4 , |ˆθK(λ) + 2λ∇λ

ˆθK(λ)| ≥ λ(1 + ηλ/4)K−1θ0η and

|∇λ(cid:96)(λ, ˆθK(λ), z)| ≥2 ·
3
8

=

3
4

· λ(1 + ηλ/4)K−1θ0η · (1 +

3
4

ηλ)Kθ0 ·

1
4

λ(1 + ηλ/4)K−1θ2

0η(1 + 3ηλ/4)K.

Finally,

||(cid:96)(λ, ˆθ(λ), z)||Lip ≥ sup
λ∈Λ
3
32
and ||(cid:96)(λ, ˆθ(λ), z)||Lip ≥ L = Ω((1 + 3

≥

16 η)K).

3
8

λ(1 + ηλ/4)K−1θ2

0η(1 + 3ηλ/4)K

(1 + η/16)K−1θ2

0η(1 + 3η/16)K := L,

C Improve Theorem 3 under Stronger Assumptions

When the inner loss ϕi is convex or strongly convex, we can get tighter bounds for L and γ in
Theorem 3. In Proposition 2, we show that L = O(K) and γ = O(K 3) when the inner loss ϕi
is convex. In this case, the dependence on K of the generalization gap (i.e., β in Theorem 2) is
O(K 2). In Proposition 3, we show that L = O(1) and γ = O(1) w.r.t. K when the inner loss ϕi is
strongly convex. In this case, the dependence on K of the generalization gap is O(1). We get these
tighter results by deriving tighter Lipschitz constants for updating functions of SGD w.r.t. θ, using
the (strongly) convex properties of ϕi. Other parts of the proof is the same as Theorem 3.

Notice that Theorem 3 implies that the learning rate η in the inner level should be of the order of 1/K
for a moderate L and γ. Therefore, η will be very small when K is very large, and the algorithm will
converge slow in practice. However, Proposition 2 and Proposition 3 imply that if we use a (strongly)
convex inner loss, η will not affect the order of L and γ, and thereby we can use a larger η in practice
in this case.
Proposition 2. Suppose Assumption 1,2,3,4 hold, ϕi(λ, θ, z) as a function of θ is convex for all
1 ≤ i ≤ n, z ∈ Z and λ ∈ Λ, and the inner level problem is solved with K steps SGD or GD with
learning rate η ≤ 2
, then ∀Str ∈ Z n, ∀z ∈ Z, ∀g ∈ Gˆθ, (cid:96)(λ, g(λ, Str), z) as a function of λ is
γϕ
L = O(K) Lipschitz continuous and γ = O(K 3) Lipschitz smooth.

Proof. The kth updating step of SGD can be written as

Gλ,k(θ) = θ − η∇θϕjk (λ, θ, ztr
jk

) = ∇θ

(cid:18) 1
2

||θ||2 − ηϕjk (λ, θ, ztr
jk

)

(cid:19)

,

where jk is randomly selected from {1, 2, · · · , n}.
The output of K steps SGD is
ˆθ(λ, Str) = Gλ,K(Gλ,K−1(· · · (Gλ,1(θ0)))) and Gˆθ is formed by iterates over (j1, j2, · · · , jK) ∈
{1, 2, · · · , n}K.

According to Lemma 2 and Assumption 3, we have

LG
1

(cid:44) sup

k,jk,Str,θ

||Gλ,k(θ)||λ∈Λ,Lip = sup
i,z,θ

||∇θ

||θ||2 − ηϕi(λ, θ, z)

(cid:19)

||λ∈Λ,Lip < ∞.

(cid:18) 1
2

Similarly, we have

γG
1

(cid:44) sup

k,jk,Str,θ

γG
3

(cid:44) sup

k,jk,Str,θ

||

∂
∂λ

||

∂
∂θ

Gλ,k(θ)||λ∈Λ,Lip < ∞, γG
2

Gλ,k(θ)||λ∈Λ,Lip < ∞, γG
4

(cid:44) sup

k,jk,Str,λ

(cid:44) sup

k,jk,Str,λ

||

∂
∂θ

||

∂
∂λ

Gλ,k(θ)||θ∈Θ,Lip < ∞,

Gλ,k(θ)||θ∈Θ,Lip < ∞.

21

Then we consider ||Gλ,k(θ)||θ∈Θ,Lip. According to the co-coercivity of ∇θϕjk (λ, θ, ztr
jk
)||2

||Gλ,k(θ) − Gλ,k(θ(cid:48))||2 =||θ − θ(cid:48)||2 + η2||∇θϕjk (λ, θ, ztr
jk
) − ∇θϕjk (λ, θ(cid:48), ztr
jk

) − ∇θϕjk (λ, θ(cid:48), ztr
jk
)(cid:11)
) − ∇θϕjk (λ, θ(cid:48), ztr
jk

)||2

− 2η (cid:10)θ − θ(cid:48), ∇θϕjk (λ, θ, ztr
≤||θ − θ(cid:48)||2 + η2||∇θϕjk (λ, θ, ztr
jk
||∇θϕjk (λ, θ, ztr
jk

− 2

jk

η
γϕ

) − ∇θϕjk (λ, θ(cid:48), ztr
jk

)||2 ≤ ||θ − θ(cid:48)||2.

), we have

sup
k,jk,Str,λ

||Gλ,k(θ)||θ∈Θ,Lip ≤ 1 (cid:44) LG

Thereby, ||Gλ,k(θ)||θ∈Θ,Lip ≤ 1 and

2 . According to
Lemma 3 and Lemma 4, ˆθ(λ, Str) is Lˆθ = KLG
1 Lipschitz continuous and γ ˆθ = O(K 3) Lips-
chitz smooth as a function of λ. By deﬁnition, Lˆθ and γ ˆθ are independent of the training dataset Str
and the random indices (j1, j2, · · · , jK) and thereby the randomness of ˆθ.
According to Lemma 2 and Assumption 2, we have
||(cid:96)(λ, θ, z)||λ∈Λ,Lip < ∞, L(cid:96)

||(cid:96)(λ, θ, z)||θ∈Θ,Lip < ∞.

L(cid:96)

2 = sup

1 = sup

θ∈Θ,z∈Z

λ∈Λ,z∈Z

Similarly, we have

γ(cid:96)
1

(cid:44) sup
θ,z

||

γ(cid:96)
3

(cid:44) sup
θ,z

||

(cid:21)

(cid:96)(λ, θ, z)

(cid:21)

(cid:96)(λ, θ, z)

(cid:20) ∂
∂λ

(cid:20) ∂
∂θ

||λ∈Λ,Lip < ∞, γ(cid:96)
2

(cid:44) sup
λ,z

||

||λ∈Λ,Lip < ∞, γ(cid:96)
4

(cid:44) sup
λ,z

||

(cid:20) ∂
∂θ

(cid:20) ∂
∂λ

(cid:21)

(cid:96)(λ, θ, z)

||θ∈Θ,Lip < ∞,

(cid:21)

(cid:96)(λ, θ, z)

||θ∈Θ,Lip < ∞.

Suppose z ∈ Z, ﬁrstly we consider the Lipschitz continuity of (cid:96)(λ, ˆθ(λ, Str), z):

||(cid:96)(λ, ˆθ(λ, Str), z)||λ∈Λ,Lip

||(cid:96)(λ, θ, z)||λ∈Λ,Lip + sup

λ∈Λ,z∈Z

||(cid:96)(λ, θ, z)||θ∈Θ,Lip · ||ˆθ(λ, Str)||λ∈Λ,Lip

≤ sup

θ∈Θ,z∈Z
1 + L(cid:96)

≤L(cid:96)

2L

ˆθ (cid:44) L.

Then we consider the Lipschitz continuity of ∂

∂
∂λ

(cid:96)(λ, ˆθ(λ, Str), z) =

(cid:20) ∂
∂λ

(cid:96)(λ, θ, z)

∂λ (cid:96)(λ, ˆθ(λ, Str), z), which can be expanded as
(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)

(cid:20) ∂
∂λ

(cid:20) ∂
∂θ

(cid:96)(λ, θ, z)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)

+

ˆθ(λ, Str)

(6)

(cid:21)

.

Taking the Lipschitz norm w.r.t. λ, we have

(cid:20) ∂
∂λ

||

(cid:20) ∂
∂θ

||

(cid:96)(λ, θ, z)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)

(cid:96)(λ, θ, z)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)

which yields

||λ∈Λ,Lip ≤ γ(cid:96)

1 + γ(cid:96)

4L

ˆθ,

||λ∈Λ,Lip ≤ γ(cid:96)

3 + γ(cid:96)

2L

ˆθ,

≤||

||

(cid:96)(λ, θ, z)

∂
∂λ
(cid:20) ∂
∂λ
(cid:20) ∂
∂θ
1 + γ(cid:96)

(cid:96)(λ, ˆθ(λ, Str), z)||λ∈Λ,Lip
(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)
(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)
ˆθ)L

ˆθ + (γ(cid:96)

(cid:96)(λ, θ, z)

3 + γ(cid:96)

4L

2L

+ ||

≤γ(cid:96)

||λ∈Λ,Lip

||λ∈Λ,LipL

ˆθ + L(cid:96)
2||

∂
∂λ

ˆθ(λ, Str)||λ∈Λ,Lip

ˆθ + L(cid:96)
2γ

ˆθ (cid:44) γ.

(7)

22

With Eq. (6) and Eq. (7), we can conclude (cid:96)(λ, ˆθ(λ, Str), z) as a function of λ is L = O(K) Lipschitz
continuous and γ = O(K 3) Lipschitz smooth. By deﬁnition, L and γ are independent of the training
dataset Str, z, the random indices (j1, j2, · · · , jK) and thereby the randomness of ˆθ. Thereby, we
have ∀Str ∈ Z n, ∀z ∈ Z, ∀g ∈ Gˆθ, (cid:96)(λ, g(λ, Str), z) as a function of λ is L = O(K) Lipschitz
continuous and γ = O(K 3) Lipschitz smooth. Similarly, the result also holds for GD.

Proposition 3. Suppose Assumption 1,2,3,4 hold, ϕi(λ, θ, z) as a function of θ is τ -strongly convex
for all 1 ≤ i ≤ n, z ∈ Z and λ ∈ Λ, and the inner level problem is solved with K steps SGD or GD
with learning rate η ≤ 1
, then ∀Str ∈ Z n, ∀z ∈ Z, ∀g ∈ Gˆθ, (cid:96)(λ, g(λ, Str), z) as a function of λ
γϕ
is L = O(1) Lipschitz continuous and γ = O(1) Lipschitz smooth w.r.t. K.

Proof. The kth updating step of SGD can be written as

Gλ,k(θ) = θ − η∇θϕjk (λ, θ, ztr
jk

) = ∇θ

(cid:18) 1
2

||θ||2 − ηϕjk (λ, θ, ztr
jk

)

(cid:19)

,

where jk is randomly selected from {1, 2, · · · , n}.
The output of K steps SGD is
ˆθ(λ, Str) = Gλ,K(Gλ,K−1(· · · (Gλ,1(θ0)))) and Gˆθ is formed by iterates over (j1, j2, · · · , jK) ∈
{1, 2, · · · , n}K.

According to Lemma 2 and Assumption 3, we have

LG
1

(cid:44) sup

k,jk,Str,θ

||Gλ,k(θ)||λ∈Λ,Lip = sup
i,z,θ

||∇θ

||θ||2 − ηϕi(λ, θ, z)

(cid:19)

||λ∈Λ,Lip < ∞.

(cid:18) 1
2

Similarly, we have

γG
1

γG
3

(cid:44) sup

k,jk,Str,θ

(cid:44) sup

k,jk,Str,θ

||

∂
∂λ

||

∂
∂θ

Gλ,k(θ)||λ∈Λ,Lip < ∞, γG
2

Gλ,k(θ)||λ∈Λ,Lip < ∞, γG
4

(cid:44) sup

k,jk,Str,λ

(cid:44) sup

k,jk,Str,λ

||

∂
∂θ

||

∂
∂λ

Gλ,k(θ)||θ∈Θ,Lip < ∞,

Gλ,k(θ)||θ∈Θ,Lip < ∞.

Then we consider ||Gλ,k(θ)||θ∈Θ,Lip. Since ϕjk (λ, θ, ztr
jk
we have ϕjk (λ, θ, ztr
jk
to the co-coercivity of ∇θ(ϕjk (λ, θ, ztr
jk

) as a function of θ is τ -strongly convex,
2 ||θ||2 as a function of θ is convex and γϕ − τ Lipschitz smooth. According

) − τ

) − τ
(cid:10)θ − θ(cid:48), ∇θϕjk (λ, θ, ztr
||∇θϕjk (λ, θ, ztr
jk

1
γϕ − τ

jk

≥

2 ||θ||2), we have
) − τ θ − ∇θϕjk (λ, θ(cid:48), ztr
jk

) − τ θ − ∇θϕjk (λ, θ(cid:48), ztr
jk

) + τ θ(cid:48)(cid:11)

) + τ θ(cid:48)||2,

which is equivalent to

(cid:10)θ − θ(cid:48), ∇θϕjk (λ, θ, ztr
||∇θϕjk (λ, θ, ztr
jk

1
γϕ + τ

jk

) − ∇θϕjk (λ, θ(cid:48), ztr
jk

)(cid:11)

) − ∇θϕjk (λ, θ(cid:48), ztr
jk

)||2 +

γϕτ
γϕ + τ

||θ − θ(cid:48)||2.

≥

As a result,

||Gλ,k(θ) − Gλ,k(θ(cid:48))||2

=||θ − θ(cid:48)||2 + η2||∇θϕjk (λ, θ, ztr
jk
− 2η (cid:10)θ − θ(cid:48), ∇θϕjk (λ, θ, ztr
) − ∇θϕjk (λ, θ(cid:48), ztr
jk
≤||θ − θ(cid:48)||2 + η2||∇θϕjk (λ, θ, ztr
jk

) − ∇θϕjk (λ, θ(cid:48), ztr
jk
)(cid:11)
) − ∇θϕjk (λ, θ(cid:48), ztr
jk

jk

)||2

)||2

− 2η(

||∇θϕjk (λ, θ, ztr
jk

) − ∇θϕjk (λ, θ(cid:48), ztr
jk

)||2 +

γϕτ
γϕ + τ

||θ − θ(cid:48)||2)

1
γϕ + τ
γϕτ
γϕ + τ

=(1 − 2η

)||θ − θ(cid:48)||2 + (η2 −

2η
γϕ + τ

23

)||∇θϕjk (λ, θ, ztr
jk

) − ∇θϕjk (λ, θ(cid:48), ztr
jk

)||2.

Since η ≤ 1
γϕ

≤ 2

γϕ+τ , we have ||Gλ,k(θ)||θ∈Θ,Lip ≤
(cid:114)

sup
k,jk,Str,λ

||Gλ,k(θ)||θ∈Θ,Lip ≤

1 − 2η

γϕτ
γϕ + τ

(cid:44) LG

2 < 1.

(cid:113)

1 − 2η γϕτ

γϕ+τ and

According to Lemma 3 and Lemma 4, ˆθ(λ, Str) as a function of λ is Lˆθ = O(1) Lipschitz continuous
and γ ˆθ = O(1) Lipschitz smooth w.r.t. K. By deﬁnition, Lˆθ and γ ˆθ are independent of the training
dataset Str and the random indices (j1, j2, · · · , jK) and thereby the randomness of ˆθ.
According to Lemma 2 and Assumption 2, we have
||(cid:96)(λ, θ, z)||λ∈Λ,Lip < ∞, L(cid:96)

||(cid:96)(λ, θ, z)||θ∈Θ,Lip < ∞.

L(cid:96)

2 = sup

1 = sup

θ∈Θ,z∈Z

λ∈Λ,z∈Z

Similarly, we have

γ(cid:96)
1

(cid:44) sup
θ,z

||

γ(cid:96)
3

(cid:44) sup
θ,z

||

(cid:21)

(cid:96)(λ, θ, z)

(cid:21)

(cid:96)(λ, θ, z)

(cid:20) ∂
∂λ

(cid:20) ∂
∂θ

||λ∈Λ,Lip < ∞, γ(cid:96)
2

(cid:44) sup
λ,z

||

||λ∈Λ,Lip < ∞, γ(cid:96)
4

(cid:44) sup
λ,z

||

(cid:20) ∂
∂θ

(cid:20) ∂
∂λ

(cid:21)

(cid:96)(λ, θ, z)

||θ∈Θ,Lip < ∞,

(cid:21)

(cid:96)(λ, θ, z)

||θ∈Θ,Lip < ∞.

Suppose z ∈ Z, ﬁrstly we consider the Lipschitz continuity of (cid:96)(λ, ˆθ(λ, Str), z):

||(cid:96)(λ, ˆθ(λ, Str), z)||λ∈Λ,Lip

||(cid:96)(λ, θ, z)||λ∈Λ,Lip + sup

λ∈Λ,z∈Z

||(cid:96)(λ, θ, z)||θ∈Θ,Lip · ||ˆθ(λ, Str)||λ∈Λ,Lip

≤ sup

θ∈Θ,z∈Z
1 + L(cid:96)

≤L(cid:96)

2L

ˆθ (cid:44) L.

Then we consider the Lipschitz continuity of ∂

∂
∂λ

(cid:96)(λ, ˆθ(λ, Str), z) =

(cid:20) ∂
∂λ

(cid:96)(λ, θ, z)

∂λ (cid:96)(λ, ˆθ(λ, Str), z), which can be expanded as
(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)

(cid:20) ∂
∂λ

(cid:20) ∂
∂θ

(cid:96)(λ, θ, z)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)

+

ˆθ(λ, Str)

(8)

(cid:21)

.

Taking the Lipschitz norm w.r.t. λ, we have

(cid:20) ∂
∂λ

||

(cid:20) ∂
∂θ

||

(cid:96)(λ, θ, z)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)

(cid:96)(λ, θ, z)

(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)

which yields

||λ∈Λ,Lip ≤ γ(cid:96)

1 + γ(cid:96)

4L

ˆθ,

||λ∈Λ,Lip ≤ γ(cid:96)

3 + γ(cid:96)

2L

ˆθ,

≤||

||

(cid:96)(λ, θ, z)

∂
∂λ
(cid:20) ∂
∂λ
(cid:20) ∂
∂θ
1 + γ(cid:96)

(cid:96)(λ, ˆθ(λ, Str), z)||λ∈Λ,Lip
(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)
(cid:21) (cid:12)
(cid:12)
(cid:12)θ=ˆθ(λ,Str)
ˆθ)L

ˆθ + (γ(cid:96)

(cid:96)(λ, θ, z)

3 + γ(cid:96)

4L

2L

+ ||

≤γ(cid:96)

||λ∈Λ,Lip

||λ∈Λ,LipL

ˆθ + L(cid:96)
2||

∂
∂λ

ˆθ(λ, Str)||λ∈Λ,Lip

ˆθ + L(cid:96)
2γ

ˆθ (cid:44) γ.

(9)

With Eq. (8) and Eq. (9), we can conclude (cid:96)(λ, ˆθ(λ, Str), z) as a function of λ is L = O(1) Lipschitz
continuous and γ = O(1) Lipschitz smooth w.r.t. K. By deﬁnition, L and γ are independent of
the training dataset Str, z, the random indices (j1, j2, · · · , jK) and thereby the randomness of ˆθ.
Thereby, we have ∀Str ∈ Z n, ∀z ∈ Z, ∀g ∈ Gˆθ, (cid:96)(λ, g(λ, Str), z) as a function of λ is L = O(1)
Lipschitz continuous and γ = O(1) Lipschitz smooth. Similarly, the result also holds for GD.

24

D UD with GD in the Outer Level

Since GD is deterministic, we can derive a high probability bound for UD with GD in the outer level.
Firstly, we deﬁne the notion of uniform stability on validation for a deterministic HO algorithm.
Deﬁnition 7. A deterministic HO algorithm A is β-uniformly stable on validation if for all validation
datasets Sval, S

(cid:48)val differ in at most one sample, we have

(cid:48)val ∈ Z m such that Sval, S

∀Str ∈ Z mtr

, ∀z ∈ Z, (cid:96)(A(Str, Sval), z) − (cid:96)(A(Str, S

(cid:48)val), z) ≤ β.

If a deterministic HO algorithm is β-uniformly stable on validation, then we have the following high
probability bound.
Theorem 5. (Generalization bound of a uniformly stable deterministic algorithm). Suppose a
deterministic HO algorithm A is β-uniformly stable on validation, Str ∼ (Dtr)n, Sval ∼ (Dval)m
and Str and Sval are independent, then for all δ ∈ (0, 1), with probability at least 1 − δ,

(cid:96)(A(Str, Sval), Dval) ≤ (cid:96)(A(Str, Sval), Sval) + β +

(cid:114)

(2βm + s((cid:96)))2 ln δ−1
2m

.

Proof. Let Φ(Str, Sval) = (cid:96)(A(Str, Sval), Dval) − (cid:96)(A(Str, Sval), Sval). Suppose Sval, S
Z m differ in at most one point, then

(cid:48)val ∈

|Φ(Str, Sval) − Φ(Str, S

(cid:48)val)|

≤|(cid:96)(A(Str, Sval), Dval) − (cid:96)(A(Str, S

(cid:48)val), Dval)| + |(cid:96)(A(Str, Sval), Sval) − (cid:96)(A(Str, S

(cid:48)val), S

(cid:48)val)|.

For the ﬁrst term,

|(cid:96)(A(Str, Sval), Dval) − (cid:96)(A(Str, S

(cid:48)val), Dval)|

=|Ez∼Dval

(cid:104)

(cid:96)(A(Str, Sval), z) − (cid:96)(A(Str, S

(cid:105)

(cid:48)val), z)

| ≤ β.

For the second term,

|(cid:96)(A(Str, Sval), Sval) − (cid:96)(A(Str, S

(cid:48)val), S

(cid:48)val)|

≤

1
m

m
(cid:88)

i=1

|(cid:96)(A(Str, Sval), zval

i

) − (cid:96)(A(Str, S

(cid:48)val
(cid:48)val), z
i

)|

≤

s((cid:96))
m

+

m − 1
m

β.

As a result,

|Φ(Str, Sval) − Φ(Str, S

(cid:48)val)| ≤

s((cid:96))
m

+ 2β.

According to McDiarmid’s inequality, we have for all (cid:15) ∈ R+,

PSval∼(Dval)m (Φ(Str, Sval) − ESval∼(Dval)m

(cid:2)Φ(Str, Sval)(cid:3) ≥ (cid:15)) ≤ exp(−2

m(cid:15)2
(s((cid:96)) + 2mβ)2 ).

Besides, we have

ESval∼(Dval)m

(cid:2)Φ(Str, Sval)(cid:3) = ESval∼(Dval)m

(cid:2)(cid:96)(A(Str, Sval), Dval) − (cid:96)(A(Str, Sval), Sval)(cid:3)

=ESval∼(Dval)m,z∼Dval
=ESval∼(Dval)m,z∼Dval

(cid:2)(cid:96)(A(Str, Sval), z) − (cid:96)(A(Str, Sval), zval
(cid:2)(cid:96)(A(Str, z, zval
, · · · , zval

)(cid:3)
) − (cid:96)(A(Str, Sval), zval

m ), zval

1

2

1

1

)(cid:3) ≤ β.

Thereby, we have for all (cid:15) ∈ R+,

PSval∼(Dval)m(Φ(Str, Sval) − β ≥ (cid:15)) ≤ exp(−2

m(cid:15)2
(s((cid:96)) + 2mβ)2 ).

25

Notice the above inequality holds for all Str ∈ Z n, we further have (cid:15) ∈ R+,

PStr∼(Dtr)n,Sval∼(Dval)m(Φ(Str, Sval) − β ≥ (cid:15)) ≤ exp(−2

m(cid:15)2
(s((cid:96)) + 2mβ)2 ).

Equivalently, we have ∀δ ∈ (0, 1),

PStr∼(Dtr)n,Sval∼(Dval)m

Φ(Str, Sval) ≤ β +

(cid:32)

(cid:114)

(2βm + s((cid:96)))2 ln δ−1
2m

(cid:33)

≥ 1 − δ.

Then we analyze the stability for UD with GD in the outer level. At each iteration in the outer level,
it updates the hyperparameter by:

λt+1 = (1 − αt+1µ)λt − αt+1∇λ ˆRval(λt, ˆθ(λt, Str), Sval),

where αt is the learning rate and µ is the weight decay.
Theorem 6. (Uniform stability of algorithms with GD in the outer level). Suppose ˆθ is a deterministic
function and ∀Str ∈ Z n, ∀z ∈ Z, (cid:96)(λ, ˆθ(λ, Str), z) as a function of λ is L-Lipschitz continuous and
γ-Lipschitz smooth. Then, solving Eq. (4) in the full paper with T steps GD, learning rate αt ≤ α
and weight decay µ ≤ min(γ, 1

α ) in the outer level is β-uniformly stable on validation with

β =

2L2
m(γ − µ)

((1 + α(γ − µ))T − 1).

Proof. Suppose Str ∈ Z n, we use F (λ, Sval, α, µ) = (1 − αµ)λ − α∇λ ˆRval(λ, ˆθ(λ, Str), Sval)
to denote the updating rule of GD, where we omit the dependency on Str for simplicity. Suppose
Sval, S
t}t≥0 be the trace of gradient descent
with Sval and S

(cid:48)val ∈ Z m differ in at most one point, let {λt}t≥0 and {λ(cid:48)
(cid:48)val respectively. Let δt = ||λt − λ(cid:48)

t||, then

δt+1 =||F (λt, Sval, αt+1, µ) − F (λ(cid:48)
≤||F (λt, Sval, αt+1, µ) − F (λ(cid:48)

≤(|1 − αt+1µ| + αt+1γ)δt +

≤(1 + α(γ − µ))δt +

2αL
m

.

(cid:48)val, αt+1, µ)||

t, S
t, Sval, αt+1, µ)|| + ||F (λ(cid:48)
2αt+1L
m

= (1 + αt+1(γ − µ))δt +

t, Sval, αt+1, µ) − F (λ(cid:48)
2αt+1L
m

t, S

(cid:48)val, αt+1, µ)||

Thereby, we have δt ≤ 2L

m(γ−µ) ((1 + α(γ − µ))t − 1) for all t ≥ 0. Finally, we have

∀z ∈ Z, |(cid:96)(λT , ˆθ(λT , Str), z) − (cid:96)(λ(cid:48)

T , ˆθ(λ(cid:48)

T , Str), z)| ≤

2L2
m(γ − µ)

((1 + α(γ − µ))T − 1).

Remark: We derive such a bound by using the recursive updates of the outer level GD with the
smoothness of the loss function and the inner level optimization. This technique can be directly applied
to traditional GD (i.e., GD with one level optimization) to get a stability bound of exponentially
increasing w.r.t T and O(1/m).

E Curse of dimensionality in CV

Lemma 5. Suppose f (λ), λ ∈ Λ = [0, 1]d is L Lipschitz continuous, {λi}T
random vectors on [0, 1]d, then E inf

f (λ) + L

√

.

1≤i≤T

f (λi) ≤ inf
λ∈Λ

d
1
d

T

i=1 are i.i.d. uniform

26

Proof. Let λ∗ = arg min

λ∈Λ

Thereby,

f (λ). Firstly, we have f (λi) ≤ f (λ∗) + L||λi − λ∗|| for all 1 ≤ i ≤ T .

inf
1≤i≤T

f (λi) ≤ f (λ∗) + L inf
1≤i≤T

||λi − λ∗||.

Taking expectation, we have

E inf

1≤i≤T

f (λi) ≤ f (λ∗) + LE inf
1≤i≤T

||λi − λ∗||.

As for E inf

1≤i≤T

||λi − λ∗||, we have

E inf

1≤i≤T

||λi − λ∗|| =

P ( inf

||λi − λ∗|| > t)dt

(cid:90) ∞

0

1≤i≤T
(cid:90) ∞

P (||λ1 − λ∗|| > t)T dt =

(1 − |B(λ∗, t) ∩ Λ|)T dt

0

(1 − |B(0, t) ∩ Λ|)T dt = E inf

1≤i≤T

||λi||

√

≤E inf

1≤i≤T

d sup
1≤j≤d

λi,j =

(cid:90) 1

√

d

0

P ( inf

1≤i≤T

sup
1≤j≤d

λi,j > t)dt

P ( sup
1≤j≤d

λ1,j > t)T dt =

(cid:90) 1

√

d

0

(1 − P (λ1,1 ≤ t)d)T dt

=

≤

(cid:90) ∞

0
(cid:90) ∞

0

(cid:90) 1

0

(cid:90) 1

√

d

=

=

≤

√

d
√

d
T 1

d

(1 − td)T dt ≤
√

e−td

dt =

0
(cid:90) ∞

0

0
(cid:90) ∞

0

d
T 1

d

(cid:90) 1

√

d

e−T td

1
d

(cid:90) T

e−td

dt

dt =
√

√

d
T 1
d
T 1

d

0

d
0
(cid:90) ∞

e−tdt

1
d =

1

d e−tdt =

t

√

d
T 1

d

Γ(1 +

1
d

) ≤

√

d
T 1

d

.

As a result,

E inf

1≤i≤T

f (λi) ≤ f (λ∗) + L

√

d
T 1

d

.

The following result implies that CV suffers from curse of dimensionality. CV requires exponentially
large T w.r.t. the dimensionality of the λ to achieve a reasonably low empirical risk.
Theorem 7. (Curse of dimensionality in CV). Suppose (1) the inner level optimization is solved
deterministically, i.e., ˆθ in Eq. (4) in the full paper is a deterministic function, (2) {λt}T
t=1 are i.i.d.
uniform random vectors taking value in Λ = [0, 1]d, (3) ∀Str ∈ Z n, ∀z ∈ Z, (cid:96)(λ, ˆθ(λ, Str), z) as
a function of λ is L Lipschitz continuous. Let Str ∼ (Dtr)n and Sval ∼ (Dval)m be independent,
then we have

(cid:105)
(cid:104) ˆRval(Acv(Str, Sval), Sval)

E

≤ E

(cid:20)

inf
λ∈Λ

ˆRval(λ, ˆθ(λ, Str), Sval)

(cid:21)

+

Proof. Let t∗ be the index of the best hyperparameter, i.e.,

t∗ = arg min
1≤t≤T

ˆRval(λt, ˆθ(λt, Str), Sval),

then the output of CV is Acv(Str, Sval) = (λt∗ , ˆθ(λt∗ , Str)).
According to Lemma 5, we have

√

L
d
T 1

d

.

(cid:20)

= E{λt}T

t=1

inf
1≤t≤T

(cid:21)
ˆRval(λt, ˆθ(λt, Str), Sval)

E{λt}T

t=1

(cid:105)
(cid:104) ˆRval(λt∗ , ˆθ(λt∗ , Str), Sval)

≤ inf
λ∈Λ

ˆRval(λ, ˆθ(λ, Str), Sval) +

√

L
d
T 1

d

.

27

Thereby,

(cid:105)
(cid:104) ˆRval(Acv(Str, Sval), Sval)

E

≤EStr,Sval

(cid:20)

inf
λ∈Λ

(cid:21)
ˆRval(λ, ˆθ(λ, Str), Sval)

+

= E{λt}T

t=1,Str,Sval
√

L
d
T 1

d

.

(cid:104) ˆRval(λt∗ , ˆθ(λt∗ , Str), Sval)

(cid:105)

F Discussion of the Boundedness Assumption of the Loss Function

The bounded assumption is mild and common (e.g., also used in Theorem 3.12 of [16] and Section 2 in
[38]). Indeed, given a machine learning model of a ﬁnite number of parameters (e.g. neural networks
of ﬁnite depth and width used in our experiments), a bounded parameter space (Assumption 1), and
a bounded input space (Assumption 1), the feature space is also bounded. Note that previous work
makes a similar assumption (at the bottom of Page 9 in [16]) as Assumption 1.

G Additional Experiments

G.1 Generalization Gap

In Figure 5, we plot the generalization gap (estimated by difference between test and validation loss)
of UD on the FL and DR experiments. When the K ≥ 6411, the generalization gap increases as K
increases. These results validate our Theorem 2 and Theorem 3.

In Figure 6, we plot the generalization gap of CV on the FL and DR experiments. There is not a clear
relationship between the generalization gap and K. These results validate our Theorem 4.

(a) Validation loss (FL)

(b) Testing loss (FL)

(c) Generalization gap (FL)

(d) Validation loss (DR)

(e) Testing loss (DR)

(f) Generalization gap (DR)

Figure 5: The generalization gap of UD in feature learning (FL) and data reweighting (DR).

G.2 Empirical Veriﬁcation of the Expectation Bound of CV
We empirically validate the O((cid:112)1/m) expectation bound of CV in Theorem 4. In the data reweight-
ing experiment, we chose ten different m from [10, 1000] such that (cid:112)1/m is distributed linearly and

11The test loss is dominated by training loss when K is too small due to underﬁtting on the training dataset.

28

(a) Validation loss (FL)

(b) Testing loss (FL)

(c) Generalization gap (FL)

(d) Validation loss (DR)

(e) Testing loss (DR)

(f) Generalization gap (DR)

Figure 6: The generalization gap of CV in feature learning (FL) and data reweighting (DR).

we plot the curve of the generalization gap v.s. (cid:112)1/m. We ﬁx T = 1000 and K = 64. We run on 5
different seeds and use the averaged result. As shown in Figure 7, the curve is approximately linear,
which accords with our Theorem 4.

Figure 7: Generalization gap v.s. (cid:112)1/m of CV in data reweighting (DR).

G.3 UD with a Smaller Learning Rate in the Inner Level

We also try a smaller learning rate η = 0.1 in the inner level on the data reweighting task. As shown
in Figure 8, it requires K = 1024 inner iterations to overﬁt. This can be explained by our Theorem 3,
which implies that a smaller η requires a larger K to make the generalization gap unchanged.

G.4 Experiments with a Smaller Number of Hyperparameters

1 + x2

We also experiment with 4 hyperparameters. We create a two dimensional toy dataset in the feature
learning task: y = x2
2 + 0.3(cid:15), where x1, x2 ∼ Uniform(0, 1) and (cid:15) ∼ N (0, 1). The number of
training data is 10 and the number of validation data is 2. The hyperparameter λ is a 2 × 2 matrix
following the input x and the parameter θ is a 2 × 1 matrix to predict the y. The learning rate of the
outer level problem is 0.01 and that of the inner level problem is 0.1 and the batch size is 1 in both
problems. K is 16 and T is 1000. In this case, the validation losses of UD and CV are comparable

29

(a) Validation loss (DR)

(b) Testing loss (DR)

(c) Generalization gap (DR)

Figure 8: Results of UD in data reweighting (DR) with a smaller learning rate η = 0.1. We run on 3
different seeds. The performance of UD is sensitive to the values of K and T .

and both algorithms ﬁt well on the validation data. However, UD overﬁts much severely, leading to a
worse testing loss than CV. The results agree with our theory.

Figure 9: Compare between CV and UD with 4 hyperparameters.

30

