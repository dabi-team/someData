9
1
0
2

y
a
M
8
2

]

G
L
.
s
c
[

1
v
2
2
7
1
1
.
5
0
9
1
:
v
i
X
r
a

A Graph Theoretic Framework of Recomputation
Algorithms for Memory-Efﬁcient Backpropagation

Mitsuru Kusumoto ∗
Preferred Networks, Inc.
mkusumoto@preferred.jp

Takuya Inoue ∗
The University of Tokyo
inoue-takuya57@g.ecc.u-tokyo.ac.jp

Gentaro Watanabe
Preferred Networks, Inc.
g.wtnb@preferred.jp

Takuya Akiba
Preferred Networks, Inc.
akiba@preferred.jp

Masanori Koyama
Preferred Networks, Inc.
masomatics@preferred.jp

Abstract

Recomputation algorithms collectively refer to a family of methods that aims to
reduce the memory consumption of the backpropagation by selectively discarding
the intermediate results of the forward propagation and recomputing the discarded
results as needed. In this paper, we will propose a novel and efﬁcient recompu-
tation method that can be applied to a wider range of neural nets than previous
methods. We use the language of graph theory to formalize the general recompu-
tation problem of minimizing the computational overhead under a ﬁxed memory
budget constraint, and provide a dynamic programming solution to the problem.
Our method can reduce the peak memory consumption on various benchmark
networks by 36%
81%, which outperforms the reduction achieved by other
methods.

∼

1 Introduction

The efﬁciency of memory usage is always one of the most important issues in the application of deep
neural nets. Modern deep neural networks used in commercial applications tend to be large, and they
require massive memory for the forward and backward computations. The inputs to the networks
can be large as well; this is particularly true for the tasks related to computer vision such as object
detection and semantic segmentation, where higher resolution images are generally more useful to
detect small objects accurately. Ample free memory is also important for the training process itself;
when the memory is insufﬁcient, the user has no choice but to choose a small batch size. This is
problematic, especially when using batch normalization [7] as the quality of batch statistics degrades.
Indeed, its impact on the resulting model quality is crucial. Recent studies enabled themselves to
use large batch sizes by reducing the memory consumption or introducing distributed computation,
and succeeded in achieving the state-of-the-art results for computer vision tasks [11, 19].

Recomputation algorithms collectively refer to a family of methods of smart memory manipulation
that can reduce the memory consumption without altering the outcome of the computation or com-
promising the accuracy of the output. In theory, backward computation requires the result of the
forward computation, and naive approach requires the user to cache all the forward computation
results for the backward computation. However, we can reduce the peak memory consumption by
deliberately discarding some parts of the intermediate results in the forward computation, and re-
compute these intermediate results gradually on a need basis during the backpropagation. Naturally,
the efﬁcacy of any recomputation method depends on its rules of what to forget and what to cache
in what order.

∗Equal contribution.

Preprint. Under review.

 
 
 
 
 
 
(cid:1)−(S)

S

(cid:1)+(S)

L (cid:2)(L)

Forward Part

W1

x

W2

a1

h1

W3

a2

h2

G=(V, E)

y

gW1

gW2

gW3

gx

gh1

ga1

gh2

ga2

gy

Backward Part

Figure 1: (Left) The computation graph of a three-layer perceptron. A part induced by intermediate
nodes is denoted by G = (V, E). (Right) Neighborhoods, lower set, and its boundary.

Indeed, the idea of recomputation is not entirely new on its own. For example, Chen et al. [2] have
proposed an efﬁcient recomputation framework for a speciﬁc family of networks that can be divided
into segments, and Gruslys et al. [4] have proposed a method specializing in RNNs. These methods,
however, do not investigate neural networks with complex structures in complete generality, and
their applications are limited either to a speciﬁc family of networks or to a group of networks to
which their ad-hoc heuristics can be applied.

In this paper, we will propose a novel and efﬁcient recomputation method that can be applied to
theoretically all types of neural nets. To the authors’ knowledge, there has been no study to date that
tackled this problem in a completely general setting, and a new formulation of the recomputation
problem is warranted. We will, therefore, deﬁne the general recomputation problem with appropriate
formality as minimization of computational overhead under a ﬁxed memory budget (Section 2 and 3),
and provide our dynamic programming (DP) solution to the problem (Section 4.1 to 4.3). We will
also show that, by solving the opposite problem of maximizing the computational overhead, we can
reduce the peak memory consumption to the level that cannot be achieved by contemporary meth-
ods (Section 4.4). We will demonstrate the efﬁcacy of our DP solution on the various benchmark
networks (ResNet, DenseNet, U-Net, PSPNet, etc.) and compare its performance against contempo-
rary methods in terms of the computational overhead and the size of the achieved memory reduction
81%, which is much
(Section 5). Our method can reduce the peak memory consumption by 36%
greater than the reduction that can be achieved by other methods.

∼

2 Preliminaries

In this ﬁrst section, we present basic concepts and deﬁnitions that will be required for understanding
our algorithms. Throughout, we will use a directed graph G = (V, E) to represent the architecture
of the network, where V is the set of variables and there is an edge (v, w)
V is directly
V . In our study, we will exclude the input nodes from the
required for the computation of w
deﬁnition of V , because the computations on the intermediate nodes tend to play a more pivotal
role than the input nodes for the memory consumption of neural networks. We also exclude from
our considerations the memory required for the model parameters as well. In computation theory,
it is customary to use computational graph to represent the computational dependency among the
variables. Figure 1-Left is the computational graph for the forward and backward computation on
a neural network. Although we do not explicitly use the computational graph in our formulation of
the problem, we will develop our algorithm by building a theory on the computational graph.

E if v

∈

∈

∈

For an arbitrary node set S
directed edge from some node in S. We deﬁne δ−(S) analogously: δ+(S) :=
V
E for some s

and δ−(S) :=

E for some s

(v, s)

V , we will use δ+(S) to denote the set of nodes to which there is a

v

{

∈

V

|

(s, v)

∈

⊆

S

S

v

.

∈

}

{

∈

|

∈

∈

}

Also, following the deﬁnitions in order theory [3], we say L
edge from V
L to L, and write L
The boundary of L is deﬁned by ∂(L) := δ−(V
set we introduced above plays a pivotal role. We denote the set of all lower sets by
argument, we can deduce that #V
#
the visual renditions of these deﬁnitions.

V is a lower set of V if there is no
L.
L. In our theory, the concept of the lower
G. By a simple
2#V holds for any graph G. See Figure 1-Right for

V . By deﬁnition, L is a lower set if and only if δ−(L)

L)

⊆

≺

≤

≤

⊆

L

L

∩

\

\

G

2

Finally, we also deﬁne the forward computation cost Tv > 0 and the memory consumption cost
Pv∈S Mv for
Mv > 0 to each node v
S
V . We do not need to consider the backward computation cost in this study, because we will
not recompute the backward computation.

Pv∈S Tv and M (S) :=

V . Likewise, we deﬁne T (S) :=

⊆

∈

Chen’s Recomputation Algorithm Before explaining our formulation, we would like to intro-
duce the work of Chen et al. [2] in order to provide some intuition for our approach. Chen et al.
proposed a method of recomputation for a family of neural nets that can be decomposed of seg-
ments. For an n layer network, Chen’s algorithm divides the graph into √n segments of length √n.
In the forward computation, the algorithm caches the values for the nodes at the boundary of each
segment and discard all other nodes from the memory. When the time comes for backpropagation of
the segment i, it recomputes the required forward propagation results using the cache of the segment
i
1. This way, the algorithm can keep the peak memory consumption within O(√n) at an addi-
tional computational cost that amounts to one round of the forward computation. As mentioned in
the beginning, however, their algorithm can be applied only to a speciﬁc type of graph. For example,
a neural net with a skip connection from every layer to the output layer cannot be divided into more
than one segment, and an extra set of heuristical rules have to be applied in order to deal with such
a situation. In general, it is difﬁcult to determine if a given arbitrary graph can be handled by their
heuristics.

−

3 General Recomputation Problem

In this study, we will extend Chen’s framework to a generic graph by reformulating the recomputa-
tion problem so that we can develop algorithms that can be applied to all types of graphs. We ﬁrst
need an additional set of deﬁnitions and notations.

As mentioned in the introduction, the efﬁcacy of any recomputation method is determined by its
k
i=1 Vi with the intention
rules of caching and its order of computation. Consider a partition V =
S
of computing Vi after Vi−1 in the forward computation. In order to make computation in the intended
order, we need to require that if (v, w)
j. If
this requirement is satisﬁed for the partition, we can construct an increasing sequence of lower sets
Vi. Indeed, we can do the construction
L2 ≺
L1 ≺
∪
{
L1 ≺
backward by starting from an arbitrary increasing sequence
and
Li−1 (Figure 2-(a)). These building blocks will be the basis of our formulation.
deﬁning Vi = Li
. . .
, we deﬁne the canonical

∈
by putting Li := V1 ∪

Vj must hold for some i

∈
V2 ∪

L2 ≺

Vi and w

E, then v

Lk = V

Lk = V

. . .

. . .

. . .

Lk = V

≺

≤

≺

∈

}

\

}

{

Now, given an arbitrary sequence of lower sets
strategy of recomputation as follows:

L1 ≺

{

≺

}

Forward computation After making the evaluations for all nodes in Vi, cache the values associated
∂(Li). Using the cache of ∂(Li), compute

with ∂(Li) and discard all the values for Vi
the values for Vi+1.
See Figure 2-(b) for a visualization of the forward computation. At the time of completion
of the computations for Vi, the nodes with solid blue color have been cached, and the nodes
with opaque blue colors have been discarded.

\

Backward computation Backward computations are to be conducted in the reverse order. After
making the evaluations for all nodes in Vi+1 (Figure 2-(c)), we will execute the following
commands in order.

1. recover the required forward values for the nodes Vi based on the cache of ∂(Li−1)

and conduct the backpropagation for the nodes in Vi. (Figure 2-(d))

2. cache the nodes in the backward part of the computational graph that corresponds to
δ+(Li−1)
Vi, and discard the nodes in both the forward and backward part that
correspond to the nodes of Vi that will not be needed in the computation in future.
This means that if there is a skip connection into v
Vi, the cache on v will not be
discarded. (Figure 2-(e))

∈

∩

In the real implementation of this canonical strategy, the gradient of the model with re-
spect to the parameters are to be reported to the user in real time during the process of
backpropagation.

3

L1

L2

L3 = V

V1

V2
(a)

(c)

(e)

V3

i = 1

i = 2
(b)

(i)

(d)

(ii)

(f)

(iii)

Figure 2: The visualization of the canonical strategy for a lower set sequence.

In principle, our formulation is based on this canonical strategy. Chen’s algorithm can be seen as an
instance of the canonical strategy that specializes in a speciﬁc form of lower sets.

Now, the efﬁcacy of this canonical strategy will depend solely on the choice of the sequence of the
lower sets. Two performance measures are of our interest: the amount of computational overhead
and the size of peak memory consumption. First, at the end of the forward computation for Vi, the
values for the following nodes Ui have been cached by the algorithm: Ui :=

i
j=1 ∂(Lj).
S
Uk. The total computational

Thus, the set of nodes subject to the recomputation is given by V
overhead of the recomputation is given by

\

k

T (V

Uk) =

T (cid:16)Vi

∂(Li)(cid:17).

\

X
i=1
We would like to minimize this value under the prescribed memory budget. Let us also denote the
computational overhead for the strategy based on the same sequence by T (
{

By the design of the canonical strategy, the memory consumption shall reach its peak during the
backward computation. The breakdown of the memory consumption of the backward computation
for Vi is as follows (Figure 2-(f)):

L1 ≺

Lk

. . .

≺

).

\

}

(1)

(i) Cache for the forward computations up to Vi requires M (Ui−1).
(ii) Altogether, the forward part and the backward part for Vi requires 2M (Vi).
(iii) In backpropagation, we need to take into account the set of nodes in Vj with j > i that are

dependent on the nodes in Vi. Such nodes require M (δ+(Li)

Li).

(iv) When there are edges from v1, v2, v3 to h, one might have to use the forward cache dedi-

cated to v1 and v2 for the gradient at v3. Such case will require M (δ−(δ+(Li))

Li).

\

\

In total, the backpropagation for the nodes in Vi requires
(i) := M (Ui−1) + 2M (Vi) + M (δ+(Li)

M

Li) + M (δ−(δ+(Li))

Li).

(2)

\

\

(i).
The peak memory consumption of the canonical strategy is therefore given by maxi=1,...,k
Again, this is a value that is solely determined by the sequence of the lower sets used in its evaluation.

M

4

Let us, therefore, use the notation M (
{
L1 ≺
the canonical strategy based on

Lk
. . .
≺
Lk = V
Deﬁnition (General Recomputation Problem). Given a neural network with graph representation
Lk =
G = (V, E) and a prescribed memory budget B, ﬁnd the increasing sequence
B.
L1 ≺
V
)
If B is too small, there can be no solution to this problem.

) to denote the peak memory consumption of
}
. We can formalize our problem as follows:
}

of lower sets that minimizes T (
{

) while satisfying M (
{

L1 ≺
. . .
≺

L1 ≺
. . .
≺

{
L1 ≺

≺
Lk
}

Lk

. . .

. . .

≤

≺

}

}

{

After solving the general recomputation problem, one can aim to further improve the efﬁciency by
executing the obtained strategy with popular heuristics like liveness analysis [1].

In order to solve the general recompuation problem in practice, we need a relative value of Tv for
each node v in the network. We can either directly measure Tv with some time scale or use some
form of approximation. In general, convolutional node tends to be heavier than other types of node
in terms of computational cost. In our experiment, we therefore set Tv = 10 for convolutional node,
and Tv = 1 for all other types of node.

4 Methods

In this section, we will provide three ways to solve the general recomputation problem: (1) a naive
exhaustive search, (2) an exact dynamic programming (DP) algorithm, and (3) an approximate DP
algorithm that can be used to obtain a near-optimal canocnical strategy fast. We also present a
memotry-centric strategy, which prioritizes the reduction of peak memory consumption over the
computational overhead.

4.1 Naive Approach: An Exhaustive Search

}

}

≤

≺

≺

. . .

Lk

. . .
. . .

Lk = V

L1 ≺

Let us ﬁrst consider a rudimentary exhaustive search algorithm based on depth-ﬁrst search (DFS).
G is ﬁnite, one can ﬁnd a solution to the general recom-
Because the number of all lower sets #
L
#V
G ) by simply computing M (
. . .
putation problem in time of order O(#
) and
{
L
T (
L1 ≺
. . . Lk
L1 ≺
. For further discussion, let us
) for every sequence
}
{
{
Lk
extend the deﬁnition of M (
L1 ≺
) for the preﬁx of the lower set sequence using Equa-
}
≺
{
(j). We can similarly deﬁne the
Li
L1 ≺
k, let M (
tion (2). For i
) := maxj=1,...,i
}
≺
{
L1 ≺
computational overhead using Equation (1). Let T (
{

\
L1 ≺
Starting from an arbitrary lower set L1, the DFS proceeds by recursively visiting
}
B. From the deﬁnition above, we can
L1 ≺
. . .
Li
such that M (
from
)
≤
}
{
≺
{
deduce T (
Vi
)+T
Li−1}
L1 ≺
. Thus, we can compute the
(cid:0)
{
computational overhead incrementally. The memory consumption can be computed incrementally
as well. In Equation (2), the only term that depends on the entire sequence
is
M (Ui−1), and all other terms can be computed directly from Li and Vi = Li
A keen reader might have noticed at this point that there is no need to retain the information of the
entire traversed path
in this DFS algorithm. Instead, it sufﬁces to keep track of the
triplets (L, t, m) where L = Li, t = T (
{

. . .
Li−1}
≺
Li
. . .
L1 ≺
≺

i
j=1 T (Vj
P

), and m = M (Ui).

L1 ≺
. . .
≺

) = T (
{

L1 ≺
Li−1.

∂(Li)
(cid:1)

L1 ≺

L1 ≺

∂(Lj)).

M
Li

) :=

{
\

. . .

. . .

. . .

. . .

. . .

Li

Li

Li

Li

≺

≺

≺

≺

≺

}

{

}

{

\

}

}

}

4.2 Exact DP Algorithm

We can use the triplet representation (L, t, m) used in the previous subsection to solve the general
recomputation problem with DP. Algorithm 1 in the Appendix summarizes our DP procedure.

S

be the set of all triplet states that can be visited during the DFS previously mentioned. Our
such that

Let
DP solution takes advantage of the fact that, if there exist (L, t, m) and (L, t, m′)
m < m′, we do not need to consider (L, t, m′) for further exploration.
Let us deﬁne an array opt with entries opt[L, t]
, where
N
∈
opt[L, t] :=
. This will serve as our DP table.
, 0] = 0, we ﬁll the DP table by visiting the states that satisfy
Starting with the initial condition opt[
the prescribed memory budget constraint. More precisely, if opt[L, t] is the current entry, the algo-
∂(L′)).
rithm loops over the set of all L′ such that L ( L′ and update opt[L′, t′] with t′ = t+T (V ′

:= min
{
whenever there is no m such that (L, t, m)
∈ S

(L, t, m)

∈ S}

∈ S

∞

m

∅

|

\

5

)

}

≺

≤

≺

Li

Li

. . .

Lk = V

. . .
) = t.

}
≺
If t∗ := min

L1 ≺
{
Li
}

L1 ≺
, it means that there is no solution.

with Li =
. . .
t0 |
. . .
, we can report t∗ as the minimum possible computational overhead in the

If opt[L, t] <
after its evaluation, it means that there exists
∞
L such that M (
L1 ≺
B and T (
{
{
opt[V, t0] <
<
∞
∞}
general recomputation problem. Conversely, if t∗ =
For the sake of argument, let us assume for now that t∗ <
. In order to obtain the choice of
∞
that achieves t∗, we need modify the algorithm slightly by making a room
L1 ≺
{
for another variable optarg[L′, t′] that stores the pointer to the L that precedes L′ in the sequence
of the recursive computation that leads to opt[L′, t′]. The optimal sequence of the lower sets can
thus be obtained by starting from opt[V, t∗] and tracing back the pointers of optarg.
Since the bottleneck of our DP algorithm is its iteration process, the computational time of our
2). From a practical perspective, it is worth noting that the use of a
algorithm is O(T (V )
sparse table for opt reduces the computation time by a large constant factor. Further, when t < t′
and opt[L, t] < opt[L, t′], we can skip the iteration for the entry opt[L, t′].

∞

#

≺

L

}

{

G

·

4.3 Approximate DP Algorithm

The DP algorithm we described in the previous section can be used to obtain the optimal canonical
strategy. However, the computation time required for the DP algorithm grows with #
G, which can
be very large for models with complex network structures. We also present an algorithm that can be
used to obtain a near-optimal canonical strategy fast. We shall emphasize here that any canonical
strategy is a legitimate recomputation strategy in the sense that it never alters the network output.

L

The modiﬁcation we would make to the DP algorithm is simple. Instead of making keys for all
members of

G in the DP table, we use a good small subset of

G at every cell.

L

L

Let us say that v is reachable from w if v = w or if there exists a path from w to v. Now, let us
V
. By deﬁnition,
deﬁne
= #V . Our approximate DP algorithm makes keys for the members

v is reachable from w

:=
{
G and #

where Lv :=

Pruned
G

v
∈
|
Pruned
G

Lv

w

∈

V

}

{

|

only. This way, we can keep the computation time under O(T (V )

L

L
Pruned
G
L
of

Pruned
G

⊆ L

L

}
#V 2).

·

Indeed, this modiﬁcation excludes some possibilities from the search pool, and we can no longer
guarantee that we will be able to ﬁnd the best canonical strategy. As we will show in our experiments,
however, near-optimal solutions obtained from our approximate DP are often “good enough” in
practice at reducing the peak memory consumption.

4.4 Memory-Centric Strategy

Liveness analysis [1] is a heuristic technique that has an effect on the reduction of peak memory
consumption, and much of a given strategy’s performance in terms of memory consumption depends
on how well this technique works in the corresponding sequence of lower sets. Experimentally,
liveness analysis tends to work well when the node-set is partitioned coarsely; that is, when each Vi
is large. Through trial and error, we discovered that we can realize this coarse partition intentionally
by using a strategy with long computational overhead. In fact, a canonical strategy with maximal
computational overhead tends to have exceptionally low peak memory consumption. Given a ﬁxed
budget constraint B, the canonical strategy with maximal computational overhead can again be
found using DP.

In general, we can ﬁnd a more economical strategy by setting the budget constraint B to a smaller
value. If the reduction of the memory consumption is the ﬁrst priority, one may set B to the lowest
value for which the set of canonical strategies is non-empty. We call the strategy found this way
a memory-centric strategy, because it prioritizes the positive effect of liveness analysis over the
computational overhead. The computational overhead of memory-centric strategy is bounded by the
computation time for one round of the forward computation. We discriminate this strategy from the
optimal canonical strategy in the previous section by calling the latter a time-centric strategy.

In the application of our methods, we recommend trying the time-centric strategy ﬁrst so that the
computational overhead is minimized as possible. If the solution to the time-centric strategy does
not exist, then try the memory-centric strategy next to prioritize memory reduction.

6

Table 1: The comparison of peak memory consumption. Each value inside the parenthesis is the
achieved memory reduction from the vanilla computation.

Network

ApproxDP + MC

ApproxDP + TC

ExactDP + MC

ExactDP + TC

Chen’s [2]

Vanilla #V

Batch

PSPNet
U-Net
ResNet50
ResNet152
VGG19
DenseNet161
GoogLeNet

2.7 GB (-71%)
5.0 GB (-45%)
3.4 GB (-62%)
2.3 GB (-75%)
4.5 GB (-36%)
1.6 GB (-81%)
5.2 GB (-39%)

3.1 GB (-67%)
6.7 GB (-26%)
4.4 GB (-51%)
2.5 GB (-73%)
5.5 GB (-22%)
1.9 GB (-78%)
5.5 GB (-36%)

2.8 GB (-70%)
4.7 GB (-48%)
3.4 GB (-62%)
2.3 GB (-75%)
4.5 GB (-36%)
1.7 GB (-80%)
5.2 GB (-39%)

3.2 GB (-66%)
5.3 GB (-42%)
4.3 GB (-51%)
2.5 GB (-73%)
5.5 GB (-22%)
1.8 GB (-78%)
5.9 GB (-31%)

4.0 GB (-58%)
7.4 GB (-18%)
3.7 GB (-59%)
2.4 GB (-74%)
4.7 GB (-34%)
1.8 GB (-79%)
6.5 GB (-24%)

9.4 GB
9.1 GB
8.9 GB
9.2 GB
7.0 GB
8.5 GB
8.5 GB

385
60
176
516
46
568
134

2
8
96
48
64
32
256

Figure 3: The tradeoff between batch size and total runtime (forward and backward propagation).

5 Experiments

We applied our algorithm to various network structures and investigated their performance in terms
of computational overhead and peak memory consumption. All networks were implemented in
Chainer [17], and the experiments were conducted on NVIDIA Tesla K40c with GPU DRAM of
size 11.4 GB. The following are the list of networks on which applied our method: ResNet [5],
VGG [15], DenseNet [6], GoogLeNet [16], U-Net [14], and PSPNet [19]. Input dimensions were
set to 572

224 for all other networks.

713 for PSPNet, and 224

572 for U-Net, 713

×

×

×

We compare our method against two methods: (1) vanilla implementation of the forward and back-
ward propagation without any recomputation methods, and (2) Chen’s algorithm implemented with
the use of their heuristic techniques. We tested both our method and Chen’s method with the liveness
analysis. In the Appendix, we present an ablation study for the effect of liveness analysis.

5.1 Memory Reduction

Table 1 summarizes the performance of various methods evaluated in terms of the size of the
achieved memory consumption. ExactDP and ApproxDP are algorithms in Section 4.2 and 4.3,
respectively. MC stands for memory-centric strategy in Section 4.4 and TC stands for time-centric
strategy. The peak memory consumption enlisted in this table includes the memory used by the
model parameters itself. Each value inside the parenthesis designates the proportional size of the
achieved memory reduction relative to the peak memory consumption of the vanilla run. For each
experiment, we selected a batch size so that the memory consumption with vanilla run roughly falls
9 GB. For the memory budget B to be used for our approach, we chose the
in the range of 7
∼
minimal value B for which the solution of the general recomputation problem exists. This value
was determined using binary search.

As we can conﬁrm on the table, our method outperforms the previous method in terms of the peak
memory consumption. In DenseNet and PSPNet, we are succeeding in reducing the memory con-

7

sumption respectively by 81% and 71%. Our method is performing better than Chen’s algorithm
particularly for complex networks like PSPNet, U-Net, and GoogLeNet. The approximate DP was
siginiﬁcantly faster to complete than the exact DP algorithm. The exact DP algorithm required more
than 80 secs to complete for GoogLeNet and PSPNet, while the approximate DP completed within
1 sec for all networks. We would like to emphasize that, for all cases we considered, the exact solu-
tion and approximate solution did not differ much in terms of performance. This is to say that the
“near-optimal” canonical strategy obtained from the Approximate DP is literally “near” optimal for
complex networks commonly used in applications, and that it can be used reliably in practice. 2

5.2 Computational Time

We investigated the memory-runtime tradeoff for our algorithm by running the experiments with
various batch sizes and compared the results against other methods. For all the experiments of
recomputation methods, we used batch sizes that are so large that the naive vanilla computation is
impossible. For each choice of batch size, we repeated each experiment four times and reported
their average runtime.

Figure 3 illustrates the results of this set of experiments. Blue curves are the results obtained from
the vanilla computation. Dotted blue lines are the linear extrapolation of the vanilla translation
results that one might have been able to obtain if there was more memory in the device. Red curves
and orange curves respectively designate the results of “ApproxDP + time-centric” and “ApproxDP
+ memory-centric” settings. Green curves represent the method obtained by Chen’s algorithm. As
we can see in the plot, our method outperforms Chen’s method by a large margin in terms of the
runtime-memory tradeoff. In terms of the runtime that will be required to conduct an experiment
on ResNet152 with the batch size that is double the maximum batch size that can be used in vanilla
computation, our method was 1.16 times faster than Chen’s algorithm. This results also verify
that our algorithm indeed seeks a strategy with small computation overhead in presence of ample
memory. For PSPNet, our method was able to increase the maximum possible batch size from 2 to
8.

6 Related Work

There are various other methods of reducing the memory consumption that do not belong to the
family of recomputation methods. Precision reduction [10] reduces the memory consumption by
regulating the precision level of the computations. Network pruning [9], on the other hand, takes
the approach of compressing the network itself. These methods are fundamentally different from
recomputation methods in that they compromise the accuracy of the output in favor of efﬁcient
memory usage, because the name recomputation refers to a family of methods that does not alter the
output of the network in anyway whatsoever. At the same time, however, the methods we mentioned
above can be used in combination with recomputation methods if the reduction of the peak memory
consumption is the ﬁrst priority. Another method is to selectively transfer some part of the memory
between CPU and GPU during the training process. Superneurons [18], vDNN [13], and LayRub [8]
are the variants of this approach. This method, too, can be used in combination with recomputation
methods.

Some other methods specialize in the memory consumption reduction for a speciﬁc family of net-
works. Pleiss et al. [12] developed a method that specializes in DenseNet by storing multiple feature
maps at a shared memory location. Gruslys et al. [4] developed a method that specializes in RNNs.

Chen et al. [2] developed a recomputation framework for a family of graphs that can be divided into
segments and provided a set of heuristic rules that can be used to extend the framework to select
networks like LSTM and ResNet. Our framework is more general and powerful than Chen’s frame-
work. Chen’s framework does not take the computational overhead into account. In contrast, our
method is based on a formalized tradeoff relationship between memory usage and the computational
overhead and makes a search on a wider space of recomputation strategies than Chen’s method.

2 For some networks, the approximate DP yielded slightly better results than the exact DP because the effect
of the liveness analysis is not taken into the account in the deﬁnition of optimality in the general recomputation
problem.

8

7 Conclusion

In this study, we proposed a new framework of recomputation method that can be applied to neural
networks of any type and formulated the goal of the recomputation in the language of graph theory.
Our framework enables much simpler treatment of the recomputation problem and possibly opens
the door to complex methods with more sophisticated machinery of discrete mathematics. Also, in
this study, we only considered a set of strategies that allows at most one recomputation per node.
One possible future studies include the extension of our current formulation to strategies that allows
multiple recomputations per node. While even more challenging, this future study may lead to even
more efﬁcient recomputation methods for neural networks.

References

[1] Andrew W. Appel and Jens Palsberg. Modern Compiler Implementation in Java. Cambridge

University Press, 2002.

[2] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear

memory cost. arXiv preprint, arXiv:1604.06174, 2016.

[3] Brian A Davey and Hilary A Priestley. Introduction to lattices and order. Cambridge Univer-

sity Press, 2002.

[4] Audrunas Gruslys, Rémi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-
efﬁcient backpropagation through time. In Advances in Neural Information Processing Systems
(NIPS), pages 4125–4133, 2016.

[5] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
770–778, 2016.

[6] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely con-
nected convolutional networks. In IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pages 2261–2269, 2017.

[7] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating deep network training
by reducing internal covariate shift. In International Conference on Machine Learning (ICML),
pages 448–456, 2015.

[8] Hai Jin, Bo Liu, Wenbin Jiang, Yang Ma, Xuanhua Shi, Bingsheng He, and Shaofeng Zhao.
Layer-centric memory reuse and data migration for extreme-scale deep learning on many-core
architectures. ACM Transactions on Architecture and Code Optimization, 15(3):37, 2018.

[9] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. ThiNet: a ﬁlter level pruning method for deep
neural network compression. In IEEE International Conference on Computer Vision (ICCV),
pages 5068–5076, 2017.

[10] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David
García, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu.
Mixed precision training. In International Conference on Learning Representations (ICLR),
2018.

[11] Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu Zhang, Kai Jia, Gang Yu, and Jian
Sun. MegDet: A large mini-batch object detector. In IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pages 6181–6189, 2018.

[12] Geoff Pleiss, Danlu Chen, Gao Huang, Tongcheng Li, Laurens van der Maaten, and
arXiv preprint,

Kilian Q Weinberger. Memory-efﬁcient implementation of densenets.
arXiv:1707.06990, 2017.

[13] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulﬁqar, and Stephen W Keckler.
vDNN: Virtualized deep neural networks for scalable, memory-efﬁcient neural network design.
In IEEE/ACM International Symposium on Microarchitecture, page 18, 2016.

[14] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for
biomedical image segmentation. In Medical Image Computing and Computer-Assisted Inter-
vention, pages 234–241, 2015.

9

[15] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale

image recognition. In International Conference on Learning Representations (ICLR), 2015.

[16] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov,
Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions.
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1–9, 2015.

[17] Seiya Tokui, Kenta Oono, Shohei Hido, and Justin Clayton. Chainer: a next-generation open
source framework for deep learning. In Proceedings of Workshop on Machine Learning Sys-
tems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing
Systems (NIPS-W), 2015.

[18] Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen Leon Song, Zenglin Xu,
and Tim Kraska. Superneurons: Dynamic GPU memory management for training deep neural
networks. In ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming,
pages 41–53, 2018.

[19] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene
parsing network. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pages 6230–6239, 2017.

10

Appendix A Pseudo-Code of Dynamic Programming Algorithm

We describe below the pseudo-code of the DP algorithm presented in Section 4. In the exact DP
to the set of all lower sets
algorithm (Section 4.2), we need to set the input family of lower sets
Pruned
.
G

G. In the approximate DP algorithm (Section 4.3), on the other hand, we need to set

L
We can obtain Memory centric strategy in Section 4.4 just by replacing min with max at line 15 in
Algorithm 1.

to

L

L

L

Algorithm 1: Dynamic programming algorithm.
input
output :Increasing sequence of lower sets

:Computational graph G = (V, E), memory budget B, and family of lower sets

L

7

8

9

10

11

12

# Init DP array
. (L

∈ L

∞
, 0] := 0.

, t = 0, . . . , T (V ))

1 opt[L, t] :=
2 opt[
∅
3 for L
∈ L
for L′
4
∈ L
V ′ := L′
:= opt[L, t] + 2M (V ′) + M (δ+(L′)

in ascending order of their set size do

s.t. L ( L′ and t = 0, . . . , T (V ) do

L.

\

5

6

M
if

M

> B then

continue # Memory constraint
∂(L′)).

t′ := t + T (V ′
m′ := opt[L, t] + M (∂(L′)
if opt[L′, t′] > m′ then

L).

\

\

# Update DP array
opt[L′, t′] := m′.
optarg[L′, t′] := (L, t).

16

opt[V, t0] <

13
14 if there exists t0 such that opt[V, t0] <
t∗ := min
t0 |
15
{
Compute increasing sequence
reverse order.
L1 ≺
return
return Impossible

17
18 else
19

∞}
{

Lk

. . .

≺

}

{

∞
.
L1 ≺

then

L′)) + M (δ−(δ+(L′))

L′).

\

\

. . .

Lk

}

≺

by traversing optarg from (V, t∗) in

Appendix B Conﬁguration on Chen’s Algorithm

In Chen’s work, the procedure of topological-order in Algorithm 2 (Memory Optimized Gradient
Graph Construction) and the deﬁnition of “candidate stage splitting points C” in Algorithm 3 (Mem-
ory Planning with Budget) are not clearly deﬁned. In our experiments, we calculated the topological-
order by performing DFS on the computation graph. Meanwhile, we deﬁne C to be a set v of nodes
such that the removal of v makes the graph disconnected, and calculated its value by enumerating
the articulation points in the computation graph.

Appendix C Memory Consumption without Liveness Analysis

As an ablation study, we examined the memory consumption of both our algorithm and Chen’s
algorithm without liveness analysis. The result is shown in Table 2.

Our algorithm without liveness analysis worked much better than Chen’s algorithm without liveness
analysis. For example, our algorithm could reduce around 57% of memory in PSPNet, while Chen’s
algorithm reduced only 13%. Both Chen’s algorithm and our algorithm, however, worked more
poorly without liveness analysis than those with liveness analysis. Because we designed memory-
centric strategy to be used with liveness analysis, the memory reduction with memory-centric strat-
egy without liveness analysis was mediocre.

11

Table 2: The memory consumption without liveness analysis.

Network

ApproxDP + MC

ApproxDP + TC

ExactDP + MC

ExactDP + TC

Chen’s [2]

Vanilla

PSPNet
U-Net
ResNet50
ResNet152
VGG19
DenseNet161
GoogLeNet

3.2 GB (-57%)
6.8 GB (-21%)
4.7 GB (-42%)
2.8 GB (-61%)
5.5 GB (-34%)
1.9 GB (-70%)
6.0 GB (-29%)

3.3 GB (-56%)
6.8 GB (-21%)
4.4 GB (-45%)
2.8 GB (-61%)
5.5 GB (-34%)
2.0 GB (-69%)
6.0 GB (-29%)

3.3 GB (-56%)
6.8 GB (-21%)
4.7 GB (-42%)
2.8 GB (-61%)
5.5 GB (-34%)
1.9 GB (-70%)
6.0 GB (-29%)

3.3 GB (-56%)
6.8 GB (-21%)
4.4 GB (-45%)
2.8 GB (-61%)
5.5 GB (-34%)
2.0 GB (-69%)
6.0 GB (-29%)

7.6 GB (-13%)
>=11.4 GB
6.7 GB (-22%)
4.1 GB (-48%)
6.3 GB (-26%)
3.4 GB (-55%)
>=11.4 GB

9.4 GB
9.1 GB
8.9 GB
9.2 GB
7.0 GB
8.5 GB
8.5 GB

For PSPNet, the approximate DP yielded slightly better results than the exact DP. This is because
a small amount of memory that will be temporarily allocated during the computation of each node
(e.g., convolutional node) is not taken into the account in our deﬁnition of the memory cost Mv and
thus such a memory allocation may slightly change the peak memory consumption from our expec-
tation. The actual difference between “ApproxDP+MC” and “ExactDP+MC” of the peak memory
consumption in PSPNet was 67 MB.

We observed that Chen’s algorithm without liveness analysis worked worse than vanilla run in U-Net
and GoogLeNet. This is because the vanilla run of Chainer conducts some local memory reduction
by default. For example, for a pair of functions of the form “h=conv(x); a=relu(h)” at an activation
layer of the network, the vanilla run of Chainer releases h after the computation of a if h is not called
from other functions. Meanwhile, Chen’s algorithm without liveness analysis does not perform
such a local-level optimization. Thus, it is possible that some recomputation algorithms without
liveness analysis perform worse than the vanilla run with a local-level optimization for some network
architectures.

12

