1
2
0
2

r
a

M
3

]

G
L
.
s
c
[

2
v
1
4
6
8
0
.
0
1
0
2
:
v
i
X
r
a

Deep Neural Dynamic Bayesian Networks
applied to EEG sleep spindles modeling(cid:63)

Carlos A. Loza[0000−0002−0509−5104] and Laura L. Colgin[0000−0002−4853−8913]

Department of Neuroscience, Center for Learning and Memory,
The University of Texas at Austin
carlos.loza@utexas.edu, colgin@mail.clm.utexas.edu

Abstract. We propose a generative model for single–channel EEG that
incorporates the constraints experts actively enforce during visual scor-
ing. The framework takes the form of a dynamic Bayesian network with
depth in both the latent variables and the observation likelihoods—while
the hidden variables control the durations, state transitions, and robust-
ness, the observation architectures parameterize Normal–Gamma distri-
butions. The resulting model allows for time series segmentation into
local, reoccurring dynamical regimes by exploiting probabilistic models
and deep learning. Unlike typical detectors, our model takes the raw
data (up to resampling) without pre–processing (e.g., ﬁltering, window-
ing, thresholding) or post–processing (e.g., event merging). This not only
makes the model appealing to real–time applications, but it also yields
interpretable hyperparameters that are analogous to known clinical cri-
teria. We derive algorithms for exact, tractable inference as a special case
of Generalized Expectation Maximization via dynamic programming and
backpropagation. We validate the model on three public datasets and
provide support that more complex models are able to surpass state–of–
the–art detectors while being transparent, auditable, and generalizable.

1

Introduction

Sleep spindles are a hallmark of stage 2 non–REM sleep. They are the result of
interactions between GABAergic reticular neurons and excitatory thalamic cells
[30]. Their proposed functions include memory consolidation [29], cortical devel-
opment, and potential biomarkers for psychiatric disorders [10]. Hence, proper
detection and modeling using Electroencephalogram (EEG) are crucial.

A sleep spindle is deﬁned as an oscillatory burst in the range 11–15 Hz
(sigma band) with duration between 0.5 and 2 s., waxing–waning envelope, and
maximal in amplitude in central EEG electrodes. EEGers usually adhere to
clinical manuals [28, 13, 22] to visually categorize multi–channel EEG traces into
sleep stages. They are also trained to identify non–brain–related artifacts (e.g.,
eye and muscle activity). Currently, the large amount of data calls for machine
learning techniques to guide principled, human–like automatic EEG scorers.

(cid:63) Supported by NSF CAREER grant 1453756. All code and data can be accessed at

https://github.com/carlosloza/DNDBN

 
 
 
 
 
 
2

C. Loza and L. Colgin

Most automatic sleep spindles detectors comprise four stages: pre–processing,
decomposition, decision making, and feature extraction [1]. The ﬁrst stage usu-
ally involves bandpassing and artifact rejection, while the second stage applies
wavelets or other windowing techniques for quasi–stationary processes. Decision
making takes the form of hard–thresholding plus cross–validation, and lastly,
feature extraction characterizes the events for further analysis. A myriad of ap-
proaches use slight variations of this pipeline [4, 34, 20, 24] (reviews in [1], [33]).
Even though this methodology provides adequate results, it suﬀers from three
main drawbacks: i) analysis based on ﬁltered EEG, ii) lack of theoretical founda-
tions for hyperparameter settings, and iii) reported results are often in–sample
predictions, which is prone to overﬁtting. The ﬁrst point not only diﬀers from
human–like scoring, it is also computationally expensive for online applications.
The latter points are consequences of a model–less framework; that is, amplitude
thresholds are data (and scorer) dependent, which severely limits generalization.
Instead, we propose a generative model with the following components: i) ro-
bustness against artifacts via observations modeled as conditional heteroscedas-
tic non–linear generalized t likelihoods, ii) high–capacity deep learning architec-
tures that parameterize such likelihoods, iii) reoccurring modes with distinctive
dynamics that characterize non–spindle and spindle regimes, iv) semi–Markovian
states that generalize the geometric regime durations of Hidden Markov Mod-
els (HMM), v) tractable exact inference via message passing routines (unlike
variational approximations [14, 8, 18, 7]), and vi) uncertainty quantiﬁcation via
posterior probabilities. The result is a model that combines probabilistic mod-
eling with deep learning in an eﬀort to mimic the constraints EEGers actively
enforce when visually scoring EEG: a Deep Neural Dynamic Bayesian Network.
We validate the model on three datasets from the DREAMS database [3] and
achieve comparable or better performance than the state–of–the–art. The results
open the door to more complex models and sophisticated inference in the future
(e.g., real–time detection). The paper continues as follows: Section 2 details the
model, Section 3 presents the results, and Section 4 concludes the paper.

2 Generative model for EEG

Let y = {yn}N
n=1 be an ordered collection of N observed random variables over
time. For our case, y is a single–channel, single–trial EEG trace embedded into
a dynamical system with K discrete states or labels (non–spindle and spindle for
our case). States are represented by a K–dimensional multinomial zn. Addition-
ally, the D–dimensional multinomial, dn, is a counter variable that keeps track
of the remaining duration of the current mode. Given a regime k, we pose each
yn as a Normal random variable with location, µn,k, and scale, σn,k, parameter-
ized by functions (either linear or non–linear) of the previous p samples, yn−1
n−p,
where p is the autoregressive order of the model. In particular, σn,k is modulated
by the conditionally Gamma distributed hidden variable, τn, that accounts for
robustness. Likewise, the parameter of the Gamma distribution, νn,k, depends
on yn−1
n−p. In short, given a state k, τn is marginally Gamma distributed and yn is

Deep Neural Dynamic Bayesian Networks applied to EEG sleep spindles

3

Fig. 1: Left. Generative model for EEG. Observations are shaded. Right. Deep
networks for Normal–Gamma models (colors paired with graphical model).

conditionally Normal given τn; thus, yn is marginally generalized t distributed:

τn|zn, yn−1

n−p; {θν,k}K

k=1 ∼ Gamma

yn|τn, zn, yn−1

n−p; {θµ,k}K

k=1, {θσ,k}K

k=1 ∼ N

µn,k,

(cid:18)

(cid:16) νn,k
2
σn,k√
τn

νn,k
2

,

(cid:19)

(cid:17)

(1)

(2)

n−p), Gθσ,k (yn−1

n−p), Hθν,k (yn−1

where θµ,k, θσ,k, and θν,k parameterize a deep neural network with dedicated
architectures, Fθµ,k (yn−1
n−p), for µn,k, σn,k, and νn,k, re-
spectively (Fig. 1). The deep observation models allow for rich representations,
whereas the deep 3–level hidden state accounts for robustness and regime dura-
tions beyond the usual geometric paradigm of HMM. Lastly, the autoregressive
structure is key to capture local temporal dependencies (e.g., sleep spindles).
The result is the Deep Neural Dynamic Bayesian Network (DNDBN) of Fig. 1.
We use a 1-of-K notation for zn and dn (e.g., zn,k means the k–th entry of zn
n=2 obey semi–Markovian dynamics:

is 1). z1 is parameterized by π, while {zn}N

p(zn|zn−1, dn−1; A) =

(cid:26) δ(zn, zn−1)

p(zn|zn−1; A)

, dn−1,1 > 1
, dn−1,1 = 1

(3)

where A is a K × K stochastic matrix, and δ(·, ·) is the Kronecker delta function.
In words, the hidden label, zn, transitions to a diﬀerent regime only when the
counter variable is equal to 1; otherwise, it remains in the same mode.

Similarly, dn samples a new duration only after a regime ends; otherwise, it
decreases by one (i.e., an explicit duration hidden semi–Markov model [9, 35]).

p(dn|zn, dn−1; {λ}K

k=1) =

(cid:26) δ(dn,j−1, dn−1,j)
p(dn|zn,k; {λ}K

k=1) = p(dn; λk)

, dn−1,1 > 1
, dn−1,1 = 1

(4)

where {λ}K

k=1 is a collection of parameters for each regime durations.

Consequently, we can write the complete data log–likelihood over parameters,

θ = {λ, A,π, {θµ,k}K

k=1, {θσ,k}K

k=1, {θν,k}K

k=1}, as (ignoring constants):

KKλkπd1d2d3z1z2z3Aτ1τ2τ3y1y2y3θν,kθσ,kθµ,kyn−1n−pp1ReLU128ReLU128ReLU128ReLU1281291softplus1301softplusµn,kσn,kνn,kLegend:fmfullyconnected(fc)layerwithmneuronsfollowedbyactivationfunctionfmfullyconnectedlayerwithmneurons,noactivationfunction4

C. Loza and L. Colgin

log p(y, τ , z, d|θ) =

K
(cid:88)

k=1

z1,k log πk +

D
(cid:88)

K
(cid:88)

i=1

k=1

z1,kd1,i log p(d1; λk)+

N
(cid:88)

K
(cid:88)

K
(cid:88)

n=2

k=1

j=1

zn−1,jzn,kdn−1,1 log Aj,k +

N
(cid:88)

K
(cid:88)

n=2

k=1

zn,kdn−1,1 log p(dn; λk)+

(5)

N
(cid:88)

K
(cid:88)

n=1

k=1

zn,k

(cid:26)

Γ ( νn,k+1
)
2
√
Γ ( νn,k
πνn,kσn,k
2 )

(cid:18)

1 +

1
νn,k

(cid:16) yn − µn,k
σn,k

(cid:17)2(cid:19)−(cid:0) νn,k +1

2

(cid:1)

(cid:27)

where Γ (·) is the gamma function, Aj,k is the j–th row, k–th column entry of A,
and z = {zn}N
n=1 (similar for d and τ ). Three tasks are key: a) given θ, calculate
the marginal log–likelihood (LL) of the observations (i.e., p(y; θ)), b) given y
and θ, estimate the most likely hidden state sequence, and c) given y, estimate
θ. We refer to the tasks as LL calculation, inference, and learning, respectively.
k=1, are re-
duced to single fully–connected layers with no non–linearities. This design choice
not only renders linear autoregressive models (i.e., inﬁnite impulse response ﬁl-
ters), but it also helps with validation while keeping the model more interpretable
without sacriﬁcing performance (as shown in the next sections).

In terms of interpretability, the location networks, {Fθµ,k (yn−1

n−p)}K

2.1 Learning model parameters

Maximizing eq. (5) analytically is intractable; rather, we exploit a variation of
the Generalized Expectation–Maximization (GEM) algorithm for HMM [27]. We
implement coordinate ascent on the log–likelihood: ﬁrst, θ is ﬁxed and we obtain
expectations of log p(y, τ , z, d|θ) under the posterior distribution of the latent
variables given y (underlined variables in eq. (5)). Then, θ is optimized (either
globally or locally) keeping the expectations ﬁxed. This constitutes one GEM
iteration and guarantees an increase of the marginal LL of the training data.
GEM continues until convergence to local optima (tracked by successive LLs).
The factorizations induced by the model yield closed–form solutions of the ex-
pectations without appealing to variational approximations that usually require
recognition networks to compute local potentials [16, 14, 8, 18, 7]. Hence, our pro-
posed learning routine does not suﬀer from the so–called amortization gap [2] nor
“posterior collapse” [12, 5]. Next, for simplicity, we outline the GEM algorithm
for the case of a single training sequence; yet, the batch case is straightforward.
1 , zn, dn) be the joint probability of the ﬁrst n
n+1|zn, dn).

observations and the n–th hidden variables, and similarly βzn,dn = p(yN
Next, by induction and d–separation [17] (no parameters to avoid clutter):

For the E–step, let αzn,dn = p(yn

αzn,dn = p(yn|zn)

(cid:88)

dn−1

p(dn|zn, dn−1)

(cid:88)

zn−1

p(zn|zn−1, dn−1)αzn−1,dn−1

(6)

βzn,dn =

(cid:88)

zn+1

p(yn+1|zn+1)p(zn+1|zn, dn)

(cid:88)

dn+1

p(dn+1|zn+1, dn)βzn+1,dn+1 (7)

Deep Neural Dynamic Bayesian Networks applied to EEG sleep spindles

5

These probabilities are the bedrock of all tasks. Namely, inference replaces the
sums in eq. (6) with “max” operators plus backtracking (i.e., Viterbi algorithm
[31] or maximum a posteriori (MAP) estimate). Likewise, LL calculation reduces
to p(y; θ) = (cid:80)

αzN,dN. Next, by Bayes theorem and d–separation:

(cid:80)

zN

dN

η(zn, dn) = p(zn, dn|y) =

αzn,dn βzn,dn
p(y; θ))

(8)

γ(zn) = p(zn|y) =

η(zn, dn)

(9)

(cid:88)

dn

ξ(zn,k, zn−1, dn) = p(zn,k, zn−1, dn, dn−1,1|y; {λ}K
αzn−1,dn−1,1 p(yn|zn)p(dn; λk)p(zn|zn−1; A)βzn,dn
p(y; θ)

k=1, A) =

(10)

where p(yn|zn) is given by a conditional generalized t distribution. By con-
struction, the entries of zn and dn are binary; hence, their expectations are the
probabilities of taking the value 1 (same color coding as eq. (5)).

For the M–step, we plug in the previous expectations into eq. (5) and optimize
each θ component. The optimal parameters for the discrete hidden variables are
simple global maximizers due to convexity (i.e., partial derivatives equal to 0):

π(t)
k ∝ γ(z1,k)

(11)

A(t)

j,k ∝

N
(cid:88)

(cid:88)

n=2

dn

ξ(zn,k, zn−1,j, dn)

(12)

λ(t)

k,i ∝ η(z1,k, d1,i) +

N
(cid:88)

(cid:88)

n=2

zn−1

ξ(zn,k, zn−1, dn,i)

(13)

where (t) denotes estimates for the t–th GEM iteration. γ(zn,k) = E{zn,k}
(analogous for η and ξ). λ(t)
k,i is the probability of duration i, regime k. The
observation parameters are updated with local optima obtained via stochastic
gradient descent and backpropagation applied to the last line of eq. (5).

3 Results

The DREAMS Sleep Spindles dataset [3] was used to validate the proposed
model. 30–minute–long, single–channel (CZ-A1 or C3-A1) recordings from 8 sub-
jects with corresponding sleep spindles expert scores (2 experts) are available. All
traces were resampled to 50 Hz and z–scored prior to feeding them to the model.
No bandpass ﬁltering, artifact rejection, nor windowing were implemented. The
data was then partitioned into 8 folds of training and test sets (i.e., 7 subjects
for training, 1 for testing). The performance measure is the by–sample Matthews
Correlation Coeﬃcient (MCC) between MAP output and expert scores (on test
set). We detail several model variants due to the modularity of the framework:

6

C. Loza and L. Colgin

a) Supervised setting (S, E1, E2) where the expert labels (either experts union,
expert 1, or expert 2, resp.) are used to ﬁt the model (i.e., no EM needed).

b) Unsupervised setting (U) where no labels are used (i.e, EM required).
c) Hidden Markov (HM) or hidden semi–Markov (HSM) latent state dynamics
(i.e., setting D = 1 and allowing self–transitions reduces the model to HM).
d) “Linear + 2 Non–Linear” or “3 Non–Linear ” depending whether the location

models, {Fθµ,k (yn−1

n−p)}K

k=1, are linear (Fig. 1) or non–linear, respectively.

We set K = 2, p = 5, and D = 1 (HM) or D = 50 × 15 (HSM). We allowed
self–transitions for non–spindles to accommodate long spindle–free intervals. For
the deep models, we used stochastic gradient descent (batch size=32), early–
stopping (75/25 split, patience=3, max epochs=10), and Adam [15] (α = 0.001,
β1 = 0.9, β2 = 0.999) on Python’s tensorﬂow probability [6]. In each fold, a sub-
ject was held–out for validation and the best model out of 3 was kept for testing
(according to MCC or LL for supervised or unsupervised cases, respectively).

Table 1 summarizes MAP results (without any post–processing) compared to
three previous eﬀorts [34, 20, 24]. The supervised–HSM setting has the highest
average MCC, while the unsupervised setting outperforms two of the state–of–
the–art methods. It is worth noting that the authors in [24] use window–based
spectral methods to suppress artifacts in a pre–processing stage and all other
methods in Table 1 bandpass the signal. Therefore, DNDBN not only performs
feature extraction and detection, but it also handles outliers in a principled
manner. All DNDBN types of Table 1 are “Linear + 2 Non–Linear”. Incidentally,
the DNDBN(S, HSM) “3 Non–Linear” variant achieves the same MCC of 0.459.
As a side note, the approach in [26] achieves an average MCC of 0.071.

1

0

For the unsupervised setting, we set sensible initial conditions: i) π = [1, 0](cid:62),
(cid:1) (no self–transitions for sleep spindles), iii) uniform durations for
ii) A = (cid:0) 0.5 0.5
non–spindles and N (1 s., 0.15 s.) for spindles, and iv) initial posterior marginals,
γ(zn), equal to the average of the labels from 8 diﬀerent detectors implemented
in the package wonambi (https://github.com/wonambi-python/wonambi): six
state–of–the–art [10, 21, 23, 19, 32, 20] and two native to wonambi. We refer to
this mixture of experts type of detector as MixDetectors.

Fig. 2 validates the model while illustrating its advantages over classic detec-
tors. DNDBN is able to not only provide binary labels (MAP estimates), but it
also quantiﬁes the uncertainty via the posterior marginal, γ(zn). The model also
provides a proxy for robustness via the expected value of the posterior τn. In par-
ticular, by conjugacy, τn remains Gamma with updated means dependent of the
observation parameters [25]. The ﬁrst 15 seconds of Fig. 2B show EEG plagued
with artifacts, while the rest is clean EEG. Likewise, the posterior τn is able to
track such changes while remaining bounded. Fig. 2C shows the clear spectral
diﬀerences between modes: the non–spindle regime depicts the well–known 1/f
spectral distribution [11] while its counterpart has clear oscillatory activity in
the sigma band. Lastly, the distributions of {νn,k}2
k=1 conﬁrm the expert opin-
ion that most artifacts appeared during non–spindle epochs (i.e, smaller values
imply longer tails of the generalized t distribution). These results highlight the
fact that detection is only one of the many attributes of the model.

Deep Neural Dynamic Bayesian Networks applied to EEG sleep spindles

7

Table 1: MCC metrics (DREAMS Spindles data). Best results marked in bold.

Algorithm

Subject
S5

S1

S3

S2

Wendt et al.
Martin et al.
Parekh et al.

S8 Mean
0.362
0.491 0.407 0.390 0.207 0.497 0.543 0.131 0.230
0.399
0.437 0.548 0.407 0.212 0.514 0.586 0.229 0.259
0.501 0.575 0.540 0.245 0.574 0.624 0.253 0.269 0.447
DNDBN(U, HSM) 0.565 0.483 0.450 0.265 0.549 0.610 0.199 0.233
0.419
DNDBN(S, HSM) 0.588 0.604 0.484 0.295 0.572 0.616 0.259 0.254 0.459

S4

S6

S7

Fig. 2: A. Example of MAP (red) and posterior marginal detections (shaded red,
larger shaded areas imply larger probabilities of a sleep spindle). B. Example
of expected value of τn under the posterior for EEG with and without artifacts.
C. Log frequency response of learned location coeﬃcients, θµ,k. D. Histogram
of learned degrees of freedom, νn,k. SS: sleep spindle, NSS: non–sleep spindle.

Next, diﬀerent local solutions can be compared also. Table 2 details predictive
NLL and shows that the parameters set by expert 2 have stronger predictive
power and follow the EEG dynamics closer (under the model in question). Also
unsurprisingly, the “3 Non–Linear” models outperform the “Linear + 2 Non–
Linear” counterparts. However, if we are interested in detection only, “Linear +
2 Non–Linear” might suﬃce, as previously conﬁrmed by the average MCC.

Fig. 3A compares several models in terms of sleep spindles durations and area
under the precision–recall curve (AUC) computed based on posterior marginals,
γ(zn). An additional fully supervised deep learning detector was implemented as
well (DNN) for benchmarking (6 128 fc layers + ReLU). Fig. 3A illustrates the
interplay between interpretability, performance, and complexity. For instance,
DNN performs poorly in both AUC and durations (no underlying probabilistic
model). MixDetectors performs well, as expected. The HM variants seem to miss
the mark when it comes to durations (see mean values). On the other hand, the
HSM counterparts seem appropriate for both supervised and unsupervised cases,
which suggests that a hidden semi–Markov state is indeed principled.

The DREAMS Subjects and Patients datasets [3] are used for our ﬁnal vali-
dation. The datasets consist of 20 and 27 whole–night recordings (≈ 8−9 hours),
respectively. We train the model (DNDBN(U, HSM), “Linear + 2 Non–Linear”)

0.51.01.52.02.53.03.54.0Time (s.)50050Amplitude (V)A01020Frequency (Hz)10010Magnitude (dB)CNSSSS51015202530Time (s.)2500250Amplitude (V)B1015n,k0.00.51.0ProbabilityDNSSSSExpertNSSSSPredictionDNDBN01|y,zn,2;{n}8

C. Loza and L. Colgin

Table 2: Average predictive negative log–likelihood (NLL). HSM variants.
Observation Model Linear + 2 Non–Linear

3 Non–Linear

Type
NLL

S

E1

E2

U

S

E1

E2

U

39876 40151 35312 39751 38553 38754 34192 38861

Fig. 3: Distribution of sleep spindles durations. A. DREAMS Sleep Spindles
dataset. Mean values marked with triangles. Areas under the precision–recall
curves in red. Out–of–sample results with the exception of MixDetectors case.
B. DREAMS Subjects and Patients datasets (stage N2 events only)

on the DREAMS Sleep Spindles dataset and perform inference on both Subjects
and Patients datasets (one central channel downsampled to 50 Hz). Fig 3B shows
the duration distributions, which resemble previous large–scale studies [33]. In
addition, Table 3 summarizes the sleep spindle rate (by–sample basis) for each
sleep stage (scoring provided in the datasets). As expected, it is evident that
stages N2 have relatively larger sleep spindles densities. Also, there are signif-
icant diﬀerences between datasets, which might be a result of the cohorts (i.e.
Subjects: healthy individuals, Patients: individuals with various pathologies).

4 Conclusion

Probabilistic models over sequences are principled frameworks for robust detec-
tion and parametrization of sleep spindles. Future work will go beyond the sigma
band to characterize and detect high–frequency oscillations in other structures
where ground truth is unavailable, e.g. hippocampal gamma rhythms.

Table 3: Average percentage of putative detected sleep spindles.

Dataset

Subjects
Patients

wake
4.26
3.54

N1
7.01
7.92

Sleep Stage
N2
13.54
9.92

N3
8.65
3.64

REM
6.23
6.32

MixDetectorsDNNDNDBN(S, HM)DNDBD(S, HSM)DNDBN(U, HM)DNDBN(U, HSM)0123Duration (s.)0.420.200.540.550.250.48A0.00.51.01.52.0Duration (s.)01234Probability densityBSubjectsPatientsDeep Neural Dynamic Bayesian Networks applied to EEG sleep spindles

9

References

1. Coppieters’t Wallant, D., Maquet, P., Phillips, C.: Sleep spindles as an electro-
graphic element: description and automatic detection methods. Neural Plasticity
2016 (2016)

2. Cremer, C., Li, X., Duvenaud, D.: Inference suboptimality in variational autoen-
coders. In: International Conference on Machine Learning. pp. 1078–1086. PMLR
(2018)

3. Devuyst, S.: The DREAMS databases and assessment algorithm (Jan 2005),

https://doi.org/10.5281/zenodo.2650142

4. Devuyst, S., Dutoit, T., Stenuit, P., Kerkhofs, M.: Automatic sleep spindles detec-
tion overview and development of a standard proposal assessment method. In: 2011
Annual international conference of the IEEE engineering in medicine and biology
society. pp. 1713–1716. IEEE (2011)

5. Dieng, A.B., Kim, Y., Rush, A.M., Blei, D.M.: Avoiding latent variable collapse
with generative skip models. In: The 22nd International Conference on Artiﬁcial
Intelligence and Statistics. pp. 2397–2405. PMLR (2019)

6. Dillon, J.V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D., Patton,
B., Alemi, A., Hoﬀman, M., Saurous, R.A.: Tensorﬂow distributions. arXiv preprint
arXiv:1711.10604 (2017)

7. Dong, Z., Seybold, B., Murphy, K., Bui, H.: Collapsed amortized variational infer-
ence for switching nonlinear dynamical systems. In: International Conference on
Machine Learning. pp. 2638–2647. PMLR (2020)

8. Ebbers, J., Heymann, J., Drude, L., Glarner, T., Haeb-Umbach, R., Raj, B.: Hidden
markov model variational autoencoder for acoustic unit discovery. In: InterSpeech.
pp. 488–492 (2017)

9. Ferguson, J.: Variable duration models for speech. In: Proc. Symposium on the

Application of Hidden Markov Models to Text and Speech (1980)

10. Ferrarelli, F., Huber, R., Peterson, M.J., Massimini, M., Murphy, M., Riedner,
B.A., Watson, A., Bria, P., Tononi, G.: Reduced sleep spindle activity in schizophre-
nia patients. American Journal of Psychiatry 164(3), 483–492 (2007)

11. Freeman, W., Quiroga, R.Q.: Imaging brain function with EEG: advanced temporal
and spatial analysis of electroencephalographic signals. Springer Science & Business
Media (2012)

12. He, J., Spokoyny, D., Neubig, G., Berg-Kirkpatrick, T.: Lagging inference networks
and posterior collapse in variational autoencoders. arXiv preprint arXiv:1901.05534
(2019)

13. Iber, C.: The AASM manual for the scoring of sleep and associated events: Rules.

Terminology and Technical Speciﬁcation (2007)

14. Johnson, M.J., Duvenaud, D., Wiltschko, A.B., Datta, S.R., Adams, R.P.: Com-
posing graphical models with neural networks for structured representations and
fast inference. In: Proceedings of the 30th International Conference on Neural In-
formation Processing Systems. pp. 2954–2962 (2016)

15. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint

arXiv:1412.6980 (2014)

16. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint

arXiv:1312.6114 (2013)

17. Koller, D., Friedman, N.: Probabilistic graphical models: principles and techniques.

MIT press (2009)

10

C. Loza and L. Colgin

18. Krishnan, R., Shalit, U., Sontag, D.: Structured inference networks for nonlinear
state space models. In: Proceedings of the AAAI Conference on Artiﬁcial Intelli-
gence. vol. 31 (2017)

19. Leclercq, Y., Schrouﬀ, J., Noirhomme, Q., Maquet, P., Phillips, C.: fMRI artefact
rejection and sleep scoring toolbox. Computational intelligence and neuroscience
2011 (2011)

20. Martin, N., Lafortune, M., Godbout, J., Barakat, M., Robillard, R., Poirier, G.,
Bastien, C., Carrier, J.: Topography of age-related changes in sleep spindles. Neu-
robiology of aging 34(2), 468–476 (2013)

21. M¨olle, M., Bergmann, T.O., Marshall, L., Born, J.: Fast and slow spindles dur-
ing the sleep slow oscillation: disparate coalescence and engagement in memory
processing. Sleep 34(10), 1411–1421 (2011)

22. Niedermeyer, E., da Silva, F.L.: Electroencephalography: basic principles, clinical

applications, and related ﬁelds. Lippincott Williams & Wilkins (2005)

23. Nir, Y., Staba, R.J., Andrillon, T., Vyazovskiy, V.V., Cirelli, C., Fried, I., Tononi,
G.: Regional slow waves and spindles in human sleep. Neuron 70(1), 153–169 (2011)
24. Parekh, A., Selesnick, I.W., Rapoport, D.M., Ayappa, I.: Sleep spindle detection
using time-frequency sparsity. In: 2014 IEEE Signal Processing in Medicine and
Biology Symposium (SPMB). pp. 1–6. IEEE (2014)

25. Peel, D., McLachlan, G.J.: Robust mixture modelling using the t distribution.

Statistics and computing 10(4), 339–348 (2000)

26. Penny, W.D., Roberts, S.J.: Dynamic models for nonstationary signal segmenta-

tion. Computers and biomedical research 32(6), 483–502 (1999)

27. Rabiner, L.R.: A tutorial on hidden Markov models and selected applications in

speech recognition. Proceedings of the IEEE 77(2), 257–286 (1989)

28. Rechtschaﬀen, A., Kales, A.: A Manual of Standardized Terminology, Techniques
and Scoring System for Sleep Stages of Human Subjects. Brain Information Ser-
vice/Brain Research Institute (1968)

29. Schabus, M., Gruber, G., Parapatics, S., Sauter, C., Kl¨osch, G., Anderer, P.,
Klimesch, W., Saletu, B., Zeitlhofer, J.: Sleep spindles and their signiﬁcance for
declarative memory consolidation. Sleep 27(8), 1479–1485 (2004)

30. Steriade, M., McCormick, D.A., Sejnowski, T.J.: Thalamocortical oscillations in

the sleeping and aroused brain. Science 262(5134), 679–685 (1993)

31. Viterbi, A.: Error bounds for convolutional codes and an asymptotically opti-
mum decoding algorithm. IEEE transactions on Information Theory 13(2), 260–269
(1967)

32. Wamsley, E.J., Tucker, M.A., Shinn, A.K., Ono, K.E., McKinley, S.K., Ely, A.V.,
Goﬀ, D.C., Stickgold, R., Manoach, D.S.: Reduced sleep spindles and spindle coher-
ence in schizophrenia: mechanisms of impaired memory consolidation? Biological
psychiatry 71(2), 154–161 (2012)

33. Warby, S.C., Wendt, S.L., Welinder, P., Munk, E.G., Carrillo, O., Sorensen, H.B.,
Jennum, P., Peppard, P.E., Perona, P., Mignot, E.: Sleep-spindle detection: crowd-
sourcing and evaluating performance of experts, non-experts and automated meth-
ods. Nature methods 11(4), 385 (2014)

34. Wendt, S.L., Christensen, J.A., Kempfner, J., Leonthin, H.L., Jennum, P.,
Sorensen, H.B.: Validation of a novel automatic sleep spindle detector with high
performance during sleep in middle aged subjects. In: 2012 Annual international
conference of the IEEE engineering in medicine and biology society. pp. 4250–4253.
IEEE (2012)

35. Yu, S.Z.: Hidden semi-markov models. Artiﬁcial intelligence 174(2), 215–243 (2010)

