0
2
0
2

y
a
M
8
2

]
E
S
.
s
c
[

1
v
5
1
0
4
1
.
5
0
0
2
:
v
i
X
r
a

MACER: A Modular Framework for Accelerated Compilation Error
Repair

Darshak Chhatbar∗

Umair Z. Ahmed†

Purushottam Kar∗
∗Indian Institute of Technology Kanpur
†National University of Singapore
{darshak,purushot}@cse.iitk.ac.in, umair@comp.nus.edu.sg

May 29, 2020

Abstract

Automated compilation error repair, the problem of suggesting ﬁxes to buggy programs that fail to
compile, has generated signiﬁcant interest in recent years. Apart from being a tool of general conve-
nience, automated code repair has signiﬁcant pedagogical applications for novice programmers who ﬁnd
compiler error messages cryptic and unhelpful. Existing approaches largely solve this problem using a
blackbox-application of a heavy-duty generative learning technique, such as sequence-to-sequence predic-
tion (TRACER) or reinforcement learning (RLAssist). Although convenient, such black-box application
of learning techniques makes existing approaches bulky in terms of training time, as well as ineﬃcient at
targeting speciﬁc error types.

We present MACER, a novel technique for accelerated error repair based on a modular segregation of
the repair process into repair identiﬁcation and repair application. MACER uses powerful yet inexpensive
discriminative learning techniques such as multi-label classiﬁers and rankers to ﬁrst identify the type of
repair required and then apply the suggested repair.

Experiments indicate that the ﬁne-grained approach adopted by MACER oﬀers not only superior error
correction, but also much faster training and prediction. On a benchmark dataset of 4K buggy programs
collected from actual student submissions, MACER outperforms existing methods by 20% at suggesting
ﬁxes for popular errors that exactly match the ﬁx desired by the student. MACER is also competitive or
better than existing methods at all error types – whether popular or rare. MACER oﬀers a training time
speedup of 2× over TRACER and 800× over RLAssist, and a test time speedup of 2 − 4× over both.

1

Introduction

The ability to code is a staple requirement in science and engineering and programmers rely heavily on
feedback from the programming environment, such as the compiler, linting tools, etc., to correct their
programs. However, given the formal nature of these tools, it is diﬃcult to master their eﬀective use without
extended periods of exposure.

Thus, and especially for beginners, these tools can pose a pedagogical hurdle. This is particularly true of
compiler error messages which, although always formally correct, can often be unhelpful in guiding the novice
programmer on how to correct their error [14]. This is sometimes due to the terse language used in error
messages. For example, see Fig 1 where the error message uses terms such as “speciﬁer” and ”statement”
which may be unfamiliar to a novice.

At other times this is due to the compiler being unable to comprehend the intent of the user. For example,
statements such as 0 = i; (where i is an integer variable) in the C programming language generate an error
informing the programmer that the “expression is not assignable” (such as with the popular LLVM compiler
[13]). The issue here is merely the direction of assignment but the compiler brings in concepts of expressions

1

 
 
 
 
 
 
Figure 1: Two examples of actual repairs carried out by MACER. The erroneous line in the ﬁrst example
requires multiple replacements to repair the error. Speciﬁcally, two occurrences of ’,’ need to be replaced
with ’;’ as indicated by the repair class description. The erroneous line the second example uses incorrect
syntax to check for equality and requires replacing the ’=’ symbol with the ’==’ symbol.

and assignability which may confuse a beginner. At still other times, there are compiler quirks, such as
reporting a missing terminal semicolon as begin present on the next line.

Several of these issues can be irritating, although not debilitating, to more experienced programmers
as well. However, for novices, this frequently means seeking guidance from a human mentor who can then
explain the program repair steps in more accessible terms. Given that educational institutions struggle to
keep up with increasing student strengths, human mentorship is not a scalable solution [5].

Consequently, automated program repair has generated a lot of interest in recent years due to its promising
applications to programming education and training. A tool that can automatically take a program with
compilation errors and suggest repairs to remove those errors can greatly facilitate programming instructors,
apart from being a source of convenience for even seasoned programmers.

In this work we report MACER, a tool for accelerated repair of programs that fail to compile.

2 Related Works

The area of automated program repair has seen much interest in recent years. The DeepFix method [9]
was one of the ﬁrst major eﬀorts at using deep learning techniques such as sequence-to-sequence models to
locate as well as repair errors. The TRACER method [1] proposed segregating this pipeline into repair line
localization and repair prediction and reported better repair performance. This work also introduced the use
of the Pred@k metric to compilation error repair which demands not just elimination of compilation errors
but actually an exact match with the ﬁx desired by the student. This was a much more stringent metric
than the prevailing repair accuracy metric which simply counted reductions in compilation errors.

Recent works have focused on several other aspects of this problem. The RLAssist method [8] introduced
self learning techniques using reinforcement learning to eliminate the need for training data. However, the
technique oﬀers slow training times. The work of [10] proposes to use generative techniques using variational
auto-encoders to introduce diversity in the ﬁxes suggested by the technique. The work of [19] focuses on
locating and repairing a special class of errors called variable-misuse errors which are logical errors where
programmers use an inappropriate identiﬁer, possibly due to confusion in identiﬁer names. The TEGCER
method [2] focuses not on error repair but rather repair demonstration by showing students, ﬁxes made by
other students on similar errors which can be argued to have greater pedagogical utility.

The works DeepFix, RLAssist and TRACER most directly relate to our work and we will be comparing to
all of them experimentally. MACER outperforms all these methods in terms of repair accuracy, exact match
(Pred@k) accuracy, training and prediction time, or all of the above.

3 Our Contributions

MACER makes the following key contributions to compilation error repair

2

MACER:AModularFrameworkforAcceleratedCompilationErrorRepairDarshakChhatbar1[0000−0002−6669−9059],UmairZ.Ahmed2[0000−0002−2203−7301],andPurushottamKar*1[0000−0003−2096−5267]1IndianInstituteofTechnologyKanpur2NationalUniversityofSingapore{darshak,purushot}@cse.iitk.ac.in,umair@comp.nus.edu.sg1voidmain(){2inti,n=5,s=0;3for(i=1,i<n,i++)4s=s+i*(i++)/2;5printf("%d",s);6}1voidmain(){2inti,n=5,s=0;3for(i=1;i<n;i++)4s=s+i*(i++)/2;5printf("%d",s);6}1voidmain(){2inti=0;3if(0=i)4i++;5}1voidmain(){2inti=0;3if(0==i)4i++;5}ErrorMessageE6:expected’;’in’for’statementspeciﬁerErrorMessageE10:expressionisnotassignableRepairClass[E6[,,][;;]](seeSec2fordetails)RepairClass[E10[=][==]](seeSec2fordetails)Fig.1.TwoexamplesofactualrepairsbyMACER.Theerroneouslineintheﬁrstexamplerequiresmultiplereplacementstorepairtheerror(twooccurrencesof’,’needtobereplacedwith’;’).Theerroneouslineinthesecondexampleusesincorrectsyntaxtocheckforequalityandrequiresreplacingthe’=’symbolwiththe’==’symbol.References1. MACER sets up a modular pipeline that, in addition to locating lines that need repair, further segregates
the repair pipeline by identifying what is the type of repair needed on each line (the repair-class of
that line), and where in that line to apply that repair (the repair-proﬁle of that line). This presents
a signiﬁcant departure from previous works like TRACER and DeepFix that rely on a heavy-duty
generative mechanism to perform the last two operations (repair type identiﬁcation and application)
in a single step to directly generate repair suggestions.

2. Although convenient, these generative mechanisms used in previous works come at a cost – not only
are they expensive at training and prediction, but their one-step approach also makes it challenging
to ﬁne tune their method to focus more on certain types of errors than others. We show that MACER
on the other hand, is able to speciﬁcally target certain error types. Speciﬁcally, MACER is able to pay
individual attention to each repair class to oﬀer superior error repair.

3. MACER introduces techniques used in large-scale multi-class and multi-label learning tasks, such as
hierarchical classiﬁcation and reranking techniques, to the problem of program repair. To the best
of our knowledge, these highly eﬃcient and scalable techniques have hitherto not been applied to the
problem of compilation error repair.

4. MACER accurately predicts the repair class (see Tab 3). Thus, instructors can manually rewrite helpful
feedback (to accompany MACER’s suggested repair) for popular repair classes which may oﬀer greater
pedagogical value.

5. We present a highly optimized implementation of an end-to-end tool-chain1 for compilation error repair
that eﬀectively uses these scalable techniques. MACER’s repair pipeline is end-to-end and entirely
automated i.e. steps such as creation of repair classes can be replicated for any programming language
for which static type inference is possible.

6. The resulting implementation of MACER not only outperforms existing techniques on various metrics,
but also oﬀers training and prediction times that are several times to orders of magnitude faster than
those of existing techniques.

4 Problem Setting and Data Preprocessing

MACER learns error repair strategies given training data in the form of several pairs of programs, with one
program in the pair failing to compile (called the source program) and the other program in the pair being
free of compilation errors (called the target program). Such a supervised setting is standard in previous works
in compilation error repair [1, 8]. Similar to [1], we train only on those pairs where the two programs diﬀer
in a single line (e.g. in Fig 1, the programs diﬀer only in line 3).

However, we stress that MACER is able to perform repair on programs where multiple lines may require
repairs as well, and we do include such datasets in our experiments. The diﬀering line in the source (resp.
target) program is called the source line (resp. target line). With every such program pair, we also receive
the errorID and message generated by the Clang compiler [13] when compiling the source program. Tab 1
lists a few errorIDs and error messages. It is clear from the table data that some error types are extremely
rarely encountered whereas others are very common.

4.1 Notation

We use angular brackets to represent n-grams. For example, the statement a = b + c; contains the unigrams
(cid:104)a(cid:105), (cid:104)=(cid:105), (cid:104)b(cid:105), (cid:104)+(cid:105), (cid:104)c(cid:105) and (cid:104);(cid:105), as well as contains the bigrams (cid:104)a =(cid:105), (cid:104)= b(cid:105), (cid:104)b +(cid:105), (cid:104)+ c(cid:105), (cid:104)c ;(cid:105) and (cid:104); EOL(cid:105).
When encoding bigrams, we include an end-of-line character EOL as well. This helps MACER distinguish this
location since several repairs (such as insertion of expression termination symbols) require edits at the end
of the line.

1The MACER tool-chain is available at https://github.com/purushottamkar/macer/

3

Table 1: Some examples of the 148 compiler errorIDs listed in decreasing order of their frequency of occur-
rence in the data (reported in the Count column). It is clear that some error types are extremely frequent
whereas other error types rarely occur in data. The symbol (cid:3) is a placeholder for program speciﬁc tokens
such as identiﬁers, reserved keywords, punctuation marks etc. For example, a speciﬁc instance of errorID
E6 is shown in Figure 1. A speciﬁc instance of errorID E1 could be “Expected ; after expression”.

ErrorID Error Message

E1
E2
E3
E6
E10
E23
E57
E76
E98
E148

Expected (cid:3) after expression
Use of undeclared identiﬁer (cid:3)
Expected expression (cid:3)
Expected (cid:3) in (cid:3) statement speciﬁer
Expression is not assignable
Expected ID after return statement
Unknown type name (cid:3)
Non-object type (cid:3) is not assignable
variable has incomplete type ID
Parameter named (cid:3) is missing

Count

4999
4709
3818
720
538
128
23
11
3
1

4.2 Feature Encoding

The source lines contain several user-deﬁned literals and identiﬁers (variable names) which can be diverse
but are not informative for error repair. To avoid overwhelming the machine learning method with these
uninformative tokens, it is common in literature to reduce the input vocabulary size. MACER does this by
replacing literals and identiﬁers with their corresponding abstract LLVM token type while retaining keywords
and symbols. An exception is string literals where format-speciﬁers (such as %d and %s) are retained as is,
since these are often a source of errors themselves.

For example, the raw or concrete statement int abc = 0; is converted into the abstract statement int
VARIABLE INT = LITERAL INT ;. An attempt is made to infer the datatypes of identiﬁers (which is possible
even though compilation fails on these programs since the compiler is often nevertheless able to generate a
partial symbol table while attempting compilation). Undeclared/unrecognized identiﬁers are replaced with a
generic token INVALID. Such abstraction is common in literature. DeepFix [9] replaces each program variable
name with a generic identiﬁer ID and removes the contents of string literals. We use the abstraction module
described in TRACER [1] that retains type information, which is helpful in ﬁxing type errors.

For our data, this abstraction process yielded a vocabulary size of 161 tokens and 1930 unique bigrams.
Both uni and bigrams were included in the representation since this feature representation will be used
to predict the repair class of the line which involves predicting which tokens (i.e. unigrams) need to be
replaced/deleted/inserted, as well as be used to predict the repair proﬁle of the line which involves predicting
bigrams as locations. Thus, having uni and bigrams natively in the representation eased the task of these
classiﬁers.
Including trigrams in the representation did not oﬀer signiﬁcant improvements but increased
training and prediction times.

MACER represents each source line as a 2239 dimensional binary vector. The ﬁrst 148 dimensions in this
representation store a one-hot encoding of the compiler errorID generated on that source line (see Tab 1
for examples). The next 161 dimensions store a one-hot unigram feature encoding of the source line and
the remaining 1930 dimensions store a one-hot bigram feature encoding of the abstracted source line. We
used one-hot encodings rather than TF-IDF encodings since the additional frequency information for uni
and bigrams did not oﬀer any predictive advantage. We also found the use of trigrams to not signiﬁcantly
increase performance but make the method slower. It is important to note that the feature creation step
does not use the target line in any manner. This is crucial to allow feature creation for test examples as well.

4

Table 2: Some examples of the 1016 repair classes used by MACER listed in decreasing order of their
frequency of occurrence (frequencies reported in the column Count). For example, Class C2 concerns the
use of undeclared identiﬁers and the solution is to replace the undeclared identiﬁer (INVALID token) with an
integer variable or literal. A ∅ indicates that no token need be inserted/deleted for that class. For example,
no token need be inserted to perform repair for repair class C22 whereas no token need be deleted to perform
repair for repair class C115. See the text for a description of the notation used in the second column.

Class ID [ErrorID [Del] [Ins]] Type

Count

C1
C2
C12
C22
C31
C64
C99
C115
C145
C190

Insert

[E1 [∅] [;]]
[E2 [INVALID] [INT]] Replace
Replace
[E6 [,] [;]]
Delete
[E23 [;] [∅]]
Replace
[E6 [,,] [;;]]
Delete
[E3 [)] [∅]]
Replace
[E45 [==] [=]]
Insert
[E3 [∅] [‘]]
Replace
[E24 [.] [->]]
Replace
[E6 [for] [while]]

3364
585
173
89
62
33
19
16
11
9

4.3 Repair Class Creation

The repair class of a source line encodes what repair to apply to that line. As noted in Table 1, the
Clang compiler oﬀers 148 distinct errorIDs. However, diverse repair strategies may be required to handle
all instances of any given errorID. For example, errorID E6 in Fig 1 can of course signal missing semicolons
within the header of a for loop as the example indicates, but it can also be used by the compiler to signal
missing semicolons ; at the end of a do-while block, as well as missing colons : in a switch case block.

To consider the above possibilities, similar to TEGCER [2], we ﬁrst expand the set of 148 compiler-
generated errorIDs into a much bigger set of 1016 repair classes. It is notable that these repair classes are
generated automatically from training data and do not require any manual supervision. For each training
instance, token abstraction (see Sec 4.2) is done on both the source and target lines and a diﬀ is taken
between the two. This gives us the set of tokens that must be deleted from the (abstracted) source line, as
well as those that must be inserted into the source line, in order to obtain the (abstracted) target line.

A tuple is then created consisting of the compiler errorID for that source line, followed by an enumeration
of tokens that must be deleted (in order of their occurrence in the source line from left to right), followed
by an enumeration of tokens that must be inserted (in order of their insertion point in the source line from
left to right). Such a tuple of the form

[ErrID [TOK−

1 TOK−

2 ...] [TOK+

1 TOK+

2 ...]]

is called a repair class. We identiﬁed 1016 such classes. A repair class requiring no insertions (resp. deletions)
is called a Delete (resp. Insert) repair class. A repair class requiring as many insertions as deletions with
insertions at exactly the locations of the deletions is called a Replace repair class. Tab 2 illustrates a few
repair classes. Repair classes exhibit a heavy tail (see Fig 2) with popular classes having hundreds of training
points whereas the vast majority of (rare) repair classes have merely single digit training instances.

4.4 Repair Proﬁle Creation

The repair proﬁle of a source line encodes where in that line to apply the repair encoded in its repair class.
For every source line, taking the diﬀ of the abstracted source and abstracted target lines (as done in Sec 4.3)
also tells us which bigrams in the abstracted source line require some edit operation (insert/delete/replace)
in order to obtain the abstracted target line.

5

Figure 2: Repair classes generated by MACER arranged in descending order of the number of training
programs associated with them. Only the 500 most popular classes are shown. The classes exhibit heavy-
tailed behavior: less than 400 of the 1016 classes have 3 or more training data points associated with them.
On the other hand, the top 10 classes have more than 200 training points each.

The repair proﬁle for a training pair stores the identity of these bigrams which require modiﬁcation for
that source line. A one-hot representation of the set of these bigrams i.e. a binary vector r ∈ {0, 1}1930 is
taken to be the repair proﬁle of that source line. We note that the repair proﬁle is a sparse ﬁxed-dimensional
binary vector (that does not depend on the number of tokens in the source line) and ignores repetition
information. Thus, even if a bigram requires multiple edit operations, or even if a bigram appears several
times in the source line and only one of those occurrences requires an edit, we record a 1 in the repair
proﬁle corresponding to that bigram. This was done in order to simplify prediction of the repair proﬁle for
erroneous programs at testing time.

4.5 Working Dataset

After the steps in Sec 4.2, 4.3, and 4.4 have been carried out, we have with us, corresponding to every
source-target pair in the training dataset, a class-label yi ∈ [1016] telling us the repair class for that source
line, a feature representation xi ∈ {0, 1}2239 that tells us the errorID, and the uni/bigram representation of
the source line, and a sparse Boolean vector ri ∈ {0, 1}1930 that tells us the repair proﬁle. Altogether, this
constitutes a dataset of the form (cid:8)(xi, yi, ri)(cid:9)n

i=1.

5 MACER: Methodology

MACER (Modular Accelerated Compilation Error Repair) segregates the repair process into six distinct steps

1. Repair Lines: Locate within the source code, which line(s) are erroneous and require repair.

2. Feature Encoding: For each of the identiﬁed lines, perform code abstraction and obtain a 2239-

dimensional feature vector (see Sec 4.2).

3. Repair Class Prediction: Use the feature vector to predict which of the 1016 repair classes is

applicable i.e. which type of repair is required.

4. Repair Localization: Use the feature vector to predict locations within the source lines at which

repairs should be applied.

5. Repair Application: Apply the predicted repairs at the predicted locations

6. Repair Concretization: Undo code abstraction and compile.

6

0100200300400500Repair Class ID101102103Num of ProgramsFigure 3: The training pipeline proposed by MACER, illustrated using the example used in Fig 1. In the
example above, L INT and V INT are shorthand for LITERAL INT and VARIABLE INT.

Although previous works do incorporate some of the above steps, e.g., TRACER incorporates code abstraction
and locating repair lines within the source code, MACER departs most notably from previous approaches
in segregating the subsequent repair process into repair class prediction, localization, and application steps.
Among other things such as greater training and prediction speed, this allows MACER to learn a customized
repair location and repair application strategy for diﬀerent repair classes which can be beneﬁcial. For
instance, if it is known that the repair required is the insertion of a semi-colon, then the location where the
repair must be performed is narrowed down signiﬁcantly.

In contrast, existing methods expect a generative mechanism such as sequence-to-sequence prediction or
reinforcement learning, to jointly perform all these tasks. This precludes any opportunity to exploit the type
of repair required to perform better on speciﬁc repair types, apart from making these techniques slow at
training and prediction. Below we detail the working of each of the above steps.

5.1 Repair Lines

One of the key tasks of a compiler is to report line numbers where an error was encountered. However, this
does not necessarily correspond to the location where the repair must be performed. In our training data
set where errors are localized to a single line, the repair line location was the same as the compiler reported
line-number in only about 80% of the cases.

Existing works have used diﬀerent techniques for repair line localization. RLAssist [8] use reinforcement
learning to perform localization by navigating the program using movement based actions to maximize a
predeﬁned reward. DeepFix [9] trains a dedicated neural-network to identify suspicious tokens (and hence
their location) to achieve around 86% repair line localization accuracy. TRACER [1] relies on compiler
reported line numbers and considers two additional lines, one above and one below the compiler error line,
and obtains a localization accuracy of around 87%. MACER uses this same technique which gave a repair
line localization recall of around 90% on our training dataset.

7

E610001011110⋮UnigramFeaturesBigramFeaturesCompiler ErrorID001⋮00Source Program (buggy)Repair Line ExtractionCode AbstractionRepair Profile CreationFeature Vector 𝐱∈0,12239Repair Profile 𝐫∈0,11930TRAINTrain OVA multi label classifiersTrain Tree and Prototype RerankersFigure 4: The repair pipeline proposed by MACER, illustrated using the example used in Fig 1. To illustrate
the beneﬁt of reranking, we depict a situation where a wrong repair class gets highest score from the
classiﬁcation tree, but reranking corrects the error. Tab 3 shows that this is indeed common. The repair
line extraction and code abstraction steps are common to training and prediction.

5.2 Repair Class Prediction

As outlined in Sec 4, MACER considers 1016 repair classes which is quite large. In order to make fast and
accurate predictions for the correct repair class that apply to a given source line, MACER uses hierarchical
classiﬁcation techniques that are popular in the domain of large-scale multi-class and multi-label classiﬁcation
problems [12, 17].

As there exists a natural hierarchy in our problem setting, we found it suitable (as suggested by [12])
to use a ﬁxed hierarchy rather than a learnt hierarchy. Given that a signiﬁcant fraction of repair classes
(around 40%) involve replacement repairs, we found it advantageous to ﬁrst segregate source lines that
require replacement repairs from others.

The classiﬁcation hierarchy used by MACER is shown in Fig 5. The root node decides whether a source
line requires a replacement or some other form of repair using a feed-forward network with 2 hidden layers
with 128 nodes each and trained on cross entropy loss. All other internal nodes use a linear one-vs-rest
classiﬁer trained on cross entropy loss to perform their respective multi-way splits.

It is well-known [11] that discriminative classiﬁers struggle to do well on rare classes due to paucity of
data. Our repair classes do exhibit signiﬁcant heavy-tailed behavior (see Fig 2) with most classes occurring
infrequently and only a few being popular. To improve MACER’s performance, we ﬁrst augment the classi-
ﬁcation tree into a ranking tree that ranks classes instead of just predicting one class, and then introduce a
reranking step which modiﬁes the ranking given by the ranking tree.

5.2.1 Repair Class Ranking

We converted the classiﬁcation tree into a probabilistic ranking tree that could assign a likelihood score to
each repair class. More speciﬁcally, given the feature representation of a source line x ∈ {0, 1}2239, the tree
(x) := P [y = c | x]. We followed a
is used to assign, for every repair class c ∈ [1016], a likelihood score stree
process similar to ([12, 17]) to obtain these likelihood scores from the tree. This construction is routine and
detailed in the appendix Sec A. Although these scores stree
(x) can themselves be used to rank the classes,
doing so does not yield the best results. This is due to the large number of extremely rare repair classes
(Tab 2 shows that only ≈ 150 of the 1016 repair classes have more than 10 training examples).

c

c

8

Feature Vector 𝐱∈0,12239Re-Ranking001⋮00OVA multi label classifier for C31PREDICTRepair ApplicationConcretizeC31Figure 5: The prediction hierarchy used by MACER to predict the repair class.

Table 3: Performance beneﬁts of reranking. The table shows the performance accuracy (in terms of various
ranking metrics) achieved by MACER in predicting the correct repair class. The ﬁrst three columns report
Top@k i.e. the fraction of test examples on which the correct errorID or correct repair tokens were predicted
within the top k locations of the ranking. The last column reports the mean-average precision i.e.
the
average reciprocal rank at which the correct repair class was predicted in terms of tokens to be inserted or
deleted. Note that in all cases, reranking signiﬁcantly boosts the performance. In particular, the last column
indicates that reranking ensures that the correct tokens to be inserted/deleted were almost always predicted
within the ﬁrst two ranks.

Reranking Oﬀ (use stree
Reranking On (use 0.8 · stree

c

c

(x) to rank repair classes)

(x) + 0.2 · sprot

c

(x) instead)

Top@1 Top@3 Top@5 MAP

0.66
0.67

0.83
0.88

0.87
0.90

0.40
0.50

5.2.2 Repair Class Reranking

To improve classiﬁcation performance on rare repair classes, MACER uses prototype classiﬁers [11, 16] that
have been found to be eﬀective and scalable when dealing with a large number of rare classes. Suppose
a repair class c ∈ [1016] is associated with nc training points. The k-means algorithm is used to obtain
kc = (cid:6) nc
c , are
taken as prototypes for this repair class. This is repeated for all repair classes.

(cid:7) clusters out of these nc training points and the centroids of these clusters, say ˜x1

c, . . . , ˜xkc

At test time, given a source line x ∈ {0, 1}2239, these prototypes are used to assign a new score to each

25

repair class as follows

sprot
c

(x) := max
k∈[kc]

(cid:18)

exp

−

1
2

(cid:19)

(cid:13)
(cid:13)x − ˜xk
c

(cid:13)
2
(cid:13)
2

Thus, for each repair class, the source line searches for the closest prototype of that repair class and uses it
to generate a score.

MACER uses the scores assigned by the probabilistic ranking tree and those assigned by the prototypes to
get a combined score as sc(x) = 0.8 · stree
(x) (the constants 0.8, 0.2 are standard in literature
c
and we did not tune them). MACER uses this combined score sc(x) to rank repair classes in decreasing
order of their applicability. Tab 3 outlines how the reranking step signiﬁcantly boosts MACER’s ability to
accurately predict the relevant compiler errorID and the repair class. Sec 6 will present additional ablation
studies that demonstrate how the reranking step boosts not just the repair class prediction accuracy, but
MACER’s error repair performance as well.

(x) + 0.2 · sprot

c

9

ROOTNON-REPLACEREPLACEC1C413…INSERTC414C640…DELETEC641C942…MISCC943C1016…5.3 Repair Localization

Having predicted the repair class, MACER proceeds to locate regions within the source line where those
repairs must be made. MACER reformulates this a problem of predicting which bigram(s) within the source
line require edits (multiple locations may require edits on the same line). Note that this is exactly the same
as predicting the repair proﬁle vector of that source line (apart from any ambiguity due to the same bigram
appearing multiple times in the line which is handled during repair application).

This observation turns repair localization into a multi-label learning problem, that of predicting the
sparse Boolean repair proﬁle vector r using the source feature representation x and the predicted repair class
ˆy as inputs. Each of the 1930 bigrams in our vocabulary, now turns into a potential “label” which if turned
on, indicates that repair is required at bigrams of that type. Fig 4 explains the process pictorially.

MACER adopts the “one-vs-rest” (OVR) approach that is a state-of-the-art in large-scale multi-label
classiﬁcation [4]. Thus, 1930 binary classiﬁers are trained (using standard implementations of decision
trees), each one predicting whether that particular bigram needs repair or not. At test time, classiﬁers
corresponding to all bigrams present in the source line are queried as to whether the corresponding bigrams
require repair or not.

MACER trains a separate OVR multi-label classiﬁer per repair class that is trained only on training
points of that class (as opposed to having a single OVR classiﬁer handle examples of all repair classes).
Training repair-class speciﬁc localizers improved performance since the kind of bigrams that require edits for
replacement repairs e.g. substituting = with ==, are very diﬀerent from the kind of bigrams that require edits
for insertion repairs e.g. inserting a semicolon ;. At test time, after predicting the repair class for a source
line, MACER invokes the OVR classiﬁer of the predicted repair class to perform repair localization. During
repair localization, we only invoke OVR classiﬁers corresponding to bigrams that are actually present in the
source line. Thus, we are guaranteed that any bigrams predicted to require edits will always be present in
the source line. In our experiments, we found this strategy to work well with MACER oﬀering a Hamming
loss of just 1.43 in terms of predicting the repair proﬁle as a Boolean vector. Thus, on an average, only
about one bigram was either predicted to require repair when it did not, or not predicted to require repair
when it actually did.

5.4 Repair Application

The above two steps provide MACER with information on what repairs need to be applied as well as where
they need to be applied. Frugal but eﬀective techniques are then used to apply the repairs which we discuss
in this subsection. Let B denote the ordered set of all bigrams (and their locations, ordered from left to
right) in the source line which were ﬂagged by the repair localizer as requiring edits. For example, if the
repair localizer predicts the bigram (cid:104), VARIABLE INT(cid:105) to require edits and this bigram appears twice in the
source line (note that this would indeed happen in the example in Fig 1), then both those bigrams would be
included in B. This is repeated for all bigrams ﬂagged by the repair localizer. Below we discuss the repair
application strategy for various repair class types.

5.4.1

Insertion Repairs

Recall that these are repairs where no token needs to be deleted from the source line but one or more tokens
need to be inserted. We observed that in an overwhelmingly large number of situations that require multiple
tokens to be inserted, all tokens need to be inserted at the same location, for instance the repair

for(i=0;i<5) → for(i=0;i<5;i++)

has the repair class [E6 [∅] [; VARIABLE INT ++]] and requires three tokens, a semicolon ;, an integer variable
identiﬁer, and the increment operator ++ to be inserted, all at the same location i.e. within the bigram (cid:104)5
)(cid:105) which is abstracted as (cid:104)LITERAL INT )(cid:105).

Thus, for insert repairs, MACER concatenates all tokens marked for insertion in the predicted repair class
and attempts insertion of this ensemble into all bigrams in the set B. Attempting insertion into a single
bigram itself requires 3 attempts since each bigram oﬀers 3 positions for insertion within itself. After each

10

attempt, MACER concretizes the resulting program (see below) and attempts to compile it. MACER stops
at a successful compilation and keeps trying otherwise.

5.4.2 Deletion Repairs

Recall that these are repairs where no token needs to be inserted into the source line but one or more tokens
need to be deleted. In this case, MACER scans the list of tokens marked for deletion in the predicted repair
class from right to left. For every such token, the ﬁrst bigram in the ordered set B (also scanned from
right to left) that has that token, gets edited by deleting that token. Once all tokens in the repair class are
exhausted, MACER concretizes the resulting program (see below) and attempts to compile it.

5.4.3 Replace Repairs

Recall that these are repairs where an insertion and a deletion, both happen at the same location and this
process may be required multiple times. In such cases, MACER scans the list of tokens marked for deletion in
the predicted repair class from right to left and also considers the corresponding token marked for insertion.
Let this pair be (TOK−, TOK+). As in the deletion repair case, the ﬁrst bigram in the ordered set B (also
scanned from right to left) that contains TOK−, gets edited by deleting TOK− from that bigram and inserting
TOK+ in its place. Once all tokens in the repair class are exhausted, MACER concretizes the resulting program
(see below) and attempts to compile it.

5.4.4 Miscellaneous Repairs

In the most general case, an unequal number of tokens may need to be inserted and deleted from the source
line, that too possibly at varied locations. Handling all cases in this situation separately is unwieldy and
thus, MACER adopts a generic approach which works well in a large number of cases. First, MACER ignores
the insertion tokens in the repair class and performs edits as if the repair class were a deletion type class.
Subsequently, it considers the insertion tokens (all deletion tokens having been considered by now) and
processes the resulting edited line as if it were an insertion type class.

5.4.5 Repair Concretization

The above repair process generates a line that still contains abstract LLVM tokens such as LITERAL_INT. In
order to make the program compilable, these abstract tokens are replaced with concrete program tokens such
as literals and identiﬁers through an approximate process reverses the abstraction. To do this, we replace
each abstract token with the most recently used concrete variable/literal of the same type, that already
exists in the current scope.

This is an approximate process since the repair application could suggest the insertion of a particular
type of variable, which does not exist in the current scope. For example, if the repair application stage
suggests the insertion of a variable of type Variable Float, then at least one ﬂoating point variable should
be declared in the same scope as the erroneous line. Nevertheless, we observe that this concretization strategy
of MACER is able to recover the correct replacement in 90+% of the instances in our datasets.

MACER considers each candidate repair line reported by the repair line localizer (recall that these include
compiler-reported lines as well as lines immediately above and below those lines). For each such candidate
repair line, MACER applies its predicted repair (including concretization) and then compiles the resulting
program. If the number of compilation errors in the program reduce, then this repair is accepted and the
process is repeated for the remaining candidate repair lines.

11

Table 4: Comparison between TRACER and MACER on the single-line and multi-line test datasets. MACER
achieves similar Pred@k and repair accuracy as TRACER on the single-line dataset. On multi-line dataset,
where programs require repairs on multiple diﬀerent lines, MACER achieves 14% improvement over TRACER.
Single-line
Pred@1 Pred@5 Rep@5

Multi-line
Rep@5

Dataset
Metric

TRACER
MACER

0.596
0.597

0.683
0.691

0.792
0.805

0.437
0.577

Table 5: Comparison of all methods on the DeepFix dataset. Values take from ∗[9] and †[8]. MACER oﬀers the
highest repair accuracy on this dataset. The nearest competitor is TRACER that is 12.5% behind. MACER
oﬀers a prediction time that is 4× faster than TRACER and 2× faster than the rest, and a train time that
is 2× faster than TRACER and more than 800× faster than RLAssist.

DeepFix RLAssist TRACER MACER

Repair Acc
Test Time
Train Time

0.27∗
<1s†
-

0.267†
<1s†
4 Days

0.439
1.66s
14 min

0.566
0.45s
7 min

6 Experiments

We compared MACER’s performance against previous works, as well as performed ablation studies to study
the relative contribution of its components. All MACER implementations2 were done using standard machine
learning libraries such as sklearn [15] and keras [6]. Experiments were performed on a system with Intel(R)
Core(TM) i7-4770 CPU @ 3.40GHz × 8 CPU having 32 GB RAM.

6.1 Datasets

We report MACER accuracy on three diﬀerent datasets. All these datasets were curated from the same
2015-2016 fall semester course oﬀering of CS-1 course at IIT-Kanpur (a large public university) where 400+
students attempted more than 40 diﬀerent programming assignments. The dataset was recorded using Pru-
tor [7], an online IDE. The DeepFix dataset3 contains 6,971 programs that fail to compile, each containing
between 75 and 450 tokens [9]. The single-line (17,669 train program pairs + 4,578 test program pairs) and
multi-line (17,451 test program pairs) datasets4 released by [1] contain program pairs where error-repair is
required, respectively, on a single line or multiple lines .

6.2 Metrics

We report our results on two metrics i) repair-accuracy, the popular metric widely adopted by repair tools,
and ii) Pred@k, a metric introduced by TRACER [1]. Repair accuracy denotes the the fraction of test programs
that were successfully repaired by a tool i.e. all compilation errors were removed, thereby producing a correct
program devoid of any compilation errors. On the other hand, Pred@k metric captures the fraction of test
programs where at least one of the top k abstract repair suggestions (since MACER and other competing
algorithms are capable of oﬀering multiple suggestions for repair in a ranked list) exactly matched the
student’s own abstract repair.

2The MACER tool-chain is available at https://github.com/purushottamkar/macer/
3https://www.cse.iitk.ac.in/users/karkare/prutor/prutor-deepﬁx-09-12-2017.zip
4https://github.com/umairzahmed/tracer

12

Figure 6: The two ﬁgures compare MACER and TRACER on the head repair classes (top 60 in terms of
popularity with 35+ training points in each class) and torso repair classes (top 60-120 with 15+ training
points in each class). To avoid clutter, only 30 classes from each category are shown. MACER has a substantial
lead over TRACER on head classes with an average of 20% higher prediction hit rate (i.e. predicting the
exact repair as desired by the student). On torso classes, MACER continues to dominate albeit with a smaller
margin. On rare classes, the two methods are competitive.

The choice of the Pred@k metric is motivated by the fact that the goal of program repair is not to generate
any program that merely compiles. This is especially true of repair tools designed for pedagogical settings.
rated by the tool is exactly same as the student generated one, at the abstraction level. The purpose of this
metric is further motivated in the Sec 6.4.

6.3 Training Details

Our training is divided into two parts, learning models to perform i) Repair Class prediction, and ii) Repair
Location prediction. For repair class prediction we followed the prediction hierarchy shown in Figure 5.
The root node uses a feed forward neural net with two hidden layers of 128 nodes each. We tried {1,2,3}
hidden layers with each layer containing nodes varying in {128,256,512}, and the structure currently used
by us (with 2 hidden layers of 128 nodes each) was found to be the best.

For re-ranking the repair classes, we created prototype(s) of each class depending on the size of class
using KMeans clustering. The second part of training is Repair Location Prediction. We followed one-
vs-rest (OVR) approach and performed binary classiﬁcation for each of the 1930 bigrams, each binary
classiﬁcation telling us whether the corresponding bigram is worthy of edits or not. We recall that these OVR
classiﬁers were trained separately for all repair classes to allow greater ﬂexibility. The binary classiﬁcation
was performed using standard implementations of decision trees that use Gini impurity to ensure node purity.
Decision trees were chosen due to their speed of prediction and relatively high accuracy.

13

0102030405060Repair Class ID0.00.20.40.60.81.0Prediction AccuracyMACERTRACER60708090100110120Repair Class ID0.00.20.40.60.81.0Prediction AccuracyMACERTRACERTable 6: Performance of MACER on several sample test instances. Pred? = Yes if MACER’s top suggestion
exactly matched the student’s abstracted ﬁx, else Pred? = No is recorded. Rep? = Yes if MACER’s top
suggestion removed all compilation errors else Rep? = Yes is recorded. ZS? records whether the example
was a “zero-shot” test example where MACER had never seen the corresponding repair class in training data.
On the ﬁrst three examples, MACER not only oﬀers successful compilation, but oﬀers a repair that exactly
matches that desired by the student. Note that the second example involves an undeclared identiﬁer. In the
next two examples, although MACER does not oﬀer the exact match desired by the student, it nevertheless
oﬀers sane ﬁxes that eliminate all compilation errors. In the ﬁfth example, MACER errs on the side of caution
and inserts a matching parenthesis rather than risk eliminating an unmatched parenthesis. The last two are
zero-shot examples. Although MACER could handle one of the zero-shot cases gracefully, it could not handle
the other case. Obtaining better performance on zero shot repair classes is a valuable piece of future work
for MACER.

# Source-line
1
2
3
4
5
6
7

scanf("%c",&a[i] ;
for (i =0;i<n;i++)
if(x==y)printf("Y"); break;
for(i=0; i=<N ;i++)
if ( (a[j]==’ ’)
int n; n=q;
c=sqrt( a^2+b^2 );

Target-line
scanf("%c",&a[i]);
for(int i=0;i<n;i++)
if(x==y)printf("Y");
for(i=0;i<=N;i++)
if(a[j]==’ ’)
int n;
c=sqrt(a*a+b*b);

MACER’s Top Prediction
scanf("%c",&a[i] );
for (int i =0;i<n;i++)
if(x==y)printf("Y") ;
for(i=0; i<N ;i++)
if((a[j]==’ ’) )
int n; n=0;
c=sqrt( a^2+b^2 );

(Pred?, Rep?, ZS?)
(Yes, Yes, No)
(Yes, Yes, No)
(Yes, Yes, No)
(No, Yes, No)
(No, Yes, No)
(No, Yes, Yes)
(No, No, Yes)

6.4 A Naive Baseline and Importance of Pred@k

We consider a naive method Kali’ that simply deletes all lines where the compiler reported an error. This is
inspired by Kali [18], an erstwhile state-of-art semantic-repair tool that repaired programs by functionality
deletion alone. This naive baseline Kali’ gets 48% repair accuracy on the DeepFix dataset whereas DeepFix [9],
TRACER [1] and MACER get respectively 27%, 44% and 56% (Tab 5). Although Kali’ seems to oﬀer better
repair accuracy than TRACER, its Pred@1 accuracy on the single-line dataset is just 4%, compared to 59.6%
and 59.7% by TRACER and MACER respectively (Tab 4). This demonstrates the weakness of reporting on
repair accuracy metric in isolation, and motivates the usage of additional complex metrics such as Pred@k,
to better capture the eﬃcacy of repair tools.

6.5 Breakup of Training Time

Of the total 7 minute train time (see Tab 5), MACER took less than 5 seconds to create repair classes and
repair proﬁles from the raw dataset. The rest of the training time was taken up more or less evenly by repair
class prediction training (tree ranking + reranking) and repair proﬁle prediction training.

6.6 Comparisons with other methods

The values for Pred@k (resp. Rep@k) were obtained by considering the top k repairs suggested by a method
and declaring success if any one of them matched the student repair (resp. removed compilation errors).
For Pred@k computations, all methods were given the true repair line and did not have to perform repair
line localization. For Rep@k computations, all methods had to localize then repair. Tabs 4 and 5 compare
MACER with competitor methods. MACER oﬀers superior repair performance at much lesser training and
prediction costs. Fig 6 shows that MACER outperforms TRACER by ≈ 20% on popular classes while being
competitive or better on others.

14

Figure 7: A graph comparing the prediction and repair hit rate of MACER on the top 120 most popular
repair classes. To avoid clutter, every 4th class is shown. For these popular classes – which still may have
as low as 15 training data points – MACER frequently achieves perfect or near perfect score in terms of
prediction accuracy or repair accuracy or both.

Figure 8: A study similar to the one presented in Fig 9 but with respect to prediction (exact match) accuracy
instead of repair accuracy. A study of the prediction accuracy oﬀered by MACER on repair classes with at
least 3 training data points – a total of 391 such classes were there. On a majority of these classes 221/391
= 56%, MACER oﬀers greater than 90% prediction accuracy. On a much bigger majority 287/391 = 73% of
these classes, MACER oﬀers more than 50% prediction accuracy. The second graph indicates that MACER’s
prediction accuracy drops below 50% only on classes which have less than around 30 points.

6.7 Ablation studies with MACER

To better understand the strengths and limitations of MACER, we report on further experiments. Fig 7
shows that for top 120 most popular repair classes (which still may have as low as 15 training data points),
MACER frequently achieves perfect or near perfect score in terms of prediction accuracy or repair accuracy
or both. Figs 8 and 9 shows that MACER is eﬀective at utilizing even small amounts of training data and
that its prediction accuracy drops below 50% only on repair classes which have less than 30 examples in the
training set. Tab 6 oﬀers examples of actual repairs by MACER. Although it performs favorably on repair
classes seen during training, it often fails on zero-shot repair classes which were never seen during training.
Tab 7 presents an explicit ablation study analyzing the diﬀerential contributions of MACER’s individual
components on the single-line dataset. Re-ranking gives 10-12% boost to both Pred@k and repair accuracy.
Predicting the repair class (resp. proﬁle) correctly accounts for 5-12% (resp. 6%) of the performance.
MACER loses a mere 6% accuracy on account of improper repair application. For all ﬁgures and tables,
details are provided in the captions.

15

020406080100120Repair Class ID0.00.20.40.60.81.0AccuracyMACER PredictionMACER Repair0.00.20.40.60.81.0Prediction Accuracy050100150200Num Repair Classes0.00.20.40.60.81.0Prediction Accuracy101102103Num Training PointsFigure 9: A study of the repair accuracy oﬀered by MACER on repair classes with at least 3 training data
points – a total of 391 such classes were there. On a majority of these classes 267/391 = 68%, MACER oﬀers
greater than 90% repair accuracy. On a much bigger majority 327/391 = 84% of these classes, MACER oﬀers
more than 50% repair accuracy. The second graph indicates that MACER’s repair accuracy drops below
50% only on classes which have less than around 30 points. This indicates that MACER is very eﬀective at
utilizing even small amounts of training data.

Table 7: An ablation study on the diﬀerential contributions of MACER’s components. ZS stands for “zero-
shot”. For the “ZS included” column all test points are considered while reporting accuracies. For the “ZS
excluded” column, only those test points are considered whose repair class was observed at least once in the
training data. RR stands for Reranking. RCP stands for Repair Class Prediction, RLP stands for Repair
Location Prediction. RCP = P (resp. RLP = P) implies that we used the repair class (resp. repair location)
predicted by MACER. RCP = G (resp. RLP = G) implies that we used the true repair class (resp. true
repair proﬁle vector). It is evident from the diﬀerence in the results of the ﬁrst two rows that (whether we
include ZS or not), reranking gives 10-12% boost in both prediction accuracy. This highlights the importance
of reranking in the presence of rare classes. Similarly, it can be seen that predicting the repair class (resp
location) correctly accounts for 5-12% (resp. 6%) of the performance. The ﬁnal row shows that MACER
loses 6-8% performance owing to improper repair application/concretization. In the last two rows, Pred@1 is
higher than Rep@1 (1-2% cases) owing to concretization failures – even though the predicted repair matched
the student’s repair in abstracted form, the program failed to compile after abstraction was removed.

ZS included

ZS excluded

RCP RLP Pred @1 Rep @1 Pred @1 Rep @1

RR
OFF P
P
ON
G
ON
G
ON

P
P
P
G

0.492
0.597
-
-

0.599
0.703
-
-

0.631
0.757
0.885
0.943

0.706
0.825
0.877
0.926

16

0.00.20.40.60.81.0Repair Accuracy050100150200250Num Repair Classes0.00.20.40.60.81.0Repair Accuracy101102103Num Training Points7 Conclusion

In this paper we presented MACER, a novel technique to accelerated compilation error-repair. A key contri-
bution of MACER is a ﬁne-grained segregation of the error repair process into eﬃciently solvable ranking and
labelling problems. These reductions are novel in this problem area where most existing techniques prefer to
directly apply a single powerful generative learning technique instead. MACER oﬀers signiﬁcant advantages
over existing techniques namely superior error repair accuracy on various error classes and increased training
and prediction speed. Targeting rare error classes and “zero-shot” cases (Tab 6) is an important area of
future improvement. A recent large scale user-study [3] demonstrated that students who received automated
repair feedback from TRACER [1] resolved their compilation errors faster on average, as opposed to human
tutored students; with the performance gain increasing with error complexity. We plan to conduct a similar
systematic user study in the future, to better understand the correlation between the Pred@k metric scores
and error resolution eﬃciency (performance) of students.

Acknowledgments

The authors thank the reviewers for helpful comments and are grateful to Pawan Kumar for support with
benchmarking experiments. P. K. thanks Microsoft Research India and Tower Research for research grants.

17

References

[1] Umair Z. Ahmed, Pawan Kumar, Amey Karkare, Purushottam Kar, and Sumit Gulwani. Compilation
error repair: for the student programs, from the student programs. In Proceedings of the 40th Inter-
national Conference on Software Engineering: Software Engineering Education and Training (ICSE-
SEET), pages 78–87, 2018. doi:10.1145/3183377.3183383.

[2] Umair Z. Ahmed, Renuka Sindhgatta, Nisheeth Srivastava, and Amey Karkare. Targeted Example
Generation for Compilation Errors. In 2019 34th IEEE/ACM International Conference on Automated
Software Engineering (ASE), pages 327–338. IEEE, 2019. doi:10.1109/ASE.2019.00039.

[3] Umair Z. Ahmed, Nisheeth Srivastava, Renuka Sindhgatta, and Amey Karkare. Characterizing the
Pedagogical Beneﬁts of Adaptive Feedback for Compilation Errors by Novice Programmers. In 42nd In-
ternational Conference on Software Engineering: Software Engineering Education and Training (ICSE-
SEET) - to appear, 2020.

[4] Rohit Babbar and Bernhard Sch¨olkopf. DiSMEC - Distributed Sparse Machines for Extreme Multi-label
Classiﬁcation. In 10th ACM International Conference on Web Search and Data Mining (WSDM), pages
721–729, 2017. doi:10.1145/3018661.3018741.

[5] Tracy Camp, Stuart H. Zweben, Ellen L Walker, and Lecia Jane Barker. Booming enrollments:
Good times? In Proceedings of the 46th ACM Technical Symposium on Computer Science Education
(SIGCSE), pages 80–81. ACM, 2015. doi:10.1145/2676723.2677333.

[6] Fran¸cois Chollet et al. Keras: The Python Deep Learning library. https://keras.io, 2015.

[7] Rajdeep Das, Umair Z. Ahmed, Amey Karkare, and Sumit Gulwani. Prutor: A System for Tutoring

CS1 and Collecting Student Programs for Analysis. arXiv:1608.03828 [cs.CY], 2016.

[8] Rahul Gupta, Aditya Kanade, and Shirish Shevade. Deep Reinforcement Learning for Syntactic Error
Repair in Student Programs. In 33rd AAAI Conference on Artiﬁcial Intelligence (AAAI), pages 930–937,
2019. doi:10.1609/aaai.v33i01.3301930.

[9] Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. DeepFix: Fixing Common C Language
Errors by Deep Learning. In 31st AAAI Conference on Artiﬁcial Intelligence (AAAI), pages 1345–1351,
2017.

[10] Hossein Hajipour, Apratim Bhattacharyya, and Mario Fritz. SampleFix: Learning to Correct Programs

by Sampling Diverse Fixes. arXiv:1906.10502v1 [cs.SE], 2019.

[11] Himanshu Jain, Yashoteja Prabhu, and Manik Varma. Extreme Multi-label Loss Functions for Recom-
mendation, Tagging, Ranking & Other Missing Label Applications. In 22nd ACM SIGKDD Confer-
ence on Knowledge Discovery and Data Mining (KDD), pages 935–944, 2016. doi:10.1145/2939672.
2939756.

[12] Kalina Jasinska, Krzysztof Dembczy´nski, R´obert Busa-Fekete, Karlson Pfannschmidt, Timo Klerx, and
In 33rd

Eyke H¨ullermeier. Extreme F-measure Maximization Using Sparse Probability Estimates.
International Conference on Machine Learning (ICML), pages 1435–1444, 2016.

[13] Chris Lattner and Vikram Adve. LLVM: A compilation framework for lifelong program analysis &
transformation. In Proceedings of the international symposium on Code generation and optimization:
feedback-directed and runtime optimization, page 75. IEEE Computer Society, 2004.

[14] Renee McCauley, Sue Fitzgerald, Gary Lewandowski, Laurie Murphy, Beth Simon, Lynda Thomas, and
Carol Zander. Debugging: A Review of the Literature from an Educational Perspective. Computer
Science Education, 18(2):67–92, 2008. doi:10.1080/08993400802114581.

18

[15] Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre
Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. Scikit-learn:
Machine Learning in Python. Journal of Machine Learning Research, 12(85):2825–2830, 2011.

[16] Yashoteja Prabhu, Anil Kag, Shilpa Gopinath, Kunal Dahiya, Shrutendra Harsola, Rahul Agrawal, and
Manik Varma. Extreme Multi-label Learning with Label Features for Warm-start Tagging, Ranking &
Recommendation. In 11th ACM International Conference on Web Search and Data Mining (WSDM),
pages 441–449, 2018. doi:10.1145/3159652.3159660.

[17] Yashoteja Prabhu, Anil Kag, Shrutendra Harsola, Rahul Agrawal, and Manik Varma. Parabel:
Partitioned Label Trees for Extreme Classiﬁcation with Application to Dynamic Search Advertis-
ing.
In 27th International World Wide Web Conference (WWW), pages 993–1002, 2018.
doi:
10.1145/3178876.3185998.

[18] Zichao Qi, Fan Long, Sara Achour, and Martin C Rinard. An analysis of patch plausibility and cor-
rectness for generate-and-validate patch generation systems. In International Symposium on Software
Testing and Analysis, pages 24–36. ACM, 2015. doi:doi.org/10.1145/2771783.2771791.

[19] Marko Vasic, Aditya Kanade, Petros Maniatis, David Bieber, and Rishabh Singh. Neural Program
Repair by Jointly Learning to Localize and Repair. In 7th International Conference on Learning Rep-
resentations (ICLR), 2019.

A Hierarchical Repair Class Ranking

Let T denote the tree in Fig 5 with root r(T ). The leaves of T correspond to individual repair classes. The
set of leaf nodes of a subtree rooted at any node t will be denoted by L(t). The set of children of a node t
will be denoted by C(t) and the parent of node t will be denoted by P (t).

The set of nodes on the path from the root r(T ) to any leaf l ∈ L(r(T )) is denoted by W (l). Note that
each leaf l corresponds to a repair class cl ∈ [1016]. For any node t, let the indicator random variable Vt
indicate if we visited node t. Then using the chain rule, we can express

P [y = c | x] = P [Vlc = 1 | x] =

(cid:89)

P (cid:2)Vt = 1 | x, VP (t) = 1(cid:3)

t∈W (lc)

The correctness of the above can be deduced from the fact that Vt = 1 implies VP (t) = 1. Given that all
nodes in our tree train their classiﬁers probabilistically using the cross entropy loss, we are readily able to,
for every internal node tn and its, say k children c1, . . . , ck, assign the probability P [Vci = 1 | x, Vt = 1] using
the sigmoidal activation (for binary split at the root), or the softmax activation (for all other multi-way
splits). This allows us to compute the score ˆsc(x) := P [y = c | x] for any repair class c by just traversing the
tree from the root node to the leaf node corresponding to the repair class c.

19

