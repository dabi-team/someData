9
1
0
2

v
o
N
8
2

]

G
L
.
s
c
[

2
v
3
8
9
9
0
.
1
1
9
1
:
v
i
X
r
a

TreeGen: A Tree-Based Transformer Architecture for Code Generation

Zeyu Sun† Qihao Zhu† Yingfei Xiong∗† Yican Sun† Lili Mou‡ Lu Zhang†
†Key Laboratory of High Conﬁdence Software Technologies (Peking University), MoE;
Software Institute, Peking University, 100871, P. R. China
{szy , zhuqh, xiongyf, sycpku, zhanglucs}@pku.edu.cn
‡University of Alberta, Edmonton, AB, Canada
doublepower.mou@gmail.com

Abstract

A code generation system generates programming language
code based on an input natural language description. State-of-
the-art approaches rely on neural networks for code genera-
tion. However, these code generators suffer from two prob-
lems. One is the long dependency problem, where a code
element often depends on another far-away code element.
A variable reference, for example, depends on its deﬁnition,
which may appear quite a few lines before. The other problem
is structure modeling, as programs contain rich structural in-
formation. In this paper, we propose a novel tree-based neural
architecture, TreeGen, for code generation. TreeGen uses the
attention mechanism of Transformers to alleviate the long-
dependency problem, and introduces a novel AST reader (en-
coder) to incorporate grammar rules and AST structures into
the network. We evaluated TreeGen on a Python benchmark,
HearthStone, and two semantic parsing benchmarks, ATIS
and GEO. TreeGen outperformed the previous state-of-the-
art approach by 4.5 percentage points on HearthStone, and
achieved the best accuracy among neural network-based ap-
proaches on ATIS (89.1%) and GEO (89.6%). We also con-
ducted an ablation test to better understand each component
of our model.

Introduction
Code generation is an important artiﬁcial intelligence prob-
lem that has the potential to signiﬁcantly boost the pro-
ductivity of programmers. Given a speciﬁcation written in
natural language, a code generation system translates the
speciﬁcation into an executable program. For example, if a
python programmer gives an instruction “initialize a dictio-
nary, Dict”, the code generator is expected to automatically
generates “Dict={ }”.

With the development deep learning techniques, re-
searchers have applied various neural architectures to this
problem, such as sequence-to-sequence (Seq2Seq) models
or sequence-to-tree (Seq2Tree) models (Sutskever, Vinyals,
and Le 2014; Ling et al. 2016; Yin and Neubig 2017;
Rabinovich, Stern, and Klein 2017; Hayati et al. 2018;

∗Yingfei Xiong is the corresponding author. The code is avail-

able at https://github.com/zysszy/TreeGen
Copyright c(cid:13) 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.

Sun et al. 2019). Especially, state-of-the-art approaches gen-
erate code by predicting a sequence of grammar rules (Yin
and Neubig 2017; Rabinovich, Stern, and Klein 2017; Sun
et al. 2019). That is to say, the system keeps a partial ab-
stract syntax tree (AST) of the already-generated code, and
predicts the grammar rule to be used to expand a particular
node.

The classiﬁcation of grammar rules faces two main chal-
lenges. The ﬁrst challenge is the long-dependency prob-
lem (Bengio, Simard, and Frasconi 1994). A code element
may depend on another far-away element. For example, a
variable reference statement “if len(a) < Max Length:”
at line 100 may depend on a variable deﬁnition statement
“Max Length = 100” at line 10. The second challenge
is the representation of code structures. It is pointed out
that the tree-structural information is crucial for modeling
code (Mou et al. 2016; Yin and Neubig 2017; Rabinovich,
Stern, and Klein 2017; Sun et al. 2019). However, a “ﬂat”
neural architecture, such as an RNN, cannot capture struc-
ture information well.

In this paper, we propose a novel neural architecture,
TreeGen, for the code generation. To address the ﬁrst chal-
lenge, TreeGen adopts the recently proposed Transformer
architecture (Vaswani et al. 2017), which is capable of
capturing long dependencies. However, the original Trans-
former architecture is not designed for programs, and can-
not utilize tree structures, i.e., the second above mentioned
challenge. A standard way of utilizing structural informa-
tion, as in graph- and tree-based convolutional neural net-
works, is to combine the vector representations of a node
and its structural neighbors as the output of a structural con-
volution sub-layer. However, a standard Transformer archi-
tecture does not have such structural convolution sub-layers,
and it is not clear where to add them.

It is tempting to add structural convolution sub-layers in
all the Transformer blocks. Our core conjecture is that when
convolving a node and its structural neighbors, the vector
representation should mainly contain the information from
the original node. As the vector representation of the nodes
is processed by more blocks in the decoder of the Trans-
former, they gradually mix in more information from other
nodes and lose their original information. Therefore, we add

 
 
 
 
 
 
AST-based code generation could be thought of as ex-
panding a non-terminal node by a grammar rule. This pro-
cess is repeated until all leaf nodes are terminal. In Figure 1,
“1: root -> Module” is an example of the grammar rules,
where the preceding number is the ID of the rules. Follow-
ing the pre-order traverse, we could obtain the sequence of
rules that generate the AST shown in the top right corner.

Formally, the probability can be factorized as the proba-
bilities of the rules generating the code following the order.

p(code) =

(cid:89)P

i=1

p(ri | NL input, ri, · · · , ri−1)

(1)

where ri is the ith rule in the rule sequence. In this way,
our task is to train a model to calculate p(ri | NL input, pi),
i.e., given the natural language description and the currently
generated partial AST the model calculates the probabilities
of the rules to expand this node.

NL Reader
The input description determines the functionality of the
code. It can be a semi-structural description as in the Hearth-
Stone dataset, or a natural language as in ATIS and GEO
semantic parsing datasets.

For an input description, we ﬁrst tokenize it into to-
kens n1, n2, · · · , nL, where L denotes the length of
the input. Each token ni
to characters
c(ni)
, where S is the number of characters
1
in ni. All the tokens and characters are represented as real-
valued vectors n1, n2, · · · , nL and c(ni)
, · · · , c(ni)
by embeddings.

is then split

, · · · , c(ni)

, c(ni)
2

, c(ni)
2

S

S

1

Input Text Representation

Character Embedding. It often happens that similar words
have similar characters (e.g., “program” and “programs”).
To utilize this property, we represent a token by character
embeddings with a fully-connected layer

i = W (c)[c(ni)
n(c)

1

; · · · ; c(ni)
M ]

(2)

where W (c) are the weights and the character sequence is
padded to a pre-deﬁned maximum length M . After the fully-
connected layer, we also apply layer normalization (Lei Ba,
Kiros, and Hinton 2016). These vectors are then fed to the
NL reader, and are integrated with the word embeddings by
a gating sub-layer.

Neural Structure of NL Reader. The NL reader is com-
posed of a stack of blocks (Nd blocks in total). Each block
contains three different sub-layers (namely, self-attention,
gating mechanism, and word convolution) to extract fea-
tures, which we introduce in detail in the following subsec-
tions. Between two sub-layers, we employ a residual con-
nection (He et al. 2016) followed by a layer normalization.

Self-Attention. The self-attention sub-layer follows the
Transformer’s architecture (Vaswani et al. 2017), and uses
multi-head attention to capture long dependency informa-
tion.

For a sequence of input tokens n1, n2, · · · , nL, we rep-
resent them as an embedding n1, n2, · · · , nL by a look-up

Figure 1: A Python AST for code “length = 10”

the structural convolution sub-layer only to the ﬁrst several
Transformer decoder blocks but not all.

Generally speaking, the TreeGen architecture consists of
three parts: (1) a natural language (NL) reader (encoder)
encodes the text description; (2) an AST reader (the ﬁrst
several Transformer decoder blocks) encodes the previously
generated partial code with the structural convolution sub-
layers; (3) a decoder (the rest Transformer decoder blocks)
combines the query (the node to be expanded in AST) and
the previous two encoders to predict the next grammar rule.
We evaluated our model on an established benchmark
dataset for Python code generation, HearthStone (Ling et
al. 2016), which is a Python implementation of a card
game HearthStone. The results show that our model sig-
niﬁcantly outperforms previous models by 4.5 percentage
points. We further evaluated our model on two semantic
parsing datasets, ATIS and GEO, which translate natural lan-
guage sentences into lambda calculus logical forms. The re-
sults show that our model has the best accuracy among pre-
vious neural models, with 89.1% and 89.6% accuracy, re-
spectively. Our evaluation also shows that adding the struc-
tural convolution sub-layer to the ﬁrst several Transformer
blocks signiﬁcantly outperforms a Transformer with struc-
tural convolution in all blocks.

Our Model

We generate code by predicting the grammar rules of the
programming language. Figure 2 shows the overall picture
of our model, which comprises three parts: an NL reader, an
AST reader, and decoder. We introduce them in detail in the
following subsections.

Grammar Rule Prediction

In this section, we introduce how to model code genera-
tion as a series of classiﬁcation problems of grammar rules.
The programs can be decomposed into several context-free
grammar rules and parsed as an AST. For example, Figure 1
shows a Python AST for the code “length = 10”, where
dotted boxes are terminal symbols and solid boxes are non-
terminal symbols.

rootModulebodyAssigntargetsNameidlengthNumn10value1: root -> Module2: Module -> body3: body -> Assign10: Assign -> targets value11: targets -> Name13: Name -> id64: id -> length18: value -> Num19: Num -> n8: n -> 10Rule Sequence:1 2 3 10 11 13 64 18 19 8Figure 2: Overview of the TreeGen.

table. We also use position embeddings to encode the infor-
mation of word positions. In particular, we adopt the variant
in Dehghani et al. (2018), and compute the position embed-
ding for the ith word in the bth Transformer block as

pb,i[2j] = sin((i + b)/(100002j/d))
pb,i[2j + 1] = sin((i + b)/(100002j/d))

(3)

(4)

where pi,b[·] indexes a dimension of the vector pi,b, and d is
the number of dimensions (i.e., embedding size).
A Transformer block learns non-linear

features by
multi-head attention, which yields a matrix Y (self)
=
b
[y(self)
b,L ](cid:62), where Y (self)
∈ RL×d. For no-
tational simplicity, we omit the subscript b. The multi-head
layer is computed by

b,2 , · · · , y(self)

b,1 , y(self)

b

Y (self) = concat(head1, · · · , headH )Wh

(5)

where H denotes the number of heads and Wh is the weight.
An attention layer is applied in each head headt, computed
by

headt = softmax(

QK (cid:62)
√
dk

)V

(6)

where dk = d/H denotes the length of each features vector.
Q, K and V are computed by

[Q, K, V ] = [x1, · · · , xL](cid:62)[WQ, WK, WV ]

(7)

where WQ, WK, WV ∈ Rd×dk are model parameters. xi
is the input of this Transformer block. For the ﬁrst block,
it is the vector sum of the table-lookup embedding and the
position embedding, i.e., ni + p1,i; for other blocks, it is the
vector sum of the lower Transformer block’s output and the
position embedding that corresponds to this block.

Gating Mechanism. After the features are computed by
self-attention, we further incorporate with the information
of character embeddings. This is given by a gating mech-
anism based on softmax. For the ith word, we compute a

i

control vector qi from y(self)
softmax weight k(c)
i
linear transformation from n(c)
i
weight k(y)
i
ear transformation from y(self)
by

by a linear transformation. The
for character embedding is given by a
in Equation 2. The softmax
for Transformer’s output is given by another lin-
. Then, the gate is computed

i

[α(y)

i,t, α(c)

i,t] = softmax{q(cid:62)

(8)
They are used to weigh the feature of the Transformer’s
layer v(y)
, lin-
i
ear transformed from y(self)

and the feature of character embeddings v(c)
i
and n(c)

i k(c)
i }

i , respectively.

i k(y)

, q(cid:62)

i

i

hi,t = [α(y)

i,tv(y)

i + α(c)

i,tv(c)

(9)
Similar to Equation 5, the output of our gating mechanism
is Y (gate) = (hi,t)i,t, where (·)i,t represents a block matrix
with the elements being hi,t.

]

i

i

1

L

y(conv,l)
i

, · · · , y(gate)

is computed by

two convolutional

layers
the gating mechanism
and to extract the local features around
, · · · , y(conv,l)
, where l denotes the layer
L

Word Convolution. Finally,
are applied to the output of
y(gate)
1
each token y(conv,l)
of convolutional layers. The y(conv,l)
= W (conv,l)[y(conv,l−1)

(10)
where W (conv,l) are the convolution weights, w = (k − 1)/2,
and k denotes the window size. In particular, y(conv,0)
de-
notes the output of gating mechanism y(gate)
. In these layers,
separable convolution (Chollet 2017) is used. The reason
is separable convolution has fewer parameters that is easy
for training. For the ﬁrst and the last words, we add zero
padding. Between these layers, we use the GELU activation
function (Hendrycks and Gimpel 2016).

; · · · ; y(conv,l−1)
i+w

i−w

]

i

i

In summary, the NL reader has a few Transformer blocks
of self-attention, the gating mechanism, and word convolu-
tion. The natural language description is encoded as features
y(NL)
1

, · · · , y(NL)
L .

, y(NL)
2

ConvGatingTreeConvNL AttentionSelf attentionCharacterEmbeddingGatingRuleDefinitionEncodingDenseNL AttentionAST AttentionSoftmax & PointerNatural Language Description(Input)+PositionEmbeddingSelf Attention+PositionEmbeddingRule Sequence(Generated Code)Tree Path(Query)NdxN1xN2xNL ReaderAST ReaderDecoderDepthEmbeddingAST Reader
We design an AST reader to model the structure of the par-
tial AST that has generated. Although our programs are gen-
erated by predicting the sequence of grammar rules, these
rules alone lack a concrete picture of the program and are
insufﬁcient for predicting the next rule. Therefore, our AST
reader considers heterogeneous information, including the
predicted rules and the tree structures.

To incorporate such program-speciﬁc information, we
ﬁrst represent the code as a sequence of rules, then encode
the rules with attention mechanism, and ﬁnally use a tree
convolution layer to combine the encoded representation of
each node with its ancestors.

AST Representation

Rule Sequence Embedding. To encode the rule informa-
tion, we use the ID of the rules. Suppose we have a sequence
of rules r1, r2, · · · , rP that are been used to generate the par-
tial AST in a decoding step, where P denotes the length of
the sequence. We represent these rules as real-valued vectors
r1, r2, · · · , rP by table-lookup embeddings.

Rule Deﬁnition Encoding. The above table-lookup em-
bedding treats a grammar rule as an atomic token, and loses
the information of the rule’s content.

To alleviate this problem, we enhance the representation

of a rule with the encoding of rule deﬁnition.

For a grammar rule i : α → β1 · · · βK, where α is the
parent node and β1 · · · βK are child nodes. They can be ei-
ther terminal or non-terminal symbols. The index i is the ID
of the rule.

Similar to Equation 2, we encode the rule content as a vec-
tor r(c) by a fully-connected layer with input being the table-
lookup embeddings α, β1, · · · , βK of respective symbols.
It is noted that the sequence is also padded to a maximum
length.

Then the rule deﬁnition features y(rule)
computed by another fully-connected layer as

1

, · · · , y(rule)

P

y(rule)
i

= W (rule)[ri; r(c); α]

are

(11)

where ri is the table-lookup embedding of the rule ri, r(c)
is
i
the content-encoding rule representation, and we emphasize
the parent node α again. After that, a layer normalization is
followed.

Position and Depth Embeddings. Since our AST reader
would use self-attention mechanisms, we need to represent
the position where a grammar rule is used.

We ﬁrst adopt the position embedding as in Equation 4,
representing when a rule is used in the sequence r1, · · · , rP .
The position embeddings are denoted by p(r)

1 · · · , p(r)

However, such position embedding does not capture the
position of a rule in the AST. We further encode such in-
formation by a depth embedding. If we expand a symbol
α by the rule r : α → β1 · · · βK, we represent the depth
of the rule by its parent node, i.e., α. In this way, we
associate another sequence of table-lookup depth embed-
dings d1, · · · , dP to the sequence of used grammar rules
r1, · · · , rP .

P

Neural Structure of AST Reader. The AST reader is
also composed of a stack of blocks (N1 blocks in total).
Each block is decomposed into four sub-layers (namely,
self-attention, a gating mechanism, NL attention, and tree
convolution). We employ a residual connection around each
sub-layer except the layer of tree convolution. After each
sub-layer, we apply a layer normalization.

1

, y(ast-self)
2

, · · · , y(ast-self)
P

Self-Attention. To capture the information of AST, we
build a Transformer-like self-attention layer, where the in-
put is sum of the rule embedding, position embedding, and
depth embedding, i.e., ri + di + p(r)
i . The self-attention sub-
layer extract features y(ast-self)
of AST
input, using the same mechanism as Equations 4, 5, 6 with
different weights but add an additional depth embedding to
p(r)
i .
Gating Mechanism. We would like

to incorporate
the content-encoding rule representation y(rule)
into the
Transformer-extracted features. We adopt a gating mecha-
nism as in Equations 8, 9, and the fused features becomes
y(ast-g)
1

after this sub-layer.

, · · · , y(ast-g)

, y(ast-g)
2

i

P

NL Attention. During the decoding step, we should be in-
formed of the input NL description. This is given by a multi-
head NL attention, similar to the Transformer decoder’s at-
tention to its encoder (Vaswani et al. 2017). The extracted
features are denoted byy(ast-nl)

, · · · , y(ast-nl)

.

, y(ast-nl)
2

1

P

Tree Convolution. Should we consider only the above
sub-layers, it would be hard for the reader to combine the
information of a node with its ancestors. A node can be far
away from its ancestors in the rule sequence but is close
in structure. Therefore, it is difﬁcult for a traditional Trans-
former to extract such structural features.

We integrate the features of a node with those of its an-
cestors. We treat the AST as a graph and use an adjacency
matrix M to represent the directed graph. If a node αi is the
parent of αj, then Mji = 1. Suppose all the nodes are pre-
sented by features f1, · · · , fn, their parents’ features can be
given by the multiplication with the adjacency matrix:

, · · · , f (par)

[f (par)
1
where f (par)
denotes the parent of the ith node. For the father
of the root node, we pad it with the feature vector of the root
node itself.

] = [f1, · · · , fn]M

(12)

n

i

The tree-based convolution window, applied to the current

sub-tree, is given by

Y (tconv, l) = f (W (tconv, l)[Y (tconv, l−1);

Y (tconv, l−1)M ; · · · ; Y (tconv, l−1)M kt−1])

(13)

where W (tconv, l) is the weights of the convolutional layer,
kt denotes the window size (we set to 3 in our experi-
ments), l is the layer of these convolutional layers. In partic-
ular, Y (tconv, 0) = [y(att)
P ], where Y (tconv, 0) ∈
Rd×P . For the last layer of the AST reader, we add addi-
tional two convoluation layers. In the equation, f is the acti-
vation function and GELU is applied between these layers.
In summary, the AST reader has N1 blocks of these four

, · · · , y(att)

, y(att)
2

1

sub-layers, and yields the features y(ast)

, y(ast)
2

, · · · , y(ast)
P .

1

Decoder
Our ﬁnal component is a decoder that integrates the infor-
mation of the generated code with the NL description, and
predicts the next grammar rule. Similar to the AST reader,
a stack of blocks (N2 blocks in total) each with several sub-
layers is used in the decoder as follows. A residual connec-
tion is also employed around each sub-layer followed by a
layer normalization.

The decoder takes the non-terminal node to be expanded
as a query. Inspired by a previous approach (Sun et al. 2019),
the querying node is represented as a path from the root to
the node to be expanded. For example, if we are going to
expand node “Assign” in Figure 1, the path should be root,
Module, body, Assign. We represent the nodes in this path as
real-valued vectors. Then we apply a fully-connected layer
like Equation 2 to these vectors and the output of the path
(querying node) is q(path)

.

We then apply two attention layers to integrate the outputs

i

of the AST reader and the NL reader.

1

P

, · · · , f (tree)
, · · · , q(path)

We ﬁrst apply an AST attention layer over the out-
put of the AST reader with queries and extract features
f (tree)
. In this layer, Q is computed from queries
1
q(path)
; K and V are computed from the code
1
P
features y(ast)
, · · · , y(ast)
P . We further integrate the features
from the input description. This integration is also imple-
mented with an NL attention, where Q is computed by fea-
ture f (tree)
, · · · , f (tree)
; and K and V are computed by the
input description y(NL)
, · · · , y(NL)
L .
Finally, a set of two fully-connected layers, where the ﬁrst
layer has a GELU activation function, are followed to ex-
tract features for prediction.

P

1

1

Training and Inference
We predict the next grammar rule, among all possible candi-
dates, by softmax based on the decoder’s last layer features.
We also introduce the pointer network (See, Liu, and
Manning 2017) (essentially, an attention) that can directly
copy a token a from the NL description. In this case, the re-
sulting grammar rule is α → a, where α is a non-terminal
symbol to be expanded and a is a terminal symbol. Such
pointer mechanism is helpful for user-deﬁned identiﬁers
(e.g., variable and function names).

The choice between softmax rule prediction and the
pointer network is given by another gating mechanism pg,
also computed from the decoder’s last feature. The overall
predicted probability of the next grammar rule is

(cid:26)pg p(ri|·)

p(ri|·) =

if i ∈ D
(1 − pg) Pr{copy word t at step i|·} if i ∈ C
(14)
where i denotes the ID of the rule, D is the set of predeﬁned
rules, and C denotes the set of rules in the form of α → a,
where a is a terminal token that occurs in the NL description.
pg is the probability of using the type of predeﬁned rules,
and the p(ri|·) (the probability of each predeﬁned rules) are
computed by two single-layer perceptrons with the sigmoid
and softmax activation functions, respectively, and the input
of these layers are the features h(dec).

Figure 3: A example of the implement of HearthStone.

The pointer network is computed by

ξt = vT tanh(W1h(dec) + W2y(NL)

t

)

Pr{copy word t at step i|·} =

exp {ξt}
j=1 exp {ξj}

(cid:80)L

(15)

where h(dec) denotes the decoder’s last feature. The model
is optimized by maximizing negative log likelihood loss
against the reference program.

The inference starts with a start rule, start : snode −→
root, expanding a special symbol snode to the root symbol.
The recursive prediction terminates if every leaf node in the
predicted AST is a terminal. During prediction, we use beam
search with a size of 5. Invalid rules are excluded during
beam search.

Evaluation
We evaluated our approach on two types of benchmarks: (1)
a Python code generation benchmark, HearthStone, and (2)
two semantic parsing benchmarks, ATIS and GEO.

Experiment: HearthStone
Dataset. We ﬁrst evaluated our approach on the Hearth-
Stone benchmark (Ling et al. 2016). The benchmark con-
tains Python code that implements 665 different cards of
HearthStone. Each card is composed of a semi-structural
description and a groundtruth Python program. The Python
programs have a length of 84 tokens on average. The de-
scription comes with several attributes such as card name,
card type, as well as a natural language description for
the functionality of the card. A Python program is mainly
decided by the natural language description where the at-
tributes decide the constants or identiﬁer names. A sample
description and its corresponding Python program are shown
in Figure 3. When preprocessing the card description into to-
ken sequences, existing approaches consider two methods.

NAME:Darkscale HealerATK:4DEF:5COST:5DUR:-1TYPE:MinionPLAYER:NeutralRACE:NILRARITY:CommonDESCRIPTION: <b>Battlecry:</b> Restore 2 Health to all friendly characters.The ﬁrst (Yin and Neubig 2017; Hayati et al. 2018) (called
plain preprocessing) treats the whole description as plain
text and delimit the tokens by standard separators such as
space or periods. The second (Rabinovich, Stern, and Klein
2017) (called structural preprocessing) treats the descrip-
tions as semi-structural and always treat an attribute as one
token. In this experiment, we consider both methods and de-
note the results corresponding to the plain preprocessing as
TreeGen-A and that corresponding to the structural prepro-
cessing as TreeGen-B. We followed the train-dev-test split
in Ling et al. (2016), and the statistic is listed in Table 2.

Metrics. We measured the performance following the
metrics in Sun et al. (2019). We computed the StrAcc, which
is the percentage of programs that has exactly the same token
sequence as the ground truth; the BLEU score, which is used
to measure the similarity between the generated code and
the reference code at the token level; and the Acc+, which
is evaluated manually, allows variable renaming on top of
StrAcc, for every test case.

Settings. For neural networks, we set the number of NL
reader layers Nd = 6, and N1 = N2 = 5 for the AST
reader as well as the decoder. The size of all embedding is
256. The hidden sizes were all set to the 256 except each
fully-connected layers, except the ﬁrst layer was 1024 di-
mensions. We applied dropout after each layer (including at-
tention layers, gating mechanism layers, convolutional lay-
ers, and fully-connected layers, where the drop rate is 0.15).
The model is optimized by Adafactor (Shazeer and Stern
2018) with default parameters.

Overall Results. We show the results in Table 1. In this
table, the structural preprocessing has a better performance
compared with the plain preprocessing.

As shown, our model achieves 6 percentage points ac-
curacy improvement with plain preprocessing and 4.5 per-
centage points accuracy improvement with structural pre-
processing. For the BLEU score, our model also achieves
the best results. These boosts in performance indicate that
TreeGen successfully alleviates the long dependency prob-
lem and effectively encodes the structural information in the
code generation.

Time Efﬁciency. We further evaluated the complexity of
our model on the HearthStone, and the result shows that our
model is faster than the previous ones. It takes 18s for an
epoch on a single Nvidia Titan XP, whereas 180s for the
CNN (Sun et al. 2019) and 49s for the RNN (Yin and Neubig
2017).

Location of Structural Convolution Sub-layer. One of
the keys of our approach is to add the structural convolution
sub-layers only to part of the Transformer blocks in the de-
coder. To evaluate whether this design decision is effective,
we evaluate four competing settings: 1) adding the struc-
tural convolution sub-layers to all Transformer blocks (i.e.,

N1 = 10); 2) adding the structural convolution sub-layers to
the ﬁrst 7 blocks in AST reader (i.e., N1 = 10(7)); 3) adding
the structural convolution sub-layers to the ﬁrst 8 blocks in
AST reader (i.e., N1 = 10(8)); 4) the other adds to none
(i.e., N1 = 0). As we can see, from Table 1 our approach
adding the sub-layer to all transformer blocks (N1 = 10)
signiﬁcantly outperforms the last setting (N1 = 0), but
slightly worse than the other two settings.

Ablation Test. We ablated our model (TreeGen-B was
used) to analyze the contribution of each component, results
also shown in Table 1. First, we compared our model with
the traditional Transformer, which is a Transformer with-
out effective structure modeling. We achieved 21 percentage
points higher accuracy (p-value is less than 0.001) and 12
higher BLEU score. This result provides strong evidence of
the effectiveness of the AST reader in our model and the im-
portance of the structural information. Next, we replaced the
tree convolutional layers in the AST Reader with two layers
of fully-connected layers, and we removed the char embed-
ding, rule deﬁnition encoding, self-attention layers in turn.
The experimental results show the identiﬁers-encoding, alle-
viating long-dependency and structural information signiﬁ-
cantly inﬂuence the accuracy. Please note that in some cases
BLEU increases while StrAcc and Acc+ decrease. Here we
consider StrAcc and Acc+ more important as they guarantee
the correctness of the generated programs and correctness is
usually crucial in code generation.

Experiment II: Semantic Parsing
Dataset. We further evaluated our approach on the seman-
tic parsing tasks. Our experiment was conducted on two se-
mantic parsing datasets, ATIS and GEO. The input of these
datasets is a natural language description, while the output
is a short piece of code in lambda calculus. We followed the
standard train-dev-test split of these datasets, and the statis-
tics are listed in Table 2.

Metrics and Settings.
In this task, we follow the evalua-
tion of the previous approaches (Dong and Lapata 2016) and
use accuracy as the metric, where the tree exact match was
considered to avoid spurious errors. In other words, the or-
der of the children can be changed within conjunction nodes.
We followed all the settings in the HS experiment except that
we changed the embedding size and the hidden sizes to 128
compared with the setting of the HS experiment.

Results. Table 3 shows the performance of our TreeGen.
As seen, the accuracy of our approach is sightly worse than
the traditional approach WKZ14 (Wang, Kwiatkowski, and
Zettlemoyer 2014), which is based on the CCG parser and
uses a large number of templates. This traditional approach
is hard to generalize new datasets. However, our model
was directly adopted from the HS dataset, and achieved the
highest accuracy, among all neural models (Dong and La-
pata 2016; Rabinovich, Stern, and Klein 2017; Dong and
Lapata 2018; Chen, Sun, and Han 2018; Xu et al. 2018;

i
a
l
P

a
r
u
t
c
u
r
t
S

Model

n LPN (Ling et al. 2016)

SEQ2TREE (Dong and Lapata 2016)
YN17 (Yin and Neubig 2017)
ASN (Rabinovich, Stern, and Klein 2017)
ReCode (Hayati et al. 2018)

TreeGen-A

l ASN+SUPATT (Rabinovich, Stern, and Klein 2017)

SZM19 (Sun et al. 2019)

TreeGen-B

Location of Structural Convolutional Sub-layer
N1 = 10, N2 = 0
N1 = 10(7), N2 = 0
N1 = 10(8), N2 = 0
N1 = 0, N2 = 10
Ablation test
Baseline: Transformer
- Tree Convolution
- Rule Deﬁnition Encoding
- Char Embedding
- Self-Attention

StrAcc Acc+ BLEU
67.1
53.4
75.8
77.6
78.4

–
6.1
1.5
–
16.2 ∼18.2
–
18.2
–
19.6

25.8

22.7
27.3

31.8

25.8
27.3
25.8
21.2

10.6 (p = 0.015)
27.3 (p = 0.015)
27.3 (p < 0.001)
15.2 (p < 0.001)
28.8 (p < 0.001)

25.8

–
30.3

33.3

27.3
28.8
28.8
22.7

12.1
27.3
28.8
18.2
28.8

79.3

79.2
79.6

80.8

80.4
78.5
78.5
79.6

68.0
80.9
81.8
72.9
81.0

Table 1: Performance of our model in comparison with previous state-of-the-art results.

Statistics

# Train
# Dev
# Test

Avg. tokens in description
Max. tokens in description
Avg. tokens in code
Max. tokens in code

Exp II

ATIS GEO

4,434
491
448

10.6
48
33.9
113

600
-
280

7.4
23
28.3
144

HS

533
66
66

35.0
76.0
83.2
403

Table 2: Statistics of the datasets we used.

Method

ATIS GEO

l ZC07 (Zettlemoyer and Collins 2007)
FUBL (Kwiatkowski et al. 2011)
KCAZ13 (Kwiatkowski et al. 2013)
WKZ14 (Wang, Kwiatkowski, and Zettlemoyer 2014)

a
n
o
i
t
i
d
a
r
T
s SEQ2SEQ (Dong and Lapata 2016)

SEQ2TREE (Dong and Lapata 2016)
ASN (Rabinovich, Stern, and Klein 2017)
ASN+SUPATT (Rabinovich, Stern, and Klein 2017)
COARSE2FINE (Dong and Lapata 2018)
TRANX (Yin and Neubig 2018)
Seq2Act (Chen, Sun, and Han 2018)
Graph2Seq (Xu et al. 2018)
SZM19 (Sun et al. 2019)

TreeGen

k
r
o
w
t
e
N

l
a
r
u
e
N

84.6
82.8
-
91.3

84.2
84.6
85.3
85.9
87.7
86.2
85.5
85.9
85.0

89.1

86.1
88.6
89.0
90.4

84.6
87.1
85.7
87.1
88.2
88.2
88.9
88.1
-

89.6

Table 3: Accuracy in semantic parsing (in percentage).

Sun et al. 2019). This experiment shows the effectiveness
and generalizability of TreeGen.

Related Work
Code generation achieves signiﬁcant progress in recent
years. The early approaches are mainly based on templates
(Zettlemoyer and Collins 2007; Zettlemoyer and Collins
2005; Kushman and Barzilay 2013; Wang, Kwiatkowski,
and Zettlemoyer 2014). With the prosperity of deep learn-
ing, the sequence-to-sequence framework has shown to be

effective in various tasks (Sutskever, Vinyals, and Le 2014).
Ling et al. (2016) applied this framework to generate code
based on tokens. Unlike natural languages, it is shown
that the code contains much more structural information.
Thus, the abstract syntax tree (AST) was used in more re-
cent works (Dong and Lapata 2016; Yin and Neubig 2017;
Rabinovich, Stern, and Klein 2017; Hayati et al. 2018;
Yin and Neubig 2018). However, these studies mainly use
recurrent neural networks (RNNs) from the long depen-
dency problem (Bengio, Simard, and Frasconi 1994). Sun
et al. (2019) proposed to use the convolutional neural net-
work (CNN) to handle the long dependency problem. Our
approach addresses this problem by Transformer’s intensive
attention mechanism (Vaswani et al. 2017). To incorporate
the structural information and the idea of self-attention, we
propose a tree-based Transformer architecture for code gen-
eration.

Conclusion
In this work, we propose TreeGen for program generation.
TreeGen uses the attention mechanism of Transformers to
alleviate the long-dependency problem and introduces the
AST reader to combine the grammar rules and the AST
structure.

The evaluation was conducted on a Python dataset,
HearthStone, and two semantic parsing datasets, ATIS and
GEO. The experimental results show that our model signiﬁ-
cantly outperforms existing approaches. We also conducted
in-depth ablation tests, which suggests that each component
in our model plays a signiﬁcant role.

Acknowledgments
This work is
sponsored by the National Key Re-
search and Development Program of China under Grant
No. 2017YFB1001803, and National Natural Science Foun-
dation of China under Grant Nos. 61672045, 61529201, and
61922003. Lili Mou is an Amii Fellow; he is supported by

[Rabinovich, Stern, and Klein 2017] Rabinovich, M.; Stern,
M.; and Klein, D. 2017. Abstract Syntax Networks for Code
Generation and Semantic Parsing. In ACL, 1139–1149.
[See, Liu, and Manning 2017] See, A.; Liu, P. J.; and Man-
ning, C. D. 2017. Get to the point: Summarization with
pointer-generator networks. In ACL, 1073–1083.
[Shazeer and Stern 2018] Shazeer, N., and Stern, M. 2018.
Adafactor: Adaptive learning rates with sublinear memory
cost. arXiv preprint arXiv:1804.04235.
[Sun et al. 2019] Sun, Z.; Zhu, Q.; Mou, L.; Xiong, Y.; Li,
G.; and Zhang, L. 2019. A grammar-based structural cnn
In AAAI, volume 33, 7055–
decoder for code generation.
7062.
[Sutskever, Vinyals, and Le 2014] Sutskever, I.; Vinyals, O.;
and Le, Q. V. 2014. Sequence to sequence learning with
neural networks. In NIPS, 3104–3112.
[Vaswani et al. 2017] Vaswani, A.; Shazeer, N.; Parmar, N.;
Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, Ł.; and Polo-
sukhin, I. 2017. Attention is all you need. In NIPS, 6000–
6010.
A.;
[Wang, Kwiatkowski, and Zettlemoyer 2014] Wang,
Kwiatkowski, T.; and Zettlemoyer, L.
2014. Morpho-
syntactic lexical generalization for CCG semantic parsing.
In EMNLP, 1284–1295.
[Xu et al. 2018] Xu, K.; Wu, L.; Wang, Z.; Yu, M.; Chen, L.;
and Sheinin, V. 2018. Exploiting Rich Syntactic Information
for Semantic Parsing with Graph-to-Sequence Model.
In
ACL, 918–924.
[Yin and Neubig 2017] Yin, P., and Neubig, G. 2017. A Syn-
tactic Neural Model for General-Purpose Code Generation.
In ACL, 440–450.
[Yin and Neubig 2018] Yin, P., and Neubig, G.
2018.
TRANX: A transition-based neural abstract syntax parser
In EMNLP, 7–
for semantic parsing and code generation.
12.
[Zettlemoyer and Collins 2005] Zettlemoyer, L. S.,
and
Collins, M. 2005. Learning to map sentences to logical
form: structured classiﬁcation with probabilistic categorial
grammars. In UAI, 658–666.
and
[Zettlemoyer and Collins 2007] Zettlemoyer,
2007. Online learning of relaxed CCG
Collins, M.
grammars for parsing to logical form. In EMNLP-CoNLL,
678–687.

L.,

the CCAI Chair Program; and he also thanks AltaML for
support.

References
[Bengio, Simard, and Frasconi 1994] Bengio, Y.; Simard, P.;
and Frasconi, P. 1994. Learning long-term dependencies
with gradient descent is difﬁcult. IEEE Trans. Neural Net-
works 5(2):157–166.
[Chen, Sun, and Han 2018] Chen, B.; Sun, L.; and Han, X.
2018. Sequence-to-Action: End-to-End Semantic Graph
Generation for Semantic Parsing. In ACL, 766–777.
[Chollet 2017] Chollet, F. 2017. Xception: Deep learning
In CVPR, 1251–
with depthwise separable convolutions.
1258.
[Dehghani et al. 2018] Dehghani, M.; Gouws, S.; Vinyals,
O.; Uszkoreit, J.; and Kaiser, Ł. 2018. Universal transform-
ers. arXiv preprint arXiv:1807.03819.
[Dong and Lapata 2016] Dong, L., and Lapata, M. 2016.
Language to Logical Form with Neural Attention. In ACL,
33–43.
[Dong and Lapata 2018] Dong, L., and Lapata, M. 2018.
Coarse-to-Fine Decoding for Neural Semantic Parsing. In
ACL, 731–742.
[Hayati et al. 2018] Hayati, S. A.; Olivier, R.; Avvaru, P.;
Yin, P.; Tomasic, A.; and Neubig, G. 2018. Retrieval-Based
Neural Code Generation. In EMNLP, 925–930.
[He et al. 2016] He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016.
In CVPR,
Deep residual learning for image recognition.
770–778.
[Hendrycks and Gimpel 2016] Hendrycks, D., and Gimpel,
K. 2016. Bridging Nonlinearities and Stochastic Regu-
larizers with Gaussian Error Linear Units. arXiv preprint
arXiv:1606.08415.
[Kushman and Barzilay 2013] Kushman, N., and Barzilay,
R. 2013. Using semantic uniﬁcation to generate regular
expressions from natural language. In NAACL, 826–836.
[Kwiatkowski et al. 2011] Kwiatkowski, T.; Zettlemoyer, L.;
Goldwater, S.; and Steedman, M. 2011. Lexical general-
ization in CCG grammar induction for semantic parsing. In
EMNLP, 1512–1523.
[Kwiatkowski et al. 2013] Kwiatkowski, T.; Choi, E.; Artzi,
Y.; and Zettlemoyer, L. 2013. Scaling semantic parsers with
on-the-ﬂy ontology matching. In EMNLP, 1545–1556.
[Lei Ba, Kiros, and Hinton 2016] Lei Ba, J.; Kiros, J. R.; and
Hinton, G. E. 2016. Layer normalization. arXiv preprint
arXiv:1607.06450.
[Ling et al. 2016] Ling, W.; Blunsom, P.; Grefenstette, E.;
Hermann, K. M.; Koˇcisk`y, T.; Wang, F.; and Senior, A.
2016. Latent Predictor Networks for Code Generation. In
ACL, 599–609.
[Mou et al. 2016] Mou, L.; Li, G.; Zhang, L.; Wang, T.; and
Jin, Z. 2016. Convolutional neural networks over tree struc-
tures for programming language processing. In AAAI, 1287–
1293.

