ACERAC: Efﬁcient reinforcement learning in ﬁne
time discretization

Jakub Łyskawa, Paweł Wawrzy´nski
Warsaw University of Technology, Institute of Computer Science, Warsaw, Poland

1

2
2
0
2

l
u
J

1
1

]

G
L
.
s
c
[

4
v
4
0
0
4
0
.
4
0
1
2
:
v
i
X
r
a

Abstract—One of the main goals of reinforcement learning
(RL) is to provide a way for physical machines to learn optimal
behavior instead of being programmed. However, effective control
of the machines usually requires ﬁne time discretization. The
most common RL methods apply independent random elements
to each action, which is not suitable in that setting. It is not
feasible because it causes the controlled system to jerk, and does
not ensure sufﬁcient exploration since a single action is not long
enough to create a signiﬁcant experience that could be translated
into policy improvement. In our view these are the main obstacles
that prevent application of RL in contemporary control systems.
To address these pitfalls,
in this paper we introduce an RL
framework and adequate analytical tools for actions that may
be stochastically dependent in subsequent time instances. We
also introduce an RL algorithm that approximately optimizes
a policy that produces such actions. It applies experience replay
to adjust likelihood of sequences of previous actions to optimize
expected n-step returns the policy yields. The efﬁciency of this
algorithm is veriﬁed against four other RL methods (CDAU,
PPO, SAC, ACER) in four simulated learning control problems
(Ant, HalfCheetah, Hopper, and Walker2D) in diverse time
discretization. The algorithm introduced here outperforms the
competitors in most cases considered.

Index Terms—Reinforcement learning, Actor-Critic, Experi-

ence Replay, Fine Time Discretization.

I. INTRODUCTION

The subject of this paper is reinforcement learning (RL)
[1]. This ﬁeld offers methods of learning to make sequential
decisions in dynamic environments. One application of such
methods is the literal implementation of “machine learning”,
i.e., enabling machines and software to learn optimal behavior
instead of being programmed.

The usual goal of RL methods is to optimize a policy that
samples an action based on the current state of a learning
agent. The only stochastic dependence between subsequent
actions is through state transition: the action moves the agent
to another state, which determines the distribution of another
action. The main analytical tools in RL are based on this
lack of other dependence between actions. For example, for
a given policy, its value function expresses the expected sum of
discounted rewards the agent may expect starting from a given
state. The sum of rewards does not depend on actions taken
before the given state has been reached. Hence, only the given
state and the policy matter.

Lack of dependence between actions beyond state transition
leads to the following difﬁculties. In the physical implemen-
tation of RL, e.g., in robotics, the lack of dependence usually

means that white noise is added to control actions. However,
this makes control discontinuous and subject to constant rapid
changes. In addition, this is often impossible to implement
since electric motors to execute these actions can not change
their output too quickly. Even if such control is possible, it
requires large amounts of energy, makes the controlled system
shake, and exposes it to damages.

Control frequency for real-life control systems can be much
higher than that of simulated environments for which RL
methods are designed. The typical frequency of the control
signal for environments commonly used as benchmarks for
RL algorithms ranges from 20 to 60 Hz [2], while the control
frequency considered for real-life robots is 10 times higher,
from 200 to 500 Hz [3] and can be even higher, up to 1000 Hz
[4]. Therefore, ﬁner time discretization should be considered
to make RL more suitable for automation and robotics.

The lack of dependence between actions beyond state transi-
tion may also reduce the efﬁciency of learning as follows. Each
action is then an independent random experiment that leads
to policy improvement. However, due to the limited accuracy
of (action-)value function approximation, the consequences
of a single action may be hard to recognize. The ﬁner the
time discretization, the more serious this problem becomes.
The consequences of a random experiment distributed over
several time instants could be more tangible and thus easier
to recognize.

Additionally, ﬁne time discretization makes policy evalua-
tion more difﬁcult, as it requires accounting for more distant
rewards. Technically, the discount factor needs to be larger,
which makes learning more difﬁcult for most RL algorithms
[5]. The above problems are serious enough to prevent RL
from wide applicability of RL in real life control systems.

To avoid the above pitfalls, we introduce in this paper
a framework in which an action is both a function of state and
a stochastic process whose subsequent values are dependent.
In particular, these subsequent values can be autocorrelated,
which makes the resulting actions close to one another. A part
of action trajectory can create a distributed-in-time random
experiment that leads to policy improvement. An RL algorithm
is also introduced that optimizes a policy based on the above
principles.

The contribution of this paper may be summarized by the

following points:

• A framework is introduced here in which a policy pro-
duces actions based on the states and values of a stochas-

0000–0000/00$00.00 © 2021 IEEE

 
 
 
 
 
 
tic process. This framework is suited for the application
of RL to optimization of control in physical systems, e.g.,
in robots.

• An ACERAC algorithm, based on Actor-Critic structure
and experience replay, is introduced that approximately
optimizes a policy in the aforementioned framework.
• An extensive study is described here with four benchmark
learning control problems (Ant, Half-Cheetah, Hopper,
and Walker2D) at diverse time discretization. The per-
formance of the ACERAC algorithm is compared using
these problems with state-of-the-art RL methods.

This paper extends [6] in several directions. We introduce
here the notion of adjusted noise which is the input to the
noise-value function. Also, when manipulating the policy
parameter, the value of the noise-value function at the end of
the action sequence is taken into account. The experimental
study of the resulting algorithm is almost entirely new.

The rest of the paper is organized as follows. The problem
considered here is formulated in Sec. II. Another section
overviews related literature. Sec. IV introduces a policy that
produces autocorrelated actions along with tools for its analy-
sis. Sec. V introduces the ACERAC algorithm that approx-
imately optimizes this policy. Sec. VI presents simulations
that compare the algorithm presented with state-of-the-art
reinforcement learning methods. The last section concludes
the paper.

II. PROBLEM FORMULATION

We consider here the standard Markov Decision Process
(MDP) model [1] in which an agent operates in discrete time
t = 1, 2, . . . . At time t the agent ﬁnds itself in a state, st ∈ S,
takes an action, at ∈ A, receives a reward, rt ∈ R, and is
transited to another state, st+1 ∼ Ps(·|st, at), where Ps is
a ﬁxed but unknown conditional probability.

The goal of the agent is to learn to designate actions to
be able to expect at each t the highest discounted rewards in
the future. To ensure exploration, there is usually a random
component introduced into the action selection.

We mainly consider the application of the MDP model to
control physical devices. Therefore, we assume that both S and
A are spaces of vectors of real numbers [7]. We also assume
ﬁne time discretization typical for such applications, which
means that designating actions the agent should account for
rewards that are quite distant in terms of discrete-time steps in
the future. This translates into a discount parameter close to 1,
e.g., γ ∈ (0.995, 1). We require the reasons for the instability
of learning with such a large γ [5] to be overcome.

To ensure applicability to control of physical machines,
we require that
the actions should generally be close for
subsequent t, even if they are random. Also, the learning
should be efﬁcient in terms of the amount of experience needed
to optimize the agent’s behavior.

III. RELATED WORK

A general way to make subsequent actions close is the
autocorrelation of the randomness on which these actions
are based. Efﬁciency in terms of experience needed can be

2

provided by experience replay. We focus on these concepts in
the literature review below.

A. Stochastic dependence between actions

An autocorrelated stationary stochastic process, now re-
ferred to as Ornstein-Uhlenbeck (OU) process, was analyzed in
[8]. This process is the only autocorrelated Gaussian stochastic
process that has the Markov property [9].

A policy with autocorrelated actions was analyzed in [10].
This policy was optimized by a standard RL algorithm that
did not account for the dependence of actions. In [11] a policy
was analyzed whose parameters were incremented by the OU
stochastic process. Essentially, this resulted in autocorrelated
random components of actions. In [12] a policy is analyzed
that produced an action that was the sum of the OU noise and
a deterministic function of the state. However, no learning
algorithm was presented in the paper that accounted for the
speciﬁc properties of this policy.

B. Reinforcement learning for ﬁne time discretization

In [13] RL in arbitrarily ﬁne time discretization is analyzed.
It is proven that RL based on the action-value function can
not be effective when time discretization becomes sufﬁciently
ﬁne and note the importance of the dependence of the action
noise in the next
timesteps. In the aforementioned work
RL algorithm called Deep Advantage Updating (DAU) for
discrete actions and its variant for continuous actions (CDAU)
are introduced. These methods are based on estimating the
advantage function and are presented as immune to time
discretization. They are based on Deep Q-Network (DQN) [14]
and Deep Deterministic Policy Gradient (DDPG) [15] algo-
rithms, respectively, and use the OU process as autocorrelated
noise.

Integral Reinforcement Learning (IRL) is an approach to
learning control policies for continuous-time environments.
IRL is based on the assumption that the control problem can be
divided into a hierarchy of control loops [16]. This assumption
is usually not satisﬁed in challenging tasks and thus IRL is not
applicable to tasks with any state transition dynamics, only
those belonging to a certain relatively narrow class [17].

C. Reinforcement learning with experience replay

The Actor-Critic architecture for RL was ﬁrst introduced
in [18]. Approximators were applied to this structure for the
ﬁrst time in [19]. Basic on-line RL algorithms use consecutive
events of the agent-environment
interaction to update the
the efﬁciency of these algorithms, expe-
policy. To boost
rience replay (ER) can be applied, i.e., storing the events
in a database, sampling, and using them for policy updates
several times per each actual event [20]. ER was combined
with the Actor-Critic architecture for the ﬁrst time in [21].

However, the application of experience replay to Actor-
Critic encounters the following problem. The learning algo-
rithm needs to estimate the quality of a given policy based
on the consequences of actions that were registered when
a different policy was in use. Importance sampling estimators

are designed to do that, but they can have arbitrarily large
variances. In [21] the problem was addressed with truncating
density ratios present in those estimators. In [22] speciﬁc
correction terms were introduced for that purpose.

Another approach to the aforementioned problem is to
prevent
the algorithm from inducing a policy that differs
too much from the one tried. This idea was ﬁrst applied in
Conservative Policy Iteration [23]. It was further extended
in Trust Region Policy Optimization [24]. This algorithm
optimizes a policy with the constraint
the Kullback-
Leibler divergence between this policy and the one being
tried should not exceed a given threshold. The K-L divergence
becomes an additive penalty in Proximal Policy Optimization
algorithms, namely PPO-Penalty and PPO-Clip [25].

that

A way to avoid the problem of estimating the quality of
a given policy based on the one tried is to approximate the
action-value function instead of estimating the value function.
Algorithms based on this approach are DQN [14], DDPG [15],
and Soft Actor-Critic (SAC) [26]. Although OU noise was
added to the action in the original version of DDPG, this
algorithm was not adapted to this fact in any speciﬁc way.
SAC uses white noise in actions and it is considered one of
the most efﬁcient in this family of algorithms.

IV. POLICY WITH AUTOCORRELATED ACTIONS

In this section, we introduce a framework for reinforcement
learning where subsequent actions are stochastically dependent
beyond state transition. We also design tools for the analysis
of such a policy.

Let an action, at, be designated as

at = π(st, ξt; θ),

(1)

where π is a deterministic transformation, st is a current state,
θ is a vector of trained parameters, and (ξt)∞
t=1 is a stochastic
process with values in Rd. We require this process to have the
following properties:

• Stationarity: The marginal distribution of ξt is the same

for each t.

• Zero mean: Eξt = 0 for each t.
• Autocorrelation decreasing with growing lag:

EξT

t ξt+k > EξT

t ξt+k+1 > 0 for k ≥ 0.

(2)

Essentially that means that values of the process are close
to each other when they are in close time instants.

• Markov property: For any t and k, l ≥ 0, the conditional

distributions

(ξt, . . . , ξt+k|ξt−1, . . . , ξt−1−l) and (ξt, . . . , ξt+k|ξt−1)
(3)
are the same. In words, dependence of future values of
the process, ξt+k, k ≥ 0, on its past is entirely carried
over by ξt−1.

Consequently, if only π (1) is continuous for all its argu-
ments, and subsequent states st are close to each other, then
the corresponding actions are close too, even though they are
random. Because they are close, they are feasible in physical
systems. Because they are random, they create a consistent

3

Fig. 1. Realization of the normal white noise ((cid:15)t), and the Ornstein-
Uhlenbeck process (ξt) (4).

distributed-in-time experiment that can give a clue to policy
improvement.

Below we analyze an example of (ξt) that meets the above

requirements.

a) Ornstein-Uhlenbeck (OU) process (ξt): Let α ∈

[0, 1), C be a positively deﬁnite matrix, and

(cid:15)t ∼ N (0, C),
ξ1 = (cid:15)1

t = 1, 2, . . .

ξt = αξt−1 +

(cid:112)

1 − α2(cid:15)t,

t = 2, 3, . . .

(4)

Fig. 1 demonstrates a realization of both the white noise ((cid:15)t)
and (ξt). Let us analyze if (ξt) has the required properties.
Their derivations can be found in Appendix A.

Both (cid:15)t and ξt have the same marginal distribution N (0, C).
Therefore, (ξt) is stationary and zero-mean. Applying induc-
tion to (4) one obtains
EξtξT

t+k = α|k|C and EξT

t ξt+k = α|k|tr(C)

for any t, k. Therefore, (ξt) is autocorrelated, and this autocor-
relation decreases with growing lag. Consequently, the values
of ξt are closer to one another for subsequent t than the values
of (cid:15)t, namely

E(cid:107)(cid:15)t − (cid:15)t−1(cid:107)2 = E((cid:15)t − (cid:15)t−1)T ((cid:15)t − (cid:15)t−1) = 2tr(C)

E(cid:107)ξt − ξt−1(cid:107)2 = E

(cid:16)

(α−1)ξt−1 +
(cid:16)

(cid:17)T

(cid:112)

1 − α2(cid:15)t
(cid:112)

(cid:17)

×

1 − α2(cid:15)t
(α − 1)ξt−1 +
= (α − 1)2tr(C) + (1 − α2)tr(C)
= (1 − α)2tr(C).

The Markov property of (ξt) directly results from how ξt (4)

is computed.

In fact, marginal distributions of the process (ξt), as well as
its conditional distributions, are normal, and their parameters
have compact forms. Let us denote
t , . . . , ξT

t+n−1]T .

(5)

¯ξn
t = [ξT
The distribution of ¯ξn
t

is normal

N (0, Ωn

0 ),

(6)

where Ωn
conditional distribution ( ¯ξn

0 (21) is a matrix dependent on n, α, and C. The

t |ξt−1) is also normal,

of the ordinary value function. In particular, we consider n-
step look-ahead equation in the form

4

N (Bnξt−1, Ωn

1 ),

(7)

W π(ut−1, st)

(11)

where both Bn (24) and Ωn
n, α, and C.

1 (25) are matrices dependent on

b) Noise-value function: In policy (1) there is a stochas-
tic dependence between actions beyond the dependence re-
sulting from the state transition. Therefore,
the traditional
understanding of policy as distribution of actions conditioned
on state does not hold here. Each action depends on the
current state, but also previous states and actions. Analytical
usefulness of the traditional value function and action-value
function is thus limited.

Our objective now is to deﬁne an analytical tool in the form

of a function that satisﬁes the following:
R1. A hard requirement: The function designates an expected
value of future discounted rewards based on entities that
this expected value is conditioned on.

R2. An efﬁciency requirement: A small change of policy
corresponds to a small change of this function. While
this is not necessary, it facilitates concurrent learning of
the policy and this function approximation.

In order to meet
an adjusted noise, (ut)∞
the same space Rd. Let

the above requirements we introduce
t=1, as follows. ut and ξt belong to

f (·; θ, s)
(8)
be a bijective function in Rd parameterized by θ and state. We
have

ξt−1 =f (ut−1; θ, st)
ut−1 =f −1(ξt−1; θ, st).
Formally, we can apply f to convert ξt−1 to ut−1 and back
whenever necessary.

(9)

As an analytical tool satisfying the aforementioned hard
requirement R1, we propose the noise-value function deﬁned
as

W π(u, s) = Eπ





(cid:88)

i≥0

γirt+i

(cid:12)
(cid:12)
(cid:12)ξt−1 = f (u; θ, s), st = s



 .

(10)
The course of events starting in time t depends on the current
state st and the value ut−1. Because of the Markov property
of (ξt) (3) and the direct equivalence between ξt−1 and ut−1,
the pair (cid:104)ut−1, st(cid:105) is a proper condition for the expected value
of future rewards.

To satisfy the aforementioned efﬁciency requirement R2,
we design the f function (8) based on π. It should make
the distribution of an initial part of the action trajectory
(at, . . . ) similar for given (cid:104)ut−1, st(cid:105), regardless of the policy
parameter θ. Therefore, when θ changes due to learning,
the arguments of the W π function (10) still deﬁne similar
circumstances in which the rewards start being collected. This
prevents large changes in the shape of W π. An example of
an appropriate f function is provided below in (18).

We can consider the pair (cid:104)ut−1, st(cid:105) a state of an extended
MDP. Therefore, the noise-value function has all the properties

(cid:18) n−1
(cid:88)

= Eπ

i=0

(cid:19)
(cid:12)
γirt+i +γnW π(f (ξt+n−1; θ, st+n), st+n)
(cid:12)
.
(cid:12)ut−1, st

It says that
the noise-value function is the expected sum
of several ﬁrst rewards, and that the rest of them are also
designated by the noise-value function itself.

The algorithm introduced below manipulates the policy π
(1) to make n-step sequences of registered actions more or
less likely in the future. Let us consider

t = [sT
¯sn
i = [aT
¯an

t , . . . , sT
t , . . . , aT

t+n−1]T ,
t+n−1]T ,

and

¯π(¯an

t , ξt−1; θ)

t |¯sn
being a probability density of the action sequence ¯an
t condi-
tioned on the sequence of visited states ¯sn
t , the preceding noise
value ξt−1, and the policy parameter θ. This density is deﬁned
by π, and the conditional probability distribution ¯ξn
t |ξt−1. The
algorithm deﬁned in the next section updates θ to manipulate
the above distribution.

(12)

c) The neural-AR policy: A simple and practical way to
implement π (1) is as follows. A feedforward neural network,

A(s; θ),

has input s and weights θ. An action is designated as

at = π(st, ξt; θ) = A(st; θ) + ξt,

(13)

(14)

for ξt in the form (4). Let us analyze the distribution ¯π (12).
In this order the density of the normal distribution with mean
µ and covariance matrix Ω will be denoted by

ϕ(· ; µ, Ω).

(15)

Let us also denote

¯A(¯sn

i ; θ) = [A(st; θ)T , . . . , A(st+n−1; θ)T ]T .

(16)

It can be seen that the distribution (¯an
namely N ( ¯A(¯sn
fore,

t ; θ) + Bnξt−1, Ωn

t |¯sn

t , ξt−1) is normal,
1 ), (see (6) and (7)). There-

¯π(¯an

t |¯sn

i , ξt−1; θ) = ϕ(¯an

t ; ¯A(¯sn
What is of paramount importance is the log-density gradient

t ; θ) + Bnξt−1, Ωn

1 ).

∇θ ln ¯π. For ¯π deﬁned as (14) it may be expressed as

∇θ ln ¯π(¯an
= ∇θ ¯A(¯sn

t |¯sn
t ; θ)(Ωn

i , ξt−1; θ)

1 )−1(¯an

t − Bnξt−1 − ¯A(¯sn

t ; θ)).

The f function (8) may have the form

ut−1 = f −1(ξt−1; θ, st) = A(st; θ) + αξt−1
ξt−1 = f (ut−1; θ, st) = α−1(ut−1 − A(st; θ)).

(17)

(18)

Then ut−1 is the expected value of at given θ, st, and ξt−1.
Consequently, this deﬁnition of f delimits differences between
noise-value functions of different policies. This is because

W π(u, s) means for any policy the expected sum of future
rewards received starting from the same point, which is the
current state equal to s and the expected action equal to u.
Therefore, if W π is accurately approximated for a current
policy and this policy is updated, the approximation of W π
needs only limited adjustment.

V. ACERAC: ACTOR-CRITIC WITH EXPERIENCE REPLAY
AND AUTOCORRELATED ACTIONS

The RL algorithm presented in this section has an actor-
critic structure. It optimizes a policy of the form (1) and uses
the critic,

W (u, s; ν),

which is an approximator of the noise-value function (10)
parametrized by the vector ν. The critic is trained to approx-
imately satisfy (11).

t |¯sn

t , ¯rn

t , ¯πn

t = ¯π(¯an

t , ξt−1; θ).

t , st+n(cid:105), where ¯πn

A constant parameter of

the algorithm is natural n.
It denotes the length of action sequences whose prob-
abilities the algorithm adjusts. For each time instant of
the agent-environment interaction, the policy (1) is applied.
Also, data is registered that enables recall of
the tuple
(cid:104)¯sn

t , ¯an
The general goal of policy training in ACERAC is to
maximize W π(uj−1, sj) for each state sj registered during
the agent-environment interaction. In this order previous time
instants are sampled, and sequences of actions that followed
these instants are made more or less probable depending
on their return. More speciﬁcally, j is sampled from {t −
M, . . . , t − n}, where M is a memory buffer length, and θ
is adjusted along with a policy gradient estimate, which is
derived in Appendix B. In other words, the conditional density
of the sequence of actions ¯an
is being increased/decreased
j
depending on the return

rj + · · · + γn−1rj+n−1 + γnW (uj+n−1, sj+n; ν)

this sequence of actions yields.

A. Actor & Critic training

At each t-th instant of agent–environment interaction expe-
rience replay is repeated several times in the form presented
in Algorithm 1 to calculate actor and critic weight updates.

j = [sT

In Line 2, the algorithm selects an experienced event to
replay with the starting time index j. In the following lines
the vectors of states ¯sn
j , . . . , sT
j =
[aT

j , . . . , aT
In Lines 3-4 Xj−1 and Xj+n−1 are appointed to be values
of the noise with which the current policy would designate the
past actions. Then, in Lines 5-6, the corresponding adjusted
noise values uj−1 and uj+n−1 values are calculated.

j+n−1]T are considered.

j+n−1]T and actions ¯an

In Line 7 a temporal difference is computed. It determines

the relative quality of ¯an
i .

In Line 8 a softly truncated density ratio is computed. The
density ratio implements two ideas. Firstly, θ is changing due
to being optimized, thus the conditional distribution (¯an
i |ξj−1)
is now different than it was at the time when the actions
¯an
accounts
i were executed. The density ratio

j |¯sn

¯π(¯an

j ,Xj−1;θ)
¯πn
j

5

Algorithm 1 Calculating weights update from a single trajec-
tory in Actor-Critic with Experience Replay and Autocorre-
lated aCtions, ACERAC

1: ACTOR AND CRITIC UPDATES()
2:
3:
4:
5:
6:

select randomly j ∈ {t − M . . . t − n}
Xj−1 ← E [ξj−1| . . . sj−1, aj−1, . . . ; θ]
Xj+n−1 ← E [ξj+n−1| . . . sj+n−1, aj+n−1, . . . ; θ]
uj−1 ← A(sj; θ) + αXj−1
uj+n−1 ← A(sj+n; θ) + αXj+n−1
j (θ, ν) = rj + · · · + γn−1rj+n−1
dn
+γnW (uj+n−1, sj+n; ν) − W (uj−1, sj; ν)

7:

8:

9:

10:

11:

(cid:17)

(cid:16) ¯π(¯an

j |¯sn

j ,Xj−1;θ)
ρj ← ψb
¯πn
j
j |¯sn
∆θ ← ∇θ ln ¯π(¯an
+γn∇θW (uj+n−1(θ), sj+n; ν)ρj
−∇θL(sj, θ)

j , Xj−1; θ)dn

∆ν ← ∇νW (uj−1(θ), sj; ν)dn
return ∆θ, ∆ν

j (θ, ν)ρj

j (θ, ν)ρj(θ)

for this discrepancy of distributions. Secondly, to limit the
variance of the density ratio, the soft-truncating function ψb
is applied. E.g.,

ψb(x) = b tanh(x/b),

(19)

for a certain b > 1. In the ACER algorithm [21], the hard trun-
cation function, min{·, b} is used for the same purpose which
is limiting density ratios necessary in designating updates due
to action distribution discrepancies. However, soft-truncating
distinguishes the magnitude of density ratio and works slightly
better than hard truncation.

j , Xj−1; θ)dn

In Line 9 an improvement direction for actor is com-
puted. The sum of ∇θ ln ¯π(¯an
j |¯sn
j (θ, ν)ρj(θ) and
γn∇θW (uj+n−1(θ), sj+n; ν)ρj is an improvement direction
estimate of W π(uj−1, sj) derived in Appendix B. It is de-
signed to increase/decrease the likelihood of occurrence of
the sequence of actions ¯an
i (θ, ν). L(s, θ)
is a loss function that penalizes the actor for producing
actions that do not satisfy constraints, e.g., they exceed their
boundaries.

i proportionally to dn

In Line 10 an improvement direction for critic, ∆ν, is
computed. It is designed to make W (·, · ; ν) approximate the
noise-value function (10) better.

In Line 7 the improvement directions ∆θ and ∆ν are
applied to update θ and ν, respectively, with the use of either
ADAM, SGD, or another method of stochastic optimization.
Implementation details of the algorithm using the neural-AR

policy (14) are presented in (17) and Appendix A.

VI. EMPIRICAL STUDY

This section presents simulations whose purpose was to
compare the algorithm introduced in Sec. V to state-of-the-
art reinforcement learning methods. We compared the new
algorithm to Actor-Critic with Experience Replay (ACER)
[21], Proximal Policy Optimization (PPO) [25], Soft Actor-
Critic (SAC) [26] and Continuous Deep Advantage Updating
(CDAU) [13]. We selected these algorithms as different state-
of-the-art approaches that also apply trajectory updates (PPO,

(a)

(b)

6

for 5 test episodes. An average sum of rewards within a test
episode was registered. Each run was repeated 5 times.

In experiments with, respectively, 3 and 10 times ﬁner time
discretization, the number of timesteps for a run and between
tests was increased, respectively, 3 and 10 times. Also, to
keep the scale of the sum of discounted rewards, the discount
parameter was increased from 0.99 to, respectively, 0.991/3
and 0.991/10, and the rewards were decreased, respectively, 3
and 10 times. The number of model updates was kept constant
for different discretization. The data buffer was increased 3
and 10 times, respectively. In ACER the λ parameter was
increased to λ1/3 and λ1/10, respectively. Also, in ACERAC,
the n coefﬁcient was increased 3 and 10 times, respectively
and α was increased to α1/3 and α1/10, respectively.

For each environment-algorithm-discretization triple hyper-
parameters such as step-sizes were optimized to yield the
highest ultimate average rewards. The values of these hyper-
parameters are reported in Appendix C.

(c)

(d)

Fig. 2. Environments used in simulations: Ant (a), HalfCheetah (b), Hopper
(c), Walker2D (d).

B. Ablation results

CDAU) or control exploration (SAC, CDAU). We used the
RLlib implementation [27] of SAC and PPO, and implemen-
tation of CDAU published by its authors1. Our experimental
software is available online.2

For the comparison of the RL algorithms to be the most in-
formative we chose four challenging tasks inspired by robotics.
They were Ant, Hopper, HalfCheetah, and Walker2D (see
Fig. 2) from the PyBullet physics simulator [28]. A simulator
that is more popular in the RL community is MuJoCo [29].3
Hyperparameters that assure optimal performance of ACER,
SAC, and PPO applied to the environments considered in
MuJoCo are well known. However, PyBullet environments
introduce several changes to MuJoCo tasks, which make them
more realistic and thus more difﬁcult. Additionally, physics in
MuJoCo and PyBullets differ slightly [30], hence we needed
to tune the hyperparameters. We based the hyperparameters
in our experiments on their values used for MuJoCo environ-
ments reported in the original papers. We also followed their
authors’ guidelines when selecting hyperparameters for tuning.
the experiments only to the original
environments. We also use modiﬁed ones with 3 and 10 times
ﬁner time discretization. This is to verify how the algorithms
work in these circumstances.

We do not

limit

We used actor and critic structures as described in [26] for
each learning algorithm. That is, both structures had the form
of neural networks with two hidden layers of 256 units each.

A. Experimental setting

Each learning run with basic time discretization lasted for 3
million timesteps. Every 30000 timesteps of training a simu-
lation was made with frozen weights and without exploration

1https://github.com/ctallec/continuous-rl
2https://github.com/lychanl/acerac
3We chose PyBullet because it is freeware, while MuJoCo is commercial

software.

Figures 3, 4, and 5, respectively, present results for AC-
ERAC, for the original, 3 times, and 10 times ﬁner time dis-
cretization, with different α and n. The primary goal of these
experiments was to verify whether the concepts introduced
with ACERAC really contribute to performance. For α = 0,
autocorrelation of actions is switched off. It is seen in the
graphs that α = 0 yields inferior performance. n deﬁnes the
length of sequences of actions whose density is manipulated
in the course of learning. It is seen that our proposed default
values usually yield optimal or close to optimal performance,
but in some cases smaller n prove better.

C. Results

Figures 6, 7, 8, respectively, present learning curves for
all four environments and all four compared algorithms. The
ﬁgures are for, respectively, the original, 3 times, and 10 times
ﬁner time discretization. Each graph shows how a sum of
rewards, in test episodes evolves in the course of learning.
Solid lines represent the average sums of rewards and shaded
areas represent their standard deviations.

It is seen in Figures 6–8 that in 12 combinations of tasks
and time discretizations, ACERAC was the algorithm to yield
the best performance in 6 cases, in 2 cases it yielded the best
performance ex equo with ACER, and in the rest of the cases
it still yielded reasonable performance.

A curious result of our experiments was the extraordinarily
high rewards obtained in some experiments with time dis-
cretization 10 times ﬁner than the original. Namely, ACER
and ACERAC obtained such results for Ant, and ACERAC for
Hopper and Walker2D. Apparently, these environments require
fast intervention of control and no algorithm is able to learn
it at coarser time discretization.

It can also be seen that for most discretizations and problems
ACERAC obtained relatively good results in the initial training
steps, which is a desirable feature in robotic control [31].

7

Fig. 3. ACERAC with different α and n, for the original time discretization:
Average sums of rewards in test trials. Base: α = 0.5, n = 2. Environments:
Ant, HalfCheetah, Hopper and Walker2D.

Fig. 4. ACERAC with different α and n, for time discretization 3 times
ﬁner than the original: Average sums of rewards in test trials. Base: α =
0.51/3, n = 2 · 3. Environments: Ant, HalfCheetah, Hopper and Walker2D.

8

Fig. 5. ACERAC with different α and n, for time discretization 10 times
ﬁner than the original: Average sums of rewards in test trials. Base: α =
0.51/10, n = 2·10. Environments: Ant, HalfCheetah, Hopper and Walker2D.

Fig. 6. Learning curves for the original time discretization: Average sums of
rewards in test trials. Environments: Ant, HalfCheetah, Hopper and Walker2D.

9

Fig. 7. Learning curves for time discretization 3 times ﬁner than the original:
Average sums of rewards in test trials. Environments: Ant, HalfCheetah,
Hopper and Walker2D.

Fig. 8. Learning curves for time discretization 10 times ﬁner than the original:
Average sums of rewards in test trials. Environments: Ant, HalfCheetah,
Hopper and Walker2D.

10

D. Discussion

ACKNOWLEDGEMENT

The performance of the algorithms in our experiments with
ﬁne time discretization can be attributed to two features.
The ﬁrst one is autocorrelated actions. ACERAC and CDAU
use them, but only ACERAC utilize their properties. Other
considered algorithms do not use them. It can be seen in
Figures 6-8 that ACERAC achieved the best performance
for 8 discretization-environment pairs out of 12. Switching
off autocorelation from actions worsen the efﬁciency. The
autocorrelated actions seem to be an efﬁcient way to orga-
nize exploration, better than actions without any stochastic
dependence beyond state transition. However, a policy with
autocorrelated actions requires specialized training, which is
provided in ACERAC.

The second factor is whether the algorithms use 1-step
returns (SAC and CDAU) or n-step returns (PPO, ACER, and
ACERAC). The impact of this parameter on performance is
complex. For ﬁne enough time discretization and large enough
discount parameter, the 1-step returns are expected to fail due
to the limited accuracy of the critic. However, if the critic is
accurate enough for the task at hand, small n values work
quite well. This is visible in the upper part of Fig. 5, where
1 proves to be the best value of n in ACERAC for Ant and
the highest analyzed time discretization. Hence, n > 1 may
be a remendy for an inaccurate critic, but an accurate one is
a better remedy.

Even though CDAU was designed in [13] to assure efﬁcient
RL in ﬁne time discretization, that algorithm yielded poor
performance in our experiments. However, it was presented as
an extension of a method, DAU, for discrete actions, and no
experimental material on CDAU was presented in the original
paper.

VII. CONCLUSIONS AND FUTURE WORK

In this paper, a framework has been introduced for the
application of reinforcement learning to policies that admit
stochastic dependence between subsequent actions beyond
state transition. This dependence is a tool that enables re-
inforcement learning in physical systems and ﬁne time dis-
cretization. It can also yield better exploration and therefore
faster learning.

An algorithm based on this framework, Actor-Critic with
Experience Replay and Autocorrelated aCtions (ACERAC),
was introduced. Its efﬁciency was veriﬁed by simulations of
four learning control problems, namely, Ant, HalfCheetah,
Hopper, and Walker2D, at diverse time discretization. The
algorithm was compared with CDAU, PPO, SAC, and ACER.
ACERAC exhibited the best performance in 8 out of 12
discretization-environment pairs.

It would be desirable to combine the framework proposed
here with adapting the amount of randomness in actions by
introducing reward for the entropy of their distribution, as is
done in PPO. Also, the framework proposed here has been
specially designed for applications in robotics. An obvious
next step in our research would be to apply it in this area,
which is more demanding than simulations.

This work was partially funded by a grant of Warsaw
University of Technology Scientiﬁc Discipline Council for
Computer Science and Telecommunications.

This research was supported in part by PL-Grid Infrastruc-

ture.

REFERENCES

[1] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.

Second edition. The MIT Press, 2018.

[2] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman,
J. Tang, and W. Zaremba, “Openai gym,” 2016, arXiv:1606.01540.
[3] P. Khosla, “Choosing sampling rates for robot control,” in IEEE Inter-
national Conference on Robotics and Automation, 1987, pp. 169–174.
[4] J. Schrimpf, “Sensor-based real-time control of industrial robots,” Ph.D.

dissertation, Norwegian University of Science and Technology, 2013.

[5] V. Francois, R. Fonteneau, and D. Ernst, “How to discount deep
learning: Towards new dynamic strategies,” 2015,

reinforcement
arXiv:1512.02011.

[6] M. Szulc, J. Łyskawa, and P. Wawrzy´nski, “A framework for reinforce-
ment learning with autocorrelated actions,” in International Conf. on
Neural Information Processing, 2020, pp. 90–101.

[7] R. Liu, F. Nageotte, P. Zanne, M. de Mathelin, and B. Dresp-Langley,
“Deep reinforcement learning for the control of robotic manipulation:
A focussed mini-review,” Robotics, vol. 10, no. 1, 2021. [Online].
Available: https://www.mdpi.com/2218-6581/10/1/22

[8] G. E. Uhlenbeck and L. S. Ornstein, “On the theory of the brownian
[Online].

motion,” Phys. Rev., vol. 36, pp. 823–841, Sep 1930.
Available: https://link.aps.org/doi/10.1103/PhysRev.36.823

[9] J. L. Doob, “The brownian movement and stochastic equations,” Annals
of Mathematics, vol. 43, no. 2, pp. 351–369, 1942. [Online]. Available:
http://www.jstor.org/stable/1968873

[10] P. Wawrzy´nski, “Control policy with autocorrelated noise in reinforce-
ment learning for robotics,” International Journal of Machine Learning
and Computing, vol. 5, no. 2, pp. 91–95, 2015.

[11] H. van Hoof, D. Tanneberg, and J. Peters, “Generalized exploration in
policy search,” Machine Learning, vol. 106, pp. 1705–1724, 2017.
[12] D. Korenkevych, A. R. Mahmood, G. Vasan, and J. Bergstra, “Autore-
gressive policies for continuous control deep reinforcement learning,”
in Proceedings of the Twenty-Eighth International Joint Conference on
Artiﬁcial Intelligence (IJCAI-19), 2019, pp. 2754–2762.

[13] C. Tallec, L. Blier, and Y. Ollivier, “Making deep q-learning methods
robust to time discretization,” in International Conference on Machine
Learning (ICML), 2019, pp. 6096–6104.

[14] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wier-
stra, and M. Riedmiller, “Playing atari with deep reinforcement learn-
ing,” 2013, arXiv:1312.5602.

[15] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
D. Silver, and D. Wierstra, “Continuous control with deep reinforcement
learning,” 2016, arXiv:1509.02971.

[16] D. Vrabie and F. Lewis, “Neural network approach to continuous-time
direct adaptive optimal control for partially unknown nonlinear systems,”
Neural Networks, vol. 22, no. 3, pp. 237–246, 2009.

[17] X. Guo, W. Yan, and R. Cui, “Integral reinforcement learning-based
adaptive nn control for continuous-time nonlinear mimo systems with
unknown control directions,” IEEE Transactions on Systems, Man, and
Cybernetics: Systems, 2019.

[18] A. G. Barto, R. S. Sutton, and C. W. Anderson, “Neuronlike adaptive
learning control problems,” IEEE
elements that can learn difﬁcult
Transactions on Systems, Man, and Cybernetics B, vol. 13, pp. 834–
846, 1983.

[19] H. Kimura and S. Kobayashi, “An analysis of actor/critic algorithms
using eligibility traces: Reinforcement learning with imperfect value
function,” in ICML, 1998.

[20] S. Mahadevan and J. Connell, “Automatic programming of behavior
based robots using reinforcement learning,” Artiﬁcial Intelligence, no.
55(2–3), pp. 311–365, 1992.

[21] P. Wawrzy´nski, “Real-time reinforcement learning by sequential ac-
tor–critics and experience replay,” Neural Networks, vol. 22, no. 10,
pp. 1484–1497, 2009.

[22] Z. Wang, V. Bapst, N. Heess, V. Mnih, R. Munos, K. Kavukcuoglu,
and N. de Freitas, “Sample efﬁcient actor-critic with experience replay,”
2016, arXiv:1611.01224.

[23] S. Kakade and J. Langford, “Approximately optimal approximate re-
inforcement learning,” in Proceedings of the Nineteenth International
Conference on Machine Learning, ICML’02, 2002, pp. 267–274.
[24] J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel, “Trust

region policy optimization,” 2015, arXiv:1502.05477.

[25] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,

“Proximal policy optimization algorithms,” 2017, arXiv:1707.06347.

[26] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic
actor,” 2018, arXiv:1801.01290.

[27] E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg, J. Gon-
zalez, M. Jordan, and I. Stoica, “RLlib: Abstractions for distributed rein-
forcement learning,” in Proceedings of the 35th International Conference
on Machine Learning, ser. Proceedings of Machine Learning Research,
Stockholmsm¨assan, Stockholm
J. Dy and A. Krause, Eds., vol. 80.
Sweden: PMLR, 10–15 Jul 2018, pp. 3053–3062.

[28] E. Coumans and Y. Bai, “Pybullet, a python module for physics
simulation for games, robotics and machine learning,” http://pybullet.org,
2016–2019.

[29] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for
model-based control,” in 2012 IEEE/RSJ International Conference on
Intelligent Robots and Systems.

IEEE, 2012, pp. 5026–5033.

[30] T. Erez, Y. Tassa, and E. Todorov, “Simulation tools for model-based
robotics: Comparison of bullet, havok, mujoco, ode and physx,” in 2015
IEEE International Conference on Robotics and Automation (ICRA),
2015, pp. 4397–4404.

[31] K.-T. Song and W.-Y. Sun, “Robot control optimization using
reinforcement learning,” Journal of Intelligent and Robotic Systems,
vol. 21, no. 3, pp. 221–238, Mar 1998.
[Online]. Available:
https://doi.org/10.1023/A:1007904418265

APPENDIX

A. Properties of Ornstein-Uhlenbeck process and the policy
based on it

In this section the key properties of the process (ξt) (4) are

derived.

a) Stationary distribution of ξt: From (4) one can see
that if for a certain t it is true that ξt−1 ∼ N (0, C), then also
ξt ∼ N (0, C). By induction this leads us to the conclusion
that ξt ∼ N (0, C) for all t.

b) Stationary distribution of ¯ξn

t : Applying induction to

(4) for k ≥ 0 one obtains that

ξt+k = αkξt +

(cid:112)

1 − α2

k−1
(cid:88)

i=0

αi(cid:15)t+k−i.

(20)

Consequently,

EξtξT

t+k = α|k|C and EξT

t ξt+k = α|k|tr(C)

Therefore,

¯ξn
t = [ξT
0 = Λn
Ωn

t , ..., ξT
0 ⊗ C, Λn

t+n−1]T ∼ N (0, Ωn
0 )

0 = [α|l−k|]l,k, 0 ≤ l, k < n.

(21)

The symbol “⊗” denotes Kronecker product of two matrices.
We have

(Λn

0 ⊗ C)−1 = (Λn

0 )−1 ⊗ C −1.

(22)

c) Conditional distribution ¯ξn

t |ξt−1: From (20) we have

that

E(ξt+k|ξt−1) = αk+1ξt−1,

11

and for 0 ≤ k ≤ l we have

cov(ξt+k, ξt+l|ξt−1)

(cid:32)

(cid:112)

1 − α2

= E

(cid:33) 


(cid:112)

1 − α2

αk−i(cid:15)t+i


T

αl−j(cid:15)t+j



l
(cid:88)

j=0

k
(cid:88)

i=0

= (1 − α2)αl−k (cid:0)1 + α2 + · · · + α2k(cid:1) C
= αl−k (cid:0)1 − α2k+2(cid:1) C.

Therefore, the conditional distribution ¯ξn

t |ξt−1 takes the form

¯ξn
t |ξt−1 ∼ N (Bnξt−1, Ωn
1 )
Bn = [αI, . . . , αnI]T
Ωn

1 ⊗ C, Λn

1 = Λn

(23)

(24)
1 = [α|l−k| − αl+k+2]l,k, 0 ≤ l, k < n.
(25)

d) Distribution of actions’ trajectory:

at = A(st; θ)+ξt and n > 0 we have ¯an
distribution of the actions that initiate a trial, ¯π(¯an
is
further actions ¯π(¯an
N ( ¯A(¯sn

For
t . The
j , ∅; θ),
0 ). The distribution of
t , ξt−1; θ) is also normal, namely
1 ).

t |¯sn
t ; θ) + Bnξt−1, Ωn

thus normal N ( ¯A(¯sn

t ; θ)+ ¯ξn
t |¯sn

t = ¯A(¯sn

t ; θ), Ωn

e) Retrieving ξt−1 and ut−1 from past actions: For the
OU processes the values of ξt−1 and ut−1 may be calculated
from actions and actor’s outputs as

ξt−1 = at−1 − A(st−1; θ)
ut−1 = A(st; θ) + αξt−1

= A(st; θ) + α(at−1 − A(st−1; θ)).

(26)

(27)

(28)

If t is an initial instance of a trial, the conditional expected
values of ξt−1 and ut−1 are calculated from A(st; θ) and at,
namely

ξt−1 = α−1(at − A(st; θ))
ut−1 = A(st; θ) + αξt−1 = at.

(29)

(30)

B. Policy gradient estimator derivation

In this section we derive a policy gradient estimator, which

is an estimator of a gradient of

Eπ

(cid:32)n−1
(cid:88)

i=0

γirj+i + γnW π(uj+n−1(θ), sj+n)

(cid:33)

(cid:12)
(cid:12)ξ∗
(cid:12)
j−1, sj

with respect to the current polity parameter θ, for constant
ξ∗
j−1 = ξj−1(θ).
Let us denote by A the action space, by θ the current policy
parameter, by π the current policy, by θj the policy parameter
used when aj was selected, by π(θj) the policy used then,
and the density ratio by

ρj(θ) =

¯π(an

j |¯sn

j−1; θ)

j , ξ∗
¯πn
j

.

We have

d
dθT Eπ(θ)
(cid:90)

=

d
dθT

(cid:90)

=

An

(cid:90)

+ γn

An
(cid:32)n−1
(cid:88)

(cid:90)

=

An

i=0

×

(cid:90)

+ γn

(cid:32)n−1
(cid:88)

(cid:12)
(cid:12)ξ∗
γirj+i + γnW π(uj+n−1(θ), sj+n)
(cid:12)
j−1, sj
(cid:33)

γirj+i + γnW π(uj+n−1(θ), sj+n)

i=0
(cid:32)n−1
(cid:88)

An

(cid:32)n−1
(cid:88)

i=0
× ¯π(¯an

j |¯sn

j , ξ∗

j−1; θ)d¯an
j

γirj+i + γnW π(uj+n−1(θ), sj+n)

i=0
× ∇θ ¯π(¯an

j |¯sn

j , ξ∗

j−1; θ)d¯an
j

∇θW π(uj+n−1(θ), sj+n)¯π(¯an

j |¯sn

γirj+i + γnW π(uj+n−1(θ), sj+n)

j |¯sn
∇θ ¯π(¯an
j , ξ∗
¯π(¯an
j , ξ∗
j |¯sn

j−1; θ)

j−1; θ)

ρj(θ)¯πn

j d¯an
j

j−1; θ)d¯an
j , ξ∗
j
(cid:33)

∇θW π(uj+n−1(θ), sj+n)ρj(θ)¯πn

j d¯an
j

An
(cid:40)(cid:34) (cid:32)n−1
(cid:88)

= Eπ(θj )

γirj+i + γnW π(uj+n−1(θ), sj+n)

(cid:33)

(cid:33)

TABLE I
COMMON PARAMETERS FOR OFFLINE ALGORITHMS (ACER, ACERAC,
SAC, CDAU). d DENOTES DISCRETIZATION INCREASE (1, 3, OR 10).

12

Parameter
Memory size
Minibatch size
Update interval
Gradient steps

Value
d · 106
256
d
1

(cid:33)

TABLE II
ACERAC HYPERPARAMETERS. d DENOTES DISCRETIZATION INCREASE
(1, 3, OR 10).

Parameter
Action std. dev.
α
Critic step-size
Actor step-size
n
b
Learning start

Value
0.3
0.5d−1
10−4
10−5
d · 2
2
d · 103

spread by a factor of 3: . . . , 10−6, 3 · 10−5, 10−5, . . . , with
the exception of the clip parameter for PPO, whose only
considered values were 0.1, 0.2, 0.3, as suggested by the
authors of this algorithm [25].

i=0
× ∇θ ln ¯π(¯an

j |¯sn

j , ξ∗

j−1; θ)

(cid:35)

(cid:41)

+ γn∇θW π(uj+n−1(θ), sj+n)

ρj(θ)

.

The analytical property

Eπ(θj )

(cid:8)∇θ ln ¯π(¯an

j |¯sn

j , ξ∗

j−1; θ)ρj(θ)(cid:9) = 0

allows us to subtract any constant baseline from the sum of
rewards above. Consequently, an unbiased estimator of the
policy gradient may take the form
(cid:34)(cid:32)n−1
(cid:88)

γirj+i +γnW π(uj+n−1(θ), sj+n)−W π(ξ∗

j−1, sj−1; ν)

(cid:33)

i=0
× ∇θ ln ¯π(¯an

j |¯sn

j , ξ∗

j−1; θ)

(cid:35)
+ γn∇θW π(uj+n−1(θ), sj+n)

(31)

ρj(θ).

The above estimator is not feasible. Firstly, it is based on
the noise-value function, which is unknown. Also, it uses
the density ratio, which could make its variance excessive.
In the feasible version of the above estimator, we use the
approximator of the noise-value function, and the density ratio
is softly truncated from above.

C. Algorithms’ hyperparameters

This section presents hyperparameters used in the simula-
tions described in Sec. VI. For the original time discretization
all algorithms used a discount factor equal to 0.99. Common
parameters for the ofﬂine algorithms (i.e., ACERAC, ACER,
SAC and CDAU) are presented in Tab. I. Hyperparameters
speciﬁc for different algorithms are depicted in Tabs. II–IX.
The hyperparameters were tuned using grid search over values

TABLE III
ACER HYPERPARAMETERS. d DENOTES DISCRETIZATION INCREASE (1, 3,
OR 10). FOR ENVIRONMENT- AND DISCRETIZATION-SPECIFIC
HYPERPARAMETERS, SEE TAB. VII

Parameter
Action std. dev.
λ
b
Learning start

Value
0.3
1 − 1−0.9
2
d · 103

d

TABLE IV
SAC GENERAL HYPERPARAMETERS. d DENOTES DISCRETIZATION
INCREASE (1, 3, OR 10). FOR ENVIRONMENT- AND
DISCRETIZATION-SPECIFIC HYPERPARAMETERS SEE TAB. VIII

Parameter
Target smoothing coef. τ
Learning start

Value
0.005
d · 104

13

TABLE V
PPO HYPERPARAMETERS. d DENOTES DISCRETIZATION INCREASE (1, 3,
OR 10). FOR ENVIRONMENT- AND DISCRETIZATION-SPECIFIC
HYPERPARAMETERS, SEE TAB. IX

Parameter
GAE parameter (λ)
Minibatch size
Horizon
Number of epochs
Value function clipping coef.
Target KL

Value
0.95
64
d · 2048
10
10
0.01

TABLE VI
CDAU HYPERPARAMETERS. d DENOTES DISCRETIZATION INCREASE (1,
3, OR 10).

Parameter
dt
Step-size for HalfCheetah and Ant
Step-size for Hopper and Walker2D
θ
σ

Value
d−1 · 0.0165
0.01
0.003
7.5
1.5

TABLE VII
ACER STEP-SIZES

Parameter
Discretization increase
Actor step-size for HalfCheetah env.
Actor step-size for Ant env.
Actor step-size for Hopper env.
Actor step-size for Walker2D env.
Critic step-size for HalfCheetah env.
Critic step-size for Ant env.
Critic step-size for Hopper env.
Critic step-size for Walker2D env.

1
10−5
10−5
10−5
10−5
10−5
10−5
10−5
10−5

Value
3
3 · 10−6
10−5
3 · 10−5
3 · 10−5
3 · 10−5
3 · 10−5
3 · 10−5
3 · 10−5

10
10−5
3 · 10−6
3 · 10−5
10−6
10−5
10−4
10−5
10−5

TABLE VIII
SAC REWARD SCALING.

Parameter
Discretization increase
Reward scaling for HalfCheetah env.
Reward scaling for Ant env.
Reward scaling for Hopper env.
Reward scaling for Walker2D env.

Value
3
10
100
10
3

10
300
3000
3
3

1
0.1
1
0.03
30

TABLE IX
PPO STEP-SIZES AND CLIP PARAMS.

Parameter
Discretization increase
Step-size for HalfCheetah env.
Step-size for Ant env.
Step-size for Hopper env.
Step-size for Walker2D env.
Clip param for HalfCheetah env.
Clip param for Ant env.
Clip param for Hopper env.
Clip param for Walker2D env.

1
3 · 10−4
3 · 10−4
3 · 10−4
3 · 10−4
0.2
0.2
0.2
0.2

Value
3
10−4
3 · 10−5
3 · 10−4
3 · 10−4
0.3
0.2
0.2
0.2

10
10−5
10−5
10−5
10−5
0.1
0.1
0.3
0.3

