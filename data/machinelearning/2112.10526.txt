2
2
0
2

g
u
A
8
1

]
h
p
-
t
n
a
u
q
[

2
v
6
2
5
0
1
.
2
1
1
2
:
v
i
X
r
a

SciPost Physics Codebases

Submission

NetKet 3: Machine Learning Toolbox for Many-Body
Quantum Systems

Filippo Vicentini1,2?, Damian Hofmann3, Attila Szab´o4,5, Dian Wu1,2, Christopher Roth6,
Clemens Giuliani1,2, Gabriel Pescia1,2, Jannes Nys1,2, Vladimir Vargas-Calder´on7,
Nikita Astrakhantsev8 and Giuseppe Carleo1,2

1 ´Ecole Polytechnique F´ed´erale de Lausanne (EPFL), Institute of Physics,
CH-1015 Lausanne, Switzerland
2 Center for Quantum Science and Engineering, ´Ecole Polytechnique F´ed´erale de Lausanne
(EPFL), CH-1015 Lausanne, Switzerland
3 Max Planck Institute for the Structure and Dynamics of Matter,
Center for Free-Electron Laser Science (CFEL),
Luruper Chaussee 149, 22761 Hamburg, Germany
4 Rudolf Peierls Centre for Theoretical Physics,
University of Oxford, Oxford OX1 3PU, United Kingdom
5 ISIS Facility, Rutherford Appleton Laboratory,
Harwell Campus, Didcot OX11 0QX, United Kingdom
6 Physics Department, University of Texas at Austin
7 Grupo de Superconductividad y Nanotecnolog´ıa, Departamento de F´ısica,
Universidad Nacional de Colombia, Bogot´a, Colombia
8 Department of Physics, University of Zurich, CH-8057 Zurich, Switzerland
? ﬁlippo.vicentini@epﬂ.ch

August 19, 2022

Abstract

We introduce version 3 of NetKet, the machine learning toolbox for many-body
quantum physics. NetKet is built around neural quantum states and provides
eﬃcient algorithms for their evaluation and optimization. This new version is
built on top of JAX, a diﬀerentiable programming and accelerated linear algebra
framework for the Python programming language. The most signiﬁcant new
feature is the possibility to deﬁne arbitrary neural network ans¨atze in pure Python
code using the concise notation of machine-learning frameworks, which allows for
just-in-time compilation as well as the implicit generation of gradients thanks to
automatic diﬀerentiation. NetKet 3 also comes with support for GPU and TPU
accelerators, advanced support for discrete symmetry groups, chunking to scale
up to thousands of degrees of freedom, drivers for quantum dynamics applications,
and improved modularity, allowing users to use only parts of the toolbox as a
foundation for their own code.

Contents

1 Introduction

1

3

 
 
 
 
 
 
SciPost Physics Codebases

Submission

1.1 What’s new
1.2 Outline
1.3

Installing NetKet

2 Quantum-mechanical primitives

2.1 Hilbert spaces
2.2 Linear operators

3 Variational quantum states

3.1 Abstract interface
3.2 Deﬁning the variational ansatz
3.3 Estimating observables
3.4 Monte Carlo samplers
3.5 Quantum geometric tensor

4 Algorithms for variational states

4.1 Ground-state search
4.2 Finding steady states
4.3 Time propagation
4.4

Implementing custom algorithms using NetKet

5 Symmetry-aware neural quantum states

5.1 Symmetry groups and representation theory
5.2 Using group convolutional neural networks (GCNNs)

6 Quantum systems with continuous degrees of freedom

6.1 Continuous Hilbert spaces
6.2 Linear operators
6.3 Samplers
6.4 Harmonic oscillators
6.5

Interacting system with continuous degrees of freedom

7 Example: Finding ground and excited states of a lattice model

7.1 Deﬁning the lattice and the Hamiltonian
7.2 Deﬁning and training a symmetric ansatz
7.3 Finding an excited state

8 Example: Fermions on a lattice

9 Example: Real-time dynamics

9.1 Unitary dynamics
9.2 Dissipative dynamics (Lindblad master equation)

10 Benchmarks

10.1 Variational Monte Carlo
10.2 MPI for NetKet
10.3 Comparison with jVMC

11 Discussion and conclusion

2

4
5
5

6
6
8

9
10
11
17
19
21

26
26
28
29
31

32
33
35

37
37
37
38
38
39

43
43
43
45

46

48
49
50

52
52
52
53

55

SciPost Physics Codebases

Submission

A Details of group convolutions

A.1 Group convolutions and equivariance
A.2 Fast group convolutions using Fourier transforms

B Implementation details of the quantum geometric tensor

B.1 Jacobians and their products
B.2 QGTJacobian
B.3 QGTOnTheFly

References

56
56
57

59
59
60
61

61

1 Introduction

During the last two decades, we have witnessed tremendous advances in machine learning
(ML) algorithms which have been used to solve previously diﬃcult problems such as image
recognition [1, 2] or natural language processing [3]. This has only been possible thanks
to sustained hardware development: the last decade alone has seen a 50-fold increase in
available computing power [4]. However, unlocking the full computational potential of modern
arithmetic accelerators, such as GPUs, used to require signiﬁcant technical skills, hampering
researchers in their eﬀorts. The incredible pace of algorithmic advances must therefore be
attributed, at least in part, to the development of frameworks allowing researchers to tap into
the full potential of computer clusters while writing high-level code [5, 6].

In the last few years, researchers in quantum physics have increasingly utilized machine-
learning techniques to develop novel algorithms or improve on existing approaches [7]. In the
context of variational methods for many-body quantum physics in particular, the method of
neural quantum states (NQS) has been developed [8]. NQS are based on the idea of using
neural networks as an eﬃcient parametrization of the quantum wave function. They are of
particular interest because of their potential to represent highly entangled states in more than
one dimension with polynomial resources [9], which is a signiﬁcant challenge for more estab-
lished families of variational states. NQS are also ﬂexible: they have been successfully used to
determine variational ground states of classical [10] and quantum Hamiltonians [11–17] as well
as excited states [13], to approximate Hamiltonian unitary dynamics [8, 18–23], and to solve
the Lindblad master equation [24–26]. In particular, NQS are currently used in the study of
frustrated quantum systems [13,15–17,27–30], which have so far been challenging to optimize
by established numerical techniques. They have also been used to perform tomographic state
reconstruction [31] and eﬃciently approximate quantum circuits [32].

A complication often encountered when working with NQS is, however, that standard
ML frameworks like TensorFlow [33] or PyTorch [34] are not geared towards these kind of
quantum mechanical problems, and it often takes considerable technical expertise to use them
for such non-standard tasks. Alternatively, researchers sometimes avoid those frameworks and
implement their routines from scratch, but this often leads to sub-optimal performance. We
believe that it is possible to foster research at this intersection of quantum physics and ML by
providing an easy-to-use interface exposing quantum mechanical objects to ML frameworks.

3

SciPost Physics Codebases

Submission

We therefore introduce version 3 of the NetKet framework [35].1 NetKet 3 is an
open-source Python toolbox expressing several quantum mechanical primitives in the diﬀer-
entiable programming framework jax [5, 36]. NetKet provides an easy-to-use interface to
high-performance variational techniques without the need to delve into the details of their
implementations, but customizability is not sacriﬁced and advanced users can inspect, mod-
ify, and extend practically every aspect of the package. Moreover, integration of our quantum
object primitives with the jax ecosystem allows users to easily deﬁne custom neural-network
architectures and compute a range of quantum mechanical quantities, as well as their gra-
dients, which are auto-generated through jax’s tracing-based approach. jax provides the
ability to write numerical code in pure Python using NumPy-like calls for array operations,
while still achieving high performance through just-in-time compilation using xla, the accel-
erated linear algebra compiler that underlies TensorFlow. We have also integrated jax and
MPI with the help of mpi4jax [37] to make NetKet scale to hundreds of computing nodes.

1.1 What’s new

With the release of version 3, NetKet has moved from internally relying on a custom C++
core to the jax framework, which allows models and algorithms to be written in pure Python
and just-in-time compiled for high performance on both CPU and GPU platforms.2 By using
only Python, the installation process is greatly simpliﬁed and the barrier of entry for new
contributors is lowered.

iFrom a user perspective, the most important new feature is the possibility of writing
custom NQS wave functions using jax, which allows for quick prototyping and deployment,
frees users from having to manually implement gradients due to jax’s support for automatic
diﬀerentiation, and makes models easily portable to GPU platforms. Other prominent new
features are

• support for (real and imaginary time) unitary and Markovian dissipative dynamics;

• support for continuous systems;

• support for composite Hilbert spaces;

• eﬃcient implementations of the quantum geometric tensor and stochastic reconﬁgura-

tion, which scale to models with millions of parameters;

• group-invariant and group-equivariant layers and architectures which support arbitrary

discrete symmetries.

A more advanced feature is an extension mechanism built around multiple dispatch [38],
which allows users to override algorithms used internally by NetKet without editing the
source itself. This can be used to make NetKet work with custom objects and algorithms
to study novel problems that do not easily ﬁt what is already available.

1This manuscript refers to NetKet v3.5, released in August 2022.
2Google’s Tensor Processing Units (TPUs) are also, in principle, supported. However, at the time of
writing they only support half-precision float16 . Some modiﬁcations would be necessary to work-around
loss of precision and gradient underﬂow.

4

SciPost Physics Codebases

Submission

(cid:240) JAX ﬂuency. Using NetKet’s high-level interface and built-in neural network

architectures does not require the user to be familiar with jax and concepts
such as just-in-time compilation and automatic diﬀerentiation. However, when
deﬁning custom classes such as neural network architectures, operators, or Monte
Carlo samplers, some proﬁciency with writing jax-compatible code will be re-
quired. We refrain from discussing jax in detail and instead point the reader
towards its documentation at jax.readthedocs.io.

1.2 Outline

NetKet provides both an intuitive high-level interface with sensible defaults to welcome
beginners, as well as a complete set of options and lower-level functions for ﬂexible use by
advanced users. The high-level interface is built around quantum-mechanical objects such as
Hilbert spaces ( netket.hilbert ) and operators ( netket.operator ), presented in Section 2.
The central object in NetKet 3 is the variational state, discussed in Section 3, which bring
together the neural-network ansatz, its variational parameters, and a Monte-Carlo sampler.
In Section 3.2, we give an example on how to deﬁne an arbitrary neural network using a
NetKet/jax-compatible framework, while Section 3.4 presents the new API of stochastic
samplers. In Section 3.5, we show how to compute the quantum geometric tensor (QGT) with
NetKet, and compare the diﬀerent implementations.

Section 4 shows how to use the three built-in optimization drivers to perform ground-state,
steady-state, and dynamics calculations. Section 5 discusses NetKet’s implementation of
spatial symmetries and symmetric neural quantum states, which can be exploited to lower
the size of the variational manifold and to target excited states in nontrivial symmetry sectors.
In Section 6, we also show how to study a system with continuous degrees of freedom, such
as interacting particles in one or more spatial dimensions.

The ﬁnal sections present detailed workﬂow examples of some of the more common use
cases of NetKet. In Section 7, we show how to study the ground state and the excited state
of a lattice Hamiltonian. Section 9 gives examples of both unitary and Lindbladian dynamical
simulations.

To conclude, Section 10 presents scaling benchmarks of NetKet running across multiple
devices and a performance comparison with jvmc [39], another library similar in scope to
NetKet.

Readers who are already familiar with the previous version of NetKet might be especially
interested in the variational state interface described in Section 3.1, which replaces what was
called machine in NetKet 2 [35], the QGT interface described in Section 3.5, algorithms for
dynamics (Section 4.3 and Section 9), and symmetry-aware NQS (Section 5).

1.3 Installing NetKet

NetKet is a package written in pure Python; it requires a recent Python version, currently at
least version 3.7. Even though NetKet itself is platform-agnostic, jax, its main dependency,
only works on MacOS and Linux at the time of writing.3 Installing NetKet is straightforward

3In principle, jax runs on Windows, but users must compile it themselves, which is not an easy process.

5

SciPost Physics Codebases

Submission

and can be achieved running the following line inside a python environment:
(cid:7)

1 pip install --upgrade netket

(cid:6)
To enable GPU support, Linux with a recent CUDA version is required and a special version
of jax must be installed. As the appropriate installation procedure can change between jax
versions, we refer the reader to the oﬃcial documentation4 for detailed instructions.

NetKet by default does not make use of multiple CPUs that might be available to the
user. Exploiting multiple processors, or even running across multiple nodes, requires MPI
dependencies, which can be installed using the command
(cid:7)

1 pip install --upgrade "netket[mpi]"

(cid:6)
These dependencies, namely mpi4py and mpi4jax , can only be installed if a working MPI
distribution is already available.

Once NetKet is installed, it can be imported in a Python session or script and its version

can be checked as
(cid:7)

1 >>> import netket as nk
2 >>> print(nk.__version__)
3 3.5.0

(cid:6)
We recommend that users use an up-to-date version when starting a new project. In code
listings, we will often refer to the netket module as nk for brevity.

NetKet also comes with a set of so-called experimental functionalities which are pack-
aged into the netket.experimental submodule which mirrors the structure of the standard
netket module. Experimental APIs are marked as such because they are relatively young
and we might want to change the function names or options keyword arguments without
guaranteeing backward compatibility as we do for the rest of NetKet. In general, we import
the experimental submodule as follows
(cid:7)

1 >>> from netket import experimental as nkx

(cid:6)
and use nkx as a shorthand for it.

2 Quantum-mechanical primitives

(cid:4)

(cid:5)

(cid:4)

(cid:5)

(cid:4)

(cid:5)

(cid:4)

(cid:5)

In general, when working with NetKet, the workﬂow is the following: ﬁrst, one deﬁnes the
Hilbert space of the system (Section 2.1) and the Hamiltonian or super-operator of interest
(Section 2.2). Then, one builds a variational state (Section 3.1), usually combining a neural-
network model and a stochastic sampler. In this section, we describe the ﬁrst step in this
process, namely, how to deﬁne a quantum-mechanical system to be modeled.

2.1 Hilbert spaces

Hilbert-space objects determine the state space of a quantum system and a speciﬁc choice of
basis. Functionality related to Hilbert spaces is contained in the nk.hilbert module; for
brevity, we will often leave out the preﬁx nk.hilbert in this section.

4https://github.com/google/jax#installation

6

SciPost Physics Codebases

Submission

All implementations of Hilbert spaces derive from the class AbstractHilbert and fall

into two classes:

• discrete Hilbert spaces, which inherit from the abstract class DiscreteHilbert and
include spin ( Spin ), qubit ( Qubit ), Fock ( Fock ) as well as fermionic orbitals (dubbed
SpinOrbitalFermions ) Hilbert spaces. Discrete spaces are typically used to describe
lattice systems. The lattice structure itself is, however, not part of the Hilbert space
class and can be deﬁned separately.

• continuous Hilbert spaces, which inherit from the abstract class ContinuousHilbert .
Currently, the only concrete continuous space provided by NetKet is Particle .

Continuous Hilbert spaces are discussed in Section 6. A general discrete space with N

sites has the structure

Hdiscrete = span{|s0i ⊗ · · · ⊗ |sN −1i | si ∈ Li, i ∈ {0, . . . , N − 1}},

(1)

where Li is the set of local quantum numbers at site i (e.g., L = {0, 1} for a qubit, L = {±1}
for a spin-1/2 system in the σz basis, or L = {0, 1, . . . , Nmax} for a Fock space with up to
Nmax particles per site). Constraints on the allowed quantum numbers are supported, result-
ing in Hilbert spaces that are subspaces of Eq. (1). For example, Spin(1/2, total_sz=0)
creates a spin-1/2 space which only includes conﬁgurations {si} that satisfy PN
i=1 si = 0. The
corresponding basis states |si span the zero-magnetization subspace. Similarly, constraints
on the total population in Fock spaces are also supported.

Diﬀerent spaces can be composed to create coupled systems by using the exponent operator
( ** ) and the multiplication operator ( * ). For example, the code below creates the Hilbert
space of a bosonic cavity with a cutoﬀ of 10 particles at each site, coupled to 6 spin− 1
degrees
2
of freedom.
(cid:7)

(cid:4)

704

1 >>> hi = nk.hilbert.Fock(10) * nk.hilbert.Spin(1/2)**6
2 >>> print("Size of the hilbert space: ", hi.n_states)
3 Size of the hilbert space:
4 >>> print("Size of the basis: ", hi.size)
5 Size of the basis: 7
6 >>> hi.random_state(jax.random.PRNGKey(0), (2,))
7 DeviceArray([[10., -1.,
8
(cid:6)
All Hilbert objects can generate random basis elements through the function
random_state(rng_key, shape, dtype) , which has the same signature as standard random
number generators in jax. The ﬁrst argument is a jax random-generator state as returned
by jax.random.PRNGKey , while the other arguments specify the number of output states and
optionally the jax data type.
In this example, an array with two state vectors has been
returned. The ﬁrst entry of each corresponds to the Fock space and is thus an integer in
{0, 1, . . . , 10}, while the rest contains the spin quantum numbers.

[ 9., 1., -1., -1., 1., -1.,

1.]], dtype=float32)

1., -1., -1.],

1., -1.,

Custom Hilbert spaces can be constructed by deﬁning a class inheriting either from
ContinuousHilbert for continuous spaces or DiscreteHilbert for discrete spaces. In the
rest of the paper, we will always be working with discrete Hilbert spaces unless stated other-
wise.

7

(cid:5)

SciPost Physics Codebases

Submission

NetKet also supports working with super-operators, such as the Liouvillian used to deﬁne
open quantum systems, and variational mixed states. The density matrix is an element of
the space of linear operators acting on a Hilbert space, B(H). NetKet represents this space
using the Choi–Jamilkowski isomorphism [40, 41] convention B(H) ∼ H ⊗ H; this “doubled”
Hilbert space is implemented as DoubledHilbert . Doubled Hilbert spaces behave largely
similarly to standard Hilbert spaces, but their bases have double the number of degrees of
freedom; for example, super-operators can be deﬁned straightforwardly as operators acting
on them.

2.2 Linear operators

NetKet is designed to allow users to work with large systems, beyond the typically small
system sizes that are accessible through exact diagonalization techniques. In order to compute
expectation values h ˆOi on such large spaces, we must be able to eﬃciently represent the
operators ˆO and work with their matrix elements hσ| ˆO |ηi without storing them in memory.
NetKet provides diﬀerent implementations for the operators, tailored for diﬀerent use
cases, which are available in the netket.operator submodule. NetKet operators are always
deﬁned relative to a speciﬁc underlying Hilbert space object and inherit from one of the
abstract classes DiscreteOperator or ContinuousOperator , depending on the classes of
supported Hilbert spaces. We defer the discussion of operators acting on a continuous space
to Section 6 and focus on discrete-space operators in the remainder of this section.

An operator acting on a discrete space can be represented as a matrix with some matrix
elements hσ| ˆO |ηi. As most of those elements are zero in physical systems, a standard ap-
proach is to store the operator as sparse matrices, a format that lowers the memory cost by
only storing non-zero entries. However, the number of non-zero matrix elements still scales
exponentially with the number of degrees of freedom, so sparse matrices cannot scale to the
thousands of lattice sites that we want to support, either. For this reason, NetKet uses one
of three custom formats to represent operators:

• LocalOperator is an implementation that can eﬃciently represent sums of K-local
operators, that is, operators that only act nontrivially on a set of K sites. The memory
cost of this format grows linearly with the number of operator terms and the number
of degrees of freedom, but it scales exponentially in K.

• PauliStrings is an implementation that eﬃciently represents a product of Pauli X, Y, Z
operators acting on the whole system. This format only works with qubit-like Hilbert
spaces, but it is extremely eﬃcient and has negligible memory cost.

• FermionOperator2nd is an eﬃcient implementation of second-quantized fermionic oper-
i , fi. It works together

ators built out of the on-site creation and annihilation operators f †
with SpinOrbitalFermions and the equivalent Fock spaces.

• Special implementations like Ising , which hard-code the matrix elements of the oper-

ator. Those are the most eﬃcient, though they cannot be customized at all.

The nk.operator submodule also contains ready-made implementations of commonly used
operators, such as Pauli matrices, bosonic ladder or projection operators, and common Hamil-
tonians such as the Heisenberg, or the Bose–Hubbard models.

8

SciPost Physics Codebases

Submission

2.2.1 Manipulating operators

Operators can be manipulated similarly to standard matrices: they can be added, subtracted,
In the example below we show how to
and multiplied using standard Python operators.
construct the operator

ˆO = (ˆσx
0

+ ˆσx
1

)2 = 2(ˆσx
0

ˆσx
1

+ 1).

(2)

starting from the Pauli X operator acting on the i-th site, σx
i
function nk.operator.spin.sigmax(hi, i) :
(cid:7)

, given by the clearly-named

1 >>> hi = nk.hilbert.Spin(1/2)**2
2 >>> op = nk.operator.spin.sigmax(hi,0) + nk.operator.spin.sigmax(hi,1)
3 >>> op = op * op
4 >>> op
5 LocalOperator(dim=2, acting_on=[[0], [0, 1], [1]], constant=0, dtype=float64)
6 >>> op.to_dense()
7 array([[2., 0., 0., 2.],
[0., 2., 2., 0.],
8
[0., 2., 2., 0.],
[2., 0., 0., 2.]])

10
(cid:6)
Note that each operator requires the Hilbert space object hi as well as the speciﬁc sites it
acts on as constructor arguments. In the last step (line 6), we convert the operator into a
dense matrix using the to_dense() method; it is also possible to convert an operator into a
SciPy sparse matrix using to_sparse() .

9

(cid:4)

(cid:5)

While it is possible to inspect those operators and (if the Hilbert space is small enough)
to convert them to dense matrices, NetKet’s operators are built in order to support eﬃcient
row indexing, similar to row-sparse (CSR) matrices. Given a basis vector |σi in a Hilbert
space, one can eﬃciently query the list of basis states |ηi and matrix elements O(σ, η) such
that

O(σ, η) = hσ| ˆO |ηi 6= 0

(3)

using the function operator.get_conn(sigma) , which returns both the vector of non-zero
matrix elements and the corresponding list of indices |ηi, stored as a matrix.5

3 Variational quantum states

In this section, we ﬁrst introduce the general interface of variational states, which can be used
to represent both pure states (vectors in the Hilbert space) and mixed states (positive-deﬁnite
density operators). We then present how to deﬁne variational ans¨atze and the stochastic
samplers needed that generate Monte Carlo states.

5This querying is currently performed in Python code, just-in-time compiled using numba [42], which runs
on the CPU. If you run your computations on a GPU with a small number of samples, this might introduce a
considerable slowdown. We are aware of this issue and plan to adapt our operators to be indexed directly on
the GPU in the future.

9

SciPost Physics Codebases

Submission

3.1 Abstract interface

A variational state describes a parametrized quantum state that depends on a (possibly
large) set of variational parameters θ. The quantum state can be either pure (denoted as
|ψθi) or mixed (written as a density matrix ˆρθ). NetKet deﬁnes an abstract interface,
netket.vqs.VariationalState , for such objects; all classes that implement this interface will
automatically work with all the high-level drivers (e.g., ground-state optimization or time-
dependent variational dynamics) discussed in Section 4. The VariationalState interface is
relatively simple, as it has only four requirements:

• The parameters θ of the variational state are exposed through the attribute parameters

and should be stored as an array or a nested dictionary of arrays.

• The expectation value h ˆAiθ of an operator ˆA can be computed or estimated by the

method expect .

• The gradient of an expectation value with respect to the variational parameters, ∂h ˆAiθ/∂θj,

is computed by the method expect_and_grad 6.

• The quantum geometric tensor (Section 3.5) of a variational state can be constructed

with the method quantum_geometric_tensor .

At the time of writing, NetKet exposes three types of variational state:

• nk.vqs.ExactState represents a variational pure state |ψθi and computes expectation
values, gradients and the geometric tensor by performing exact summation over the full
Hilbert space.

• nk.vqs.MCState (short for Monte Carlo state) represents a variational pure state and
computes expectation values, gradients and the geometric tensor by performing Markov
chain Monte Carlo (MCMC) sampling over the Hilbert space.

• nk.vqs.MCMixedState represents a variational mixed state and computes expectation

values by sampling diagonal entries of the density matrix.

Variational states based on Monte Carlo sampling are the main tools that we expose to users,
together with a wide variety of high-performance Monte Carlo samplers. More details about
stochastic estimates and Monte Carlo sampling will be discussed in Section 3.3 and Section 3.4.

Dispatch and algorithm selection. With three diﬀerent types of variational state and
several diﬀerent operators supported, it is hard to write a well-performing algorithm that
In order not to
works with all possible combinations of types that users might require.
sacriﬁce performance for generic algorithms, NetKet uses the approach of multiple dispatch
based on the plum module [38]. Combined with jax’s just-in-time compilation, this solution
bears a strong resemblance to the approach commonly used in the Julia language [6].

6For complex-valued parameters θj ∈ C, expect_and_grad returns the conjugate gradient ∂h ˆAiθ/∂θ∗
j in-
stead. This is done because the conjugate gradient corresponds to the direction of steepest ascent when
optimizing a real-valued function of complex variables [43].

10

SciPost Physics Codebases

Submission

Every time the user calls VariationalState.expect or .expect_and_grad , the types of
the variational state and the operator are used to select the most speciﬁc algorithm that
applies to those two types. This allows NetKet to provide generic algorithms that work
for all operators, but keeps it easy to supply custom algorithms for speciﬁc operator types if
desired.

This mechanism is also exposed to users: it is possible to override the algorithms used by
NetKet to compute expectation values and gradients without modifying the source code of
NetKet but simply by deﬁning new dispatch rules using the syntax shown below.
(cid:7)

(cid:4)

1 @nk.vqs.expect.dispatch
2 def expect(vstate: MCState, operator: Ising):
3

# more efficient implementation than default one
#
# expectation_value = ...
#
return expectation_value

4

5

6

7
(cid:6)

(cid:5)

3.2 Deﬁning the variational ansatz

The main feature deﬁning a variational state is the parameter-dependent mapping of an input
conﬁguration to the corresponding probability amplitude or, in other words, the quantum wave
function (for pure states)

or quantum density matrix (for mixed states)

(θ, s) 7→ ψθ(s) = hs|ψθi

(θ, s, s0) 7→ ρθ(s, s0) = hs| ˆρθ

(cid:12)
(cid:12)s0(cid:11) .

(4)

(5)

In NetKet, this mapping is called a model (of the quantum state).7 In the case of NQS,
the model is given by a neural network. For deﬁning models, NetKet primarily relies on
flax [44], a jax-based neural-network library8. Ans¨atze are implemented as flax modules
that map input conﬁgurations (the structure of which is determined by the Hilbert space) to
the corresponding log-probability amplitudes. For example, pure quantum states are evalu-
ated as

ln ψθ(s) = module.apply (θ, s).

(6)

The use of log-amplitudes has the beneﬁt that the log-derivatives ∂ ln ψθ(s)/∂θj, often needed
in variational optimization algorithms, are directly available through automatic diﬀerentiation

7The notion of “model” in NetKet 3 is related to the “machine” classes in NetKet 2 [35]. However, while
NetKet 2 machines both deﬁne the mapping (4) and store the current parameters, this has been decoupled
in NetKet 3. The model only speciﬁes the mapping, while the parameters are stored in the variational state
classes.

8While our primary focus has been the support of flax, NetKet can in principle be used with any
jax-compatible neural network model. For example, NetKet currently includes a compatibility layer which
ensures that models deﬁned using the Haiku framework by DeepMind [45] will work automatically as well.
functions (as used, e.g., in the stax
Furthermore, any model represented by a pair of
framework included with jax) is also supported.

init and apply

11

SciPost Physics Codebases

Submission

of the model. It also makes it easier for the model to learn amplitudes with absolute values
ranging over several orders of magnitude, which is common for many types of quantum states.

, Real and complex amplitudes. NetKet supports both real-valued and

complex-valued model outputs. However, since model outputs correspond to
log-amplitudes, real-valued networks can only represent states that have exclu-
sively non-negative amplitudes, ln ψθ(s) ∈ R ⇒ ψθ(s) ≥ 0.
Since the input conﬁgurations s are real, in many pre-deﬁned NetKet models
the data type of the network parameters (θ ∈ RNp or θ ∈ CNp) determines
whether an ansatz represents a general or a real non-negative state. This should
be kept in mind in particular when optimizing Hamiltonians with ground states
that can have negative amplitudes.

3.2.1 Custom models using Flax

The recommended way to deﬁne a custom module is to subclass flax.linen.Module and
to provide a custom implementation of the __call__ method. As an example, we deﬁne a
simple one-layer NQS with a wave function of the form

ln ψ(s) =

M
X

j=1

tanh[W s + b]j.

(7)

with the number of visible units N matching the number of physical sites, a number of hidden
units M , and complex parameters W ∈ CM ×N (the weight matrix) and b ∈ CM (the bias
vector). Using NetKet and flax, this ansatz can be implemented as follows:
(cid:7)

(cid:4)

1 import netket as nk
2 import jax.numpy as jnp
3 import flax
4 import flax.linen as nn
5
6 class OneLayerNQS(nn.Module):
7

# Module hyperparameter:

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

n_hidden_units: int

@nn.compact
def __call__(self, s):

n_visible_units = s.shape[-1]
# define parameters
# the arguments are: name, initializer, shape, dtype
W = self.param(

"weights",
nn.initializers.normal(),
(self.n_hidden_units, n_visible_units),
jnp.complex128,

)
b = self.param(
"bias",
nn.initializers.normal(),
(self.n_hidden_units,),

12

SciPost Physics Codebases

Submission

25

26

27

28

29

30

31

32

33

34

jnp.complex128,

)

# multiply with weight matrix over last dimension of s
y = jnp.einsum("ij,...j", W, s)
# add bias
y += b
# apply tanh activation and sum
y = jnp.sum(jnp.tanh(y), axis=-1)

return y

35
(cid:6)
The decorator flax.linen.compact used on __call__ (line 10) makes it possible to deﬁne
the network parameters directly in the body of the call function via self.param as done above
(lines 15 and 21). For performance reasons, the input to the module is batched. This means
that, instead of passing a single array of quantum numbers s of size N , a batch of multiple
state vectors is passed as a matrix of shape (batch_size, N) . Therefore, operations like the
sum over all feature indices in the example above need to be explicitly performed over the
last axis.

(cid:5)

(cid:240) Just-in-time compilation. Note that the network will be just-in-time (JIT)

compiled to eﬃcient machine code for the target device (CPU, GPU, or TPU)
using jax.jit , which means that all code inside the __call__ method needs
to written in a way compatible with jax.jit .
In particular, users should use jax.numpy for NumPy calls that need to happen
at runtime; explicit Python control ﬂow, such as for loops and if statements,
should also be avoided, unless one explicitly wants to have them evaluate once at
compile time. We refer users to the jax documentation for further information
on how to write eﬃcient JIT-compatible code.

The module deﬁned above can be used by ﬁrst initializing the parameters using module.init

and then computing log-amplitudes through module.apply :
(cid:7)

1 >>> module = OneLayerNQS(n_hidden_units=16)
2 # init takes two arguments, a PRNG key for random initialization
3 # and a dummy array used to determine the input shape
4 # (here with a batch size of one):
5 >>> params = module.init(nk.jax.PRNGKey(0), jnp.zeros((1, 8)))
6 >>> module.apply(params, jnp.array([[-1, 1, -1, 1, -1, 1, -1, 1]]))
7 DeviceArray([-0.00047843+0.07939122j], dtype=complex128)

(cid:6)

(cid:4)

(cid:5)

3.2.2 Network parametrization and pytrees

Parameter data types. NetKet supports models with both real-valued and complex-
valued network parameters. The data type of the parameters does not determine the output
type. It is possible to deﬁne a model with real parameters that produces complex output.
A simple example is the sum of two real-valued and real-parameter networks, representing

13

SciPost Physics Codebases

Submission

real and imaginary part of the log-amplitudes (and thus phase and absolute value of the wave
function) [29, 31]:

ln ψ(θ,η)(s) = fθ(s) + igη(s) ⇔ |ψ(θ,η)(s)| = exp[fθ(s)],

arg ψ(θ,η)(s) = gη(s)

(8)

(where all θi, ηi ∈ R and f (s), g(s) ∈ R).

More generally, any model with Np complex parameters θ = α+iβ can be represented by a
model with 2Np real parameters (α, β). While these parametrizations are formally equivalent,
the choice of complex parameters can be particularly useful in the case where the variational
mapping is holomorphic or, equivalently, complex diﬀerentiable with respect to θ. This is the
case for many standard network architectures such as RBMs or feed-forward networks, since
both linear transformations and typical activation functions are holomorphic (such as tanh,
cosh, and their logarithms) or piecewise holomorphic (such as ReLU), which is suﬃcient in
practice. Note, however, that there are also common architectures, such as autoregressive
In the holomorphic case, the computational cost of
networks, that are not holomorphic.
diﬀerentiating the model, e.g., to compute the quantum geometric tensor (Section 3.5), can
be reduced by exploiting the Cauchy–Riemann equations [46],

i∇αψ(α,β)(s) = ∇βψ(α,β)(s).

(9)

Note that NetKet generally supports models with arbitrary parametrizations (i.e., real
and both holomorphic and non-holomorphic complex parametrizations). The default assump-
tion is that models with complex weights are non-holomorphic, but some objects (most notably
the quantum geometric tensor) accept a ﬂag holomorphic=True to enable a more eﬃcient
code path for holomorphic networks.

, It is the user’s responsibility to only set holomorphic=True for models that are,

in fact, holomorphic.
If this is incorrectly speciﬁed, NetKet code may give
incorrect results. To check whether a speciﬁc architecture is holomorphic, one
can verify the condition

∂ψθ(s)/∂θ∗
j

= (∂/∂αj + i∂/∂βj)ψθ(s) = 0,

(10)

which is equivalent to Eq. (9).

In NetKet, model parameters do not need to be stored as a contiguous vector.
Pytrees.
Instead, models can support any collection of parameters that forms a so-called pytree. Pytree
is jax terminology for collections of numerical tensors stored as the leaf nodes inside layers of
nested standard Python containers (such as lists, tuples, and dictionaries).9 Any object that is
not itself a pytree, in particular NumPy or jax arrays, is referred to as a leaf. Networks deﬁned
as flax modules store their parameters in a (potentially nested) dictionary, which provides
name-based access to the network parameters.10 For the OneLayerNQS deﬁned above, the
parameter pytree has the structure:

9See https://github.com/google/jax/blob/jax-v0.2.28/docs/pytrees.md for a detailed introduction of pytrees.
10Speciﬁcally, flax stores networks parameters in an immutable FrozenDict object, which otherwise has
the same semantics as a standard Python dictionary and, in particular, is also a valid pytree. The parameters
can be modiﬁed by converting to a standard mutable dict via flax.core.unfreeze(params) .

14

(cid:4)

(cid:5)

(cid:4)

(cid:5)

SciPost Physics Codebases

Submission

(cid:7)

1 # For readability, the actual array data has been replaced with ... below.
2 >>> print(params)
3 FrozenDict({
4

params: {

5

6

weights: DeviceArray(..., dtype=complex128),
bias: DeviceArray(..., dtype=complex128),

},

7
8 })

(cid:6)
The names of the entries in the parameter dictionary correspond to those given in the param
call when deﬁning the model. NetKet functions often work directly with both plain arrays
and pytrees of arrays. Furthermore, any Python function can be applied to the leaves of a
pytree using jax.tree_map . For example, the following code prints a pytree containing the
shape of each leaf of params , preserving the nested dictionary structure:
(cid:7)

1 >>> print(jax.tree_map(jnp.shape, params))
2 FrozenDict({
3

params: {

4

5

weights: (16, 8),
bias: (16,),

},

6
7 })

(cid:6)
Functions accepting multiple leaves as arguments can be mapped over the corresponding
number of pytrees (with compatible structure) using jax.tree_map .

For example, the diﬀerence of two parameter pytrees of the same model can be computed
using delta = jax.tree_map(lambda a, b: a - b, params1, params2) . NetKet provides
an additional set of utility functions to perform linear algebra operations on such pytrees in
the nk.jax submodule.

3.2.3 Pre-deﬁned ans¨atze included with NetKet

NetKet provides a collection of pre-deﬁned modules under nk.models , which allow quick
access to many commonly used NQS architectures (Table 1):

• Jastrow: The Jastrow ansatz [47, 48] is an extremely simple yet eﬀective many-body
ansatz that can capture some inter-particle correlations. The log-wavefunction is the
linear function log ψ(σ) = P
i σiWi,jσj. Evaluation of this ansatz is very fast but it is
also the least powerful model implemented in NetKet;

• RBM: The restricted Boltzmann machine (RBM) ansatz is composed by a dense layer
If the Hilbert space has N degrees of freedom of size d,
followed by a nonlinearity.
RBM has αN features in the dense layer. This ansatz requires param_dtype=complex
to represent states that are non-positive valued.
RBMMultiVal is a one-hot encoding
layer followed by an RBM with αdN features in its dense layer. Finally, RBMModPhase
consists of two real-valued RBMs that encode respectively the modulus and phase of
the wavefunction as log ψ(σ) = RBM (σ) + i RBM (σ). This ansatz only supports real
parameters.
If considering Hilbert spaces with local dimension d > 2, plain RBMs
usually require a very large feature density α and RBMMultiVal s perform better.

15

SciPost Physics Codebases

Submission

Name

Jastrow ansatz

NetKet class

Jastrow

Restricted
(RBM)

Boltzmann

machine

RBM , RBMMultiVal , RBMModPhase

Symmetric RBM

Group-Equivariant
Neural Network

Convolutional

RBMSymm

GCNN

References

[47, 48]

[8]

[8]

Section 5
[49]

ARNNDense ,

ARNNConv1D ,

Autoregressive Neural Network

ARNNConv2D ,

FastARNNConv1D ,

[50]

Neural Density Matrix

FastARNNConv2D

NDM

[24, 51]

Table 1: List of models included in NetKet’s nk.models submodule, together with
relevant references.

• RBMSymm: A symmetry-invariant RBM. Only symmetry groups that can be repre-
sented as permutations of the computational basis are supported (see Section 5). This
architecture has fewer parameters than an RBM, but it is more expensive to evaluate.
It requires param_dtype=complex to represent states that are non-positive valued.

• GCNN: A symmetry-equivariant feed-forward network (see Section 5.2). Only symme-
try groups that can be represented as permutations of the computational basis are
supported. This model is much more complex and computationally intensive than
RBMSymm , but can also lead to more accurate results.
It can also be used to target
an excited state of a lattice Hamiltonian. When working with states that are real but
non-positive, one can use real parameters together with complex_output=True . If the
states are to have a complex phase, param_dtype=complex is required.

• Autoregressive networks: ARNNs are models that can produce uncorrelated samples
when sampled with nk.sampler.ARNNSampler . Those architectures can be eﬃciently
sampled on GPUs, but they are much more expensive than traditional RBMs.

• NDM: A positive-semideﬁnite density matrix ansatz, comprised of a component de-
scribing the pure part and one describing the mixed part of the state. The pure part
is equivalent to an RBM with feature density α, while the mixed part is an RBM with
feature density β. This network only supports real parameters.

3.2.4 Custom layers included with Flax and NetKet

The nk.nn submodule contains generic modules such as masked dense, masked convolutional
and symmetric layers to be used as building blocks for custom neural networks. Those layers

16

SciPost Physics Codebases

Submission

are complementary to those provided by flax and can be combined together to develop novel
neural-network architectures.11

As an example, a multi-layer NQS with two convolutional and one ﬁnal dense layer acting

as a weighted sum can be deﬁned as follows:
(cid:7)

1 class MultiLayerCNN(nn.Module):
2

features1: int
features2: int
kernel_size: int

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25
(cid:6)

@nn.compact
def __call__(self, s):
# define layers
layer1 = nn.Conv(

features=self.features1,
kernel_size=self.kernel_size,

)
layer2 = nn.Conv(

features=self.features2,
kernel_size=self.kernel_size,

)
weighted_sum = nn.Dense(features=1)

# apply layers and tanh activations
y = jnp.tanh(layer1(s))
y = jnp.tanh(layer2(y))
y = weighted_sum(y)
# last axis only has one entry, so we just return that
# but keep the batch dimension
return y[..., 0]

(cid:4)

(cid:5)

flax network layers are available from the flax.linen submodule (imported as nn in

the example above), NetKet layers from netket.nn .

3.3 Estimating observables

For any variational ansatz, it is crucial to also have eﬃcient algorithms for computing quan-
tities of interest, in particular observables and their gradients. Since evaluating the wave
function on all conﬁgurations is infeasible for larger Hilbert spaces, NQS approaches rely on
Monte Carlo sampling of quantum expectation values.

Pure states. The quantum expectation value of an operator ˆA on a non-normalized pure
state |ψi can be written as a classical expectation value over the Born distribution p(s) ∝
|ψ(s)|2 using the identity

h ˆAi = hψ| ˆA|ψi

hψ|ψi

= X
s

|ψ(s)|2
hψ|ψi

˜A(s) = X
s

p(s) ˜A(s) = E[ ˜A],

(11)

11In the past flax had minor issues with complex numbers and therefore NetKet included versions of
some standard layers, such as Dense and Conv , that handle complex numbers properly. Starting with flax
version 0.5, released in May 2022, those issues have been addressed and we now recommend the use of flax
layers also with complex numbers.

17

SciPost Physics Codebases

Submission

where ˜A is the local estimator

˜A(s) = hs| ˆA|ψi

hs|ψi

= X
s0

ψ(s0)
ψ(s) hs| ˆA|s0i,

(12)

also known as the local energy when ˆA is the Hamiltonian [52]. Even though the sum in
Eq. (12) runs over the full Hilbert space basis, the local estimator can be eﬃciently computed
if the operator is suﬃciently sparse in the given basis, i.e., all but a tractable number of
matrix elements hs| ˆA|s0i are zero. Thus, an eﬃcient algorithm is required that, given s, yields
all connected conﬁgurations s0 together with their respective matrix elements, as described in
Section 2.2. Given the derivatives of the log-amplitudes
Oi(s) = ∂ ln ψθ(s)

(13)

,

∂θi

gradients of expectation values can also be evaluated. Deﬁne the force vector as the covariance

˜fi = Cov[Oi, ˜A] = E[O∗

i

( ˜A − E[ ˜A])].

Then, if θi ∈ R is a real-valued parameter,
∂h ˆAi
∂θi

= 2 Re[ ˜fi].

If θi ∈ C and the mapping θi 7→ ψθ(s) is complex diﬀerentiable (holomorphic),

∂h ˆAi
∂θ∗
i

= ˜fi.

(14)

(15)

(16)

In case of a non-holomorphic mapping, Re[θi] and Im[θi] can be treated as two independent
real parameters and Eq. (15) applies to each.

The required classical expectation values are then estimated by averaging over a se-
of conﬁgurations distributed according to the Born distribution p(s) ∝ |ψ(s)|2;

quence {si}Ns
i=1
e.g., Eq. (11) becomes

E[ ˜A] ≈

1

NsX

Ns

i=1

˜A(s).

(17)

For some models, in particular autoregressive neural networks [50], one can eﬃciently draw
samples from the Born distribution directly. For a general ansatz, however, this is not possible
and Markov-chain Monte Carlo (MCMC) sampling methods [52] must be used: these generate
a sequence (Markov chain) of samples that asymptotically follows the Born distribution. Such
a chain can be generated using the Metropolis–Hastings algorithm [53], which is implemented
in NetKet’s sampler interface, described in the next section.

Mixed states. When evaluating observables for mixed states, it is possible to exploit a
slightly diﬀerent identity,

h ˆAi =

Trhˆρ ˆA
i
Tr[ˆρ]

= X
s

ρ(s, s)
Tr[ˆρ]

˜Aρ(s) = E[ ˜Aρ],

(18)

18

SciPost Physics Codebases

Submission

which rewrites the quantum expectation value as a classical expectation over the probability
distribution deﬁned by the diagonal of the density matrix p(s) ∝ ρ(s, s). Here, ˜Aρ denotes
the local estimator of the observable over a mixed state,

˜Aρ(s) = hs|ˆρ ˆA|si
hs| ˆρ |si

= X
s0

ρ(s, s0)
ρ(s, s) hs0| ˆA|si.

(19)

It is then possible to follow the same procedures detailed in the previous paragraph for pure
states to compute the gradient of an expectation value of an operator over a mixed state
by replacing the probability distribution over which the average is computed and the local
estimator.

3.3.1 Reducing memory usage with chunking

The number of variational state evaluations required to compute the local estimators (12)
typically scales superlinearly12 in the number of sites N . For optimal performance, NetKet
by default performs those evaluations in a single call using batched inputs. However, for large
Hilbert spaces or very deep models it might be impossible to ﬁt all required intermediate
buﬀers into the available memory, leading to out-of-memory errors. This is encountered
particularly often in calculations on GPUs, which have more limited memory.

To avoid those errors, NetKet’s nk.vqs.VariationalState exposes an attribute called
chunk_size , which controls the maximum number of conﬁgurations for which a model is
evaluated at the same time13. The chunk size eﬀectively bounds the maximum amount of
memory required to evaluate the variational function at the expense of an increased compu-
tational cost in some operations involving the derivatives of the model. For this reason, we
suggest using the largest chunk size that ﬁts in memory.

Chunking is supported for the majority of operations, such as computing expectation
values and their gradients, as well as the evaluation the quantum geometric tensor. If a chunk
size is speciﬁed but an operation does not support it, NetKet will print a warning and
attempt to perform the operation without chunking.

3.4 Monte Carlo samplers

The sampling algorithm used to obtain a sequence of conﬁgurations from the probability
distribution deﬁned by the variational ansatz is speciﬁed by sampler classes inheriting from
nk.sampler.AbstractSampler . Following the purely functional design of jax, we deﬁne the
sampler to be a stateless collection of settings and parameters, while storing all mutable state
such as the PRNG key and the statistics of acceptances in an immutable sampler state object.
Both the sampler and the sampler state are stored in the variational state, but they can be
used independently, as they are decoupled from the rest of NetKet.

The Metropolis–Hastings algorithm is used to generate samples from an arbitrary proba-
bility distribution. In each step, it suggests a transition from the current conﬁguration s to a

12The exact scaling depends on the sparsity of the observable in the computational basis (which in a lattice

model primarily depends on the locality of the operator and the dimension of the lattice).

13The chunk size can be speciﬁed at model construction and freely changed later. Chunking can also be

disabled at any time by setting VariationalState.chunk_size = None .

19

SciPost Physics Codebases

Submission

proposed conﬁguration s0. The proposal is accepted with probability

Pacc(s → s0) = min

(cid:18)

1,

P (s0)
P (s)

g(s | s0)
g(s0 | s)

(cid:19)

,

(20)

where P is the distribution being sampled from and g(s0 | s) is the conditional probability
of proposing s0 given the current s. We use L(s, s0) = log[g(s | s0)/g(s0 | s)] to denote the
correcting factor to the log probability due to the transition kernel. This factor is needed
for asymmetric kernels that might propose one move with higher probability than its reverse.
Simple kernels, such as a local spin ﬂip or exchange, are symmetric, therefore L(s, s0) =
L(s0, s) = 1, but other proposals, such as Hamiltonian sampling, are not necessarily symmetric
and need this factor.

lis sampler:

At the time of writing, NetKet exposes four types of rules to use with the Metropo-
MetropolisLocal , which changes one discrete local degree of freedom in each
transition; MetropolisExchange , which exchanges two local degrees of freedom respecting a
conserved quantity (e.g., total particle number or magnetization); MetropolisHamiltonian ,
which transitions the conﬁguration according to the oﬀ-diagonal elements of the Hamiltonian;
and MetropolisGaussian , which moves a conﬁguration with continuous degrees of freedom
according to a Gaussian distribution.

The diﬀerent transition kernels in these samplers are represented by MetropolisRule
objects. To deﬁne a Metropolis sampling algorithm with a new transition kernel, one only
needs to subclass MetropolisRule and implement the transition method, which gives s0
and L(s, s0) in each transition. For example, the following transition rule changes the local
degree of freedom on two sites at a time:
(cid:7)

(cid:4)

1 from netket.hilbert.random import flip_state
2 from netket.sampler import MetropolisRule
3 from netket.utils.struct import dataclass
4
5 # To be jax-compatible, it must be a dataclass
6 @dataclass
7 class TwoLocalRule(MetropolisRule):
8

def transition(rule, sampler, machine, parameters, state, key, σ):

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24
(cid:6)

# Deduce the number of MCMC chains from input shape
n_chains = σ.shape[0]
# Load the Hilbert space of the sampler
hilb = sampler.hilbert
# Split the rng key into 2: one for each random operation
key_indx, key_flip = jax.random.split(key, 2)
# Pick two random sites on every chain
indxs = jax.random.randint(

key_indx, shape=(n_chains, 2), minval=0, maxval=hilb.size

)
# flip those sites
σp, _ = flip_state(hilb, key_flip, σ, indxs)

# If this transition had a correcting factor L, it’s possible
# to return it as a vector in the second value
return σp, None

20

(cid:5)

SciPost Physics Codebases

Submission

Type

Name

Usage

MCMC (Metropolis)

MetropolisLocal

discrete Hilbert spaces

MetropolisExchange

MetropolisHamiltonian

permutations of local states, conserving
total magnetization in spin systems

preserving symmetries of the Hamilto-
nian

MetropolisGaussian

continuous Hilbert spaces

Direct

ExactSampler

small Hilbert spaces, performs MC
sampling from the exact distribution

ARDirectSampler

autoregressive models

Table 2: List of samplers in NetKet with their class names and a description.

Once a custom rule is deﬁned, a MCMC sampler using such rule can be constructed with
the command sampler = MetropolisSampler(hilbert, TwoLocalRule()) . Besides Metropo-
lis algorithms, more advanced Markov chain algorithms can also be implemented as NetKet
samplers. Currently, parallel tempering is provided as an experimental feature.

Some models allow us to directly generate samples that are exactly distributed according
to the desired probability, without the use of Markov chains and the issue of autocorrelation,
which often leads to more eﬃcient sampling. In this case, direct samplers can be implemented
with an interface similar to Markov chain samplers. Currently NetKet has implemented
ARDirectSampler to be used with ARNNs. For benchmarking purposes, NetKet also pro-
vides ExactSampler , which allows direct sampling from any model by computing the full
Born distribution |ψ(s)|2 for all s. Table 2 is a list of all the samplers.

3.5 Quantum geometric tensor

The quantum geometric tensor (QGT) [54] of a pure state is the metric tensor induced by the
Fubini–Study distance [55, 56]

d(ψ, φ) = cos−1

s

hψ|φihφ|ψi
hψ|ψihφ|φi

,

(21)

which is the natural and gauge-invariant distance between two pure quantum states |ψi and
|φi. The QGT is commonly used for time evolution (see Section 4.3) and for quantum natural
gradient descent [57], which was originally developed in the VMC community under the name
of stochastic reconﬁguration (SR) [58]. Quantum natural gradient descent is directly related
to the natural gradient descent developed in the machine learning community [59].

From now on, we assume that the state |ψθi is parametrized by a set of parameters θ.
Assuming further that |φi = |ψθ+δθi, the distance (21) can be expanded to second order in
the inﬁnitesimal parameter change δθ as d(ψθ, ψθ+δθ)2 = (δθ)†G(δθ), where G is the quantum

21

SciPost Physics Codebases

Submission

geometric tensor. For a holomorphic mapping θ 7→ |ψθi, the QGT is given by

Gij(θ) =

D

∂θiψθ

E

(cid:12)
(cid:12)
(cid:12)∂θj ψθ

hψ|ψi

D
∂θiψθ

−

E D

(cid:12)
(cid:12)
ψθ
(cid:12)ψθ
hψ|ψi2

E

(cid:12)
(cid:12)
(cid:12)∂θj ψθ

(22)

where the indices i, j label the parameters and hx|∂θiψθi = ∂θi hx|ψθi. Similar to expectation
values and their gradients, Eq. (22) can be rewritten as a classical covariance with respect to
the Born distribution ∝ |ψ(s)|2:

Gij(θ) = Cov[Oi, Oj] = E [O∗
i

(Oj − E[Oj])] ,

(23)

where Oi are the log-derivatives (13) of the ansatz.14 This allows the quantum geometric
tensor to be estimated using the same sampling procedure used to obtain expectation val-
ues and gradients. The QGT or its stochastic estimate is also commonly known as the S
matrix [52] or quantum Fisher matrix (QFM) in analogy to the classical Fisher information
matrix [57, 59, 60].

For applications such as quantum natural gradient descent or time-evolution it is usually
not necessary to access the full, dense matrix representation Gij(θ) of the quantum geometric
tensor, but only to compute its product with a vector, ˜vi = P
j Gij(θ) vj. When the variational
ansatz ψθ has millions of parameters, the QGT can indeed be too large to be stored in memory.
Exploiting the Gram matrix structure of the geometric tensor [61], we can directly compute
its action on a vector without ever calculating the full matrix, trading memory requirements
for an increased computational cost.

Given a variational state vs , a QGT object can be obtained by calling:

(cid:7)

1 >>> qgt = vs.quantum_geometric_tensor()

(cid:6)
This qgt object does not store the full matrix, but can still be applied to a vector with the
same shape as the parameters:
(cid:7)

1 >>> vec = jax.tree_map(jnp.ones_like, vs.parameters)
2 >>> qgt_times_vec = qgt @ vec

(cid:6)
It can be converted to a dense matrix by calling to_dense :
(cid:7)

(cid:4)

(cid:5)

(cid:4)

(cid:5)

(cid:4)

1 # get the matrix (2d array) of the qgt
2 >>> qgt_dense = qgt.to_dense()
3 # flatten vec into a 1d Array
4 >>> grad_dense, unravel = nk.jax.tree_ravel(vec)
5 >>> qgt_times_vec = unravel(qgt_dense @ vec_dense)

(cid:6)
The QGT can then be used together with a direct solver, such as jnp.linalg.eigh , jnp.linalg.svd ,
or jnp.linalg.qr .

(cid:5)

Mixed states. When working with mixed states, which are encoded in a density matrix, it
is necessary to pick a suitable metric to induce the QGT. Even if the most physical distances
for density matrices are the spectral norm or other trace-based norms [62, 63], it is generally

14Strictly speaking, this estimator is only correct if ψθ(s) = 0 =⇒ ∂θk ψθ(s) = 0. This is because we

multiplied and divided by ψθ(s) in the derivation of the estimator, which is only valid if ψθ(s) 6= 0.

22

SciPost Physics Codebases

Submission

hard to use them to deﬁne an expression for the QGT that can be eﬃciently sampled and
computed at polynomial cost. While this might be regarded as a barbaric choice, it leads to
an expression equivalent to Eq. (23), where the expectation value is over the joint-distribution
of the of row and column labels (s, s0) of the squared density matrix ∝ |ρ(s, s0)|2. Therefore,
when working with mixed states, we resort to the L2 norm, which is equivalent to treating
the density matrix as a vector (“pure state”) in an enlarged Hilbert space.

3.5.1 Implementation diﬀerences

There is some freedom in the way one can calculate the QGT, each diﬀerent implementation
taking a diﬀerent tradeoﬀ between computational and memory cost. In the example above,
we have relied on NetKet to automatically select the best implementation according to some
internal heuristics, but if one wants to push variational methods to the limits, it is useful to
understand the two diﬀerent implementations on oﬀer:

• QGTJacobian , which computes and stores the Jacobian Oi(s) of the model (cf. Ap-
pendix B.1) at initialization (using reverse-mode automatic diﬀerentiation) and can be
applied to a vector by performing two matrix-vector multiplications;

• QGTOnTheFly , which lazily computes the application of the quantum geometric tensor

on a vector through automatic diﬀerentiation.

QGTOnTheFly is the most ﬂexible and can be scaled to arbitrarily large systems.
It
is based on the observation that the QGT is the Jacobian of the model multiplied with its
conjugate, which means that its action can be calculated by combining forward and reverse-
mode automatic diﬀerentiation. At initialization, it only computes the linearization (forward
pass), and then it eﬀectively recomputes the gradients every time it is applied to a vector.
However, since it never has to store these gradients, it is not limited by the available memory,
which also makes it perform well for shallow neural-network models like RBMs. This method
works with both holomorphic and non-holomorphic ans¨atze with no diﬀerence in performance.

QGTJacobian. For deep networks with ill-conditioned15 quantum geometric tensors, re-
computing the gradients at every step in an iterative solver might be very costly. QGTJacobian
can therefore achieve better performance at the cost of considerably higher memory require-
ments because it precomputes the Jacobian at construction and stores it. The downside
is that it has to store a matrix of shape Nsamples × Nparameters, which might not ﬁt in the
memory of a GPU. We note that there are two diﬀerent implementations of QGTJacobian :
QGTJacobianDense and QGTJacobianPyTree . The diﬀerence among the two is that in the for-
mer the Jacobian is stored contiguously in memory, leading to a better throughput on GPUs,
while the latter stores them in the same structure as the parameters (so each parameter block
is separated from the others). Converting from the non-contiguous ( QGTJacobianPyTree )
to the contiguous ( QGTJacobianDense ) format has, however, a computational and mem-
ory cost which might shadow its beneﬁt. Moreover, the dense format does not work with

15The number of steps required to ﬁnd a solution with an iterative linear solver grows with the condition
number of the matrix. Therefore, an ill-conditioned matrix requires many steps of iterative solver. For a
discussion on this issue, see the paragraph on Linear Systems of this section.

23

SciPost Physics Codebases

Submission

Implementation

Extra arguments Use-cases

Limitations

QGTOnTheFly

None

QGTJacobianDense

QGTJacobianPyTree

mode
holomorphic

rescale_shift

networks,
shallow
large numbers of pa-
rameters and samples,
few solver steps

deep networks with
narrow layers

deep networks with
heterogeneous param-
eters

for

Might be more com-
ex-
putationally
pensive
deep
networks compared to
QGTJacobian .
requires homogeneous
parameter
types,
memory bound

memory bound

Table 3: Overview of the three QGT implementations currently provided by
NetKet with their respective options and limitations.

non-homogeneous parameter data types. The basic QGTJacobian algorithm supports both
holomorphic and non-holomorphic NQS, but a better performing algorithm for holomorphic
ans¨atze can be accessed instantiating it with the option holomorphic=True .

Key diﬀerences between the diﬀerent QGT implementations are summarized in Table 3.

Implementations can be selected, and options passed to them, as shown below:
(cid:7)

QGTJacobianPyTree, QGTJacobianDense, QGTOnTheFly

1 >>> from netket.optimizer.qgt import (
2 ...
3 ... )
4 >>> qgt1 = vs.quantum_geometric_tensor(QGTOnTheFly)
5 >>> qgt2 = vs.quantum_geometric_tensor(QGTJacobianPyTree(holomorphic=True))
6 >>> qgt3 = vs.quantum_geometric_tensor(QGTJacobianDense)

(cid:6)

(cid:4)

(cid:5)

Holomorphicity. When performing time evolution or natural gradient descent, one does
for ans¨atze with real parameters, as
not always need the full quantum geometric tensor:
well as in the case of non-holomorphic wave functions,16 only the real part of the QGT
is used. The real and imaginary parts of the QGT are only required when working with a
holomorphic ansatz. (An in-depth discussion of why this is the case can be found at [64, Table
1].) For this reason, NetKet’s QGT implementations return the full geometric tensor only
for holomorphic complex-parameter ans¨atze, and its real part in all other cases.

3.5.2 Solving linear systems

For most applications involving the QGT, a linear system of equations of the kind

Gijδj = fi

X

j

(24)

16Non-holomorphic functions of complex parameters are internally handled by both jax and NetKet as

real-parameter functions that take the real and imaginary parts of the “complex parameters” separately.

24

SciPost Physics Codebases

Submission

(cid:4)

(cid:5)

(cid:4)

(cid:5)

needs to be solved, where Gi,j is the quantum geometric tensor of a NQS, and fi is a gradient.
This can be done using the standard jax/NumPy functions, assuming f is a pytree with the
same structure as the variational parameters:
(cid:7)

1 >>> # iterative solver
2 >>> x, info = jax.scipy.sparse.linalg.cg(qgt, f)
3 >>> # direct solver, acting on the dense matrix
4 >>> x, info = jax.numpy.linalg.cholesky(qgt.to_dense(), f)

(cid:6)
However, we recommend that users call the solve method on the QGT object, which allows
some additional optimization that may improve performance:
(cid:7)

1 >>> x, info = qgt.solve(jax.scipy.sparse.linalg.cg, f)
2 >>> x, info = qgt.solve(nk.optimizer.solver.cholesky, f)

(cid:6)
While this works with any of the representations it is advisable to only use Jacobian based
implementations ( QGTJacobianPyTree or QGTJacobianDense ) with direct solvers, since con-
structing the QGT matrix from QGTOnTheFly requires multiplication with all basis vectors,
which is not as eﬃcient. Finally, we highlight the fact that users can write their own func-
tions to solve the linear system (24) using advanced regularization schemes (see for instance
ref. [21]) and use them together with qgt.solve , as long as they respect the standard NumPy
solver interface.

When working with iterative solvers such as cg, gmres or minres, the number of steps
required to ﬁnd a solution grows with the condition number of the matrix. Therefore, an
ill-conditioned geometric tensor requires many steps of iterative solver, increasing the compu-
tational cost. Even then or when using non-iterative methods such as singular value decom-
position (SVD), the high condition number can cause instabilities by amplifying noise in the
right-hand side of the linear equation [20, 21, 23]. This is especially true for NQS, which typ-
ically feature QGTs with a spectrum spanning many orders of magnitude [60], often making
QGT-based algorithms challenging to stabilize [15, 21, 23].

To counter that, there is empirical evidence that in some situations, increasing the number
of samples used to estimate the QGT and gradients helps to stabilize the solution [23]. Fur-
thermore, it is possible to apply various regularization techniques to the equation. A standard
option is to add a small diagonal shift (cid:15) to the QGT matrix before inverting it, thus solving
the linear equation

X

(Gi,j + (cid:15))δ0
j

= fi.

(25)

j

When (cid:15) is small, the solution δ0 will be close to the desired solution. Otherwise it is biased
towards the plain force f , which is still acceptable in gradient-based optimization. To add
this diagonal shift in NetKet, one of the following approaches can be used:
(cid:7)

(cid:4)

1 >>> qgt_1 = vs.quantum_geometric_tensor(QGTOnTheFly(diag_shift=0.001))
2 QGTOnTheFly(diag_shift=0.001)
3 >>> qgt_2 = qgt_1.replace(diag_shift=0.005)
4 QGTOnTheFly(diag_shift=0.005)
5 >>> qgt_3 = qgt_2 + 0.005
6 QGTOnTheFly(diag_shift=0.01)

(cid:6)

(cid:5)

25

SciPost Physics Codebases

Submission

Regularizing the QGT with a diagonal shift is an eﬀective technique that can be used when
performing SR/natural gradient descent for ground state search (see Section 4.1). Note, how-
ever, that since the diagonal shift biases the solution of the linear equation towards the plain
gradient, it may bias the evolution of the system away from the physical trajectory in cases
such as real-time evolution. In those cases, non-iterative solvers such as those based on SVD
can be used, the stability of which can be controlled by suppressing smaller singular values.
It has also been suggested in the literature to improve stability by suppressing particularly
noisy gradient components [21,39]. This is not currently implemented in NetKet, but planned
for a future release. SVD-based regularization also comes at the cost of potentially suppress-
ing physically relevant dynamics [23], making it necessary to ﬁnd the right balance between
stabilization and physical accuracy, and increased computational time as SVD is usually less
eﬃcient than iterative solvers.

4 Algorithms for variational states

The main use case of NetKet is variational optimization of wave function ans¨atze. In the
current version NetKet, three algorithms are provided out of the box via high-level driver
classes: variational Monte Carlo (VMC) for ﬁnding ground states of (Hermitian) Hamiltoni-
ans, time-dependent variational Monte Carlo (t-VMC) for real- and imaginary-time evolution,
and steady-state search of Liouvillian open-system dynamics.

These drivers are part of the nk.driver module but we also export them from the nk
namespace. They are constructed from the relevant physical model (e.g., a Hamiltonian), a
variational state, and other objects used by the optimization method. They all support the
run method, which performs a number of optimization steps and logs their progress (e.g.,
variational energies and network parameters) in a variety of output formats.

We highlight that these drivers are built on top of the functionalities described in Sections 2
and 3, and users are free to implement their own drivers or optimization loops, as demonstrated
in Section 4.4.

4.1 Ground-state search

NetKet provides the variational driver nk.VMC for searching for minimal-energy states
using VMC [52].
In the simplest case, the VMC constructor takes three arguments: the
Hamiltonian, an optimizer and the variational state (see Section 3.1). NetKet makes use of
optimizers provided by the jax-based optax library [65],17 which can be directly passed to
VMC , allowing the user to build complex training schedules or custom optimizers. In each
optimization step, new samples of the variational state are drawn and used to estimate the
gradient of the Hamiltonian with respect to the parameters θ of the ansatz [52] based on the
force vector [compare Eq. (14)]

˜fi = Cov[Oi, ˜H] = E[O∗

i

( ˜H − E[ ˜H])],

(26)

where ˜H is the local estimator (12) of the Hamiltonian, known as the local energy, and Oi is
the log-derivative (13) of the wave function. All expectation values in Eq. (26) are evaluated

17The nk.optimizer submodule includes a few optimizers for ease of use and backward compatibility: these

are simply re-exports from optax.

26

SciPost Physics Codebases

Submission

over the Born distribution ∝ |ψ( · )|2 and can therefore be estimated by averaging over the
Monte Carlo samples. Given the vector ˜f , the direction of steepest descent is given by the
energy gradient

f ≡ ∇θh ˆHi = 2 Re[ ˜f ]

(real)

or complex co-gradient [43]

f ≡ ∇θ∗h ˆHi = ˜f

(complex holomorphic).

(27)

(28)

Here we have distinguished the case of i) real parameters and ii) complex parameters with a
variational mapping that is holomorphic with respect to θ. For non-holomorphic ans¨atze (cf.
Section 3.2.2), complex parameters can be treated pairs of separate real-valued parameters
(real and imaginary part) in the sense of eq. (27). Therefore, this case can be considered
equivalent to the real parameter case.

The gradients are then passed on to the optax optimizer, which may transform them
(using, e.g., Adam) further before updating the parameters θ. Using the simple stochastic
gradient descent optimizer optax.sgd (alias nk.optimizer.Sgd ), the update rule is

θi 7→ θi − ηfi,

(29)

where the gradient f is given by Eq. (27) or (28) as appropriate.

Below we give a short snippet showing how to use the VMC driver to ﬁnd the ground-

state of the Ising Hamiltonian.
(cid:7)

1 # Define the geometry of the lattice
2 g = nk.graph.Hypercube(length=10, n_dim=1, pbc=False)
3 # Hilbert space of spins on the graph
4 hi = nk.hilbert.Spin(s=1 / 2, N=g.n_nodes)
5 # Construct the Hamiltonian
6 hamiltonian = nk.operator.Ising(hi, graph=g, h=0.5)
7
8 # define a variational state with a Metropolis Sampler
9 sa = nk.sampler.MetropolisLocal(hi)
10 vstate = nk.vqs.MCState(sa, nk.models.RBM())
11
12 # Construct the VMC driver
13 vmc = nk.VMC(hamiltonian,
14

nk.optimizer.Sgd(learning_rate=0.1),
variational_state=vstate)

15

16
17 # run the optimisation for 300 steps
18 output = vmc.run(300)
(cid:6)

To improve on plain stochastic gradient descent, the VMC interface allows passing a key-
word argument preconditioner . This must be a function that maps a variational state and
the gradient vector fi to the vector δi to be passed to the optimizer as gradients instead of fi.
An important use case is stochastic reconﬁguration [52], where the gradient is preconditioned
by solving the linear system of equations

X

Re[Gij]δj = fi = 2 Re[ ˜fi]

(real)

j

(30)

27

(cid:4)

(cid:5)

SciPost Physics Codebases

Submission

or

Gijδj = fi

(complex holomorphic).

(31)

X

j

The corresponding preconditioner can be created from a QGT class and a jax-compatible
linear solver (the default is jax.scipy.sparse.linalg.cg ) using nk.optimizer.SR :
(cid:7)

(cid:4)

1 # Construct the SR object with the chosen algorithm
2 sr = nk.optimizer.SR(
3 qgt = nk.optimizer.qgt.QGTOnTheFly,
4

solver=jax.scipy.sparse.linalg.bicgstab,
diag_shift=0.01,

5
6 )
7
8 # Construct the VMC driver
9 vmc = nk.VMC(
10

hamiltonian,
nk.optimizer.sgd(learning_rate=0.1), # The optimizer
variational_state=vstate,
preconditioner=sr,

# The variational state
# The preconditioner

# The Hamiltonian to optimize

11

12

13
14 )
(cid:6)

(cid:5)

4.2 Finding steady states

In order to study open quantum systems, NetKet provides the nk.SteadyState variational
driver for determining the variational steady-state ˆρss deﬁned as the stationary point of an
arbitrary super-operator L,

0 = dˆρ
dt
The search is performed by minimizing the Frobenius norm of the time-derivative [24], which
deﬁnes the cost function

= Lˆρ.

(32)

C(θ) = kLˆρk2
2
kˆρk2
2

=

Tr hˆρ†L†Lˆρ
i
Tr [ˆρ† ˆρ]

,

(33)

which has a global minimum for the steady state. The stochastic gradient is estimated over
the probability distribution of the entries of the vectorized density matrix according to the
formula:

fi ≡

∂
∂θ∗
i

kLˆρk2
2
kˆρk2
2

= E h ˜L∇∗

i

i
˜L

− E[O∗
i

˜L2],

(34)

where ˜L(s, s0) = P
m,m0 L(s, s0; m, m0)ρ(m, m0)/ρ(s, s0) is the local estimator proposed in [24],
and the expectation values are taken with respect to the “Born distribution” of the vec-
torized density matrix, p(s, s0) ∝ |ρ(s, s0)|2. The optimization works like the ground-state
optimization provided by nk.VMC : the gradient is passed to an optax optimizer, which may
transform it further before updating the parameters θ. The simplest optimizer, optax.sgd ,
would update the parameters according to the equation

θi → θi − ηfi.

28

(35)

SciPost Physics Codebases

Submission

To improve the performance of the optimization, it is possible to pass the keyword argu-
ment preconditioner to specify a gradient preconditioner, such as stochastic reconﬁguration
that uses the quantum geometric tensor to transform the gradient. The geometric tensor is
computed according to the L2 norm of the vectorized density matrix (see Section 3.5).

As an example, we provide a snippet to study the steady state of a transverse-ﬁeld Ising

chain with 10 spins and spin relaxation corresponding to the Lindblad master equation

Lˆρ = −i

h ˆH, ˆρ

i + X
i

ˆσ−
i

ˆρˆσ+

i −

1
2

nˆσ+

i

o
ˆσ−
i , ˆρ
.

(36)

We ﬁrst deﬁne the Hamiltonian and a list of jump operators, which are stored in a LocalLiouvillian
object, which is a lazy representation of the super-operator L. Next, a variational mixed state
is built by deﬁning a sampler over the doubled Hilbert space and optionally a diﬀerent sam-
pler for the diagonal distribution p(s) ∝ ρ(s, s), which is used to estimate expectation values
of operators. The number of samples used to estimate super-operators and operators can be
speciﬁed separately, as shown in the example by specifying n_samples and n_samples_diag .
(cid:7)

(cid:4)

1 # Define the geometry of the lattice
2 g = nk.graph.Hypercube(length=10, n_dim=1, pbc=False)
3 # Hilbert space of spins on the graph
4 hi = nk.hilbert.Spin(s=1 / 2, N=g.n_nodes)
5
6 # Construct the Liouvillian Master Equation
7 ha = nk.operator.Ising(hi, graph=g, h=0.5)
8 j_ops = [nk.operator.spin.sigmam(hi, i) for i in range(g.n_nodes)]
9 # Create the Liouvillian with Hamiltonian and jump operators
10 lind = nk.operator.LocalLiouvillian(ha, j_ops)
11
12 # Observable
13 sz = sum([nk.operator.spin.sigmam(hi, i) for i in range(g.n_nodes)])
14
15 # Neural Density Matrix
16 sa = nk.sampler.MetropolisLocal(lind.hilbert)
17 vs = nk.vqs.MCMixedState(
18
19 )
20 # Optimizer
21 op = nk.optimizer.Sgd(0.01)
22 sr = nk.optimizer.SR(diag_shift=0.01)
23
24 ss = nk.SteadyState(lind, op, variational_state=vs, preconditioner=sr)
25 out = ss.run(n_iter=300, obs={"Sz": sz})
(cid:6)

sa, nk.models.NDM(beta=1), n_samples=2000, n_samples_diag=500

(cid:5)

4.3 Time propagation

Time propagation of variational states can be performed by incorporating the time dependence
in the variational parameters and deriving an equation of motion that gives a trajectory
in parameters space θ(t) that approximates the desired quantum dynamics. For real-time
dynamics of pure and mixed NQS, such an equation of motion can be derived from the time-
dependent variational principle (TDVP) [64, 66, 67]. When combined with VMC sampling to

29

SciPost Physics Codebases

Submission

estimate the equation of motion (EOM), this is known as time-dependent variational Monte
Carlo (t-VMC) [52, 68] and is the primary approach currently used in NQS literature [8, 18,
19, 21–23]. For complex holomorphic parametrizations18, the TDVP equation of motion is

X

Gij(θ) ˙θj = γfi(θ, t),

j

(37)

with the QGT G and force vector f deﬁned in Sections 4.1 and 4.2. After solving Eq. (37),
the resulting parameter derivative ˙θ can be passed to an ODE solver. The factor γ determines
the type of evolution:

• For γ = −i, the EOM approximates the real-time Schr¨odinger equation on the varia-
tional manifold, the simulation of which is the main use case for the t-VMC implemen-
tation provided by NetKet.

• For γ = −1, the EOM approximates the imaginary-time Schr¨odinger equation on the
variational manifold. When solved using the ﬁrst-order Euler scheme θ(t + dt) =
θ(t) + ˙θ dt, this EOM is equivalent to stochastic reconﬁguration with learning rate dt.
Imaginary-time propagation with higher-order ODE solvers can therefore also be used
for ground state search as an alternative to VMC. This can result in improved conver-
gence in some cases [17].

• For γ = 1 and with the Lindbladian super-operator taking the place of the Hamiltonian
in the deﬁnition of the force f , this ansatz yields the dissipative real-time evolution
according to the Gorini-Kossakowski-Lindblad-Sudarshan master equation [69]. Our
implementation uses the QGT induced by the vector norm [25] as discussed in the last
paragraph of Section 3.5.

The current version of NetKet provides a set of Runge–Kutta (RK) solvers based on
jax and a driver class TDVP implementing the t-VMC algorithm for the three use cases
listed above. At the time of writing, these features are provided as a preview version in the
netket.experimental namespace as their API is still subject to ongoing development. The
ODE solvers are located in the submodule netket.experimental.dynamics , the driver under
netket.experimental.TDVP .

Runge-Kutta solvers implement the propagation scheme [70]

using a linear combination of slopes

θ(t + dt) = θ(t) + dtXL

l=1

blkl

(cid:18)

kl = F

θ(t) + Xl−1

m=1

almkm, t + cl dt

(cid:19)

,

(38)

(39)

each determined by the solution F (θ, t) = ˙θ of the equation of motion (37) at an intermediate
time step. The coeﬃcients {alm}, {bl}, and {cl} determine the speciﬁc RK scheme and its
order. NetKet further supports step size control when using adaptive RK schemes. In this

18The TDVP can be implemented for real-parameter wavefunctions by taking real parts of the right-hand
side and QGT similar to VMC (Section 4.1) [39,46]. This is not yet available in the current version of NetKet,
but will be added in a future release.

30

SciPost Physics Codebases

Submission

case, the step size is dynamically adjusted based on an embedded error estimate that can
be computed with little overhead during the RK step (38) [70]. Step size control requires a
norm on the parameters space in order to estimate the magnitude of the error. Typically, the
Euclidean norm kδk = pP
i |δi|2 is used. However, since diﬀerent directions in parameters
space inﬂuence the quantum state to diﬀerent degrees, it can be beneﬁcial to use the norm
kδkG = pP
i Gijδj induced by the QGT G as suggested in Ref. [21], which takes this
curvature into account and is also provided as an option with the NetKet time-evolution
driver.

i δ∗

An example demonstrating the use of NetKet’s time evolution functionality is provided

in Sec. 9.

4.4 Implementing custom algorithms using NetKet

While key algorithms for energy optimization, steady states, and time propagation are pro-
vided out of the box in the current NetKet version, there are many more applications of
NQS. While we wish to provide new high-level driver classes for additional use cases, such as
quantum state tomography [31] or general overlap optimization [32], it is already possible and
encouraged for users to implement their own algorithms on top of NetKet. For this reason,
we provide the core building blocks of NQS algorithms in a composable fashion.

For example, it is possible to write a simple loop that solves the TDVP equation of
motion (37) for a holomorphic variational ansatz and using a ﬁrst-order Euler scheme [i.e.,
θ(t+dt) = θ(t)+ ˙θ(t) dt] very concisely, making use of the elementary building blocks provided
by the VariationalState class:
(cid:7)

(cid:4)

1 def custom_simple_tdvp(
2

hamiltonian: AbstractOperator,
vstate: VariationalState,
t0: float,
dt: float,
t_end: float,

# Hamiltonian
# variational state
# initial time
# time step
# end time

t = t0
while t < t_end:

# compute the energy gradient f
energy, f = vstate.expect_and_grad(hamiltonian)
G = vstate.quantum_geometric_tensor()
# multiply the gradient by -1.0j for unitary dynamics
gamma_f = jax.tree_map(lambda x: -1.0j * x, f)
# Solve the linear system using any solver, such as CG
# (or write your own regularization scheme)
dtheta, _ = G.solve(jax.scipy.sparse.linalg.cg, gamma_f)
# update the parameters (theta = theta + dt * dtheta)
vs.parameters = jax.tree_map(

lambda x, y: x + dt * y, vs.parameters, dtheta

)
t = t + dt

22
(cid:6)
While the included TDVP driver (Section 4.3) provides many additional features (such as error
handling, step size control, or higher-order integrators) and makes use of jax’s just-in-time
compilation, this simple implementation already provides basic functionality and shows how

(cid:5)

31

3

4

5

6
7 ):
8

9

10

11

12

13

14

15

16

17

18

19

20

21

SciPost Physics Codebases

Submission

NetKet can be used for quick prototyping.

5 Symmetry-aware neural quantum states

NetKet includes a powerful set of utilities for implementing NQS ans¨atze that are symmetric
or transform correctly under the action of certain discrete symmetry groups. Only groups
that are isomorphic to a set of permutations of the computational basis are supported. This
is useful for modeling symmetric (e.g., lattice) Hamiltonians, whose eigenstates transform
under irreducible representations of their symmetry groups. Restricting the Hilbert space
to individual symmetry sectors can improve the convergence of variational optimization [71]
and the accuracy of its result [14, 16, 49, 72]. Additionally, symmetry restrictions can be used
to ﬁnd excited states [13, 16, 30], provided they are the lowest energy level in a particular
symmetry sector.

While there is a growing interest for other symmetry groups, such as continuous ones
like SU (2) or SO(3), they cannot be compactly represented in the computational basis and
therefore the approach described in this chapter cannot be used. Finding eﬃcient encodings
for continuous groups is still an open research problem and it’s not yet clear which strategy
will work best [16].

NetKet uses group convolutional neural networks (GCNNs) to build wave functions that
are symmetric over a ﬁnite group G. GCNNs generalize convolutional neural networks, invari-
ant under the Abelian translation group, to general symmetry groups G which may contain
non-commuting elements [73]. GCNNs were originally designed to be invariant, but they can
be modiﬁed to transform under an arbitrary irreducible representation (irrep) of G, using the
projection operator [74]

|ψχi = dχ
|G|

X

χ∗

g g|ψi,

(40)

g∈G
where g runs over all symmetry operations in G, with corresponding characters χg. Under
the trivial irrep, where all characters are unity, the invariant model is recovered.

NetKet can infer the full space group of a lattice, deﬁned as a set of permutations of
lattice sites, starting from a geometric description of its point group.
It can also gener-
ate nontrivial irrep characters [to be used in (40) for states with nonzero wave vectors or
transforming nontrivially under point-group symmetries] using a convenient interface that
approximates standard crystallographic formalism [75]. In addition, NetKet provides pow-
erful group-theoretic algorithms for arbitrary permutation groups of lattice sites, allowing new
symmetry elements to be easily deﬁned.

Pre-built GCNNs are then provided in the nk.models submodule, which can be con-
structed by specifying few parameters, such as the number of features in each layer, and
the lattice or permutation group under which the network should transform. Symmetric
RBMs [8] are also implemented as one-layer GCNNs that aggregate convolutional features
using a product rather than a sum. These pre-built network architectures are made up of
individual layers found in the nk.nn submodule, which can be used directly to build custom
symmetric ans¨atze.

Section 5.1 describes the NetKet interface for constructing space groups of lattices and
their irreps. Usage of GCNNs is described in Section 5.2, while appendix A provides mathe-
matical and implementation details.

32

SciPost Physics Codebases

Submission

5.1 Symmetry groups and representation theory

NetKet supports symmetry groups that act on a discrete Hilbert space deﬁned on a lattice.
On such a Hilbert space, space-group symmetries act by permuting sites; most generally, there-
fore, arbitrary subgroups of the symmetric group SN of a lattice of N sites are supported. A
symmetry group can be speciﬁed directly as a list of permutations, as in the following example,
which enforces the symmetry ψ(s0, s1, s2, s3) = ψ(s3, s1, s2, s0) for all four-spin conﬁgurations
s = (s0, . . . , s3), si = ±1:
(cid:7)

1 hi = nk.hilbert.Spin(1/2, N=4)
2 symms = [
3

[0, 1, 2, 3], # identity element
[3, 1, 2, 0], # swap first and last site

4
5 ]
6 model = nk.models.RBMSymm(symms, alpha=1)

(cid:6)
The listed permutations are required to form a group and, in particular, the identity operation
e : s 7→ s must always be included as the ﬁrst element.

It is inconvenient and error-prone to specify all space-group symmetries of a large lattice
by their indices. Therefore, NetKet provides support for abstract representations of per-
mutation and point groups through the nk.utils.group module, complete with algorithms
to compute irreducible representations [76–78]. The module also contains a library of two-
and three-dimensional point groups, which can be turned into lattice-site permutation groups
using the graph class nk.graph.Lattice (but not general Graph objects, for they carry no
geometric information about the system):
(cid:7)

1 from netket.utils.group.planar import D
2 from netket.graph import Lattice
3
4 # construct a centred rectangular lattice
5 lattice = Lattice(
6

7

8

basis_vectors = [[2,0], [0,1]], # each row is a lattice vector
extent = (5,5),
site_offsets = [[0,0], [1,0.5]], # each row is the position of a site in
the unit cell
point_group = D(2) # the point group of the lattice, here Z_2 x Z_2

9
10 )
(cid:6)
NetKet contains specialized constructors for some lattices (e.g., Square or Pyrochlore ),
which come with a default point group; however, these can be overridden in methods like
Lattice.space_group :
(cid:7)

(cid:4)

(cid:5)

(cid:4)

(cid:5)

(cid:4)

1 from netket.utils.group.planar import rotation, reflection_group, D
2 from netket.utils.group import PointGroup, Identity
3 from netket.graph import Honeycomb
4
5 # construct the D_6 point group of the honeycomb lattice by hand
6 cyclic_6 = PointGroup(
7

[Identity()] + [rotation(360 / n * i) for i in range(1, n)],
ndim=2,

8
9 )
10 # the @ operator returns the Cartesian product of groups

33

SciPost Physics Codebases

Submission

11 # but doesn’t check for group structure
12 dihedral_6 = reflection_group(angle=0) @ cyclic_6
13
14 assert dihedral_6 == D(6)
15
16 lattice = Honeycomb([6,6])
17
18 # returns the full space group of ‘lattice‘ as a PermutationGroup
19 space_group = lattice.space_group()
20 # the space group is spanned by 6ˆ2 translations and 12 point-group symmetries
21 assert len(space_group) == 12 * 6 * 6
22
23 # do this if the Hamiltonian breaks reflection symmetry
24 # can also be used for generic Lattices that have no default point group
25 space_group = lattice.space_group(cyclic_6)
(cid:6)

Irreducible representation (irrep) matrices can be computed for any point or permutation
group object using the method irrep_matrices() . Characters (the traces of these matrices)
are returned by the method character_table() as a matrix, each row of which lists the
characters of all group elements. Character tables closer to the format familiar from quantum
chemistry texts are produced by character_table_readable() . Irrep matrices and charac-
ter tables are calculated using adaptations of Dixon’s [76] and Burnside’s [77] algorithms,
respectively.

It would, however, be impractical to inspect irreps of a large space group directly to
specify the symmetry sector on which to project a GCNN wave function. Exploiting the
semidirect-product structure of space groups [78], space-group irreps are usually19 described
in terms of a set of symmetry-related wave vectors (known as a star) and an irrep of the
subgroup of the point group that leaves the same invariant (known as the little group) [75].
Irreps can be constructed in this paradigm using SpaceGroupBuilder objects returned by
Lattice.space_group_builder() :
(cid:7)

1 from netket.graph import Triangular
2
3 lattice = Triangular([6,6])
4 momentum = [0,0]
5 # space_group_builder() takes an optional PointGroup argument
6 sgb = lattice.space_group_builder()
7
8 # choosing a representation
9 # this one corresponds to the B_2 irrep at the Gamma point
10 chi = sgb.space_group_irreps(momentum)[3]
(cid:6)
The irrep chi , generated using the little group, is equivalent to one of the irreps in the table
Lattice.space_group().character_table() and can thus be used for symmetry-projecting
GCNN ans¨atze. The order in which irreps of the little group are returned can readily be
checked in an interactive session:
(cid:7)

(cid:5)

(cid:4)

(cid:5)

(cid:4)

19Representation theory for wave vectors on the surface of the Brillouin zone in a nonsymmorphic space

group is much more complicated [78] and is not currently implemented in NetKet.

34

SciPost Physics Codebases

Submission

1 >>> sgb.little_group(momentum).character_table_readable()
2 ([’1xId()’, ’2xRot(60)’, ’2xRot(120)’, ’1xRot(180)’, ’3xRefl(0)’,

’3xRefl(-30)’],

5

3 array([[ 1., 1., 1.,
[ 1.,
1., 1.,
4
[ 1., -1., 1., -1.,
[ 1., -1., 1., -1., -1.,
0.,
[ 2.,
0.,
[ 2., -1., -1.,

1.],
1.,
1.,
1., -1., -1.],
1., -1.],
1.],
0.],
0.]]))

1., -1., -2.,
2.,

6

7

8
(cid:6)

# this is irrep A1
# A2
# B1
# B2
# E1
# E2

(cid:5)

The main caveat in using this machinery is that the point groups predeﬁned in NetKet
all leave the origin invariant (except for cubic.Fd3m which represents the “nonsymmorphic
point group” of the diamond/pyrochlore lattice) and thus only work well with lattices in which
the origin has full point-group symmetry. This behaviour can be changed (see the deﬁnition
of cubic.Fd3m for an example), but it is generally safer to deﬁne lattices using the proper
Wyckoﬀ positions [75], of which the origin is usually maximally symmetric.

5.2 Using group convolutional neural networks (GCNNs)

NetKet uses GCNNs [49, 73] to create NQS ans¨atze that are symmetric under space groups
of lattices. These networks consist of alternating group convolutional layers and pointwise
nonlinearities. The former can be thought of as a generalization of convolutional layers to
a generic ﬁnite group G. They are equivariant, that is, if their inputs are transformed by
some space-group symmetry, features in all subsequent layers are transformed accordingly.
As a result, the output of a GCNN can be understood as amplitudes of the wave functions
g|ψi for all g ∈ G, which can be combined into a symmetric wave function using the projec-
tion operator (40). Further details about equivariance and group convolutions are given in
Appendix A.1.

GCNNs are constructed by the function nk.models.GCNN . Symmetries are speciﬁed either
as a PermutationGroup or a Lattice . In the latter case, the symmetry group is given by
space_group() ; an optional point_group argument to GCNN can be used to override the
default point group. By default, output transforms under the trivial irrep χg ≡ 1, that is, all
output features are averaged together to obtain a wave function that is fully symmetric under
the whole space group. Other irreps can be speciﬁed through the characters argument,
which takes a vector of the same size as the space group.
(cid:7)

(cid:4)

1 from netket.graph import Triangular
2 from netket.models import GCNN
3 from netket.utils.group.planar import C
4
5 lattice = Triangular([6,6])
6 momentum = [0,0]
7 sgb = lattice.space_group_builder()
8 chi = sgb.space_group_irreps(momentum)[3]
9
10 # This transforms as the trivial irrep Gamma A_1
11 gcnn1 = GCNN(lattice, layers = 4, features=4)
12
13 # This transforms as Gamma B_2
14 gcnn2 = GCNN(lattice.space_group(), characters=chi, layers=4, features=4)

35

SciPost Physics Codebases

Submission

Can be used for

any group

only space groups

mode="irreps"

mode="fft"

Symmetries can be speciﬁed
by

Kernel memory footprint
per layer

Evaluation time per layer
per sample

Preferable for

• Lattice
• PermutationGroup
• Symmetry permutations
and irrep matrices

and

• Lattice
•
PermutationGroup
shape of translation group
• Symmetry permutations,
product table, and shape of
translation group

O(finfout|G|)

O(finfout|G||P |)

P

O[(fin + fout)|G|2 +
]
a d3
finfout
a
• large point groups
• if expanded "fft" ker-
nels don’t ﬁt in memory

O[(fin + fout)|G| log |T | +
finfout|G||P |]
• small point groups
• very large batches (see
App. A.2)

Table 4: Comparison of GCNN implementations. fin,out stands for the number of
input and output features, |G|, |P |, |T | for the sizes of the space group, point group,
and translation group, respectively. da are the dimensions of irreps of G; in a large
space group, P

scales as |G||P |.

a d3
a

15
16 # This does not enforce reflection symmetry
17 gcnn3 = GCNN(lattice, point_group=C(6), layers=4, features=4)
(cid:6)

(cid:5)

NetKet currently supports two implementations of GCNNs, one based on group Fourier
transforms ( mode="irreps" ), the other using fast Fourier transforms on each coset of the
translation group ( mode="fft" ): these are discussed in more detail in Appendix A.2. Their
behavior is equivalent, but their performance and calling sequence is diﬀerent, as explained
in Table 4. A default mode="auto" is also available. For spin models, parity symmetry
(taking σz to −σz) is a useful extension of the U(1) spin symmetry group enforced by ﬁxing
magnetization along the σz axis. Parity-enforcing GCNNs can be constructed using the
parity argument, which can be set to ±1.

In addition to deep GCNNs, fully symmetric RBMs [8] are implemented in nk.models.

RBMSymm as a single-layer GCNN from which the wave function is computed as

ψ = Y
i,g∈G

cosh f (i)

g

=⇒ log ψ = X
i,g∈G

ln cosh f (i)
g .

(41)

Due to the products (rather than sums) used, this ansatz only supports wave functions that
transform under the trivial irrep. An RBM-like structure closer to that of ref. [72] can be
achieved using a single-layer GCNN:
(cid:7)

(cid:4)

1 from netket.models import GCNN, RBMSymm
2 from netket.nn import logcosh

36

SciPost Physics Codebases

Submission

3
4 # fully symmetric RBM
5 rbm1 = RBMSymm(group, alpha=4)
6
7 # symmetrized RBM similar to (Nomura, 2021)
8 rbm2 = GCNN(group, layers=1, features=4, output_activation=logcosh)

(cid:6)

6 Quantum systems with continuous degrees of freedom

In this section we will introduce the tools provided by NetKet to study systems with con-
tinuous degrees of freedom. The interface is very similar to the one introduced in Section 2
for discrete degrees of freedom.

6.1 Continuous Hilbert spaces

Similar to the discrete Hilbert spaces, the bosonic Hilbert space of N particles in continuous
space has the structure

Hcontinuous = span{|x0i ⊗ · · · ⊗ |xN −1i : xi ∈ Li, i ∈ {0, . . . , N − 1}}

(42)

where Li is the space available to each individual boson:
for example, Li is Rd for a free
particle in d spatial dimensions, and [0, L]d for particles conﬁned to a d-dimensional box of
side length L.
In the case of ﬁnite simulation cells, the boundaries can be equipped with
periodic boundary conditions.

In the following snippet, we deﬁne the Hilbert space of ﬁve bosons in two spatial dimen-

sions, conﬁned to [0, 10]2 with periodic boundary conditions:
(cid:7)

1 >>> hilb = nk.hilbert.Particle(N=5, L=(10.0, 10.0), pbc=True)
2 >>> print("Size of the basis: ", hilb.size)
3 Size of the basis: 10
4 >>> hilb.random_state(nk.jax.PRNGKey(0), (2,))
5 [[0.02952452 0.21660899 2.836163
9.163713
8.14864
6
0.4667458 2.211265
8.508119

3.5628846
9.263418
4.1587596
0.08060825]]

6.5165453 7.3764215

4.5622005

6.104126

4.250165

[9.85617

]

7

5.9473248

6.69916

8
(cid:6)
As we discussed in Section 2.1, the Hilbert objects only deﬁne the computational basis. For
that reason, the ﬂag pbc=True only aﬀects what conﬁgurations can be generated by samplers
and how to compute the distance between two diﬀerent sets of positions. This option does
not enforce any boundary condition for the wave-function, which would have to be accounted
for into the variational ansatz.

(cid:5)

(cid:4)

(cid:5)

6.2 Linear operators

Similar to the discrete-variable case, expectation values of operators can be estimated as
classical averages of the local estimator

˜O(x) = hx| ˆO |ψi

hx|ψi

37

(43)

SciPost Physics Codebases

Submission

over the (continuous) Born distribution p(x) ∝ |ψ(x)|2. NetKet provides the base class
ContinuousOperator to write custom (local) operators and readily implements Hamiltonians
of the form ((cid:126) = 1 in our units)

ˆH = −

1
2

X

i

1

mi

∇2
i

+ ˆV (cid:0){xi}(cid:1)

(44)

(cid:4)

(cid:5)

using the predeﬁned operators KineticEnergy and PotentialEnergy . For example, a har-
monically conﬁned system described by ˆH = − 1
(cid:7)
2

can be implemented as

i ∇2
i

+ 1
2

i ˆx2
i

P

P

1 # This function takes a single vector and returns a scalar
2 def v(x):
3

return 0.5*jnp.linalg.norm(x) ** 2

4
5 # Construct the Kinetic energy term with unit mass
6 H_kin = nk.operator.KineticEnergy(hilb, mass=1.0)
7 # Construct the Potential energy term using the potential defined above
8 H_pot = nk.operator.PotentialEnergy(hilb, v)
9
10 # Sum the two objects into a single Operator
11 H = H_kin + H_pot
(cid:6)

Operators deﬁned on continuous Hilbert spaces cannot be converted to a matrix form
or used in exact diagonalization, in contrast to those deﬁned on discrete Hilbert spaces.
Continuous operators can still be used to compute expectation values and their gradients
with a variational state.

6.3 Samplers

Out of the built-in samplers in the current version of NetKet (Section 3.4), only the Markov
chain Monte Carlo sampler MetropolisSampler supports continuous degrees of freedom, as
both ExactSampler and the autoregressive ARNNSampler rely on the sampled basis being
countable. For continuous spaces, we provide the transition rule sampler.rules.GaussianRule
which proposes new states by adding a random shift to every degree of freedom sampled from
a Gaussian of customizable width. More complex transition rules can be deﬁned following the
instructions provided in Section 3.4.

6.4 Harmonic oscillators

As a complete example of how to use continuous-space Hilbert spaces, operators, variational
states, and the VMC driver together, consider 10 particles in three-dimensional space, conﬁned
by a harmonic potential V (x) = x2/2. The exact ground-state energy of this system is known
to be E0 = 15. We use the multivariate Gaussian ansatz log ψ(x) = −xTΣ−1x, where Σ = T T (cid:124)
and T is randomly initialized using a Gaussian with zero mean and variance one. Note that
the form of Σ ensures that it is positive deﬁnite.
(cid:7)

(cid:4)

1 import netket as nk
2 import jax.numpy as jnp
3
4 def v(x):

38

SciPost Physics Codebases

Submission

Figure 1: VMC energy estimate as a function of the optimization step for a
continuous-space system of N = 10 particles in d = 3 spatial dimensions subject
to a harmonic conﬁnement.

5

return 0.5*jnp.linalg.norm(x) ** 2

6
7 hilb = nk.hilbert.Particle(N=10, L=(jnp.inf,jnp.inf,jnp.inf), pbc=False)
8 ekin = nk.operator.KineticEnergy(hilb, mass=1.0)
9 pot = nk.operator.PotentialEnergy(hilb, v)
10 ha = ekin + pot
11
12 sa = nk.sampler.MetropolisGaussian(hilb, sigma=0.1, n_chains=16, n_sweeps=32)
13 model = nk.models.Gaussian(param_dtype=float)
14 vs = nk.vqs.MCState(sa, model, n_samples=10 ** 4, n_discard=2000)
15
16 op = nk.optimizer.Sgd(0.05)
17 sr = nk.optimizer.SR(diag_shift=0.01)
18
19 gs = nk.VMC(ha, op, sa, variational_state=vs, preconditioner=sr)
20 gs.run(n_iter=100, out="HO_10_3d")
(cid:6)
We show the training curve of above snippet in Fig. 1; exact ground-state energy is recovered
to a very high accuracy.

(cid:5)

6.5 Interacting system with continuous degrees of freedom

In this example we want to tackle an interacting system of bosonic Helium particles in one
continuous spatial dimension. The two-body interaction is given by the Aziz potential which
qualitatively resembles a Lennard-Jones potential [79–81]. The Hamiltonian reads

H = −

(cid:126)2
2m

X

∇2
i

i

+ X
i<j

VAziz(rij)

(45)

39

SciPost Physics Codebases

Submission

= 0.3˚A−1 with N = 10 particles in units
We will examine the system at a density ρ = N
L
where (cid:126) = m = kb = 1. To conﬁne the system it is placed in a box of size L equipped with
periodic boundary conditions. The Hilbert space and sampler are initialized as shown above
(rm is the length-scaled deﬁned in the Aziz potential):
(cid:7)

(cid:4)

1 import netket as nk
2
3 N = 10
4 d = 0.3 # 1/Angstrom
5 rm = 2.9673 # Angstrom
6 L = N / (0.3 * rm)
7 hilb = nk.hilbert.Particle(N=N, L=L, pbc=True)
8 sab = nk.sampler.MetropolisGaussian(hilb, sigma=0.05, n_chains=16, n_sweeps=32)

(cid:6)

(cid:5)

6.5.1 Deﬁning the Hamiltonian

We can deﬁne the Hamiltonian through the action of the interaction-potential on a sample of
positions x, and combine it with the predeﬁned kinetic energy operator. Since we are using
periodic boundary conditions, we will use the Minimum Image Convention (MIC) to compute
distances between particles. In the following snippet the Aziz potential (in the units above)
is deﬁned and the Hamiltonian is instantiated:
(cid:7)

(cid:4)

1
2 def minimum_distance(x, sdim):
3

"""Computes distances between particles using mimimum image convention"""
n_particles = x.shape[0] // sdim
x = x.reshape(-1, sdim)

4

5

6

7

8

9

10

11

12

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

distances = (-x[jnp.newaxis, :, :] + x[:, jnp.newaxis, :])[

jnp.triu_indices(n_particles, 1)

]
distances = jnp.remainder(distances + L / 2.0, L) - L / 2.0

return jnp.linalg.norm(distances, axis=1)

13
14 def potential(x, sdim):
15

"""Compute Aziz potential for single sample x"""
eps = 7.846373
A = 0.544850 * 10 ** 6
alpha = 13.353384
c6 = 1.37332412
c8 = 0.4253785
c10 = 0.178100
D = 1.241314

dis = minimum_distance(x, sdim)
return jnp.sum(

eps
* (

A * jnp.exp(-alpha * dis)
- (c6 / dis ** 6 + c8 / dis ** 8 + c10 / dis ** 10)
* jnp.where(dis < D, jnp.exp(-((D / dis - 1) ** 2)), 1.0)

40

SciPost Physics Codebases

Submission

31

32

)

)

33
34 ekin = nk.operator.KineticEnergy(hilb, mass=1.0)
35 pot = nk.operator.PotentialEnergy(hilb, lambda x: potential(x, 1))
36 ha = ekin + pot
(cid:6)

(cid:5)

6.5.2 Deﬁning and training the variational Ansatz

There are two properties that the variational Ansatz for this system must obey:

1. It must be invariant with respect to the permutations of its particles, because they are

bosons;

2. As the interaction resembles a Lennard-Jones potential we have a strong divergence in
the potential energy when particles get close to each other. This divergence must be
compensated by the kinetic energy.

We satisfy the permutation-invariance by using a neural network architecture called DeepSets.
DeepSets exploit the fact that any function f (x1, ..., xN ) which is invariant under permutations
of its inputs can be decomposed as [82]:

f (x1, ..., xN ) = ρ

X

φ(xi)

!

i

(46)

where ρ and φ are arbitrary functions. In this speciﬁc example, xi denotes a single-particle
position and ρ and φ are parameterized with dense feed forward neural networks.

The second requirement is fulﬁlled by using Kato’s cusp condition which states that [83]

lim
r→0

∇2ψc(r)
ψc(r)

!

+ V (r)

< ∞

(47)

where r denotes the distance between the particles and ψc is the cusp wave-function. For the
case of a Lennard-Jones potential (∝ r−12-divergence), we have

"

ψc(r) = exp

−

(cid:19)5#

,

1
2

(cid:18) b
r

(48)

where b is a variational parameter.

We also need to handle the periodic conditions, making sure that the wave-function does
not exhibit divergent behaviour at the edges of the (periodic) box. To this end we will
use a surrogate distance function for the minimum image convention, namely dsin(xi, xj) =
(cid:1) in the variational Ansatz. Additionally we replace the single-particle coordinates
L
2
in Eq. (46) by the two-particle distances dsin(xi, xj)2, such that all in all our variational Ansatz
reads

sin(cid:0) π

L rij

ψ(x1, ..., xN ) = exp


ρ





X

i<j

φ(dsin(xi, xj)2)






 · exp


−

1
2

b
dsin(xi, xj)

!5


(49)

41

 
 
 
SciPost Physics Codebases

Submission

Figure 2: VMC energy estimate as a function of the optimization step for a
continuous-space system of N = 10 particles in d = 1 spatial dimensions subject
to a LJ-like interaction potential placed within a periodic box. The green dashed
line is the result given in the supplementary material of [80].

The variational ansatz we described here is implemented in NetKet as DeepSeetRelDistance ,
and a more in-depth discussion can be found in this reference [84].

Having deﬁned the ansatz, we run the VMC driver with the given variatianal Ansatz to

ﬁnd an estimation of the ground-state energy of the system. This is done as follows:
(cid:7)

(cid:4)

1 model = nk.models.DeepSetRelDistance(
2

3

4

5

6

hilbert=hilb,
cusp_exponent=5,
layers_phi=2,
layers_rho=3,
features_phi=(16, 16),
features_rho=(16, 16, 1),

7
8 )
9 vs = nk.vqs.MCState(sab, model, n_samples=4096, n_discard_per_chain=128)
10
11 op = nk.optimizer.Sgd(0.001)
12 sr = nk.optimizer.SR(diag_shift=0.01)
13
14 gs = nk.VMC(ha, op, sab, variational_state=vs, preconditioner=sr)
15 gs.run(n_iter=1000, out="Helium_10_1d")
(cid:6)
The result of this optimization and a comparison to literature results is displayed in Fig. 2.

(cid:5)

42

SciPost Physics Codebases

Submission

7 Example: Finding ground and excited states of a lattice

model

In this example, we deﬁne the J1–J2 Heisenberg Hamiltonian

H = J1

X

hiji

~σi · ~σj + J2

X

hhijii

~σi · ~σj,

(50)

on a 10×10 square lattice and use the VMC code introduced in Section 4.1 to ﬁnd a variational
approximation of its ground state. This model gives rise to several phases of matter, including
magnetically ordered states, a valence bond solid, and a quantum spin liquid. Here, we set
J1 = 1, J2 = 0.5, inside the spin liquid phase [30, 85].

Our example is optimized to run on a single GPU with 16 GB of memory. We will make

note of what should be changed when running the simulation on CPUs.

7.1 Deﬁning the lattice and the Hamiltonian

We use the Lattice class to deﬁne the square lattice and generate its space-group sym-
metries. By passing max_neighbor_order=2 to the constructor, we generate graph edges
for both nearest and next-nearest neighbours. The pre-deﬁned Heisenberg class supports
passing diﬀerent coupling constants for both types of edge.
(cid:7)

(cid:4)

1 import netket as nk
2 import numpy as np
3 import json
4 from math import pi
5
6 L = 10
7 # Build square lattice with nearest and next-nearest neighbor edges
8 lattice = nk.graph.Square(L, max_neighbor_order=2)
9 hi = nk.hilbert.Spin(s=1 / 2, total_sz=0, N=lattice.n_nodes)
10 # Heisenberg with coupling J=1.0 for nearest neighbors
11 # and J=0.5 for next-nearest neighbors
12 H = nk.operator.Heisenberg(hilbert=hi, graph=lattice, J=[1.0, 0.5])
(cid:6)

(cid:5)

7.2 Deﬁning and training a symmetric ansatz

To enforce all spatial symmetries of (50), we use the GCNN ansatz described in Section 5. By
default, the GCNN projects onto the symmetric representation, which contains the ground
state for this geometry. We select singlet states by only sampling basis states with P
= 0
and setting spin parity using parity=1 . We use a model with four layers, each contain-
ing four feature vectors (i.e., four hidden units for each of the 8L2 space-group symme-
tries). To exploit the high degree of parallelism of GPUs, we set sampler.n_chains equal to
vstate.n_samples 20. When using CPUs, n_chains should be set to a smaller value.

i Sz
i

20Note that this results in a somewhat non-standard MC scheme where, instead of an ensemble of chains
with generally non-zero internal autocorrelation, the sampler produces an ensemble of independently drawn

43

SciPost Physics Codebases

Submission

(cid:7)

1 machine = GCNN(
2

symmetries=lattice,
parity=1,
layers=4,
features=4,
param_dtype=np.complex128,

6
7 )
8 sampler = nk.sampler.MetropolisExchange(
9

hilbert=hi,
n_chains=1024,
graph=lattice,
d_max=2,

12
13 )
14 opt = nk.optimizer.Sgd(learning_rate=0.02)
15 sr = nk.optimizer.SR(diag_shift=0.01)
16 vstate = nk.vqs.MCState(
17

sampler=sampler,
model=machine,
n_samples=1024,
n_discard_per_chain=0,
chunk_size=4096,

3

4

5

10

11

18

19

20

21
22 )
23 gs = nk.VMC(H, opt, variational_state=vstate, preconditioner=sr)
24 gs.run(n_iter=200, out="ground_state")
25
26 data = json.load(open("ground_state.log"))
27 print(np.mean(data["Energy"]["Mean"]["real"][-20:])/400)
28 # Output: -0.49562531096409457
29 print(np.std(data["Energy"]["Mean"]["real"][-20:])/400)
30 # Output: 0.0002492304032505182
(cid:6)

(cid:4)

(cid:5)

We specify chunk_size=4096 in the variational state in order to reduce memory consump-
tion. As we have L2 = 100 sites, at every VMC step we will need to evaluate the network
for O(NsamplesL2) = O(103 · 102) diﬀerent conﬁgurations, but the memory available on com-
mercial GPUs will not be enough to perform this computation in a single pass. Instead, by
setting chunk_size NetKet will split the calculation in many smaller sub-calculations (see
Section 3.3.1 for more details).

This calculation, which takes about 30 minutes on an NVIDIA A100 GPU, already delivers
a fairly accurate variational energy. The evolution of variational energy during the training
procedure is shown in Fig. 3.

We note that a typical initialization of a GCNN gives rise to ferromagnetic correlations,
which can make training an antiferromagnetic Hamiltonian unstable [15, 49]. Therefore, it
is often good practice to pre-optimize the phases by restricting all amplitudes to unity by
setting equal_amplitudes=True switch and training only the phases of the network. These
parameters can then be loaded into a model with equal_amplitudes=False .

single conﬁgurations ( n_samples_per_chain==1 ). Since the sampler state of the previous VMC step is used as
initial state for the next step, there can still be a residual autocorrelation with the previous samples, which is,
however, alleviated by the sampler performing Nsites intermediate updates before yielding the single requested

44

SciPost Physics Codebases

Submission

Figure 3: Energy evolution of variational ground states (green) and excited states
after transfer learning (blue), compared to the lowest known variational energy for
the 10 × 10 square-lattice J1–J2 model [30] (black). The variational energies can be
further improved by allowing more training steps, Monte Carlo samples, etc. The
plot on the right zooms in on the lowest energies.

7.3 Finding an excited state

We can also ﬁnd low-lying excited states using this procedure, by projecting the wavefunction
onto a diﬀerent irrep. Here, we consider the ﬁrst gapless mode in the Anderson tower of states
of the N´eel antiferromagnet [86], a triplet at wave vector (π, π) that transforms trivially under
all point-group symmetries. This mode is still gapless in the quantum spin liquid; we project
out spin-singlets by focusing on parity odd states.

We expedite this calculation by using parameters optimized for the ground-state sector as
an initial guess. The resulting wave function will already have a low variational energy (as
shown in Fig. 3) and correlations typical for low-energy eigenstates.
(cid:7)

(cid:4)

1 # store the optimized ground-state parameters
2 saved_params = vstate.parameters
3 # Compute the characters of the first excited state
4 characters = lattice.space_group_builder().space_group_irreps(pi, pi)[0]
5 # Construct a model respecting the first-excited state symmetries
6 machine = GCNN(
7

symmetries=lattice,
characters=characters,
parity=-1,
layers=4,
features=4,
param_dtype=complex,

12
13 )
14 vstate = nk.vqs.MCState(
15

sampler=sampler,
model=machine,
n_samples=1024,
n_discard_per_chain=0,

8

9

10

11

16

17

18

sample with the settings used here.

45

SciPost Physics Codebases

Submission

chunk_size=4096,

19
20 )
21 # assign the old parameters to the new variational state
22 vstate.parameters = saved_params
23 gs = nk.VMC(H, opt, variational_state=vstate, preconditioner=sr)
24 gs.run(n_iter=50, out=’excited_state’)
25
26 data = json.load(open("excited_state.log"))
27 print(np.mean(data["Energy"]["Mean"]["real"][-10:])/400)
28 # Output: -0.49301426054097885
29 print(np.std(data["Energy"]["Mean"]["real"][-10:])/400)
30 # Output: 0.0003802670752071611
(cid:6)

8 Example: Fermions on a lattice

NetKet can also be used to simulate fermionic systems with a ﬁnite number of orbitals.
Functionality related to fermions is kept in the netket.experimental in order to signal that
some parts of the API might still slightly change while we gather feedback from the community.
We usually import this namespace as nkx as follows:
(cid:7)

1 from netket import experimental as nkx

(cid:6)
and then use nkx freely.

The Hilbert space for discrete fermionic systems is called SpinOrbitalFermions . It sup-
ports fermions, with and without a spin- degree of freedom, which occupy a set of orbitals
(such as the sites of a lattice). Internally, it uses a tensor product of a Fock space for each
spin component. For a set of spin-1/2 fermions, we can ﬁx the number of fermions with up (↑)
and down (↓) spins through the n_fermions keyword argument. The SpinOrbitalFermions
generates samples that correspond to occupation numbers |ni = |n1,↑, ..., nNo,↑, n1,↓, ..., nNo,↓i,
for a given ordering of the No orbitals (or sites).

In the example below, we determine the ground state of the Fermi-Hubbard model on a

square lattice

where ni,σ = f †

i,σfi,σ.

ˆH = −t X
hi,ji

X

i,σfj,σ + h.c. + U X
f †

ni,↑ni,↓

σ={↑,↓}

i

(51)

NetKet implements a class FermionOperator2nd that represents an operator in sec-
Internally, the
on an orbital i

ond quantization. This class does not separate spin and orbital indices.
FermionOperator2nd computes matrix elements of a fermion operator f †
i
through the Jordan-Wigner transformation

(cid:5)

(cid:4)

(cid:5)





f †
i →

O



Zj



j<i

(cid:18) Xi + iYi
2

(cid:19)

or in terms of occupation numbers

(cid:10)n(cid:12)

(cid:12)f †
i

(cid:12)
(cid:12)n0(cid:11) = (−1)

P

j<i

nj δn0

i+1,ni

Y

j6=i

δnj ,n0

j

46

(52)

(53)

SciPost Physics Codebases

Submission

(cid:4)

(cid:5)

One can create a FermionOperator2nd object from an external FermionOperator object
from the OpenFermion library, which is popular for symbolic manipulation of fermionic
operators [87]. The small intermezzo code below shows how this works in practice for an
operator f †
(cid:7)

1 f2 + 4f3f †

0

1 from openfermion import FermionOperator
2 from netket.operator import FermionOperator2nd
3
4 of_operator = FermionOperator("1ˆ 2") + 4*FermionOperator("3 0ˆ")
5 nk_operator1 = FermionOperator2nd.from_openfermion(of_operator)
6
7 nk_operator2 = FermionOperator2nd("1ˆ 2") + 4*FermionOperator2nd("3 0ˆ")

(cid:6)
where nk_operator1 and nk_operator2 are equivalent.

The mapping between fermions and qubit degrees of freedom is not unique [88], and the
Jordan-Wigner transformation is one well-know example of such a transformation. However,
by interfacing with toolboxes specialized in symbolic manipulation, we open up a range of
possibilities, especially in combination with PauliStrings.from_openfermion , which converts
openfermion.QubitOperator from OpenFermion to a PauliStrings object in NetKet.
This allows one to, for example, use a wider range of alternatives to the Jordan-Wigner
transformation implemented in OpenFermion, or other variations.

Going back to our example of the Fermi-Hubbard model, NetKet also implements a more
easy to use set of creation and annihilation operators that clearly separate the orbitals and
spin indices

• f †
i,σ

: nkx.operator.fermion.create

• fi,σ: nkx.operator.fermion.destroy

• ni,σ: nkx.operator.fermion.number

Each operator takes a site and spin projection ( sz ) in order to ﬁnd the right position in the
Hilbert space samples. We will create a helper function to abbreviate the creation, annihilation
and number operators in the example below.
(cid:7)

(cid:4)

1 from netket import experimental as nkx
2 from netket.experimental.operator.fermion import (
3

create as c, destroy as cdag, number as nc)

4
5 # create the graph our fermions can hop on
6 L = 4
7 g = nk.graph.Hypercube(length=L, n_dim=2, pbc=True)
8 n_sites = g.n_nodes
9
10 # create a hilbert space with 2 up and 2 down spins
11 hi = nkx.hilbert.SpinOrbitalFermions(n_sites, s=1 / 2, n_fermions=(2, 2))
12
13 t = 1
14 U = 0.01
15

# tunneling/hopping
# coulomb

47

SciPost Physics Codebases

Submission

16 up, down = +0.5, -0.5
17 ham = 0.0
18 for sz in (up, down):
19

for u, v in g.edges():

20

ham += -t * (cdag(hi, u, sz) * c(hi, v, sz) +

21
22 for u in g.nodes():
23
(cid:6)

ham += U * nc(hi, u, up) * nc(hi, u, down)

cdag(hi, v, sz) * c(hi, u, sz))

(cid:5)

Sampling: To run a VMC optimization, we need a proper sampling algorithm that takes
into account the constraints of the computational basis we are working with. As the SpinOrbitalFermions
basis consereves total spin-magnetization, we cannot use samplers like MetropolisLocal
which randomly change the population ni,σ on a site, thus changing the total spin. We can
instead use MetropolisExchange , which moves fermions around according to the physical
lattice graph of L × L vertices, but the computational basis deﬁned by the Hilbert space
contains (2s + 1)L2 occupation numbers. By taking a disjoint copy of the lattice, we can
move the fermions around independently for both spins and therefore conserve the number of
fermions with up and down spin. Notice that in the chosen representation, where is no need
to anti-symmetrize our ansatz.
(cid:7)

(cid:4)

1 sa = nk.sampler.MetropolisExchange(hi,
2

graph=nk.graph.disjoint_union(g, g),
n_chains=16)

3

4
5 ma = nk.models.RBM(alpha=1, param_dtype=complex)
6 vs = nk.vqs.MCState(sa, ma, n_discard_per_chain=100, n_samples=512)
7
8 opt = nk.optimizer.Sgd(learning_rate=0.01)
9 sr = nk.optimizer.SR(diag_shift=0.1)
10
11 gs = nk.driver.VMC(ham, opt, variational_state=vs, preconditioner=sr)
12 gs.run(500, out=’fermions’)
(cid:6)

(cid:5)

9 Example: Real-time dynamics

We demonstrate the simulation of NQS dynamics in the transverse-ﬁeld Ising model (TFIM)
on an L site chain with periodic boundaries, using a restricted Boltzmann machine (RBM)
as the NQS ansatz. The Hamiltonian reads

ˆHIsing = J X

ˆσz
i

j − h X
ˆσz

ˆσx
i

hiji

i

(54)

with J = 1 and h = 1 and periodic boundary conditions. We will estimate the expectation
value of the transverse magnetization ˆSx = P
i

ˆσx
i
We simulate the dynamics starting from an initial state |ψ(t0)i that is the ground state for
the TFIM Hamiltonian with h = 1/2. The random weight initialization of a neural network
yields a random initial state. Therefore, we determine the weights corresponding to this initial

along the way.

48

SciPost Physics Codebases

Submission

state by performing a ground-state optimization. Even though the TFIM ground state can be
parametrized using an RBM ansatz with real-valued weights, we need to use complex-valued
weights here, in order to describe the complex-phase of the wave function that arises during
the time evolution21. In this example, we work with a chain of L = 20 sites, which can be
easily simulated on a typical laptop with the parameters below.
(cid:7)

(cid:4)

1 import netket as nk
2 import netket.experimental as nkx
3
4 # Spin Hilbert space on 20-site chain with PBC
5 L = 20
6 chain = nk.graph.Chain(L)
7 hi = nk.hilbert.Spin(s=1/2) ** L
8
9 # Define RBM ansatz and variational state
10 rbm = nk.models.RBM(alpha=1, param_dtype=complex)
11 sampler = nk.sampler.MetropolisLocal(hi)
12 vstate = nk.vqs.MCState(sampler, rbm, n_samples=4096)
13
14 # Hamiltonian at J=1 (default) and external field h=1/2
15 ha0 = nk.operator.Ising(hi, chain, h=0.5)
16 # Observable (transverse magnetization)
17 obs = {"sx": sum(nk.operator.spin.sigmax(hi, i) for i in range(L))}
18
19 # First, find the ground state of ha0 to use it as initial state
20 optimizer = nk.optimizer.Sgd(0.01)
21 sr = nk.optimizer.SR()
22 vmc = nk.VMC(ha0, optimizer, variational_state=vstate, preconditioner=sr)
23 # We run VMC with SR for 300 steps
24 vmc.run(300, out="ising1d_groundstate_log", obs=obs)
(cid:6)

(cid:5)

9.1 Unitary dynamics

Starting from the ground state, we can compute the dynamics after a quench of the transverse
ﬁeld strength to h = 1. We use the second-order Heun scheme for time stepping, with a step
size of dt = 0.01, and explicitly specify a QGT implementation (compare Sec. 3.5) in order to
make use of the more eﬃcient code path for holomorphic models.
(cid:7)

(cid:4)

1 # Quenched Hamiltonian
2 ha1 = nk.operator.Ising(hi, chain, h=1.0)
3 # Heun integrator configuration
4 integrator = nkx.dynamics.Heun(dt=0.001)
5 # QGT options
6 qgt = nk.optimizer.qgt.QGTJacobianDense(holomorphic=True)
7 # Creating the time-evolution driver
8 te = nkx.TDVP(ha1, vstate, integrator, qgt=qgt)
9 # Run the t-VMC solver until time T=1.0
10 te.run(T=1.0, out="ising1d_quench_log", obs=obs)
(cid:6)

(cid:5)

21It is possible to ﬁrst use a real-weight RBM for the initial state preparation and then switch to complex-
valued weights for the dynamics. For the sake of simplicity, we leave out this extra step in the present example.

49

SciPost Physics Codebases

Submission

Figure 4: Comparison between the exact (dashed line) and variational dynamics
of a quench on the transverse-ﬁeld Ising model. (Left): Expectation value of the
quenched Hamiltonian, which is conserved by the unitary dynamics. The shaded
area represents the uncertainty due to Monte Carlo sampling. (Right): Expectation
value of the total magnetization along the x axis during the evolution.

In Fig. 5 we show the results of this calculation, comparing against an exact solution

computed using QuTiP [89, 90].

9.2 Dissipative dynamics (Lindblad master equation)

In Section 4.3 we have shown that the time-dependent variational principle can also be used to
study the dissipative dynamics of an open quantum system. In this section we give a concrete
example, studying the transverse-ﬁeld Ising model coupled to a zero-temperature bath. The
coupling is modeled through the spin depolarization operators ˆσ−
i

acting on every site i.

As the dissipative dynamics converges to the steady state, which is also an attractor of
the non-unitary dynamics, we will use a weak-simulation of the dynamics22 to determine
the steady-state more eﬃciently than using the natural gradient descent scheme proposed in
ref. [24]. This scheme is similar to what was proposed in Ref. [25].

We employ the positive-deﬁnite RBM ansatz proposed in Ref. [51]. A version of that net-
work with complex-valued parameters is provided in NetKet with the name nk.models.NDM .
(cid:7)

(cid:4)

1 # The graph of the Hamiltonian
2 g = nk.graph.Chain(L, pbc=False)
3 # Hilbert space
4 hi = nk.hilbert.Spin(0.5)**g.n_nodes
5 # The Hamiltonian
6 ha = nk.operator.Ising(hi, graph=g, h=-1.3, J=0.5)
7 # Define the list of jump operators
8 j_ops = [nk.operator.spin.sigmam(hi, i) for i in range(g.n_nodes)]
9 # Construct the Liouvillian
10 lind = nk.operator.LocalLiouvillian(ha, j_ops)

22We use weak and strong simulation in the sense of the theory of numerical SDE schemes [91]. This means
that weak integration is an integration which is not accurate at ﬁnite times but which converges to the right
state at long times. A strong integration yields the correct state at every time t.

50

SciPost Physics Codebases

Submission

Figure 5: Comparison between the exact (dashed line) and variational dynamics of
a random initial density matrix evolved according to the Lindblad Master equation.
(Left): Expectation value of the hhρ|L†L|ρii convergence estimator. (Right): Ex-
pectation value of the total magnetization along the ˆx axis during the evolution. We
remark that the evolution is near-exact in the region where the dissipative terms
dominate the dynamics, while there is a sizable error when the unitary dynamics
starts to play a role. The error could be reduced by considering smaller time steps.

11
12 # observable
13 Sx = sum([nk.operator.spin.sigmax(hi, i) for i in range(g.n_nodes)])/g.n_nodes
14
15 # Positive-definite RBM-like ansatz (Torlai et al.)
16 ma = nk.models.NDM(alpha=2, beta=3)
17 # MetropolisLocal sampling on the Choi’s doubled space.
18 sa = nk.sampler.MetropolisLocal(lind.hilbert, n_chains=16)
19 # Mixed Variational State. Use less samples for the observables.
20 vs = nk.vqs.MCMixedState(
21
22 )
23 # Setup the ODE integrator and QGT.
24 integrator = nkx.dynamics.Heun(dt=0.01)
25 # The NDM ansatz is not holomorphic because it uses conjugation
26 qgt = nk.optimizer.qgt.QGTJacobianPyTree(holomorphic=False, diag_shift=1e-3)
27 te = nkx.TDVP(lind, variational_state=vs, integrator=integrator, qgt=qgt)
28
29 # run the simulation and compute observables
30 te.run(T=6.0, obs={"Sx": Sx, "LdagL": lind.H @ lind})
(cid:6)

sa, ma, n_samples=12000, n_samples_diag=1000, n_discard_per_chain=100

In the listing above, we ﬁrst construct the Liouvillian by assembling the Hamiltonian and
the jump operators, then we construct the variational mixed state. We chose a diﬀerent
number of samples for the diagonal, used when sampling the observables, as that happens
on a smaller space with respect to the full system. For the geometric tensor, we choose the
QGTJacobianPyTree and specify that the ansatz is non-holomorphic (while this is already the
default, a warning would be printed otherwise, asking the user to be explicit). The choice
is motivated by the fact that the TDVP driver by default uses an SVD-based solver, which

(cid:5)

51

SciPost Physics Codebases

Submission

works best QGTJacobian -based implementations. However, NDM uses a mix of complex and
real parameters which is not supported by QGTJacobianDense , and would throw an error.
Normally, to simulate a meaningful dynamics you’d want to keep the diagonal shift small, but
since we are striving for a weak simulation a large value helps stabilize the dynamics.

10 Benchmarks

10.1 Variational Monte Carlo

We benchmark NetKet by measuring its performance on the 1D/2D transverse-ﬁeld Ising
model deﬁned as in Eq. (54) with J = 1 and h = 1 and periodic boundary conditions.

We ﬁrst monitor the scaling behavior of NetKet’s VMC implementation by running an
energy optimization consisting of 100 steps. In order to carry out a meaningful benchmark, we
run a ﬁrst VMC step to trigger the JIT-compiler on the relevant jax and Numba functions,
while all reported timings are for the evaluation of JIT compiled functions only. The left
panel of Fig. 6 depicts the scaling behavior of the computational time as a function of the
complexity of the NQS model, using three implementations of the QGT. Hereby, we increase
the complexity of the NQS by optimizing a DNN with an increasing amount of layers, where
depth d represents the number of dense layers (with α = 1), each followed by a sigmoid
activation function. Such a DNN with d layers has O((d − 1)L2 + L) free parameters.

10.2 MPI for NetKet

We benchmark the scaling behavior of NetKet as a function of the computational resources
available to perform parallel computations. Therefore, NetKet uses MPI for jax through
mpi4jax [37]. The eﬀectiveness of the MPI implementation is illustrated in Fig. 7 for a VMC
optimization with and without Stochastic Reconﬁguration (SR). Throughout our analyses,
we provide each MPI rank with 2 CPU cores. We introduce the speedup factor τr = ∆t1/∆tr
where ∆tr refers to the time required to perform the computations on r MPI ranks. Similarly,
we deﬁne τn as the speedup factor when using n nodes. The right panel of Fig. 7 demonstrates
that even on a single node, our MPI implementation can introduce signiﬁcant speedups by
running multiple Markov Chain samplers in parallel. This is consistent with the fact that jax
is not able to make use of multiple CPU cores unless working on very large matrices.

The performance of NetKet on challenging Hamiltonians, as well as its scalability with
both system size and model complexity depends on the implementation details of the quantum
geometric tensor [see Eq. (22)] and its matrix multiplication with the gradient vector, as
discussed in Section 3.5. We therefore isolate these operations and benchmark the QGT
constructor, combined with 1000 matrix multiplications with the gradient vector (where the
latter imitates many steps in the iterative solver). In Fig. 7, we show the scaling behavior
of these operations with respect to both the number of ranks (on a single node), and its
scaling behavior with respect the number of nodes (thereby including communication over
the Inﬁniband network). One can observe that the scaling behavior is close to optimal,
especially when the number of samples per rank is suﬃciently large.

52

SciPost Physics Codebases

Submission

Figure 6: (Left): Benchmark of NetKet’s VMC implementation. Each data point
shows the minimum time spent (out of 5 repetitions) to evolve a DNN with depth
d layers and complex weights over 100 VMC steps for the 1D transverse-ﬁeld Ising
model with L = 256 and Nsamples = 214 = 16384. (Right): Scaling behavior of the
required computational time as a function of the number of MPI ranks on a single
node. We repeat 5 VMC optimizations and report the minimum time required for
100 steps for the 1D Ising model with L = 256 and an RBM with α = 1 with complex
weights. The black line represents ideal scaling behavior.

10.3 Comparison with jVMC

We compare NetKet with jvmc [39], another open-source Python package supporting VMC
optimization of NQS. Results are shown in Fig. 6. We remark that since both NetKet and
jvmc are jax-based, performance on sampling, expectation values and gradients is roughly
equivalent when using the same hyperparameters [39]. However, a performance diﬀerence
arises in algorithms requiring the use of the QGT, such as TDVP or natural gradient (SR).
Such diﬀerence will vanish in cases where the cost of solving the QGT linear system is small
with respect to the cost of computing the energy and its gradient.

At the time of writing, jvmc only implements a singular-value decomposition (SVD) solver
to invert the QGT matrix. The same type of solver can be used also in NetKet (for a
detailed discussion, see Section 3.5). We limit the computations to models with less than 7000
parameters in order for the QGT to have less than 49 · 103 elements, which is approximately
the maximum matrix size that can be diagonalized in reasonable time on the GPUs we have
access to23. For that reason we chose the size of L = 64 spins.

As shown in Table 5, NetKet outperforms by almost an order of magnitude jvmc on a 32-
core CPU using SVD-based solvers. On GPU, jvmc requires signiﬁcantly less computational

23Using distributed linear-algebra libraries such as ScaLaPack [92], ELPA [93] or the recent [94] would allow
us to avoid this barrier, however we are not aware of any Python binding for those libraries. Regardless, if
those libraries exposed a distributed linear solver to Python, using it with NetKet would be as simple as
using it as the linear solver for the QGT.

53

SciPost Physics Codebases

Submission

Figure 7: (Left): Speedup factor τ observed by increasing the number of MPI
ranks r on a single node while keeping the problem size constant (strong scaling).
(Center): Speedup factor observed by scaling across multiple nodes n, and scaling
the number of samples accordingly (weak scaling). (Right): Scatter plot of the
absolute wall clock time in seconds for the runs reported in the weak scaling (center)
plot. There are 4 points for every color, but they overlap for most implementations
because of the almost-ideal weak scaling. We repeat 5 iterations of constructing the
QGT and 1000 matrix multiplications with the gradient vector for the 2D Ising model
with L = 8 and a DNN with 9 layers and complex weights. In the left panel, we keep
the number of samples Nsamples = 214 constant, while in the center panel, we increase
the number of samples to 214 ×n, while we correct the timing by the number of nodes
n to show the speedup factor for a constant number of 214 samples. We remark that
while the speedup factor is resistant to changes in the network architecture, the
absolute timing might favor one or another implementation depending on several
details and can change depending on the architecture and problem at hand.

time than on CPU, yet, NetKet outperforms jvmc by about 50% in a full VMC iteration.
We remark that in this benchmark both packages scale poorly when going from a single GPU
to two. This is because the diagonalization of the QGT, in this case the bottleneck, cannot
be parallelized.

In order to scale eﬃciently to many GPUs, our QGT implementations can be combined
with iterative solvers to scale up to potentially millions of parameters, as well as signiﬁcantly
larger system sizes. As expected from Fig. 7, increasing the number of MPI ranks reduces
the total time by a factor nearing the number of ranks (with eventually a saturation in the
speedup). Notice also that the CG-solver becomes signiﬁcantly more eﬃcient on GPU.

54

SciPost Physics Codebases

Submission

SVD solver

CG solver

NetKet

jVMC

CPU (32 cores)

GPU (×1)
GPU (×2)
CPU (32 cores)

CPU (32 cores, MPIx16)

GPU (×1)
GPU (×2)

48

24

20

86

7.5

3.9

1.7

337

44

36

N/A

N/A

N/A

N/A

Table 5: Comparison of performance between NetKet and jvmc. All times are
indicated in seconds and have been taken on a workstation with an AMD Ryzen
Threadripper 3970X 32-Core processor and 2xNvidia RTX 3090 GPUs. Timings
are for one VMC step using a complex-valued RBM model with hidden unit den-
sity of α = Nhidden/L = 1 on the 1-dimensional TFIM model (54) with L = 64
sites. Other parameters are: 214 total samples, 210 independent Markov chains
per GPU (or across all CPUs). Calculations for NetKet where performed us-
ing QGTJacobianDense(holomorphic=True) . NetKet multi-GPU calculations use
CUDA-enabled MPI for inter-process communication, while jvmc uses jax built-in
mechanism. The run labeled with (MPI×16) is run on the same workstation but
16 MPI processes are used to better take advantage of the multiple cores of the
processors.

11 Discussion and conclusion

We have presented NetKet 3, a modular Python toolbox to study complex quantum-
mechanical problems with machine learning-inspired tools. Compared to version 2 [35], the
major new feature is the ability to deﬁne arbitrary neural-network ans¨atze for either wave
functions or density matrices using the ﬂexible jax framework; we believe this makes NetKet
much more useful to non-technical users. Another signiﬁcant improvement is that NetKet is
now completely modular. Users of NetKet 3 can decide to use only the neural-network ar-
chitectures, the stochastic samplers, the quantum geometric tensor, or the operators without
necessarily requiring the VariationalState interface, which is convenient but geared towards
the most common applications of variational NQS. Care has been taken to ensure that the
algorithms implemented can scale to very large systems and models with millions of param-
eters thanks to more eﬃcient implementations of the geometric tensor and other algorithmic
bottlenecks. Thanks to its jax foundations, NetKet 3 can now also make eﬀective use of
GPU hardware, without any need for manual low-level programming for these platforms.

Even with all the new features that have been introduced with this version, there are
many things that we would like to see integrated into NetKet in the future. To name a
few: native support for fermionic systems; support for more general geometries in continuous
systems; improvements to the dynamics submodule in order to support a wider variety of
ODE solvers and more advanced regularization and diagnostic schemes [21, 23]; new drivers
for quantum state reconstruction [31] and overlap optimization [32]; a more general way to

55

SciPost Physics Codebases

Submission

deﬁne arbitrary cost functions to be optimized. However, we think that our new jax-based
core is very welcoming to contributors, and we believe that this constitutes a solid foundation
upon which to build in the future. Moreover, we are now explicitly committed to stability of
the user-facing API, in order to make sure that code written today will keep working for a
reasonable time, while we iterate and reﬁne NetKet.

Since a project is only as big as its community, the most important developments are
probably those related to documentation, learning material, and developing a community
where users answer each other’s questions in the spirit of open, shared science. We are taking
steps to make all of this happen.

Acknowledgments

We are grateful to the authors and maintainers of jax [5], flax [44], and optax [65] for guid-
ance and constructive discussions on how to best use their tools. F. V. thanks W. Bruinsma
for adapting plum-dispatch to our needs and making life more Julian in the Python world.
We also thank all the users who engaged with us on GitHub reporting bugs, sharing ideas,
and ultimately helping us to produce a better framework.

Funding information. This work is supported by the Swiss National Science Foundation
under Grant No. 200021 200336. D. H. acknowledges support by the Max Planck-New York
City Center for Nonequilibrium Quantum Phenomena. A. Sz. gratefully acknowledges the
ISIS Neutron and Muon Source and the Oxford–ShanghaiTech collaboration for support of the
Keeley–Rutherford fellowship at Wadham College, Oxford. J. N. was supported by Microsoft
Research.

A Details of group convolutions

A.1 Group convolutions and equivariance

As explained in Section 5.2, GCNNs generate the action of every element g of the symmetry
group G on a wave function |ψi, written as |ψgi = g|ψi, which are then combined into a
symmetric wave function using the projection operator (40). Amplitudes of the computational
basis states |σi are related to one another in these wave functions as

ψg(σ) = hσ|g|ψi = ψ(g−1σ).

(55)

Therefore, feature maps inside the GCNN are indexed by group elements rather than lattice
sites, and all layers must be equivariant: that is, if their input is transformed by a space-group
symmetry, their output must be transformed the same way, so that (55) would always hold.
Pointwise nonlinearities clearly ﬁt this bill [73]; we now consider what linear transformations
are allowed.

First, input feature maps naturally deﬁned on lattice sites must be embedded into group-

valued features:

fg = X

Wg−1~rσ~r = X

Wrσg~r = X

W~r(g−1σ)~r,

~r

~r

~r

(56)

56

SciPost Physics Codebases

Submission

consistent with (55). We also see that the embedding (56) is equivariant:
transformed by some symmetry operation u, the output transforms as

if the input is

X

Wg−1~rσu~r = X

Wg−1u−1~rσ~r = fug.

~r

~r

(57)

This layer is implemented in NetKet as nk.nn.DenseSymm .

To build deeper GCNNs, we also need to map group-valued features onto one another in
an equivariant fashion. This is achieved by group convolutional layers, which transform input
features as24

φg

= X
h∈G

Wh−1gfh.

(58)

This layer is implemented in NetKet as nk.nn.DenseEquivariant . It is equivariant under
multiplying with a group element u from the left:

X

Wh−1gfuh = X

Wh−1ugfh = φug,

h∈G

h∈G

(59)

which is consistent with how the embedding layer (56) is equivariant, cf. (57). Indeed, it can
be composed with (56):

φg

= X
h

Wh−1g

X

W0

h−1~rσ~r = X

(cid:18) X

Wh−1W0

h−1g−1~r

(cid:19)
σ~r ≡ X

W00

g−1~rσ~r

(60)

~r

~r

h

~r

as the expression in brackets only depends on g−1~r.

Finally, the output features of the last layer of the GCNN are turned into the wave
(cid:17) and projected on an irrep using (40) (we drop the irrelevant

exp(cid:16)

functions ψg(σ) = P
i
constant prefactor):

f (i)
g

ψ(σ) = X
i,g

χ∗
g

exp(cid:16)

(cid:17)

,

f (i)
g

(61)

where χg are the characters of the irrep. In addition to allowing nontrivial symmetries, our
choice of summing a large number of terms in the ansatz appears to improve the stability of
variational optimization for sign-problematic Hamiltonians [15, 30, 49, 72].

A.2 Fast group convolutions using Fourier transforms

The simplest implementation of a group convolutional layer is expanding each of the finfout
kernels, containing |G| entries, to a |G| × |G| matrix deﬁned as

˜W (a,b)
g,h

= W (a,b)
g−1h,

(62)

where a, b index input and output features, respectively. The resulting tensor of size finfout|G|2
can then be contracted straightforwardly with the input features:

φ(b)
h

= X
a,g

˜W (a,b)

g,h f (a)

g

(63)

24Our convention diﬀers from that of Ref. [73], which in fact implements group correlation rather than
convolution. The two conventions are equivalent (the indexing of the kernels diﬀers by taking the inverse of
each element); we use convolutions to simplify the Fourier transform-based implementations of Sec. A.2.

57

SciPost Physics Codebases

Submission

is equivalent to (58). Embedding layers (56) can be constructed analogously. This method
is the easiest to interpret and code, serving as a useful check on other methods; however,
the enlarged kernels require substantial amounts of memory, which already becomes a serious
problem on modestly sized lattices and networks. Furthermore, evaluating a convolution using
this method takes O(finfout|G|2) time. NetKet implements two approaches to improve on
this scaling.

The ﬁrst approach uses group Fourier transforms, which generalize the usual discrete
Fourier transform for arbitrary ﬁnite groups. The forward and backward transformations are
deﬁned by

ˆf (ρ) = X
g∈G

f (g)ρ(g);

f (g) =

1

|G|

X

dρ Tr h ˆf (ρ)ρ(g−1)i

.

(64)

ρ

In the forward transformation, ρ is a representation of the group G; f (g) is a function deﬁned
on group elements, while ˆf (ρ) is a matrix of the same shape as the representatives ρ(g). The
sum in the backward transformation runs over all inequivalent irreps ρ, of dimension dρ, of
the group. Since P
= |G|, this transformation does not increase the amount of memory
needed to store inputs, outputs, or kernels.25 Group convolutions can readily be implemented
by multiplying the Fourier transform matrices (we drop feature indices for brevity):

ρ d2
ρ

ˆφ(ρ) = X
g

φgρ(g) = X

fhWh−1gρ(h)ρ(h−1g) = ˆf (ρ) ˆW (ρ).

(65)

g,h

To calculate a convolution using this approach, the input features are Fourier transformed
[O(fin|G|2) as there is no generic fast Fourier transform algorithm for group Fourier trans-
) for an irrep
forms], multiplied with the kernel Fourier transform for each irrep [O(finfoutd3
ρ
of dimension dρ], and the output is transformed back [O(fout|G|2)], yielding the total run-
time O[(fin + fout)|G|2 + finfout
]. In a large space group, most irreps are deﬁned on
a star of |P | wave vectors (P is the point group) and thus have dimension |P |; accordingly,
P

ρ ≈ |G||P |.
ρ d3
The second approach, based on Ref. [73], exploits the fact that the translation group T is
a normal subgroup of the space group G, so each g ∈ G can be written as tgpg, where tg is a
translation and pg is a ﬁxed coset representative (in symmorphic groups, we can choose these
to be point-group symmetries). Now, we can deﬁne the expanded kernels (we drop feature
indices again to reduce clutter)

ρ d3
ρ

P

˜W (pg,ph)
t

≡ Wp−1

g tph

such that

φh = X

fgWg−1h = X

g

tg,pg

ftgpg Wpgt−1

g thph

≡ X
tg,pg

˜f (pg)
tg

˜W (pg,ph)
th−tg

.

(66)

(67)

In the last form, we split the space-group feature map f into cosets of the translation group
and observe that the latter is Abelian. In fact, the translation group is equivalent to the set of
valid lattice vectors, so the sum over tg in (67) is a standard convolution. Ref. [73] proposes to
perform this convolution using standard cuDNN routines. However, we are usually interested
in convolutions that span the entire lattice in periodic boundary conditions: these can be
performed more eﬃciently using fast Fourier transforms (FFTs) as the Fourier transform of

25If some irreps cannot be expressed as matrices with real entries, the Fourier transform of real

in-

puts/outputs/kernels is complex too, temporarily doubling the amount of memory used.

58

SciPost Physics Codebases

Submission

a convolution is the product of Fourier transforms. Therefore, we FFT the kernels ˜W and
features ˜f , contract them as appropriate, and FFT the result back:
(cid:16) ˜W (a,b;pg,ph)(cid:17) (cid:21)
,

˜φ(b,ph) = F −1

(cid:16) ˜f (a,pg)(cid:17)

(cid:20) X

(68)

F

F

a,pg

where the Fourier transform is understood to act on the omitted translation-group indices,
and the Fourier transforms are multiplied pointwise.

Calculating a convolution in this approach involves fin|P | forward FFTs [O(|T | log |T |)
each], |T | tensor inner products [O(finfout|P |2 each], and fout|P | backward FFTs; as |G| =
|T ||P |, this yields a total of O[(fin + fout)|G| log |T | + finfout|G||P |]. For large lattices, which
bring out the better asymptotic scaling of FFTs, this improves signiﬁcantly on the runtime of
the group Fourier transform-based approach, especially in the pre- and postprocessing stages.
By contrast, the group Fourier transform approach is better for large point groups, as it avoids
constructing the |P |2 reshaped kernels ˜W pgph, which can be prohibitive for large lattices.

In practice, as both FFTs and group Fourier transforms involve steps more complicated
than simple tensor multiplication, their performance is hard to assess beyond asymptotes,
especially on a GPU. On CPUs, the FFT-based approach tends to be faster. On GPUs,
computation time tends to scale sub-linearly with the number of operations so long as the
process is eﬃciently parallelized. As all operations of the group Fourier transform implemen-
tation involve multiplications of large matrices, it can fully exploit the large GPU registers
even with relatively few samples. By contrast, FFTs cannot be fully vectorized, meaning that
larger batches are required to make full use of the computing power of the GPU. In practice
therefore, the FFT-based approach may not perform better until most of the GPU memory
becomes involved in evaluating a batch.

B Implementation details of the quantum geometric tensor

In the following, we discuss our implementations of the quantum geometric tensor, introduced
in section 3.5, in more detail. In particular, we show how the action of the quantum geometric
tensor on a vector can be computed eﬃciently without storing the full matrix. Appendix B.1
introduces relevant automatic-diﬀerentiation concepts in general terms; the concrete algo-
rithms used by QGTJacobian and QGTOnTheFly are discussed in Appendices B.2 and B.3,
respectively.

B.1 Jacobians and their products

We assume that our NQS is modeled by the scalar parametric function f (s) = ln ψθ(s), where
θ is a vector of variational parameters and s is a basis vector of the Hilbert space. Consistent
with the notation of the main text, Oj(s) = ∂θj
ln ψθ(s) are the log-derivatives (13) of the
NQS.

We also assume that f can be vectorized and evaluated for a batch of inputs {sk}k=1...Ns

,

yielding the vector fk = ln ψθ(sk). The Jacobian of this function is therefore the matrix
Jkl = Ol(sk) = ∂ ln ψθ(sk)

;

(69)

∂θl

59

SciPost Physics Codebases

Submission

each row corresponds to the gradient of f evaluated at a diﬀerent input sk, so k = 1 . . . Ns
and l = 1 . . . Nparameters.

The Jacobian matrix can be computed in jax with jax.jacrev(log_wavefunction)(s) ,
which returns a matrix.26 However, it is often not needed to have access to the full Jacobian:
for example, when computing the gradient (26) of the variational energy, we only need the
product of the Jacobian with a vector, namely ∆Eloc(sk) = ˜H(sk) − E[ ˜H].

A vector can be contracted with the Jacobian along its dimension corresponding to either

parameters or outputs:

• Jacobian–vector products (Jvp), ˜v = Jv, can be computed using forward-mode auto-

matic diﬀerentiation;

• vector–Jacobian products (vJp), ˜v = vT J, can be computed through backward-mode

automatic diﬀerentiation (backward propagation).

Modern automatic-diﬀerentiation frameworks like that of jax implement primitives that eval-
uate Jvp and vJp, and construct higher-level functions such as jax.grad or jax.jacrev on
top of those functions; that is, one can extract the best performance from jax by making use
of vJp and Jvp as much as possible [95].

B.2 QGTJacobian

Writing the estimator (23) of the quantum geometric tensor explicitly in terms a ﬁnite number
of samples sk, we obtain

i Oj] − E [Oi]∗ E [Oj]
Gij = E [O∗
NsX

Oi(sk)∗Oj(sk) −

≈

1

1

(cid:18) NsX

(cid:19)∗(cid:18) NsX

Oi(sk)

(cid:19)

Oj(sk)

k=1
!∗  

2

Ns

Oi(sk)
Ns
!∗  

Jkj −

Oj(sk) −

!

NsX

k=1

Jkj
Ns

Oi(sk) −

Jki −

NsX

k=1

NsX

k=1

Jki
Ns

k=1

!

NsX

k=1

Oj(sk)
Ns

Ns

1

Ns

1

Ns

1

Ns

k=1
NsX

k=1
NsX

k=1
NsX

k=1

=

=

=

(∆Jki)∗ (∆Jkj) ,

(70)

where we have deﬁned the centered Jacobian ∆Jki ≡ Jki − PNs
k=1
is equivalent to

Jki
Ns

. In matrix notation, this

G =

∆J †
√
Ns

∆J
√
Ns

.

(71)

The Jacobian-based implementation of the quantum geometric tensor computes27 and

26More precisely, it returns a PyTree with a structure similar to the PyTree that stores the parameters; each

leaf gains an additional dimension of length Ns.

27The full Jacobian can be computed row by row, performing vJps with vectors that have a single nonzero
entry. In practice, as the network is evaluated independently for all samples in a batch, each of these products
requires us to back-propagate the network for only one sample at a time.

60

 
 
SciPost Physics Codebases

Submission

stores28 the full Jacobian matrix Jkl for the given samples upon construction.Then, QGT–
vector products ˜v = Gv are computed without ﬁnding the full matrix G, in two steps:

∆w =

∆J
√
Ns

v;

˜v =

∆J †
√
Ns

∆w =

(cid:18) ∆J
√
Ns

(cid:19)†

;

∆w†

(72)

the ﬁnal form has the advantage that the Hermitian transpose of a vector is simply its con-
jugate. Evaluating Eq. (72) is usually less computationally expensive than constructing the
full quantum geometric tensor.

B.3 QGTOnTheFly

In some cases one might have so many parameters or samples that it is impossible to store
the full Jacobian matrix in memory. In that case, we still evaluate a set of equations similar
to Eq. (72), but without pre-computing the full Jacobian, only using vector–Jacobian and
Jacobian–vector products.

It would be impractical to perform a vJp using the centered Jacobian; however, Eq. (23)

can be rewritten as Gij = E [O∗
i

(Oj − E [Oj])], which yields
1

G =

J †∆J,

(73)

Ns
where we have substituted one of the two centered Jacobians with a plain Jacobian. Then,
we note that the centered-Jacobian–vector product can be expressed as

∆w ≡ ∆J v =

J −

!

1T J
Ns

v = w −

1T w
Ns

,

(74)

where 1T is a row vector all entries of which are 1, used to express averaging the columns
of the Jacobian in the matrix formalism. Therefore, QGTOnTheFly performs the following
calculations:

w =

1

Ns

Jv;

∆w = w − hwi ;

˜v = J † ∆w,

(75)

where the ﬁrst and the last step are implemented with jax.vjp and jax.jvp , respectively.

References

[1] A. Krizhevsky, I. Sutskever and G. E. Hinton, Imagenet classiﬁcation with deep convolu-
tional neural networks, In F. Pereira, C. J. C. Burges, L. Bottou and K. Q. Weinberger,
eds., Advances in Neural Information Processing Systems, vol. 25. Curran Associates,
Inc. (2012).

[2] C. Chen, O. Li, D. Tao, A. Barnett, C. Rudin and J. K. Su, This looks like that: Deep
learning for interpretable image recognition, In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox and R. Garnett, eds., Advances in Neural Information Processing
Systems, vol. 32, pp. 8928–8939. Curran Associates, Inc. (2019).

28To speed up the evaluation of (72), we actually store ∆J/

√

Ns.

61

 
SciPost Physics Codebases

Submission

[3] D. W. Otter, J. R. Medina and J. K. Kalita, A survey of the usages of deep learning for
natural language processing, IEEE Trans. Neural Netw. Learn. Syst. 32(2), 604 (2021),
doi:10.1109/tnnls.2020.2979670.

[4] Y. Sun, N. B. Agostini, S. Dong and D. Kaeli, Summarizing CPU and GPU design trends

with product data (2019), arXiv:1911.11313.

[5] R. Frostig, M. J. Johnson and C. Leary, Compiling machine learning programs via high-

level tracing, Systems for Machine Learning (2018).

[6] J. Bezanson, A. Edelman, S. Karpinski and V. B. Shah, Julia: A fresh approach to

numerical computing, SIAM Rev. 59(1), 65 (2017), doi:10.1137/141000671.

[7] G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto and
L. Zdeborov´a, Machine learning and the physical sciences, Reviews of Modern Physics
91(4) (2019), doi:10.1103/revmodphys.91.045002.

[8] G. Carleo and M. Troyer, Solving the quantum many-body problem with artiﬁcial neural

networks, Science 355(6325), 602 (2017), doi:10.1126/science.aag2302.

[9] D.-L. Deng, X. Li and S. Das Sarma, Quantum entanglement in neural network states,

Phys. Rev. X 7, 021021 (2017), doi:10.1103/PhysRevX.7.021021.

[10] D. Wu, L. Wang and P. Zhang,

ational
doi:10.1103/PhysRevLett.122.080602.

autoregressive networks,

Solving statistical mechanics using vari-
(2019),

Phys. Rev. Lett. 122,

080602

[11] R. Kaubruegger, L. Pastori and J. C. Budich, Chiral topological phases from artiﬁcial
neural networks, Physical Review B 97(19) (2018), doi:10.1103/physrevb.97.195136.

[12] I. Glasser, N. Pancotti, M. August, I. D. Rodriguez and J. I. Cirac, Neural-network
quantum states, string-bond states, and chiral topological states, Phys. Rev. X 8, 011006
(2018), doi:10.1103/PhysRevX.8.011006.

[13] K. Choo, G. Carleo, N. Regnault and T. Neupert, Symmetries and many-body ex-
citations with neural-network quantum states, Phys. Rev. Lett. 121, 167204 (2018),
doi:10.1103/PhysRevLett.121.167204.

[14] T. Vieijra, C. Casert, J. Nys, W. De Neve, J. Haegeman, J. Ryckebusch and F. Verstraete,
Restricted Boltzmann machines for quantum states with non-{Abelian} or anyonic sym-
metries, Phys. Rev. Lett. 124(9), 097201 (2020), doi:10.1103/physrevlett.124.097201.

[15] A. Szab´o and C. Castelnovo, Neural network wave functions and the sign problem, Phys.

Rev. Research 2, 033075 (2020), doi:10.1103/PhysRevResearch.2.033075.

[16] T. Vieijra and J. Nys, Many-body quantum states with exact conservation of non-abelian
and lattice symmetries through variational monte carlo, Phys. Rev. B 104, 045123 (2021),
doi:10.1103/PhysRevB.104.045123.

[17] M. Bukov, M. Schmitt and M. Dupont, Learning the ground state of a non-stoquastic
quantum Hamiltonian in a rugged neural network landscape, SciPost Phys. 10, 147 (2021),
doi:10.21468/SciPostPhys.10.6.147.

62

SciPost Physics Codebases

Submission

[18] S. Czischek, M. G¨arttner and T. Gasenzer, Quenches near ising quantum critical-
ity as a challenge for artiﬁcial neural networks, Physical Review B 98(2) (2018),
doi:10.1103/physrevb.98.024311.

[19] G. Fabiani and J. Mentink,

Investigating ultrafast quantum magnetism with machine

learning, SciPost Physics 7(1) (2019), doi:10.21468/scipostphys.7.1.004.

[20] I. L. Guti´errez and C. B. Mendl, Real time evolution with neural-network quantum states,

Quantum 6, 627 (2022), doi:10.22331/q-2022-01-20-627.

[21] M. Schmitt and M. Heyl,

sions with artiﬁcial neural networks,
doi:10.1103/PhysRevLett.125.100503.

Quantum many-body dynamics

in two dimen-
Phys. Rev. Lett. 125, 100503 (2020),

[22] G. Fabiani, M. D. Bouman and J. H. Mentink,

tion in two-dimensional antiferromagnets,
doi:10.1103/physrevlett.127.097202.

Phys. Rev. Lett. 127(9)

Supermagnonic propaga-
(2021),

[23] D. Hofmann, G. Fabiani, J. H. Mentink, G. Carleo and M. A. Sentef, Role of stochastic
noise and generalization error in the time propagation of neural-network quantum states,
SciPost Phys. 12, 165 (2022), doi:10.21468/SciPostPhys.12.5.165.

[24] F. Vicentini, A. Biella, N. Regnault and C. Ciuti, Variational neural-network ansatz
for steady states in open quantum systems, Phys. Rev. Lett. 122, 250503 (2019),
doi:10.1103/PhysRevLett.122.250503.

[25] A. Nagy and V. Savona, Variational quantum monte carlo method with a neural-
network ansatz for open quantum systems, Phys. Rev. Lett. 122, 250501 (2019),
doi:10.1103/PhysRevLett.122.250501.

[26] M.

J. Hartmann and G. Carleo,

Neural-network

approach

tive quantum many-body dynamics,
doi:10.1103/PhysRevLett.122.250502.

Phys. Rev. Lett. 122,

to

dissipa-
250502 (2019),

[27] C.-Y. Park and M. J. Kastoryano, Expressive power of complex-valued restricted boltz-

mann machines for solving non-stoquastic hamiltonians (2021), 2012.08889.

[28] N. Astrakhantsev, T. Westerhout, A. Tiwari, K. Choo, A. Chen, M. H. Fischer, G. Carleo
and T. Neupert, Broken-symmetry ground states of the heisenberg model on the pyrochlore
lattice, Physical Review X 11(4) (2021), doi:10.1103/physrevx.11.041021.

[29] L. L. Viteritti, F. Ferrari and F. Becca, Accuracy of Restricted Boltzmann Machines for
the one-dimensional J1 − J2 Heisenberg model, doi:10.48550/ARXIV.2202.07576 (2022).

[30] Y. Nomura and M. Imada, Dirac-type nodal spin liquid revealed by machine learning,

arXiv preprint arXiv:2005.14142 (2020).

[31] G. Torlai, G. Mazzola, J. Carrasquilla, M. Troyer, R. Melko and G. Carleo,
Nature Physics 14(5), 447 (2018),

Neural-network quantum state tomography,
doi:10.1038/s41567-018-0048-5.

63

SciPost Physics Codebases

Submission

[32] B. J´onsson, B. Bauer and G. Carleo, Neural-network states for the classical simulation

of quantum computing (2018), 1808.05232.

[33] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis,
J. Dean, M. Devin, S. Ghemawat, I. Goodfellow et al., TensorFlow: Large-scale machine
learning on heterogeneous systems, Software available from tensorﬂow.org (2015).

[34] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf et al., Pytorch: An imperative style,
high-performance deep learning library, In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox and R. Garnett, eds., Advances in Neural Information Processing
Systems 32, pp. 8024–8035. Curran Associates, Inc. (2019).

[35] G. Carleo, K. Choo, D. Hofmann, J. E. Smith, T. Westerhout, F. Alet, E. J. Davis,
S. Efthymiou, I. Glasser, S.-H. Lin, M. Mauri, G. Mazzola et al., NetKet: A ma-
chine learning toolkit for many-body quantum systems, SoftwareX 10, 100311 (2019),
doi:10.1016/j.softx.2019.100311.

[36] J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Nec-
ula, A. Paszke, J. VanderPlas, S. Wanderman-Milne and Q. Zhang, JAX: composable
transformations of Python+NumPy programs, URL http://github.com/google/jax (2018).

[37] D. H¨afner and F. Vicentini, mpi4jax: Zero-copy MPI communication of JAX arrays,

Journal of Open Source Software 6(65) (2021), doi:10.21105/joss.03419.

[38] W. Bruinsma, Plum: Multiple dispatch in python, doi:10.5281/zenodo.5774909, URL

https://github.com/wesselb/plum (2021).

[39] M. Schmitt and M. Reh, Jvmc: Versatile and performant variational monte carlo lever-

aging automated diﬀerentiation and gpu acceleration (2021), arXiv:2108.03409.

[40] M.-D. Choi, Completely positive linear maps on complex matrices, Linear Algebra and

its Applications 10(3), 285 (1975), doi:10.1016/0024-3795(75)90075-0.

[41] A. Jamio lkowski, Linear transformations which preserve trace and positive semideﬁnite-
ness of operators, Reports on Mathematical Physics 3(4), 275 (1972), doi:10.1016/0034-
4877(72)90011-0.

[42] S. K. Lam, A. Pitrou and S. Seibert, Numba: A llvm-based python jit compiler,

In
Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC, pp.
1–6, doi:10.1145/2833157.2833162 (2015).

[43] K. Kreutz-Delgado, The complex gradient operator and the cr-calculus (2009), arXiv:

0906.4835.

[44] J. Heek, A. Levskaya, A. Oliver, M. Ritter, B. Rondepierre, A. Steiner and M. van Zee,
Flax: A neural network library and ecosystem for JAX, URL http://github.com/google/
ﬂax (2020).

[45] T. Hennigan, T. Cai, T. Norman and I. Babuschkin, Haiku: Sonnet for JAX, URL

http://github.com/deepmind/dm-haiku (2020).

64

SciPost Physics Codebases

Submission

[46] J. Stokes, B. Chen and S. Veerapaneni, Numerical and geometrical aspects of ﬂow-based

variational quantum monte carlo, doi:10.48550/ARXIV.2203.14824 (2022).

[47] R. Jastrow, Many-body problem with strong forces, Phys. Rev. 98(5), 1479 (1955),

doi:10.1103/physrev.98.1479.

[48] D. A. Huse and V. Elser,

two-
dimensional heisenberg spin-½ antiferromagnets, Phys. Rev. Lett. 60, 2531 (1988),
doi:10.1103/PhysRevLett.60.2531.

variational wave

functions

Simple

for

[49] C. Roth and A. H. MacDonald, Group convolutional neural networks improve quantum

state accuracy, arXiv preprint arXiv:2104.05085 (2021).

[50] O. Sharir, Y. Levine, N. Wies, G. Carleo and A. Shashua, Deep autoregressive models
for the eﬃcient variational simulation of many-body quantum systems, Phys. Rev. Lett.
124, 020503 (2020), doi:10.1103/PhysRevLett.124.020503.

[51] G. Torlai and R. G. Melko, Latent space puriﬁcation via neural density operators, Phys.

Rev. Lett. 120, 240503 (2018), doi:10.1103/PhysRevLett.120.240503.

[52] F. Becca and S. Sorella,

Systems,
doi:10.1017/9781316417041 (2017).

Cambridge University Press,

Quantum Monte Carlo Approaches for Correlated
ISBN 9781107129931, 9781316417041,

[53] W. K. Hastings, Monte carlo sampling methods using markov chains and their applica-

tions 57(1), 97 (1970), doi:10.1093/biomet/57.1.97.

[54] M. V. Berry, The quantum phase, ﬁve years after, Geometric phases in physics pp. 7–28

(1989).

[55] G. Fubini, Sulle metriche deﬁnite da una forme hermitiana, Atti R. Ist. Veneto Sci. Lett.

Arti 63, 502 (1904).

[56] E. Study, K¨urzeste Wege im komplexen Gebiet, Mathematische Annalen 60(3), 321

(1905), doi:10.1007/BF01457616.

[57] J. Stokes, J. Izaac, N. Killoran and G. Carleo, Quantum Natural Gradient, Quantum 4,

269 (2020), doi:10.22331/q-2020-05-25-269.

[58] S. Sorella, Green function monte carlo with stochastic reconﬁguration, Physical review

letters 80(20), 4558 (1998).

[59] S.-i. Amari, Natural gradient works eﬃciently in learning, Neural Comput. 10(2), 251

(1998), doi:10.1162/089976698300017746.

[60] C.-Y. Park and M. J. Kastoryano, Geometry of learning neural quantum states, Physical

Review Research 2(2) (2020), doi:10.1103/physrevresearch.2.023232.

[61] R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge University Press, Cambridge

New York, ISBN 9781139020411, doi:10.1017/cbo9781139020411 (2009).

65

SciPost Physics Codebases

Submission

[62] I. Contreras, E. Ercolessi and M. Schiavina, On the geometry of mixed states and
the ﬁsher information tensor, Journal of Mathematical Physics 57(6), 062209 (2016),
doi:10.1063/1.4954328.

[63] H. Weimer, Variational principle for steady states of dissipative quantum many-body
systems, Phys. Rev. Lett. 114, 040402 (2015), doi:10.1103/PhysRevLett.114.040402.

[64] X. Yuan, S. Endo, Q. Zhao, Y. Li and S. C. Benjamin, Theory of variational quantum

simulation, Quantum 3, 191 (2019), doi:10.22331/q-2019-10-07-191.

[65] M. Hessel, D. Budden, F. Viola, M. Rosca, E. Sezener and T. Hennigan, Optax:
composable gradient transformation and optimisation, in jax!, URL http://github.com/
deepmind/optax (2020).

[66] P. Kramer and M. Saraceno, eds., Geometry of the Time-Dependent Variational Principle
in Quantum Mechanics, Springer Berlin Heidelberg, doi:10.1007/3-540-10579-4 (1981).

[67] J. Haegeman, J. I. Cirac, T. J. Osborne, I. Piˇzorn, H. Verschelde and F. Verstraete, Time-
dependent variational principle for quantum lattices, Physical Review Letters 107(7)
(2011), doi:10.1103/physrevlett.107.070601.

[68] G. Carleo, F. Becca, M. Schir´o and M. Fabrizio, Localization and glassy dynamics of
many-body quantum systems, Scientiﬁc Reports 2(1) (2012), doi:10.1038/srep00243.

[69] H. Breuer and F. Petruccione, The Theory of Open Quantum Systems, OUP Oxford,

ISBN 9780199213900, doi:10.1093/acprof:oso/9780199213900.001.0001 (2007).

[70] J. Stoer and R. Bulirsch,

Introduction to Numerical Analysis, Springer New York,

doi:10.1007/978-0-387-21738-3 (2002).

[71] S. d’Ascoli, L. Sagun, J. Bruna and G. Biroli, Finding the Needle in the Haystack
with Convolutions: on the beneﬁts of architectural bias, arXiv:1906.06766 (2019), arxiv:
1906.06766.

[72] Y. Nomura, Helping restricted Boltzmann machines with quantum-state representation by
restoring symmetry, J. Phys. Condens. Matter 33(17), 174003 (2021), doi:10.1088/1361-
648x/abe268.

[73] T. Cohen and M. Welling, Group equivariant convolutional networks, In M. Balcan and
K. Q. Weinberger, eds., Proceedings of the 33nd International Conference on Machine
Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, vol. 48 of JMLR
Workshop and Conference Proceedings, pp. 2990–2999. JMLR.org (2016).

[74] V. Heine, Group Theory in Quantum Mechanics: An Introduction to Its Present Usage,
vol. 91 of International Series in Natural Philosophy, Pergamon, ISBN 978-0-08-009242-
3, doi:10.1016/C2013-0-01646-5 (1960).

[75] M.

ed.,
I. Aroyo,
for Crystallography,
doi:10.1107/97809553602060000114 (2016).

Space-group symmetry,
Wiley, New York,

vol. A of

International Tables
ISBN 978-0-470-97423-0,

2 edn.,

66

SciPost Physics Codebases

Submission

[76] J. D. Dixon, Computing irreducible representations of groups, Math. Comput. 24(111),

707 (1970), doi:10.1090/s0025-5718-1970-0280611-6.

[77] W. Burnside, The Theory of Groups, Cambridge University Press (1911).

[78] C. J. Bradley and A. P. Cracknell, The Mathematical Theory of Symmetry in Solids:
ISBN

Representation Theory for Point Groups and Space Groups, Clarendon Press,
9780198519201 (1972).

[79] G. Bertaina, M. Motta, M. Rossi, E. Vitali and D. E. Galli, Supplemental Material:
One-dimensional liquid 4 He: dynamical properties beyond Luttinger liquid theory PATH-
INTEGRAL GROUND STATE METHOD pp. 1–5.

[80] G. Bertaina, M. Motta, M. Rossi, E. Vitali and D. E. Galli, One-Dimensional Liquid
He 4: Dynamical Properties beyond Luttinger-Liquid Theory, Physical Review Letters
116(13), 1 (2016), doi:10.1103/PhysRevLett.116.135302, 1412.7179.

[81] R. A. Aziz and H. H. Chen, An accurate intermolecular potential for argon, The Journal

of Chemical Physics 67(12), 5719 (1977), doi:10.1063/1.434827.

[82] M. Zaheer, S. Kottur, S. Ravanbhakhsh, B. P´oczos, R. Salakhutdinov and A. J. Smola,
Deep sets, Advances in Neural Information Processing Systems 2017-December(ii),
3392 (2017), 1703.06114.

[83] T. Kato,
ics,
doi:10.1002/cpa.3160100201.

On the eigenfunctions of many-particle systems in quantum mechan-
Communications on Pure and Applied Mathematics 10(2), 151 (1957),

[84] G. Pescia, J. Han, A. Lovato, J. Lu and G. Carleo, Neural-network quantum states
for periodic systems in continuous space, Phys. Rev. Research 4, 023138 (2022),
doi:10.1103/PhysRevResearch.4.023138.

[85] F. Ferrari and F. Becca, Gapless spin liquid and valence-bond solid in the J1 − J2 heisen-
berg model on the square lattice: Insights from singlet and triplet excitations, Physical
Review B 102(1) (2020), doi:10.1103/physrevb.102.014417.

[86] P. W. Anderson, An approximate quantum theory of the antiferromagnetic ground state,

Phys. Rev. 86, 694 (1952), doi:10.1103/PhysRev.86.694.

[87] J. R. McClean, N. C. Rubin, K. J. Sung, I. D. Kivlichan, X. Bonet-Monroig, Y. Cao,
C. Dai, E. S. Fried, C. Gidney, B. Gimby, P. Gokhale, T. H¨aner et al., OpenFermion:
The electronic structure package for quantum computers, Quantum Sci. Technol. 5(3),
034014 (2020), doi:10.1088/2058-9565/ab8ebc.

[88] J. Nys and G. Carleo, Variational solutions to fermion-to-qubit mappings in two spatial

dimensions, arXiv preprint arXiv:2205.00733 (2022).

[89] J. Johansson, P. Nation and F. Nori, QuTiP: An open-source python framework for the
dynamics of open quantum systems, Computer Physics Communications 183(8), 1760
(2012), doi:10.1016/j.cpc.2012.02.021.

67

SciPost Physics Codebases

Submission

[90] J. Johansson, P. Nation and F. Nori, QuTiP 2: A python framework for the dynamics
of open quantum systems, Computer Physics Communications 184(4), 1234 (2013),
doi:10.1016/j.cpc.2012.11.019.

[91] P. E. Kloeden and E. Platen, Numerical Solution of Stochastic Diﬀerential Equations,

Springer Berlin Heidelberg, doi:10.1007/978-3-662-12616-5 (1992).

[92] J. Choi, J. Dongarra, R. Pozo and D. Walker, ScaLAPACK: a scalable linear algebra
library for distributed memory concurrent computers, In [Proceedings 1992] The Fourth
Symposium on the Frontiers of Massively Parallel Computation. IEEE Comput. Soc.
Press, doi:10.1109/fmpc.1992.234898 (1992).

[93] T. Auckenthaler, V. Blum, H.-J. Bungartz, T. Huckle, R. Johanni, L. Kr¨amer, B. Lang,
H. Lederer and P. Willems, Parallel solution of partial symmetric eigenvalue prob-
lems from electronic structure calculations, Parallel Computing 37(12), 783 (2011),
doi:10.1016/j.parco.2011.05.002.

[94] M. Gates, J. Kurzak, A. Charara, A. YarKhan and J. Dongarra, SLATE, In Proceedings
of the International Conference for High Performance Computing, Networking, Storage
and Analysis. ACM, doi:10.1145/3295500.3356223 (2019).

[95] A. Griewank and A. Walther,

Evaluating Derivatives,

Society for Industrial
ISBN 9780898716597, 9780898717761,

and Applied Mathematics, Philadelphia, PA,
doi:10.1137/1.9780898717761 (2008).

68

