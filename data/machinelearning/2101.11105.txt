1
2
0
2

n
a
J

8
2

]
h
p
-
p
m
o
c
.
s
c
i
s
y
h
p
[

2
v
5
0
1
1
1
.
1
0
1
2
:
v
i
X
r
a

A Taylor Based Sampling Scheme for Machine
Learning in Computational Physics

Paul Novello∗†

Gaël Poëtte†

David Lugato†

Pietro Marco Congedo∗

Abstract

Machine Learning (ML) is increasingly used to construct surrogate models for
physical simulations. We take advantage of the ability to generate data using
numerical simulations programs to train ML models better and achieve accuracy
gain with no performance cost. We elaborate a new data sampling scheme based on
Taylor approximation to reduce the error of a Deep Neural Network (DNN) when
learning the solution of an ordinary differential equations (ODE) system.

1

Introduction

Computational physics allow simulating various physical phenomena when real measures and
experiments are very difﬁcult to perform and even sometimes not affordable. These simulations
can themselves come with a prohibitive computational cost so that very often they are replaced by
surrogate models [15, 10]. Recently, Machine Learning (ML) has proven its efﬁciency in several
domains, and share similarities with classical surrogate models. The question of replacing some
costly parts of simulations code by ML models thus becomes relevant. To reach this long term
objective, this paper tackles a ﬁrst step by investigating new ways to increase ML models accuracy
for a ﬁxed performance cost.

To this end we leverage the ability to control the sampling of the training data. Several previous
works have hinted towards the importance of the training set, in the context of Deep Learning [2],
Active Learning [13] and Reinforcement Learning [8]. Our methodology relies on adapting the data
generation procedure to gain accuracy with a same ML model exploiting the information coming
from the derivatives of the quantity of interest w.r.t the inputs of the model. This approach is similar
to Active learning, in the sense that we adapt our training strategy to the problem, but still differs
because it is prior to the training of the model. Therefore, it is complementary with active learning
methods (see [13] for a review and [6, 16] for more recent applications). The efﬁciency of this
original methodology is tested on the approximation by a Deep Neural Network (DNN) of a stiff
Bateman Ordinary Differential Equation (ODE) system. Solving this system at a moderate cost is
essential in many physical simulations (neutronic [3, 5], combustion [4], detonic [11], etc.).

2 Methodology

⊂

→

Rni

To introduce the general methodology, we consider the problem of approximating a function f :
Rno where S is a subspace deﬁned by the physics of interest. Let a model fθ,
S
whose parameters θ have to be found in order to minimize a loss function L(θ) =
. In
fθ(cid:107)
−
, and their
supervised learning, we are usually given a training data set of N points,
pointwise values
. These points allow to statistically estimate L(θ) and then to
}
use optimization algorithms to ﬁnd a minimum of L(θ) w.r.t. θ.
Amongst ML techniques, we chose fθ to be a DNN for two reasons. First, DNNs outperform most of
1 which is often true in computational physics.
other ML models in high dimensions i.e. ni, no (cid:29)

f
(cid:107)
X1, ..., XN }
{

f (X1), ..., f (XN )
{

∗INRIA Paris Saclay, {paul.novello,pietro.congedo}@inria.fr
†CEA CESTA, {paul.novello, gael.poette, david.lugato}@cea.fr

Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019), Vancouver, Canada.

 
 
 
 
 
 
Second, recent advances in Deep Learning frameworks have made DNNs much more efﬁcient. Better
optimization algorithms as well as the compatibility with GPUs and TPUs have greatly increased its
performances.

Sampling hypothesis - The methodology is based on the assumption that a given DNN yields better
accuracy when the dataset focuses on regions where f is locally steeper. To identify these regions,
we make use of the Taylor approximation (multi-index notation) for order n on f :

f (X + (cid:15)) =

(cid:107)(cid:15)(cid:107)→0

(cid:88)

0≤|k|≤n

(cid:15)k ∂kf (X)
k!

+ O((cid:15)n).

(1)

Dn

(cid:15) (X) =

(cid:88)

1≤|k|≤n

(cid:15)k (cid:107)

∂kf (X)
k!

(cid:107)

.

(2)

(cid:107)

−

Dn

Dn
{

(cid:15) (XN )
}

(cid:15) (X1), ..., Dn

∂kf (X)
(cid:107)

(cid:15) is evaluated using

can be computed along with

(cid:15) , given by (2). Notice that Dn

{
f (X1), ..., f (XN )
}
{

∂kf (X1), ..., ∂kf (XN )
.

Quantity f (X + (cid:15))
f (X) derived using (1) gives an indication of how much f changes around X.
By neglecting the orders above (cid:15)n, it is then possible to ﬁnd the regions of interest by focusing on
instead of ∂kf (X) for derivatives
Dn
not to cancel each other. The next steps are to evaluate and sample from Dn
(cid:15) .
Evaluating Dn
(cid:15) (x) - (2) involves the computation of derivatives of f . Usually in supervised
learning, only
are provided and the derivatives of f are unknown. However,
f (X1), ..., f (XN )
}
{
here the dataset is drawn from a numerical simulation software. It is therefore possible either to
use ﬁnite difference to approximate the derivatives, or to compute them exactly using automatic
differentiation if we have access to the implementation. In any case,
, and
}
then
Sampling procedure - According to the previous assumption, we want to sample more where Dn
(cid:15)
is higher. To this end, we can build a probability density function (pdf) from Dn
(cid:15) , which is possible
since Dn
0. It remains to normalize it but in practice it is enough considering a distribution
(cid:15) ≥
(cid:15) . Here, to approximate d we use a Gaussian Mixture Model (GMM) with pdf dGMM that we
d
∝
using the Expectation-Maximization (EM) algorithm. N (cid:48) new data
Dn
(cid:15) (X1), ..., Dn
ﬁt to
(cid:15) (XN )
}
{
¯X1, ..., ¯XN (cid:48)
, can be sampled, with ¯X
dGMM. Finally, using the simulation software, we
points
{
}
f ( ¯X1), ..., f ( ¯XN (cid:48))
and train our DNN on the whole dataset.
, add it to
obtain
}
{

∼
f (X1), ..., f (XN )
}
Methodology recapitulation - Our methodology, which we call Taylor Based Sampling (TBS) is
recapitulated in Algorithm . Line 1: The choices of (cid:15), the number of Gaussian distribution nGMM
and N (cid:48) are not mandatory at this stage. Indeed, they are not a prerequisite to the computation of
∂kf (x), which is the computationally costly part of evaluating Dn
(cid:15) . It allows to choose parameters (cid:15)
and nGMM a posteriori. In this work, our choice criterion is to avoid sparsity of
over
S. We use the Python package scikit-learn [12], and more speciﬁcally the GaussianMixture
class. Line 2: Usually in physical simulations, the input subspace S is bounded. Without a priori
informations on f , we sample the ﬁrst points uniformly in a subspace S. Line 3-4: To compute the
derivatives of f and because we have access to its implementation, we use the python package jax
[9], which allows automatic differentiation of numpy code. Line 7-13: Because the support of a
GMM is not bounded, some points can be sampled outside S. We recommend to discard these points
and sample until all points are inside S. This rejection method is equivalent to sampling points from
a truncated GMM.

¯X1, ..., ¯XN (cid:48)
{

}

{

3 Application to an ODE system

We apply our method to the resolution of the Bateman equations, which is an ODE system :

(cid:26)∂tu(t) = vσa
∂tη(t) = vΣr

, with initial conditions

(cid:26)u(0) = u0,
η(0) = η0.

,

η(t)u(t),
η(t)u(t),

·
·
RM , Σr ∈

∈

a ∈

R+, η

(R+)M , σT

RM ×M . Here, f : (u0, η0, t)

with u
∈
For physical applications, M ranges from tens to thousands. We consider the particular case M = 1
so that f : R3
R2, with f (u0, η0, t) = (u(t), η(t)). The advantage of M = 1 is that we have
access to an analytic, cheap to compute solution for f . It allows to conduct extensive analyses for the
design of our methodology. Of course, this particular case can also be solved using a classical ODE
solver, which allows us to test it end to end. It can thus be generalized to higher dimensions (M > 1).
All DNN trainings have been performed in Python, with Tensorflow [1]. We used a fully connected
DNN with hyperparameters chosen using a simple grid search. The ﬁnal values are: 2 hidden layers,

(u(t), η(t)).

→

→

2

(S)
(cid:46) Note that order k = 0 builds

using the simulation software.

f (X1), ..., f (XN )
{

}

D(cid:15) with a GMM using EM algorithm to obtain a density dGMM

(cid:46) Rejection method to prevent EM to sample outside S

Algorithm: Taylor Based Sampling (TBS)

≤

∼ U

1 Require: (cid:15), N , N (cid:48), nGMM, n
2 Sample
, with X
X1, ..., XN }
{
3 for 0
n do
k
≤
∂kf (X1), ..., ∂kf (XN )
Compute
4
{
}
(cid:15) (X1), ..., Dn
Dn
5 Compute
using (2)
6 Approximate d
∼
7 i
1
←
N (cid:48) do
8 while i
≤
Sample ¯Xi ∼
9
if ¯Xi /
S then
∈
discard ¯Xi

(cid:15) (XN )
}

dGMM

{

10

11

12

else
i
13
14 Compute
15 Add

←
{

i + 1
f ( ¯X1), ..., f ( ¯XN (cid:48))
}
to
f (X1), ..., f (XN )
}
{

f ( ¯X1), ..., f ( ¯XN (cid:48))
}

{

"ReLU" activation function, and 32 units for each layer, trained with the Mean Squared Error (MSE)
loss function using Adam optimization algorithm with a batch size of 50000, for 40000 epochs and
R, with an
on N + N (cid:48) = 50000 points, with N = N (cid:48). We ﬁrst trained the model for (u(t), η(t))
∈
uniform sampling, that we call basic sampling (BS) (N (cid:48) = 0), and then with TBS for several values
10−4, n = 2
of n, nGMM and (cid:15) = (cid:15)(1, 1, 1), to be able to ﬁnd good values. We ﬁnally select (cid:15) = 5
and nGMM = 10. The data points used in this case have been sampled with an explicit Euler scheme.
This experiment has been repeated 50 times to ensure statistical signiﬁcance of the results.

×

4 Results and discussion

−

fθ(X)

f (X)
(
|

Table 1 summarizes the MSE, i.e. the L2 norm of the error of fθ and L∞ norm, with L∞(θ) =
) obtained at the end of the training phase. This last metric is important because
max
|
X∈S
the goal in computational physics is not only to be averagely accurate, which is measured with MSE,
but to be accurate over the whole input space S. Those norms are estimated using a same test data set
of Ntest = 50000 points. The values are the means of the 50 independent experiments displayed with
a 95% conﬁdence interval. These results reﬂect an error reduction of 6.6% for L2 and of 45.3% for
L∞, which means that TBS mostly improves the L∞ error of fθ. Moreover, the L∞ error conﬁdence
intervals do not intersect so the gain is statistically signiﬁcant for this norm.

Table 1: Comparison between BS and TBS

Sampling L2 error (
BS
TBS

1.22
1.14

×
0.13
0.15

10−4) L∞ (
5.28
2.96

±
±

10−1) AEG(

10−2) AEL(

10−2)

×

±
±

0.47
0.37

-
2.51

×

0.07

±

×

-
0.42

0.008

±

(cid:15) increases when U0 →

Figure 1a shows how the DNN can perform for an average prediction. Figure 1b illustrates the
beneﬁts of TBS relative to BS on the L∞ error (Figure 2b). These 2 ﬁgures conﬁrm the previous
observation about the gain in L∞ error. Finally, Figure 2a displays u0, η0 →
(cid:15) (u0, η0, t)
w.r.t. (u0, η0) and shows that Dn
0. TBS hence focuses on this region.
Note that for the readability of this plots, the values are capped to 0.10. Otherwise only few
points with high Dn
gθT BS (u0, η0), with
2
2 where θBS and θT BS denote the parameters
fθ(u0, η0, t)
max
gθ : u0, η0 →
(cid:107)
0≤t≤10(cid:107)
obtained after a training with BS and TBS, respectively. It can be interpreted as the error reduction
achieved with TBS. The highest error reduction occurs in the expected region. Indeed more points are
sampled where Dn
(cid:15) is higher. The error is slightly increased in the rest of S, which could be explained
by a sparser sampling on this region. However, as summarized in Table 1, the average error loss (AEL)
of TBS is around six times lower than the the average error gain (AEG), with AEG = Eu0,η0(Z(cid:49)Z>0)

(cid:15) are visible. Figure 2b displays u0, η0 →

gθBS (u0, η0)

f (u0, η0, t)

max
0≤t≤10

Dn

−

−

3

and AEL = Eu0,η0 (Z(cid:49)Z<0) where Z(u0, η0) = gθBS (u0, η0)
and AEL are estimated using uniform grid integration, and averaged on the 50 experiments.

gθT BS (u0, η0). In practice, AEG

−

t → fθ(u0, η0, t) for randomly chosen (u0, η0), for fθ obtained with the two samplings. 1b: t →
1a:
fθ(u0, η0, t) for (u0, η0) resulting in the highest point-wise error with the two samplings. 2a: u0, η0 →
max
0≤t≤10

(cid:15) (u0, η0, t) w.r.t. (u0, η0). 2b: u0, η0 → gθBS (u0, η0) − gθT BS (u0, η0),

Dn

nn+1
i

derivatives for each Dn

∂kf (x)
, i.e. no ×
|
|

Results are promising for this case of the Bateman equations (M = 1). We selected a simple
numerical simulation to efﬁciently test the initial assumption and elaborate our methodology. Its
application to more expensive physical simulations is the next step of this work. By then, several
problems have to be tackled. First, TBS does not scale well to higher dimensions, because it involves
the computations of
(cid:15) . This issue is less important
when using automatic differentiation than ﬁnite difference, and we can compute the derivatives for
only few points (i.e. chose N < N (cid:48)) to ease it, but we will investigate on how to traduce the initial
assumption for high dimensions. Moreover, the exploration of the input space is more difﬁcult in this
case, because the initial sampling is more sparse for a same N when ni increases. In this work, (cid:15), N (cid:48),
n have only been empirically chosen, and we arbitrarily selected a GMM to approximate d. These
choices have to be questioned in order to improve the methodology. Finally, this paper focused on
accuracy rather than performance, whereas performance is the goal of using a surrogate in place of a
simulation code. Extending TBS to higher dimensions will allow to investigate this aspect. Beside,
DNN may not be the best ML model in every situation, in terms of performances, but the advantage
of TBS is that it can be applied to any ML model.

5 Conclusion

We described a new approach to sample training data based on a Taylor approximation in the context
of ML for approximation of physical simulation codes. Though non speciﬁc to Deep Learning, we
applied this method to the approximation of the solution of a physical ODE system by a Deep Neural
Network and increased its accuracy for a same model architecture.
In addition to the leads mentioned above, the idea to use the derivatives of numerical simulations to
better train ML models should be explored. This idea has already been investigated in other ﬁelds
such as gradient-enhanced kriging [7] or Hermite interpolation [14] and could lead to ML approaches
based on higher orders. An example of application could be to include the derivatives as new training
points and making a DNN learn these derivatives.

4

0246810t0.00.51.01.52.02.53.03.5f(u0,η0,t)Figure1afu0,η0,t)fθ(u0,η0,t)withBSfθ(u0,η0,t)withTBS0246810t0.00.51.01.52.0f(u0,η0,t)Figure1bf(u0,η0,t)fθ(u0,η0,t)withBSfθ(u0,η0,t)withTBS012η00.00.51.0U0Figure2a0.000.020.040.060.080.10Dn(cid:15)012η00.00.51.0U0Figure2b−0.010−0.0050.0000.0050.010L2errorgainReferences

[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey
Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens,
Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda
Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng.
TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from
tensorﬂow.org.

[2] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning.

In
Proceedings of the 26th Annual International Conference on Machine Learning, ICML ’09, pages 41–48,
New York, NY, USA, 2009. ACM.

[3] Adrien Bernède and Gaël Poëtte. An unsplit monte-carlo solver for the resolution of the linear boltzmann
equation coupled to (stiff) bateman equations. Journal of Computational Physics, 354:211–241, 02 2018.

[4] M. Bisi and L. Desvillettes. From reactive boltzmann equations to reaction–diffusion systems. Journal of

Statistical Physics, 124(2):881–912, Aug 2006.

[5] Jan Dufek, Dan Kotlyar, and Eugene Shwageraus. The stochastic implicit euler method – a stable coupling

scheme for monte carlo burnup calculations. Annals of Nuclear Energy, 60:295 – 300, 10 2013.

[6] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data. In
Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, pages
1183–1192. JMLR.org, 2017.

[7] Zhong-Hua Han, Yu Zhang, Chen-Xing Song, and Ke-Shi Zhang. Weighted gradient-enhanced kriging for
high-dimensional surrogate modeling and design optimization. AIAA Journal, 55(12):4330–4346, 2017.

[8] Tang Jie and Pieter Abbeel. On a connection between importance sampling and the likelihood ratio policy
gradient. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors,
Advances in Neural Information Processing Systems 23, pages 1000–1008. Curran Associates, Inc., 2010.

[9] Matt Johnson, Roy Frostig, Dougal Maclaurin, and Chris Leary. Jax: Autograd and xla. https://github.

com/google/jax.

[10] Lu Lu, Xuhui Meng, Zhiping Mao, and George E. Karniadakis. Deepxde: A deep learning library for

solving differential equations. CoRR, abs/1907.04502, 2019.

[11] D. Lucor, C. Enaux, H. Jourdren, and P. Sagaut. Stochastic design optimization: Application to reacting

ﬂows. Computer Methods in Applied Mechanics and Engineering, 196(49):5047 – 5062, 2007.

[12] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011.

[13] Burr Settles. Active Learning. Synthesis Lectures on Artiﬁcial Intelligence and Machine Learning. Morgan

& Claypool Publishers, 2012.

[14] A. Spitzbart. A generalization of hermite’s interpolation formula. The American Mathematical Monthly,

67(1):42–46, 1960.

[15] B. Sudret, S. Marelli, and J. Wiart. Surrogate models for uncertainty quantiﬁcation: An overview. In 2017
11th European Conference on Antennas and Propagation (EUCAP), pages 793–797, March 2017.

[16] Christoph Zimmer, Mona Meister, and Duy Nguyen-Tuong. Safe active learning for time-series modeling
with gaussian processes. In Proceedings of the 32Nd International Conference on Neural Information
Processing Systems, NIPS’18, pages 2735–2744, USA, 2018. Curran Associates Inc.

5

