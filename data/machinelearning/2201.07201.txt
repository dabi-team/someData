2
2
0
2

n
a
J

8
1

]
E
S
.
s
c
[

1
v
1
0
2
7
0
.
1
0
2
2
:
v
i
X
r
a

1

Studying Popular Open Source Machine
Learning Libraries and Their Cross-Ecosystem
Bindings

Hao Li, Cor-Paul Bezemer

Abstract—Open source machine learning (ML) libraries allow developers to integrate advanced ML functionality into their own
applications. However, popular ML libraries, such as TensorFlow, are not available natively in all programming languages and software
package ecosystems. Hence, developers who wish to use an ML library which is not available in their programming language or
ecosystem of choice, may need to resort to using a so-called binding library. Binding libraries provide support across programming
languages and package ecosystems for a source library. For example, the Keras .NET binding provides support for the Keras library in
the NuGet (.NET) ecosystem even though the Keras library was written in Python. In this paper, we conduct an in-depth study of 155
cross-ecosystem bindings and their development for 36 popular open source ML libraries. Our study shows that for most popular ML
libraries, only one package ecosystem is ofﬁcially supported (usually PyPI). Cross-ecosystem support, which is available for 25% of the
studied ML libraries, is usually provided through community-maintained bindings, e.g., 73% of the bindings in the npm ecosystem are
community-maintained. Our study shows that the vast majority of the studied bindings cover only a small portion of the source library
releases, and the delay for receiving support for a source library release is large.

Index Terms—Software engineering for machine learning, Software package ecosystems, Cross-ecosystem library usage.

(cid:70)

1 INTRODUCTION

M ACHINE learning (ML) has become extremely popular

in the last decade. Nowadays, there exist many ML
applications in our daily lives, such as email spam ﬁlters,
recommendation systems, and voice assistants. To provide
ML features in an application, most developers rely on well-
developed open source ML libraries, such as TensorFlow [1]
or PyTorch [43].

These open source ML libraries provide easy-to-use in-
terfaces for software developers to use ML techniques in
their projects. However, these libraries often target only
one programming language and publish to one software
package ecosystem. For example, scikit-learn [44], a pop-
ular ML library which provides various ML algorithms,
is written in Python and publishes to PyPI. Thus, Python
developers can download and use the source code of scikit-
learn directly from its GitHub repository or include the
published package through PyPI. But, developers in other
programming languages cannot use this library as easily.

There exist several workarounds that allow a developer
to use a library that was not written in their preferred
language. First, they could choose an alternative but similar
library that is written in their preferred language. However,
such a similar library may not exist, and even if it does,
it may only provide a subset of the required functionality.
Another workaround is to recode the library from scratch,
but this approach is error-prone and requires a large amount
of work. Finally, the developer can use a binding for the
library in their preferred language, which would allow the
developer to use the original library’s functionality without

• Hao Li and Cor-Paul Bezemer are with the Analytics of Software, GAmes
And Repository Data (ASGAARD) Lab, University of Alberta, Edmon-
ton, AB, Canada. Email: li.hao@ualberta.ca, bezemer@ualberta.ca.

recoding the library. However, there is not much known
about this type of reuse.

In our study, we focus on popular open source ML
libraries that are supported across software package ecosys-
tems. We perform a large-scale study of 155 cross-ecosystem
bindings for 36 popular open source ML libraries in the
software package ecosystems of 13 programming languages.
In particular, we address the following research ques-
tions (RQs):

RQ1. What are the most popular ML libraries and in
which software package ecosystems are they avail-
able?
We found 146 popular ML libraries and 99% of these
libraries support at least one software package ecosys-
tem ofﬁcially. Notably, 85% of them are published to
PyPI.

RQ2. Which cross-ecosystem bindings exist for popular

ML libraries?
25% of the popular ML libraries can be found in mul-
tiple software package ecosystems. The most common
combinations of publishing these cross-ecosystem
bindings are PyPI with other package ecosystems,
such as PyPI and npm, or PyPI and NuGet.

RQ3. How are cross-ecosystem bindings for popular ML

libraries maintained?
Cross-ecosystem bindings have low coverage for the
releases of popular ML libraries and high delays
for supporting new releases. Notably, the situation
is worse (i.e., lower coverage and higher delays) in
the bindings that are not maintained by the ofﬁcial
organization of the source library.

Our results can help developers to choose a suitable

 
 
 
 
 
 
software package ecosystem before integrating a popular
ML library. In addition, developers can use our method-
ology to evaluate a chosen binding before adapting it.
Finally, this paper provides suggestions for communities of
software package ecosystems to better support and main-
tain cross-ecosystem bindings for popular ML libraries. We
have provided a replication package [35], containing details
about 36 popular cross-ecosystem ML libraries and their 155
bindings. In addition, it contains the results of our analysis
in which we matched 3,838 binding releases to 3,756 ML
library releases.

Paper Organization. The rest of this paper is orga-
nized as follows. Section 2 gives background information
about our study. Section 3 discusses related work. Section 4
presents our methodology. Section 5 presents the ﬁndings
of our three RQs. Section 6 discusses the implications of
our ﬁndings. Section 7 outlines threats to the validity of our
study. Section 8 concludes this paper.

2 BACKGROUND
In this section, we give background information about soft-
ware package ecosystems and cross-ecosystem bindings for
ML libraries.

2.1 Software Package Ecosystems

Traditionally, developers of open source libraries published
their source code in a source code repository like git [12].
Developers who wish to use those libraries could then
download these libraries directly from the source code
repositories. However, developers had to resolve the li-
brary’s dependencies and build the library manually. To
help developers integrate a library more easily, the releases
of software libraries can be published to a software package
ecosystem.

Most modern programming languages come with an of-
ﬁcial package manager and a package registry. This package
manager, the package registry and all the packages are the
key components of a software package ecosystem. Usually,
a package manager helps developers to manage the depen-
dencies of their applications, for example, by downloading
a speciﬁc version of a dependency when the application is
installed. In addition, package managers help developers
publish their applications to the software package ecosys-
tem. Most software package ecosystems of programming
languages will provide a website for developers to search
and browse the information of stored packages. Some ex-
amples of software package ecosystems are Maven for Java,
PyPI for Python, and npm for JavaScript.

2.2 Cross-Ecosystem Bindings for ML Libraries

We consider the source code repository of an ML library
as the source library, and its supportive packages in differ-
ent software package ecosystems as cross-ecosystem bindings
for the source library. For example, tensorﬂow in PyPI
and tfjs in npm are cross-ecosystem bindings for the same
ML library (TensorFlow), even though they have different
names and reside in different software package ecosystems.
Moreover, cross-ecosystem bindings can be maintained by
the community or the ofﬁcial organization of the source

2

library. For instance, tensorﬂow in PyPI and tfjs in npm
are both maintained by the ofﬁcial organization of the
TensorFlow source library (i.e., the owners of their source
code repositories are the same), and we consider these
two bindings as ofﬁcially-maintained bindings. In contrast,
TensorFlowSharp in NuGet, a binding for TensorFlow, is
considered as a community-maintained binding since its owner
is not the same as that of TensorFlow, but an individual
developer. Note that the owner of a community-maintained
binding can also be an organization that is not the same as
the ofﬁcial organization of the source library.

ML libraries and their cross-ecosystem bindings do not
necessarily share the same releases. In our study, we con-
sider the tags on the source code repositories as the releases
of the ML libraries. Cross-ecosystem bindings can have
different versions, release policies, and version numbers
than their source library. For instance, a cross-ecosystem
binding may choose to only support a proportion of the
releases of its source library. Also, after the source library
publishes a release, there may be a delay before a cross-
ecosystem binding supports that release (if at all).

Cross-ecosystem bindings can be written in a different
programming language (i.e., the primary supported lan-
guage of the package ecosystem) than their source library.
These bindings are often built on top of the source library
and call the source library using a foreign function inter-
face (FFI). For example, the tfjs-node binding in npm calls
the TensorFlow C binary in the backend. In addition, some
bindings choose to reimplement their source library in a
different programming language. For instance, tfjs in npm
is written in JavaScript and does not rely on the TensorFlow
source library.

3 RELATED WORK

In this section, we discuss prior empirical studies of ML
libraries, and related work on software ecosystems and the
foreign function interface.

3.1 Empirical Studies of ML Libraries

Many researchers have studied different concepts of ML
libraries [2] [26] [27] [29] [55] [58]. However, no studies
have considered ML libraries together with software pack-
age ecosystems. To the best of our knowledge, this is the
ﬁrst paper to focus on ML libraries in software package
ecosystems, especially, ML libraries that can be found in
multiple software package ecosystems.

Several studies focused on the problems that developers
could face when using ML libraries. Islam et al. [29] mined
Q&A of ten ML libraries on StackOverflow, and reported
that three types of problems occurred frequently (i.e., type
mismatch, data cleaning, and parameter selection). Zhang
et al. [58] extracted bugs of TensorFlow applications from
Q&A pages on StackOverflow and investigated the root
causes of these problems. They provided strategies for
developers to locate bugs and ﬁx them. Thung et al. [55]
extracted bugs from issue reports for three ML libraries,
and noticed that most bugs are related to algorithms and
methods.

Several researchers have conducted comparison studies
of multiple ML libraries. Guo et al. [26] compared the devel-
opment and deployment processes of four ML libraries un-
der the same conﬁguration for training of the same models.
They found that using different ML libraries can lead to dif-
ferent levels of accuracy. Bahrampour et al. [2] investigated
ﬁve ML libraries to compare their performance in extensibil-
ity, hardware utilization, and speed. Han et al. [27] collected
projects that depend on PyTorch, TensorFlow, and Theano
on GitHub, and observed four frequent applications (i.e.,
image and video, NLP, model theory, and acceleration).
In addition, most projects depend on these three libraries
directly instead of transitively.

3.2 Software Ecosystems

“Software ecosystems” are studied from several angles and
even using different deﬁnitions [23], [38], [40]. The “software
package ecosystems” term covers a subset of the software
ecosystems term. In this paper, we study ML libraries that
can be found across multiple software package ecosystems.
Very few studies have focused on cross-ecosystem bind-
ings. Constantinou et al. [15] investigated twelve software
package ecosystems and observed that only a small propor-
tion of packages are cross-ecosystem packages. However,
these cross-ecosystem packages could have a high impact in
ecosystems through the dependency network.

Many studies have focused on software package ecosys-
tems. In our prior work [36], we studied the release-level
deprecation mechanism in Cargo (Rust) ecosystem and
found that the deprecated releases propagate through the
dependency network and lead to broken releases. German
et al. [25] mined packages in CRAN and reported that most
dependencies point to a core set of packages in the ecosys-
tem. This phenomenon was also observed in another active
software package ecosystem – npm [56]. Cogo et al. [14] stud-
ied dependency downgrades in npm and found three rea-
sons behind the downgrades: defects, unexpected changes,
and incompatibilities. Constantinou and Mens [16] [17]
studied social aspects in ecosystems and found that the
developers are more likely to abandon an ecosystem if
they do not participate in the community, and the Ruby
[33] proposed
ecosystem is being abandoned. Kula et al.
a model for visualizing dependencies in ecosystems, and
show that CRAN packages tend to use the latest releases, but
Maven packages stay with the older versions. Decan and
Mens [20] investigated package releases in three software
package ecosystems and observed that most pre-releases do
not become ≥ 1.0.0 releases. Moreover, software package
ecosystems have different practices, policies, and tools for
handling breaking changes [7] [8].

In addition, researchers studied other types of software
ecosystems. Blincoe et al.
[5] [6] proposed a reference
coupling method to identify software ecosystems in GitHub
as well as the dependencies in the ecosystems. Osman
and Baysal [42] identiﬁed the Bitcoin software ecosystem
in GitHub and assessed it as a healthy ecosystem. Fur-
thermore, many researchers studied the health of software
ecosystems [18] [30]. Kula and Robles [32] investigated
four abandoned software ecosystems and observed that
all these ecosystems had a successor or their components

TABLE 1: Overview of the dataset

3

Ecosystem

Language

# of packages

# of releases

JavaScript
PHP
Python
C#
Java
Ruby

npm
Packagist
PyPI
NuGet
Maven
RubyGems
CocoaPods Objective-C
CPAN
Cargo
Clojars
CRAN
Hackage
Pub

Perl
Rust
Clojure
R
Haskell
Dart

1,277,221
313,575
232,050
199,671
184,890
161,650
68,085
37,496
35,695
24,295
16,710
14,484
10,143

11,400,714
1,766,576
1,752,770
2,445,003
2,799,513
1,055,874
365,782
290,847
195,011
116,945
94,716
98,572
75,388

Total

2,575,965

22,457,711

were adopted by other systems. Bavota et al.
[3] found
that projects in the Apache ecosystem get updates when
the dependencies published releases for breaking changes
or bug ﬁxes. Souza et al.
[19] studied social aspects in
proprietary mobile software ecosystems and observed that
most developers chose a speciﬁc ecosystem based on oth-
ers’ recommendation. Researchers also studied information
security and business factors in mobile software ecosys-
tems [51] [52].

3.3 Foreign Function Interfaces

Foreign function interfaces (FFI) bridge the gap between
different programming languages and allow developers to
reuse libraries written in other languages. To verify the cor-
rectness of existing bindings, Furr and Foster [24] presented
a static checking system that analyzes both bindings and
their source library. In addition, Lee et al. [34] built bug
detection tools for the FFI in Jave and Python by performing
dynamic analysis. Nakata et al. [41] categorized link models
and fault models of FFI and proposed a logging framework
to track the information ﬂow for each model.

Moreover, wrapping up a function to call a library from
another programming language is not always applicable,
Chiba [13] proposed a framework based on code migra-
tion to solve this problem. Besides writing the codes of
FFI manually, Finne et al. [22] used an interface deﬁnition
language to allow Haskell to communicate with both C and
COM. In addition, Reppy and Song [45] developed a tool to
generate foreign interfaces for high-level languages to use
the libraries written in C.

4 METHODOLOGY

In this section, we introduce the methodology of our study
of popular ML libraries and their cross-ecosystem bindings.
Figure 1 gives an overview of our methodology.

4.1 Collecting ML Packages in Software Package
Ecosystems
We used the Libraries.io dataset [31] which was up-
dated on January 12, 2020 as our primary data source. It
contains information (e.g., releases, creation date, depen-
dencies) of 4,612,919 packages from 38 software package

4

Fig. 1: Overview of our methodology.

ecosystems. We excluded ecosystems that focus on a spe-
ciﬁc domain like Sublime and WordPress, have a very
small number of packages, like Shards (33 packages) and
PureScript (384 packages), or do not store information
about releases like Go. Also, we excluded ecosystems that
contain duplicated packages of other ecosystems, for ex-
ample, most packages in Bower can be found in npm.
Finally, we selected 13 software package ecosystems. Table 1
shows the supported programming language, the number of
packages, and the number of releases in these 13 ecosystems.
Braiek et al. [4] proposed a keyword-matching approach
to extract ML projects from GitHub. We used a similar
approach to collect ML packages in software ecosystems. We
searched the “Name”, “Description”, “Keywords”, “Repos-
itory Name with Owner”, “Repository Description”, and
“Repository Keywords” ﬁelds of the packages in the dataset
for the following keywords: “machine learning”, “deep
learning”, “statistical learning”, “neural network”, “super-
vised learning”, “unsupervised learning”, “reinforcement
learning”, and “artiﬁcial intelligence”. If any of the key-
words are found, the package is identiﬁed as an ML pack-
age. The regular expressions we used to identify ML pack-
ages also search for keywords which contain the characters
“ ”, “-”, or “,” between words such as “machine learning”.
We extracted 7,963 ML packages from the dataset.

4.2 Collecting Popular ML Libraries and Their Bindings

First, we went through the collected 7,963 ML packages to
ﬁlter out the packages that do not have a link to their source
code repositories (e.g., GitHub and GitLab). After going

through the non-forked repositories of these packages, we
extracted 299 repositories that have more than 1,000 stars to
select popular ones. Two authors manually inspected these
299 repositories to identify whether they are ML libraries
or just applications in which ML is used, then merged the
results and discussed the disagreements. In this identiﬁ-
cation process, we also excluded repositories that provide
supplemental tools, perform visualization of data, collect
datasets and models, give examples and tutorials, etc. Then,
we ﬁltered out duplicate repositories of the same ML library,
such as the source code repository of bindings for Tensor-
Flow in Cargo (TensorFlow Rust ) and npm (TensorFlow.js).
As suggested by Borges and Valente [9], we excluded 24
libraries which received most of their stars during a viral
growth. Finally, we collected 146 ML libraries. The list of
studied libraries is available in our replication package [35].

To collect bindings for these 146 ML libraries, we
searched each name of
the libraries in “Name”, “De-
scription”, “Keywords”, “Repository Name with Owner”,
“Repository Description”, and “Repository Keywords”
ﬁelds of the packages in the dataset. The matched packages
were considered as the candidates of bindings for these
libraries. Afterwards, we manually inspected these candi-
dates and ﬁltered out the ones which are not a binding to
the 146 ML libraries. Here is an example of the false positive
candidates: PySceneDetect [11] is a library for detecting and
analyzing video scenes based on the OpenCV [10] library.
Even though we found “OpenCV” in its “Description” ﬁeld,
it is not a binding for OpenCV. Finally, we selected 265
bindings for the studied 146 ML libraries.

Libraries.ioSelect ML packages4.1 Collecting ML Packages in Software Package Ecosystems4.2 Collecting Popular ML Libraries and Their BindingsSelect popular  ML repositories299 repositories7,963 packages4.3 Collecting Releases of Cross-Ecosystem ML Libraries3,756 tags  & 3,838 versions36 ML libraries & 155 bindingsExtract tags of ML  libraries and versions of their bindingsLibraries.io: versionsand tags RQ1. What are the most popular     ML libraries and in which software     ecosystems are they available? RQ3. How are cross-ecosystem     bindings for popular ML libraries     maintained? RQ2. Which cross-ecosystem     bindings exist for popular ML    libraries?146 ML libraries & 265 bindingsIdentify ML libraries that  have cross-ecosystem bindingsIdentify ML libraries and collect their bindings5

Fig. 3: Delays between the creation dates of the ﬁrst tag
of popular ML source repositories and their ﬁrst published
release in a software package ecosystem.

Findings. The ofﬁcial organizations of popular ML source
libraries prefer to focus on one software package ecosys-
tem. There are 145 out of 146 popular ML libraries that
published 178 ofﬁcially-maintained bindings to one or more
software package ecosystems. In other words, 99% of the
popular ML libraries are supported in at least one ecosystem
by their ofﬁcial organization. In addition, we observed that
86% (125 out of 145) of these libraries only published 1
ofﬁcially-maintained binding to an ecosystem. Particularly,
Tesseract OCR [48] is the only library that did not publish
an ofﬁcially-maintained binding, but we found community-
maintained bindings for it in 9 software package ecosys-
tems.

PyPI is the most popular software package ecosys-
tem for ML libraries to publish the ofﬁcially-maintained
bindings. Figure 2 shows that 88% (129 out of 146) of
the popular ML libraries support an ofﬁcially-maintained
binding in PyPI. Moreover, we noticed that 65% (95 out of
146) of the popular ML libraries use Python as their primary
programming language. This observation is in line with
Braiek et al. [4], who also reported that Python is the most
popular language for machine learning development. Since
PyPI is the ofﬁcial software package ecosystem of Python,
it could explain the reason why PyPI is the most popular
ecosystem for ML libraries. In contrast, the Pub ecosystem
does not have any ofﬁcially-maintained binding for these
popular ML libraries.

43% of the popular ML libraries published releases to
an ecosystem before the ﬁrst release in their source repos-
itories. Figure 3 shows that the median delays between the
ﬁrst tag of popular ML source repositories and their ﬁrst
release in an ecosystem is 0, and 58 out of 134 of the libraries
have negative delays. We noticed that 76% (44 out of 58)
of these libraries published pre-releases to an ecosystem
before their ﬁrst tag on GitHub. The reason could be that
the owners of these libraries gained conﬁdence after these
pre-releases and started publishing tags for more stable
versions. Besides, 10% (6 out of 58) of these libraries have
a negative small delay (within one week) and each of them
used the same version number for the ﬁrst release and the
ﬁrst tag.

(cid:3)

(cid:2)

RQ1 Summary: PyPI dominates the availability
of ofﬁcially-maintained bindings for popular open
source ML libraries. Most popular open source ML
libraries are ofﬁcially supported in one ecosystem
only.

(cid:0)

(cid:1)

Fig. 2: Distribution of 178 ofﬁcially-maintained bindings for
the 146 studied popular ML source libraries in software
package ecosystems.

4.3 Collecting Releases of Cross-Ecosystem ML Li-
braries

We went through the 146 popular ML source libraries and
their 265 bindings and selected libraries that have bindings
in at least two ecosystems as cross-ecosystem ML libraries.
For example, spaCy [28] is an ML library with three cross-
ecosystem bindings: spacy in PyPI, spacy-nlp in npm, and
spacyr in CRAN. We found 36 out of 146 ML source li-
braries that have cross-ecosystem bindings and collected
155 bindings for them. Moreover, Libraries.io records
the published versions of packages in ecosystems and tags
of repositories. The information includes the name of the
version/tag and the published timestamp. Eventually, we
extracted 3,756 tags of the 36 cross-ecosystem ML libraries
and 3,838 releases of their 155 cross-ecosystem bindings.

5 RESULTS
This section presents the results of our three RQs. For each
RQ, we present the motivation, approach, and ﬁndings.

5.1 RQ1: What are the most popular ML libraries and in
which software package ecosystems are they available?

Motivation. In this research question, we investigate how
popular ML libraries are distributed across software pack-
age ecosystems. In particular, we investigate the bindings
that are published by the ofﬁcial organization of the source
library. The results of this research question can give us an
overview of the popularity of software package ecosystems
for ML libraries.

Approach. To understand the preferred software package
ecosystems of these ML libraries, we selected 178 bindings
that are maintained by the ofﬁcial organization of the li-
braries from the collected 146 popular ML libraries. Next,
we analyzed the distribution of these ofﬁcially-maintained
bindings in ecosystems. Additionally, we extracted the cre-
ation date of the ﬁrst tag of these popular ML source
repositories and the date of the ﬁrst published release of
the ofﬁcially-maintained bindings. After this, we calculated
the delays between the ﬁrst tag of these ML source reposi-
tories and the ﬁrst published releases in the ecosystems. We
excluded the libraries that do not have any tags in the source
code repository and collected the creation dates for 134 of
the 146 popular ML source libraries.

PyPIMavennpmNuGetCRANCocoaPodsCargoRubyGemsHackagePackagistClojarsCPANPub020406080100120Number of bindings2001000100200300Days0TABLE 2: Basic information about the popular ML libraries that have cross-ecosystem bindings.

6

ML library

# ecosystems

# stars Description

BigDL
brain.js
DeepSpeech
dlib
Face Recognition
H2O
HanLP
HDBSCAN
ImageAI
Implicit
InterpretML
Keras
LightGBM
Magenta
Mahout
Malm ¨o
mlpack
MXNet
NLTK
ONNX
ONNX Runtime
OpenAI Gym
OpenCV
PredictionIO
PySyft
PyTorch
Rasa
scikit-learn
Seldon Core
spaCy
StanfordNLP
TensorFlow
Tesseract OCR
Vowpal Wabbit
XGBoost
xLearn

2
2
4
6
2
4
2
2
2
2
2
5
3
2
2
2
4
7
8
4
4
7
11
8
2
6
3
3
2
3
2
10
9
7
8
2

Recommendation algorithms for implicit feedback datasets

3,177 Distributed deep learning library for Apache Spark
10,584
Support for neural networks in JavaScript
12,710 A speech-to-text engine based on TensorFlow
8,412 A toolkit for making real world machine learning and data analysis applications
30,898 A framework to recognize and manipulate faces based on deep learning
4,513 A platform for distributed machine learning
17,627 An NLP library
1,447 An implementation of HDBSCAN clustering
4,252 A library for deep learning and computer vision
1,680
2,224 A library for training interpretable glassbox models and explaining blackbox systems
45,995 A framework to provide human-friendly APIs based on TensorFlow
10,242 A gradient boosting framework
14,419 A library for generating music and art based on machine learning
1,582 A distributed linear algebra framework
3,342 An environment of Minecraft for AI experimentation and research
3,016 A library to provide machine learning algorithms
18,251 A deep learning framework
8,498 An NLP library
7,595
1,561 A runtime engine for ONNX models
19,351 A toolkit for developing and comparing reinforcement learning algorithms
41,126 A computer vision library
12,226 A machine learning server for infrastructure management
4,121 A library for secure and private Deep Learning
35,004 A machine learning framework
7,436 A machine learning framework to automate text-and voice-based conversations
38,756 A framework to provide machine learning algorithms
1,296 An MLOps framework based on Kubernetes
15,161 An NLP library
2,597 An NLP library

Support for ML interoperability

139,939 A machine learning framework
32,078 An OCR engine that uses deep learning
6,767
17,996 A gradient boosting framework
2,587 A machine learning library for large-scale sparse data

Techniques to solve interactive machine learning problems

5.2 RQ2: Which cross-ecosystem bindings exist for
popular ML libraries?

Motivation. Some ML libraries get extremely popular,
which in turn attracts more community support for the
source library. However, as shown in RQ1, the vast ma-
jority of popular ML libraries is only ofﬁcially supported
in one ecosystem. As these ML libraries become mature,
developers in other ecosystems may wish to use them as
well. In this research question, we investigate which popular
ML libraries have such cross-ecosystem bindings. Also, we
investigate whether certain (combinations of) ecosystems
are favoured for cross-ecosystem bindings for popular ML
libraries.

Approach. We collected the basic information (e.g., number
of stars and creation date) of the 36 cross-ecosystem ML
libraries (see our replication package [35]) and analyzed the
distribution of 155 bindings for these 36 libraries across
ecosystems. To better understand in which combinations
of software package ecosystems these libraries reside, we
counted the ecosystem-pairs for each library. For example, if
a library has bindings in PyPI, npm, and NuGet, we will
count three ecosystem-pairs PyPI-npm, PyPI-NuGet, and
npm-NuGet. If an ecosystem-pair appears more frequently
than others, it implies that those two ecosystems are more
likely to be supported together by a popular ML library.

Findings. 25% of the popular ML libraries have cross-

ecosystem bindings. There are 36 out of 146 popular ma-
chine learning libraries that are supported in multiple soft-
ware ecosystems. Table 2 presents the number of stars, the
number of supported ecosystems, and a short description of
these 36 ML libraries with cross-ecosystem bindings.

Popular ML libraries with cross-ecosystem bindings
are supported in a median of three software package
ecosystems. Figure 4 shows the number of ecosystems that
are supported by popular cross-ecosystem ML libraries. The
most-supported ML library is OpenCV which is supported
in 11 ecosystems, followed by TensorFlow and Tesseract
OCR which are supported in 10 and 9 ecosystems respec-
tively. In addition, we observed that 42% (15 out of 36
libraries) of these cross-ecosystem ML libraries resided in
2 different ecosystems. For example, cross-ecosystem bind-
ings of the NLP library HanLP can be found in PyPI and
Maven.

Fig. 4: The distribution of the number of software package
ecosystems in which cross-ecosystem ML libraries reside.

234567891011Number of ecosystems37

Fig. 6: The process of identifying which version of the source
library is supported by a speciﬁc release from a binding.

5.3 RQ3: How are cross-ecosystem bindings for popu-
lar ML libraries maintained?

Motivation. Cross-ecosystem bindings for popular ML li-
braries can be developed and maintained differently than
their source library. If a binding only supports a small
proportion of the releases of its source library and has a
high delay to get an update, developers who rely on this
binding might have to stick with a version for a long time.
This situation will be problematic when a bug was ﬁxed
in the source library but not updated in the binding. As
RQ1 showed, the vast majority of ML libraries are only
ofﬁcially supported in one ecosystem. Hence, many cross-
ecosystem bindings are dependent on community support.
In this research question, we investigate the releases of pop-
ular ML source libraries and their cross-ecosystem bindings
to understand the development and maintenance of these
bindings. The results can be helpful for developers when
they are choosing a binding.

Approach. We extracted the releases which have a valid
version number (e.g., v1.0.0, release-1.1.0, and 1.0.0rc1) from
all releases from the 36 popular ML libraries and their 155
cross-ecosystem bindings by using the version handling
library packaging [53]. Afterwards, we investigated for each
release of the cross-ecosystem bindings which release of the
source library it supports. We searched for evidence of this
support in the following places of the binding releases:

1. README: When the supported version of the source

library is mentioned explicitly.

2. Git Submodules [12]: When the source code of the
supported version of the source library is included as a
submodule.

(cid:1)

Fig. 5: Combinations of software package ecosystems in
which cross-ecosystem ML libraries are available. The di-
agonal elements represent the number of cross-ecosystem
bindings that can be found in that ecosystem. The other
elements represent the number of bindings that can be
found in both ecosystems (i.e., ecosystems in the row and
column).

58% of the popular ML libraries with cross-ecosystem
bindings can be found in both PyPI and npm. Figure 5
presents the relations of each ecosystem-pair that cross-
ecosystem ML libraries chose to support. We noticed that
PyPI-npm is the most popular ecosystem-pair. While 97% of
the cross-ecosystem bindings in PyPI are maintained by the
ofﬁcial organization, 73% of the bindings in npm are main-
tained by the community. One reason could be that Python
is the most popular language for ML development [4] and
JavaScript has been the most commonly used programming
language for nine years [50]. Hence, there could be a need
for ML in npm, resulting in more community support for
such bindings. In addition, we observed other popular pairs
like npm-NuGet, CRAN-npm, and RubyGems-NuGet.

Maven is not a popular ecosystem for cross-ecosystem
ML libraries. Although Maven is a popular software ecosys-
tem which has more ofﬁcially-maintained bindings for
the 146 studied ML libraries than RubyGems and CRAN
(according to Figure 2), both RubyGems and CRAN have
more bindings for the 36 cross-ecosystem ML libraries.
Especially, RubyGems has a relatively small number of
ofﬁcially-maintained bindings for ML libraries but has the
fourth-highest number of bindings for cross-ecosystem ML
libraries. The reason could be that creating bindings for ML
libraries in R (CRAN) or Ruby (RubyGems) is easier than
Java (Maven) or that Java has better native ML libraries.

(cid:0)

(cid:3)

RQ2 Summary: 25% of popular ML libraries have
cross-ecosystem bindings and 58% of these cross-
ecosystem bindings are distributed to at least 3
software package ecosystems. Moreover, the most
popular combination for publishing these cross-
ecosystem bindings is PyPI and npm.

(cid:2)

CargoClojarsCocoaPodsCPANCRANHackageMavennpmNuGetPackagistPubPyPIRubyGemsCargoClojarsCocoaPodsCPANCRANHackageMavennpmNuGetPackagistPubPyPIRubyGems10440835109221074731334562276436052466216501010110100118350143613101114833213443401445441641377211281056013372215322113966110471520322013222010233414322101112212221076114412212042341576518481313321517051015202530Number of librariesNoYesFind evidence in README?Find the bindingrelease in theecosystemDownload files ofthis release fromthe ecosystemNoYesFind evidence in build files or test cas-es (if available)?YesNoIs there a link to the repository of thisbinding?Find this specificrelease in therepositoryFind evidence in README?NoFind evidence in submodules or CI/CD config?YesFind evidencein build files or test cas-es (if available)?Record the resultNot foundNoRecord the resultYesYesNoStart8

Fig. 7: Three examples of matching binding releases and
source releases: (a) all source releases are supported by the
binding; (b) 2 out of 3 source releases are supported; (c) no
source releases are supported.

3. Build Files: When the supported version of the source li-
brary that is going to be built for developers is mentioned
explicitly, e.g., a binding might indicate the supported
version in CMakeLists.txt or Rakeﬁle.

4. Test Cases: When the supported version of the source

library is veriﬁed explicitly by the tests.

5. Conﬁgurations of continuous integration or continuous
delivery (CI/CD): When the supported version of the
source library is indicated explicitly in the conﬁguration
ﬁle, such as .travis.yml, to set up the CI/CD environment.

The identiﬁcation process is shown in Figure 6. For
each release of a cross-ecosystem binding, we inspected the
README and downloaded the ﬁles of this release from the
software package ecosystem. If no evidence could be found,
we checked out the source code repository of this binding.
To locate the corresponding check point (i.e., a git commit)
of a speciﬁc release in the repository, we investigated all
tags of the repository and extracted the one that had the
same version as the release of the binding. However, some
repositories do not have tags for published releases. In this
case, we tracked the modiﬁcation history of the meta-data
ﬁle (which stores the version number) to locate the check
point. Some examples of meta-data ﬁles are package.json in
npm, setup.py in PyPI, and pom.xml in Maven. After locating
the check point of the speciﬁc release, we inspected the ﬁles
to ﬁnd out which release of the source library is supported.
We performed this process for all releases of cross-ecosystem
bindings and matched them with the releases of the source
library. The matching results can be found in our replication
package [35].

In addition, we extracted the delay (also known as tech-
nical lag [57]) between the matched releases and the source
releases in days and calculated the coverage for each binding
b in software package ecosystem e as follows:

coveragee,b =

#matched source releasese,b
#releasesource,b

where the numerator is the number of source releases that
are supported by b and the denominator is the number of
releases of the source library. We only consider the releases
of the source library that were published since the binding
started to provide support. The coverage of a binding b will

Fig. 8: The distributions of the coverage of cross-ecosystem
bindings for popular ML source libraries across ecosystems.

be 100% if we can ﬁnd a matched binding release for every
source release. If we cannot ﬁnd any matched release in
b, the coverage will be 0%. The coverage metric captures
the overall support that a binding offers for an ML library.
Figure 7 presents three matching results, the denominators
are 3 for all three examples and the numerators are 3, 2,
and 0 respectively. Hence, the coverage values for these
examples are 100%, 67%, and 0%.

Next, we compared our ﬁndings between ofﬁcially-
maintained bindings and community-maintained bindings
by performing the Mann-Whitney U test [39] at a signiﬁ-
cance level of α = 0.05 to determine whether the differences
are signiﬁcant. Also, we computed Cliff’s delta d [37] effect
size to quantify the difference. To explain the value of d, we
use the thresholds which are provided by Romano et al. [46]:

Eﬀect size =






negligible,
small,
medium,
large,

if |d| ≤ 0.147
if 0.147 < |d| ≤ 0.33
if 0.33 < |d| ≤ 0.474
if 0.474 < |d| ≤ 1

Findings. Developers in PyPI are more likely to ﬁnd a
matched release of a cross-ecosystem binding for popular
ML source libraries than developers in other software
package ecosystems. Figure 8 shows that PyPI has the
highest coverage among the 13 studied ecosystems and the
median value is 78%. In contrast, other ecosystems have
relatively small coverage values. We observed that the main
reason for the low coverage of these bindings is that they
only support some of the releases from their source libraries.
For example, a binding of dlib in RubyGems has 13 releases
but only supports 2 out of 50 versions of the source library.
Speciﬁcally, releases 1.0.0 to 1.0.3 of this binding support
version v18.13 of the source library, then it skipped nine
versions (i.e., v18.14 to v19.3) of the source library and
published 9 releases to support v19.4. This phenomenon
can also be found in other bindings with low coverage. In
addition, we noticed that software package ecosystems like
Clojars, npm, Pub, and RubyGems have a median of 0
coverage since we cannot ﬁnd any matched release for most
bindings in these ecosystems.

After a release of the ML library was published, their
bindings in PyPI, npm, and Maven received a correspond-
ing update more quickly than the ones in other software
ecosystems. Figure 9 shows that the median numbers of
days between matched releases and source releases in PyPI,
npm, and Maven are less than one day and most of the

1.0.01.1.01.2.00.1.00.2.00.3.00.4.00.5.01.0.01.1.01.2.00.1.00.2.00.3.00.4.00.5.01.0.01.1.01.2.00.1.00.2.00.3.00.4.00.5.0librarylibrarylibrarybindingbindingbinding(a)(b)(c)CargoClojarsCocoaPodsCPANCRANHackageMavennpmNuGetPackagistPubPyPIRubyGems0.00.20.40.60.81.0Coverage9

Fig. 10: Comparisons of the cross-ecosystem bindings for
ML libraries which are maintained by the ofﬁcial orga-
nization and the community: (a) the distributions of the
coverage; (b) the distributions of the technical lag between
a release of the source library and the corresponding release
of the binding.

6 IMPLICATIONS
In this section, we discuss the implications of our ﬁndings
for developers, owners of ML source libraries and their
cross-ecosystem bindings, and researchers.

6.1 Implications for Developers

Developers are not always limited to using the same
source programming language as a popular ML library
when they wish to use this library. PyPI has 88% of the
popular ML libraries (Section 5.1) and developers in PyPI
are more likely to ﬁnd an ML library for their own projects.
However, developers might prefer to start a project in their
most familiar ecosystem. Since 25% of the popular ML
libraries can be found in multiple ecosystems (Secion 5.2),
developers may ﬁnd a binding of their desired ML library
in the chosen software package ecosystem. We suggest
that developers should put the choice of ecosystems into
consideration before starting a project.

Developers should consider the number of supported
releases and the delay of getting a corresponding update
when choosing the binding for an ML library. Usually,
cross-ecosystem bindings for popular ML libraries do not
support all releases of their source library (Section 5.3). Our
ﬁndings show that it is not sufﬁcient to look at the number
of binding releases only. Bindings with low coverage could
publish many releases but only support one or two versions
of their source library. If developers are going to adapt
such a binding, they should consider that it might not
support a needed version in the future. For a binding with
high coverage, developers should also consider how long
it takes to update for a version of their source library and
whether such delays are acceptable to them. By checking the
maintenance history of the chosen binding, developers can
have an expectation about the binding and consider whether
they wish to adapt it or not.

Fig. 9: The distributions of the technical lag between releases
of popular ML source libraries and their cross-ecosystem
bindings.

technical lag was within one week. Besides, we noticed
that 11% of the technical lag was negative and 3% of the
matched releases in bindings were published more than
one day before the source releases. For example, release
0.11.0 of MXNet in PyPI was published on August 20,
2017 and the corresponding release of the source library was
published on September 5, 2017. This phenomenon implies
that sometimes the owners of ML source libraries publish a
binding release before they create a tag in the source code
repository.

Compared to community-maintained bindings for
popular ML libraries, the ofﬁcially-maintained bindings
have a higher coverage and less technical lag. There
are 68 ofﬁcially-maintained bindings and 87 community-
maintained bindings (44% versus 56%) for the 36 cross-
ecosystem ML libraries. Figure 10 shows that the coverage of
community-maintained bindings are mostly below 0.2 and
the technical lag of the source library are mostly between 22
and 163 days. In contrast, the ofﬁcially-maintained bindings
have a relatively large coverage and less technical lag. The
Mann-Whitney U test shows that the distributions of the
coverage and the distributions of the technical lag are both
signiﬁcantly different in ofﬁcially-maintained bindings and
community-maintained bindings. In addition, the values of
Cliff’s Delta d are 0.670 and −0.882 respectfully, which
indicate that the effect sizes are both large.

51% of the cross-ecosystem bindings do not follow the
version numbers of their source library. During the process
of matching binding releases with their source releases,
we noticed that 79 out of 155 bindings did not reuse any
version number from their source library. Hence, the version
numbers of these bindings are not aligned with the source
libraries and developers could be confused by the different
version numbers in bindings. In contrast, 19% (30 out of 155)
of these bindings follow the version numbers of their source
library for more than 75% of the releases, and 8% (13 out of
155) bindings follow the version numbers in all releases.

(cid:0)

(cid:3)

RQ3 Summary: Generally,
the coverage of re-
leases of cross-ecosystem bindings for their source
library is low and the technical
lag is large.
Ofﬁcially-maintained bindings support more re-
leases than community-maintained bindings and
have a shorter technical lag.

(cid:2)

6.2 Implications for ML Package Owners

Owners of cross-ecosystem bindings for popular ML li-
braries should explicitly indicate the matching between
releases of the binding and releases of the source li-
brary. In Section 5.3, we found that some software package
ecosystems have a median of 0 coverage for bindings of

(cid:1)

CargoClojarsCocoaPodsCPANCRANHackageMavennpmNuGetPackagistPubPyPIRubyGems-10.0-1.00.01.010.0100.01000.0Days of delayofficialcommunity(a)0.00.20.40.60.81.0Coverageofficialcommunity(b)-1.00.01.010.0100.0100.0Days of delaypopular ML source libraries. Such a low coverage indicates
that either no source releases are supported at all, or it is
not possible to ﬁnd out which versions are supported (even
after our thorough investigation). In addition, we noticed
that only 8% of all cross-ecosystem bindings completely
follow the version numbers of their source library. We
recommend that owners of cross-ecosystem bindings use the
same version number as their source libraries and indicate
that in their README. For example, the binding of mlpack
in CRAN mentions that “the version number of MLPACK
is used as the version number of this package” [21]. Also,
we recommend adding an extra number after the original
version number, for example, changing the version number
from “1.2.3” to “1.2.3.0”. This way, the owner can increase
the extra number for ﬁxing bugs in the binding without
causing confusion for developers.

Owners of popular ML source libraries should take
notice of the community-maintained bindings for their
libraries. 56% of the cross-ecosystem bindings for popu-
lar ML libraries are maintained by the community (Sec-
tion 5.3). These community-maintained bindings help de-
velopers to use the functionalities from their source library
in an ecosystem which the ofﬁcial organization does not
support. We recommend that ofﬁcial organizations keep an
eye on the community-maintained bindings. For example,
the ofﬁcial organization could inform popular community-
maintained bindings about important updates, e.g., those
that ﬁx security vulnerabilities. We noticed that some li-
braries list the community-maintained bindings in their
README or the ofﬁcial website. Furthermore, we ob-
served that OpenCV even adopted a popular community-
maintained binding [49].

6.3 Implications for Researchers

Researchers should further investigate the differences
between ofﬁcially and community-maintained bindings.
Our results show that community-maintained bindings and
ofﬁcially-maintained bindings have a different coverage and
technical lag (Section 5.3). Future studies should investigate
what causes these differences. One factor could be simply
ﬁnancial incentives (e.g., because contributors to ofﬁcially
maintained bindings work for the company driving the
binding), but there could also be socio-technical factors. For
example, developers may be more motivated to contribute
to ofﬁcially-maintained bindings as such contributions are
considered more valuable or prestigious. In addition, future
studies should investigate how the communication between
community-maintained bindings and the source library can
be improved.

Researchers should not simply use keyword searching
approaches to select ML libraries. During the identiﬁcation
process of ML libraries from the selected repositories, we
noticed that 118 out of 299 (39%) repositories that contained
ML keywords were not from ML libraries. These non-
ML libraries provide supplemental tools, datasets, tutorials,
etc. Notably, there are 10 out of 299 (3%) repositories that
provide datasets or pre-trained models like TensorFlow
Datasets [54], and 12 out of 299 (4%) repositories provide
visualization tools like TensorWatch [47].

10

Researchers should study automatic matching tools for
releases of ecosystem bindings to match with releases of
their source library. It is a complex task to identify which
releases of the source library are supported by an ecosystem
binding. Automatic version matching tools for ecosystem
bindings can help developers to ﬁnd a suitable release
without going through all the related ﬁles of a binding (like
we did in Section 5.3) or trying the releases one by one in
their project.

7 THREATS TO VALIDITY

In this section, we discuss the threats to validity of our
study about popular ML libraries and their cross-ecosystem
bindings.

7.1 Internal Validity

We selected popular ML libraries from packages in software
ecosystems based on the number of stars of their source code
repository. However, 19% (1,479 out of 7,963) of the selected
ML packages do not provide a link to their repository. In this
paper, we might have excluded some popular ML libraries
due to the exclusion of packages that do not have a link to
their repository.

We used the tags on GitHub as the source releases
for matching releases in the cross-ecosystem bindings. Sec-
tion 5.1 shows that bindings can have pre-releases that were
published before the ﬁrst release of the source library. We
cannot match these pre-releases in bindings if there was no
corresponding source release. In addition, we might miss
some source release if an ML source library was migrated to
GitHub from another source code repository.

A low coverage value does not always mean that a
binding has a bad support. For example, a library can be
backward compatible, thereby making it easier to support
newer versions without changing the binding. However,
developers have to manually verify whether this binding
will work for a speciﬁc version of the ML library that they
are going to use, as it is not indicated anywhere.

We

classiﬁed cross-ecosystem bindings of popu-
lar ML libraries
into ofﬁcially-maintained bindings
some
and community-maintained bindings. However,
community-maintained bindings could be adopted by the
ofﬁcial organization and become an ofﬁcially-maintained
binding. In our study, we only consider the latest informa-
tion and do not take into account the history of ownership.
In addition, the owner of a community-maintained binding
could be a member of the ofﬁcial organization of the source
library. If the binding is not owned by the ofﬁcial organiza-
tion and there is no evidence in its README, we consider
such bindings as community-maintained bindings.

Some software package ecosystems like PyPI allow
the owner of a package to delete a published release.
Libraries.io does not record the history of releases of a
package, hence, we only consider the information of releases
when the dataset was collected. In addition, some ecosys-
tems also support the owner of a package to deprecate a
release instead of deleting a release, e.g., npm and Cargo.
In our study, we consider all releases which also include the
deprecated ones.

7.2 External Validity

In our empirical study, we studied cross-ecosystem bindings
for popular ML libraries. The results of our study might
not apply directly to all cross-ecosystem bindings. Still,
our methodology can be applied to analyze other cross-
ecosystem bindings for libraries in other domains. In ad-
dition, we focused on open source ML libraries as many
popular ML libraries are open source. Future studies should
investigate if our ﬁndings hold for proprietary ML libraries.

8 CONCLUSION

In this paper, we extracted 7,963 machine learning packages
from 13 software package ecosystems in the Libraries.io
dataset and selected 146 popular ML libraries from these
packages. Then, we identiﬁed 36 popular ML libraries
which have cross-ecosystem bindings, and we collected
their releases. We studied the distribution of popular ML li-
braries across software package ecosystems, the population
of cross-ecosystem bindings for popular ML libraries, and
the development and maintenance of these cross-ecosystem
bindings. We shared the collected data in our replication
package [35]. The most important ﬁndings of our study are:

1. Popular ML libraries and their cross-ecosystem bindings
are most commonly published in the PyPI ecosystem.
2. Most of the cross-ecosystem bindings for popular ML
libraries have a low coverage of the releases of their
source libraries, and 51% of these bindings did not reuse
any version number from their source library, making it
hard to identify which source versions are supported.
3. Ofﬁcially-maintained bindings support more releases
than community-maintained bindings and have less tech-
nical lag.
Our ﬁndings show that developers who wish to use a
popular ML library are not limited to using the program-
ming language the library was written in, as there exist
many cross-ecosystem bindings. However, they should care-
fully check the coverage and technical lag of these bindings
before they commit to using one. In addition, we suggest
that maintainers of cross-ecosystem bindings should follow
the version number of their source library and add an extra
number after it, to account for bug ﬁxes, and make it easier
for developers to identify which version of the source library
is supported by the binding.

ACKNOWLEDGMENTS

The work described in this paper has been supported by the
ECE-Huawei Research Initiative (HERI) at the University of
Alberta.

REFERENCES

[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,
S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg,
R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasude-
van, P. Warden, M. Wicke, Y. Yu, and X. Zheng, “TensorFlow: A
system for large-scale machine learning,” in Proceedings of the 12th
USENIX Conference on Operating Systems Design and Implementation,
ser. OSDI’16. USA: USENIX Association, 2016, pp. 265–283.
S. Bahrampour, N. Ramakrishnan, L. Schott, and M. Shah, “Com-
parative Study of Deep Learning Software Frameworks,” 2016.

[2]

11

[3] G. Bavota, G. Canfora, M. D. Penta, R. Oliveto, and S. Panichella,
“The evolution of project inter-dependencies in a software ecosys-
tem: The case of Apache,” in 2013 IEEE International Conference on
Software Maintenance, ser. ICSM ’13. USA: IEEE Computer Society,
2013, pp. 280–289.

[4] H. Ben Braiek, F. Khomh, and B. Adams, “The Open-Closed Prin-
ciple of Modern Machine Learning Frameworks,” in Proceedings
of the 15th International Conference on Mining Software Repositories,
ser. MSR ’18. New York, NY, USA: Association for Computing
Machinery, 2018, pp. 353–363.

[5] K. Blincoe, F. Harrison, and D. Damian, “Ecosystems in GitHub
and a method for ecosystem identiﬁcation using reference cou-
pling,” in Proceedings of the 12th Working Conference on Mining
Software Repositories, ser. MSR ’15.
IEEE Press, 2015, pp. 202–207.
[6] K. Blincoe, F. Harrison, N. Kaur, and D. Damian, “Reference cou-
pling: An exploration of inter-project technical dependencies and
their characteristics within large software ecosystems,” Information
and Software Technology, vol. 110, pp. 174–189, 2019.

[7] C. Bogart, C. K¨astner, J. Herbsleb, and F. Thung, “How to Break an
API: Cost Negotiation and Community Values in Three Software
Ecosystems,” in Proceedings of the 2016 24th ACM SIGSOFT Inter-
national Symposium on Foundations of Software Engineering, ser. FSE
2016. New York, NY, USA: Association for Computing Machinery,
2016, pp. 109–120.

[8] ——, “When and how to make breaking changes: Policies and
practices in 18 open source software ecosystems,” ACM Transac-
tions on Software Engineering and Methodology (TOSEM), vol. 30,
no. 4, 2021.

[9] H. Borges and M. Tulio Valente, “What’s in a GitHub star? under-
standing repository starring practices in a social coding platform,”
Journal of Systems and Software, vol. 146, pp. 112–129, 2018.

[10] G. Bradski, “The OpenCV Library,” Dr. Dobb’s Journal of Software

Tools, vol. 25, pp. 120–125, 2000.

[11] B. Castellano.

cut
detection and video splitting tool. [Online]. Available: https:
//pyscenedetect.readthedocs.io/en/latest/

(2021) PySceneDetect:

Intelligent

scene

[12] S. Chacon and B. Straub, Pro Git, 2nd ed. USA: Apress, 2014.
[13] S. Chiba, “Foreign language interfaces by code migration,” in
Proceedings of the 18th ACM SIGPLAN International Conference on
Generative Programming: Concepts and Experiences, ser. GPCE 2019.
New York, NY, USA: Association for Computing Machinery, Oct.
2019, pp. 1–13.

[14] F. R. Cogo, G. A. Oliva, and A. E. Hassan, “An Empirical Study of
Dependency Downgrades in the npm Ecosystem,” IEEE Transac-
tions on Software Engineering, pp. 1–1, 2019.

[15] E. Constantinou, A. Decan, and T. Mens, “Breaking the borders:
An investigation of cross-ecosystem software packages,” in Pro-
ceedings of the 17th Belgium-Netherlands Software Evolution Workshop,
Delft, the Netherlands, December 10th - to - 11th, 2018, ser. CEUR
Workshop Proceedings, G. Gousios and J. Hejderup, Eds., vol.
2361. CEUR-WS.org, 2018, pp. 1–5.

[16] E. Constantinou and T. Mens, “An empirical comparison of devel-
oper retention in the RubyGems and npm software ecosystems,”
Innovations in Systems and Software Engineering, vol. 13, no. 2, pp.
101–115, 2017.

[17] ——, “Socio-technical evolution of

the Ruby ecosystem in
GitHub,” in 2017 IEEE 24th International Conference on Software
Analysis, Evolution and Reengineering (SANER), 2017, pp. 34–44.
[18] S. da Silva Amorim, J. D. McGregor, E. S. de Almeida, and C. von
Flach G. Chavez, “Software ecosystems architectural health: Chal-
lenges x practices,” in Proccedings of the 10th European Conference on
Software Architecture Workshops, ser. ECSAW ’16. New York, NY,
USA: Association for Computing Machinery, 2016.

[19] C. R. de Souza, F. Figueira Filho, M. Miranda, R. P. Ferreira,
C. Treude, and L. Singer, “The social side of software platform
ecosystems,” in Proceedings of the 2016 CHI Conference on Human
Factors in Computing Systems. New York, NY, USA: Association
for Computing Machinery, 2016, pp. 3204–3214.

[20] A. Decan and T. Mens, “How Magic Is Zero? An Empirical Anal-
ysis of Initial Development Releases in Three Software Package
Distributions,” in Proceedings of the IEEE/ACM 42nd International
Conference on Software Engineering Workshops, ser. ICSEW’20. New
York, NY, USA: Association for Computing Machinery, Jun. 2020,
pp. 695–702.

[21] D. Eddelbuettel and J. J. Balamuta. (2020) Rcppmlpack. [Online].
Available: https://github.com/rcppmlpack/RcppMLPACK1

[22] S. Finne, D. Leijen, E. Meijer, and S. Peyton Jones, “H/Direct:
A binary foreign language interface for Haskell,” in Proceedings
of the Third ACM SIGPLAN International Conference on Functional
Programming, ser. ICFP ’98. New York, NY, USA: Association for
Computing Machinery, 1998, p. 153–162.

[23] O. Franco-Bedoya, D. Ameller, D. Costal, and X. Franch, “Open
source software ecosystems: A systematic mapping,” Information
and Software Technology, vol. 91, pp. 160–185, 2017.

[24] M. Furr and J. S. Foster, “Checking type safety of foreign function
calls,” ACM Trans. Program. Lang. Syst., vol. 30, no. 4, pp. 18:1–
18:63, Aug. 2008.

[25] D. M. German, B. Adams, and A. E. Hassan, “The Evolution of
the R Software Ecosystem,” in 17th European Conference on Software
Maintenance and Reengineering, ser. CSMR.
IEEE Press, Mar. 2013,
pp. 243–252.

[26] Q. Guo, S. Chen, X. Xie, L. Ma, Q. Hu, H. Liu, Y. Liu, J. Zhao, and
X. Li, “An Empirical Study Towards Characterizing Deep Learning
Development and Deployment Across Different Frameworks and
Platforms,” in 2019 34th IEEE/ACM International Conference on
Automated Software Engineering (ASE), Nov. 2019, pp. 810–822.
[27] J. Han, S. Deng, D. Lo, C. Zhi, J. Yin, and X. Xia, “An Empirical
Study of the Dependency Networks of Deep Learning Libraries,”
in 2020 IEEE International Conference on Software Maintenance and
Evolution (ICSME).
IEEE Press, Sep. 2020, pp. 868–878, iSSN:
2576-3148.

[28] M. Honnibal and I. Montani, “spaCy 2: Natural language under-
standing with Bloom embeddings, convolutional neural networks
and incremental parsing,” To appear, vol. 7, no. 1, pp. 411–420, 2017.
[29] M. J. Islam, H. A. Nguyen, R. Pan, and H. Rajan, “What Do
Developers Ask About ML Libraries? A Large-scale Study Using
Stack Overﬂow,” 2019.

[30] S. Jansen, “Measuring the health of open source software ecosys-
tems: Beyond the scope of project health,” Information and Software
Technology, vol. 56, no. 11, pp. 1508–1519, 2014, special issue on
Software Ecosystems.

[31] J. Katz, “Libraries.io Open Source Repository and Dependency
Metadata,” Jan. 2020. [Online]. Available: https://doi.org/10.
5281/zenodo.3626071

[32] R. G. Kula and G. Robles, The Life and Death of Software Ecosystems.

Singapore: Springer Singapore, 2019, pp. 97–105.

[33] R. G. Kula, C. D. Roover, D. M. Germ´an, T. Ishio, and K. Inoue,
“A generalized model for visualizing library popularity, adoption,
and diffusion within a software ecosystem,” in 25th International
Conference on Software Analysis, Evolution and Reengineering, ser.
SANER 2018.

IEEE Computer Society, 2018, pp. 288–299.

[34] B. Lee, B. Wiedermann, M. Hirzel, R. Grimm, and K. S. McKinley,
“Jinn: synthesizing dynamic bug detectors for foreign language
interfaces,” SIGPLAN Not., vol. 45, no. 6, pp. 36–49, Jun. 2010.

[35] H. Li and C.-P. Bezemer.

January) The replication
package of our
study on popular open source machine
learning libraries and their cross-ecosystem bindings. [Online].
Available:
https://github.com/asgaardlab/lihao-compare ml
bindings-reproduce code

(2022,

[36] H. Li, F. R. Cogo, and C.-P. Bezemer, “[under review] An empirical
study of yanked releases in the Rust package registry,” IEEE
Transactions on Software Engineering, 2021.

[37] J. D. Long, D. Feng, and N. Cliff, Ordinal Analysis of Behavioral Data.

American Cancer Society, 2003, ch. 25, pp. 635–661.

[38] K. Manikas, “Revisiting software ecosystems research: A longitu-
dinal literature study,” Journal of Systems and Software, vol. 117, pp.
84–103, 2016.

[39] H. B. Mann and D. R. Whitney, “On a Test of Whether one of Two
Random Variables is Stochastically Larger than the Other,” The
Annals of Mathematical Statistics, vol. 18, no. 1, pp. 50 – 60, 1947.

[40] T. Mens, M. Claes, P. Grosjean, and A. Serebrenik, Studying
Berlin,

Evolving Software Ecosystems based on Ecological Models.
Heidelberg: Springer Berlin Heidelberg, 2014, pp. 297–326.
[41] S. Nakata, M. Sugaya, and K. Kuramitsu, “Fault model of foreign
function interface across different domains,” in 2011 IEEE/IFIP
41st International Conference on Dependable Systems and Networks
Workshops (DSN-W), Jun. 2011, pp. 248–253, iSSN: 2325-6664.
[42] K. Osman and O. Baysal, “Health is wealth: Evaluating the health
of the bitcoin ecosystem in GitHub,” in 2021 IEEE/ACM 4th
International Workshop on Software Health in Projects, Ecosystems and
Communities (SoHeal), 2021, pp. 1–8.

[43] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison,

12

A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy,
B. Steiner, L. Fang, J. Bai, and S. Chintala, “PyTorch: An imperative
style, high-performance deep learning library,” in Advances in
Neural Information Processing Systems 32, ser. NeurIPS. Curran
Associates, Inc., 2019, pp. 8024–8035.

[44] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg,
J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot,
and E. Duchesnay, “Scikit-Learn: Machine learning in Python,” the
Journal of machine Learning research, vol. 12, pp. 2825–2830, Nov.
2011.

[45] J. Reppy and C. Song, “Application-speciﬁc foreign-interface gen-
eration,” in Proceedings of the 5th international conference on Genera-
tive programming and component engineering, ser. GPCE ’06. New
York, NY, USA: Association for Computing Machinery, Oct. 2006,
pp. 49–58.

[46] J. Romano,

J. Coraggio,

J. D. Kromrey,

J. Skowronek, and
L. Devine, “Exploring methods for evaluating group differences
on the NSSE and other surveys: Are the t-test and Cohen’s d
indices the most appropriate choices,” in annual meeting of the
Southern Association for Institutional Research. Citeseer, 2006, pp.
1–51.

[47] S. Shah, R. Fernandez, and S. Drucker, “A system for real-time
interactive analysis of deep learning training,” in Proceedings of
the ACM SIGCHI Symposium on Engineering Interactive Computing
Systems, ser. EICS ’19. New York, NY, USA: Association for
Computing Machinery, 2019.

[48] R. Smith, “An overview of the Tesseract OCR engine,” in Proceed-
ings of the Ninth International Conference on Document Analysis and
Recognition, ser. ICDAR ’07, vol. 02. USA: IEEE Computer Society,
2007, pp. 629–633.

[49] A. Smorkalov.

to OpenCV organization on Github.
https://github.com/opencv/opencv-python/pull/479

(2021) Documentation update after migration
[Online]. Available:

[50] StackOverﬂow. (2021) Stack Overﬂow Annual Developer Survey
[Online]. Available: https://insights.stackoverﬂow.com/

2021.
survey/2021

[51] C. Steglich, A. Majdenbaum, S. Marczak, and R. Santos, “A study
on organizational it security in mobile software ecosystems litera-
ture,” in 2020 IEEE International Conference on Software Architecture
Companion (ICSA-C), 2020, pp. 234–241.

[52] C. Steglich, S. Marczak, R. Santos, L. H. Mosmann, L. P. Guerra,
C. de Souza, F. F. Filho, and M. Perin, “How do business factors
affect developers in mobile software ecosystems?” in XVI Brazilian
Symposium on Information Systems, ser. SBSI’20. New York, NY,
USA: Association for Computing Machinery, 2020.

[53] D. Stufft and individual contributors. (2021) packaging: Version
[Online]. Available: https://packaging.pypa.io/en/

Handling.
latest/version.html

[54] TensorFlow. (2021) TensorFlow Datasets, a collection of ready-to-
use datasets. [Online]. Available: https://www.tensorﬂow.org/
datasets

[55] F. Thung, S. Wang, D. Lo, and L. Jiang, “An Empirical Study of
Bugs in Machine Learning Systems,” in IEEE 23rd International
Symposium on Software Reliability Engineering, ser. ISSRE 2012, Nov.
2012, pp. 271–280.

[56] E. Wittern, P. Suter, and S. Rajagopalan, “A look at the dynamics
of the JavaScript package ecosystem,” in Proceedings of the 13th
International Conference on Mining Software Repositories, ser. MSR
’16. New York, NY, USA: Association for Computing Machinery,
2016, pp. 351–361.

[57] A. Zerouali, T. Mens, J. Gonzalez-Barahona, A. Decan, E. Con-
stantinou, and G. Robles, “A formal framework for measuring
technical lag in component repositories — and its application to
npm,” Journal of Software: Evolution and Process, vol. 31, no. 8, p.
e2157, 2019, e2157 smr.2157.

[58] Y. Zhang, Y. Chen, S.-C. Cheung, Y. Xiong, and L. Zhang, “An
Empirical Study on TensorFlow Program Bugs,” in Proceedings of
the 27th ACM SIGSOFT International Symposium on Software Testing
and Analysis, ser. ISSTA 2018. New York, NY, USA: Association
for Computing Machinery, Jul. 2018, pp. 129–140.

