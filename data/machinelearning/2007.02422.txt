Piecewise Linear Regression via a Difference of Convex Functions

Ali Siahkamari * 1 Aditya Gangrade * 2 Brian Kulis 1 Venkatesh Saligrama 1

0
2
0
2

v
o
N
3
1

]
L
M

.
t
a
t
s
[

3
v
2
2
4
2
0
.
7
0
0
2
:
v
i
X
r
a

Abstract

We present a new piecewise linear regression
methodology that utilizes ﬁtting a difference of
convex functions (DC functions) to the data.
These are functions f that may be represented
φ2 for a choice of con-
as the difference φ1 −
vex functions φ1, φ2. The method proceeds by
estimating piecewise-liner convex functions, in a
manner similar to max-afﬁne regression, whose
difference approximates the data. The choice
of the function is regularised by a new semi-
norm over the class of DC functions that con-
trols the ℓ∞ Lipschitz constant of the estimate.
The resulting methodology can be efﬁciently im-
plemented via Quadratic programming even in
high dimensions, and is shown to have close to
minimax statistical risk. We empirically validate
the method, showing it to be practically imple-
mentable, and to have comparable performance
to existing regression/classiﬁcation methods on
real-world datasets.

1. Introduction

}i∈[n], where xi ∈

The multivariate nonparametric regression problem is a fun-
damental statistical primitive, with a vast history and many
approaches. We adopt the following setup: given a dataset,
Rd are predictors, assumed
(xi, yi)
{
drawn i.i.d. from a law PX , and yi are responses such that
yi = f (xi) + εi, for a bounded, centered, independent
random noise εi, and bounded f , the goal is to recover an
estimate ˆf of f such that on new data, the squared error
E[(y

ˆf (x))2] is small.

−

The statistical challenge of the problem lies in the fact that
f is only weakly constrained - for instance, f may only be
known to be differentiable, or Lipschitz. In addition, the

*Equal contribution 1Department of Electrical and Computer
Engineering, Boston University 2Division of Systems Engineer-
ing, Boston University. Correspondence to: Ali Siahkamari
<siaa@bu.edu>.

Proceedings of the 37 th International Conference on Machine
Learning, Online, PMLR 119, 2020. Copyright 2020 by the au-
thor(s).

problem is algorithmically challenging in high-dimensions,
and many approaches to the univariate problem do not scale
well with the dimension d. For instance, piecewise linear re-
gression methods typically involve a prespeciﬁed grid, and
thus the number of grid points, or knots, grows exponen-
tially with dimension. Similarly, methods like splines typ-
ically require both stronger smoothness guarantees and ex-
ponentially more parameters to ﬁt with dimension in order
to avoid singularities in the estimate.

This paper is concerned with regression over the class of
functions that are differences of convex functions, i.e., DC
functions. These are functions f that can be represented as
φ2 for a choice of two convex functions. DC
f = φ1 −
functions constitute a very rich class - for instance, they
2 functions. Such functions have
are known to contain all
been applied in a variety of contexts including non-convex
optimization (Yuille & Rangarajan, 2002; Horst & Thoai,
1999), sparse signal recovery (Gasso et al., 2009) and re-
inforcement learning (Piot et al., 2014).

C

The principal contribution of this paper is a method for
piecewise linear regression over the class of DC functions.
At the heart of the method is a representation of piecewise
linear DC functions via a set of linear constraints, in a
manner that generalises the representations used for max-
afﬁne regression. The choice of the function is regularised
for smoothness by a new seminorm that controls the ℓ∞-
Lipschitz constant of the resulting function. The resulting
estimate is thus a piecewise linear function, represented
as the difference of two piecewise linear convex functions,
that are smooth in the sense of having bounded gradients.

The method enjoys two main advantages:

1. It is agnostic to any knowledge about the function, and
requires minimal parameter tuning.

2. It can be implemented efﬁciently, via quadratic program-
ming, even in high dimensions. For n data points in Rd,
the problems has 2n(2d + 1) + 1 decision variables, and
n2 linear constraints, and can be solved in the worst case in
O(d2n5) time by interior-point methods. Furthermore the
algorithm does not need to specify partitions for piece-wise
linear parts and avoids ad-hoc generalizations of splines or
piece-wise linear methods to multi-dimensions.

 
 
 
 
 
 
Piecewise Linear Regression via a Difference of Convex Functions

In addition, the method is shown to be statistically viable,
in that it is shown to attain vanishing risk as the sample
size grows at a non-trivial rate, under the condition that the
ground truth has bounded DC seminorm. The risk further
adapts to structure such as low dimensional supports.

Lastly, the solution obtained is a piecewise-linear ﬁt, and
thus enjoys interpretability in that features contribution
heavily to the value can be readily identiﬁed. Further, the
ﬁtting procedure naturally imposes ℓ1 regularisation on the
weights of the piecewise linear parts, thus enforcing a spar-
sity of local features, which further improves interpretabil-
ity.

≤

≤

To establish practical viability, we implement the method
on a number of regression and classiﬁcation tasks. The set-
103,
tings explored are those of moderate data sizes - n
102. We note that essesntially all non-
and dimensions d
parametric models are only viable in these settings - typi-
cal computational costs grow with n and become infeasi-
ble for large datasets, while for much higher dimensions,
the sample complexity - which grows exponentially with
d - cause small datasets to be non-informative. More prag-
matically, all nonparametric methods we compare against
have been evaluated on such data. Within these constraints,
the method is shown to have better error performance and
ﬂuctuation with respect to popular methodologies such as
multivariate adaptive regression splines, nearest neighbour
methods, and two-layer perceptrons, evaluated on both syn-
thetic and real world data-sets.

1.1. Connections to Existing Methodologies

Piecewise Linear Regression is popular since such regres-
sors can can model the local features of the data without
affecting the global ﬁt. In higher than 1 dimensions, piece-
wise linear functions are usually ﬁt via choosing a partition
of the space and ﬁtting linear functions on each part. The
principle difﬁculty thus lies in choosing these partitions.
The approach is usually a rectangular grid - for instance,
a variable rectangular partition of the space is studied in
(Toriello & Vielma, 2012) and solved optimally. However
the rectangulization becomes prohibitive in high dimension
as the number of parts grow exponentially with the dimen-
sion. Other approaches include Bayesian methods such as
(Holmes & Mallick, 2001), which rely on computing poste-
rior means for the parameters to be ﬁt via MCMC methods.

Max-Afﬁne Regression is a nonparametric approxima-
tion to convex regression, originating in (Hildreth, 1954;
Holloway, 1979) that recovers the optimal piece-wise lin-
ear approximant to a convex function with the form f =
+ bi for some K. Smoothness of the esti-
maxi∈[K]h
mate can be controlled by constraining the convex function
to be Lipschitz. The problem is generic in that it is easily ar-
gued that piecewise linear convex functions can uniformly

ai, xii

approximate any Lipschitz convex function on a bounded
domain. Parametric approaches, i.e., with a ﬁxed K, are
popular, but can be computationally intensive due to the
induced combinatorics of which of the K planes is max-
imised at which data point, and various heuristics and par-
titioning techniques have to be applied (Magnani & Boyd,
2009; Hannah & Dunson, 2013; Ghosh et al., 2019). The
nonparametric case, where K grows with n, has been ex-
tensively analysed in the works (Bal´azs et al., 2015; Bal´azs,
2016).

On the other hand, if K = n, i.e. if the number of afﬁne
functions used is the same as the number of datapoints, then
the problem becomes amenable to convex programming
techniques - when estimating the parameters ai, bi, one can
remove the nonlinearity induced by the max, and instead
enforce the same via n linear constraints. This simple fact
allows efﬁcient algorithmic approaches to max-afﬁne func-
tions. The heart of our method for DC function regression
is an extension of this trick to DC functions.

Smoothing splines are an extremely popular regression
methodology in low dimensions. The most popular of these
are the L2 smoothing splines, which, in one dimension, in-
volve ﬁxing a ‘knot’ at each data point, and estimating the
gradients of the function at each point, with regularisation
2. Unfortunately this L2 regularisation
of the form
3 dimensions, and methods
leads to singularities in d
≥
R
such as thin plate splines generalising these to higher di-
mensions resort of regularising up to the dth derivative of
the estimate, leading to an explosion in the number of pa-
rameters to be estimated (Wahba, 1990).

ˆf ′′

|

|

Our method is closer in relation to L1 regularised splines,
which in the univariate case regularise
- it is shown
in Appx. A that in one dimension our method reduces to
these. As a consequence, one may view this method as a
new generalisation of the L1-spline regressor.

ˆf ′′

R

|

|

A number of alternative methods for multivariate splines
have been proposed, with several, such as general additive
models modelling the data via low dimensional projections
and assumptions. The most relevant multivariate spline
methods are the adaptive regression splines, originating in
(Friedman et al., 1991), which is a greedy procedure for re-
cursively reﬁning a partition of the data, and ﬁtting new
polynomials over the parts.

Previous DC Modeling Finally, let us mention that the ﬁ-
nal chapter of the doctoral thesis of Bal´azs (2016) antici-
pates our study of DC function regression, but gives neither
algorithms nor analyses. Subsequently, Cui et al. (2018) in-
troduces the same DC modeling as us in a broader context,
where the loss function can also be a DC function. However
their problem ends up being non-convex. They focus on
developing a majorization-minimization algorithm to ﬁnd

Piecewise Linear Regression via a Difference of Convex Functions

⊂
R

an approximate solution with desirable guarantees such as
convergence to a directional stationary solution.

2. A brief introduction to DC functions

Difference of convex functions are functions which can be
represented as difference of two continuous convex func-
tion over a domain Ω

Rd, i.e., the class

(Ω) ,

{

→

DC

f : Ω

φ1, φ2 convex, continuous s.t.
|∃
.
f = φ1 −
Throughout the text we will set Ω =
.
}
We will assume that the noise and the ground truth func-
tion are bounded so that
M. As a
2M .
consequence,

{
, supx∈Ω |

k∞ ≤

φ2}

f (x)

| ≤

x :

R

x

k

y

ε

|

|

|

| ≤

One of the ﬁrst characterizations of DC functions is due
to Hartman et al. (1959): a univariate function is a DC
function if and only if its directional derivatives each has
bounded total variation on all compact subsets of Ω. For
higher dimensions it is known that DC functions are a sub-
class of locally-Lipschitz functions and include C2 func-
tions. Therefore, any continuous function can be uniformly
approximated by DC functions. For a recent review see
Baˇc´ak & Borwein (2011).

In the following section we show that D.C functions can
ﬁt any sample data. Thus, to allow a bias-variance trade-
off, we regularise the selection of DC functions via the DC
seminorm

f

k

k

, inf
φ1,φ2

sup
x

v1k1 +

sup
vi∈∂∗φi(x) k

k
s.t. φ1, φ2 are convex, φ1 −

v2k1

φ2 = f,

c

k

k

where ∂∗φi denotes the set of subgradients of φi. The
above function is not a norm because every constant func-
tion satisﬁes
= 0. Indeed, if we equate DC functions
up to a constant, then the above seminorm turns into a norm.
We leave a proof of the fact that the above is a seminorm to
Appx. B.1. Note that the DC seminorm offers strong con-
trol on the ℓ∞-Lipschitz constant of the convex parts of at
least one DC representation of the function (and in turn on
the Lipschitz constant of the function).

We will principally be interested in DC functions with
bounded DC seminorm, and thus deﬁne

DCL ,

f

{

∈ DC

:

f

k

k ≤

L

.

}

The bulk of this paper concentrates on piecewise linear DC
functions. This is justiﬁed because piecewise linear func-
tions are known to uniformly approximate bounded varia-
tion functions, and structural results (Kripfganz & Schulze,

1987; Ovchinnikov, 2002) showing that every piecewise lin-
ear function may be represented as difference of two con-
vex piecewise linear functions - i.e., max-afﬁne functions.
Since the term is used very often, we will abbreviate “piece-
wise linear DC” as PLDC, and symbolically deﬁne

,
pl-
DC
DCL ,

pl-

f

f

{

{

: f is piecewise linear
,
∈ DC
}
∈ DCL : f is piecewise linear
}

.

The following bound on the seminorm of PLDC functions
is useful. The proof is obvious, and thus omitted.

Proposition 1. Every f
difference of two max-afﬁne functions

DC

pl-

∈

can be represented as a

f (x) = max

ak, x

k∈[K]h

+ ck −

i

max
k∈[K]h

bk, x

i

+ c′
k

for some ﬁnite K. For such an f ,
maxk k
2.1. Expressive Power of Piecewise linear DC functions

maxk k

bkk1.

akk1 +

k ≤

k

f

We begin by arguing that PLDC functions can interpolate
any ﬁnite data. The principle characterisation for DC func-
tions is as follows:

Proposition 2. For any ﬁnite data
{
ists a DC function h : Rd
yi if and only if there exist ai, bi ∈
such that:

}i∈[n], there ex-
R, that takes values h(xi) =
R, i
Rd, zi ∈
[n]

(xi, yi)

→

∈

yi −

yj + zi −
zi −

zj ≥ h
zj ≥ h

aj, xi −
bj, xi −

xji
,
xji
,

i, j

i, j

[n]

[n]

∈

∈

(1)

Further, if there exists a DC function that interpolates a
given data, then there exists a PLDC function that also in-
terpolates the data.

Proof. Assuming h = φ1 −
φ2 for convex functions φ1 and
φ2, take aj and bj to be sub-gradients of respectively φ1
and φ2 then (1) holds by convexity. Conversely, assuming
(1) holds, deﬁne h as

h(x) = max
i∈[n]h

ai, x

xii

+ yi + zi −

−

max
i∈[n]h

bi, x

xii

−

+ zi

∈ DC

since it is expressed as the difference of two max-
h
afﬁne functions. Further, it holds that h(xk) = yk for any
Indeed, the ﬁrst condition implies that for any
k
i

∈
= k,

[n].

ai, xk −
h

xii

+ yi + zi ≤

yk + zk,

with equality when i = k. Thus, the ﬁrst maximum simply
takes the value yk+zk at the input xk. Similarly, the second
maximum takes the value zk at this input, and thus h(xk) =
(yk + zk)

zk = yk.

−

6
Piecewise Linear Regression via a Difference of Convex Functions

Notice that the interpolating function given in the above is
actually piecewise-linear. Thus, if a DC function ﬁts the
given data, then extracting the vectos ai, bi and constants
zi as in the ﬁrst part of the proof, and constructing the inter-
polant in the second part yields a PLDC function that ﬁts
the data.

The principal utility of the conditions stated above is that
we can utilise these to enforce the shape restriction of get-
ting a DC estimate in an efﬁcient way when performing em-
pirical risk minimisation. Indeed, suppose we wish to ﬁt a
DC function to some data (xi, yi). We may then introduce
decision variables ˆyi, zi, ai, bi, where the ˆyi represent the
value of our ﬁt at the various xi, and then enforce the lin-
ear constraints of the above proposition (with yi replaced
ˆyi)2.
by ˆyi) while minimising a loss of the form
Since these constraints are linear, the resulting program is
thus convex, and can be efﬁciently implemented. This ob-
servation forms the core of our algorithmic proposal in §3

(yi −

P

The above characterisation relies on existence of vectors
that may serve as subgradients for the two convex functions.
This condition can be removed, as in the following.

Proposition 3. Given any ﬁnite data
that yi 6
which interpolates this data.

}i∈[n], such
= xj, there exists a PLDC function

= yj =

(xi, yi)

xi 6

⇒

{

Proof. The interpolating function is constructed by adding
and subtracting a quadratic function to the data. Let

yi −
|
xi −
Then the piecewise linear function

C , max
i,j

k

yj|
xjk

.

2
2

h(x) , max
i∈[n]h

Cxi, x

xii

−

+

max
i∈[n]h

Cxi, x

+

xii

−

−

1
2
1
2

C

xik

k

2 +

2

C

xik

k

−

1
2
1
2

yi

yi

(2)

2

k

h

2 C

k ≤

xik1.

k
xi −

xjk
−
xjk1, and

1
convex function at xj is equal to 1
2 yj which
together results in h(xj) = yj. Note that h(x) has the
ℓ∞-Lipschitz constant 2C maxi,j k
2C maxi k
Next, in order to contextualise the expressiveness of DC
functions, we argue that the popular parametric class of
ReLU neural networks can be represented by PLDC func-
tions, and vice versa. This is also argued in (Cui et al.,
2018) for a 2 layer network.
Proposition 4. A fully connected neural network f , with
ReLU activations, and D layers with weight matrices
W 1, . . . , W D, i.e,

f (x) =

wD+1
j

aD
j

j
X

al+1
i = max(

wl+1

i,j al

j, 0), D > l

j
X

a1
i = max(

w1

i,jxj , 0),

j
X

1

≥

is a PLDC function with the DC seminorm bounded as

f

k

k ≤ |

wD+1

T

|

D

W l

|

|

~1,
(cid:17)

(cid:16)

Yl=1

where
is the entry-wise absolute value. The above is
proved in Appx. B.2 via an induction over the layers using
the relations

| · |

max(max(a, 0)
max(b, 0)
max(a, b) + max(c, d) = max(a + b, a + d, b + c, b + d).

max(b, 0), 0) = max(a, b, 0)

−

−

Proposition 5. Every PLDC function with K hyper-planes
can be represented by a ReLU net with 2
layers
and maximum width of 8K.

log2 K

⌉

⌈

The proof is constructed in Appx. B.2 using the relations

satisﬁes the requirements. Indeed, the function is DC, since
it the difference of two max-afﬁne funcitons. The argument
proceeds similarly to the previous case - at any xj, we have:

max(a, b, c, d) = max (max(a, b), max(c, d))

max(a, b) = max(a

b, 0) + max(b, 0)

max(

−

−

b, 0).

−

+

xii

1
2

C

xik

k

2 +

1
2

yi

2 +

1
2

yi

C
2 k
yi −
2

xi −
yj|

|

−

−

xjk
1
2

+

yi

max
i∈[n]h

C

Cxi, xj −
1
2
1
2

xjk

C

k

k

2

2

xjk
1
2 +
2

xjk

= max
i∈[n]

max
i∈[n]
1
2

C

k

≤

≤

yj.

3. Algorithms

We begin by motivating the algorithmic approach we take.
This is followed by separate section developing key por-
tions of the algorithm.

Suppose we observe a data-set Sn =
ated iid from some distribution
function ℓ : R
minimize the expected risk

}i∈[n] gener-
and a convex loss
R+ bounded by c. The goal is to

{
X × Y

(xi, yi)

→

×

R

However the upper bound is reached by choosing i = j
in the max operator. Similarly the value of the second

E[(f (x)

min
f ∈DC

y)2]

−

(3)

Piecewise Linear Regression via a Difference of Convex Functions

by choosing an appropriate function f from the class of
functions. Note instead of the squared error, the above
DC
could be generalised to any bounded, Lipschitz losses
ℓ(f (x), y). Note also that the squared loss is bounded in
our setting because of our assumption that both the ground
truth and noise are bounded.

There are two basic problems with the above - the distri-
bution is unknown, so the objective above cannot be evalu-
ated, and that the class of DC functions is far too rich, and
so the problem is strongly underdetermined. In addition,
directly optimising over all DC functions is an intractable
problem.

To begin with, we reduce the problem by instead ﬁnding
the values that a best ﬁt DC function must take at the dat-
apoint xi, and then ﬁtting a PLDC functions with convex
parts that are max-afﬁne over precisely n linear functionals
on this. This has the signiﬁcant advantage of reducing the
optimisation problem to a set of convex constraints. Quan-
titatively, this step is justiﬁed in §4, which argues that the
error induced by this approximation via PLDC functions is
dominated by the intrinsic risk of the problem.

To handle the lack of knowledge of the distribution, we re-
sort to uniform generalisations bounds in the literature. Our
approach to relies on the following result, which mildly
generalises the bounds of Bartlett & Mendelson (2002),
and uniformly controls the generalisation error of an empir-
ical estimate of the expectation (specialised to our context):
Theorem 1. Let
}i∈[n], be i.i.d. data, with n as-
sumed to be even. Let the empirical maximum discrepancy
of the class

(xi, yi)

{

DCL, be deﬁned as,

ˆDn(

DCL) , sup

f ∈DCL

2
n 

n/2

n

f (xi)

−

f (xi)



.

i=1
X

Xi=n/2+1


δ over the data, it holds holds uni-



1

With probability
formly over all f

≥
−
∈ DC ∩ {|

that

M

}

f

| ≤
n

E[(f (x)

(cid:12)
(cid:12)
(cid:12)
12M ˆDn(
(cid:12)
(cid:12)

≤

y)2]

1
n

−

−

DC2kf k+2)

(f (xi)

yi)2

−

i=1
X

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

+ 45M 2

f
C max(2, log2 k
n

k

s

) + ln(1/δ)

,

(4)

(cid:1)

where C is a constant independent of f, M, R.

The above statement essentially arises from a doubling trick
over a Rademacher complexity based bound for a ﬁxed
DCL ⊂ DCL′ for
. The broad idea is that since
f
k
L′, we can separately develop Rademacher complex-
L
ity based bounds over L of the form 2j, each having the
more stringent high-probability requirement δj = δ2−j. A

k
≤

⌈

k⌉

=
log2 k
S

can be used. See §C for details.

union bound over these then extends these bounds to all of
j≥1 DC2j , and for a particular f , the bound for
f

DC
j =
Optimising the empirical upper bound on E[(f (X)
Y )2]
implied by the above directly leads to a structural risk mini-
mization over the choice of L. The crucial ingredient in the
practicality of this is that for DC functions, ˆDn(
DCL) =
L ˆDn(
DC1) can be computed via
linear programming. Thus, the term ˆDn above serves as
a natural, efﬁciently computable penalty function, and acts
exactly as a regularisation on the DC seminorm.

DC 1), and further, ˆDn(

−

3.1. Computing empirical maximum discrepancy.

Throughout this and the following sections, we use ˆyi to
denote ˆf (xi), where ˆf is the estimated function.

The principle construction relies on the characterisation of
Proposition 2.

Theorem 2. Given data
program computes ˆDn(

(xi, yi)
}

{
DCL)

, the following convex

n/2

n

max
ˆyi,zi,ai,bi

2
n 

ˆyi −

ˆyi −
zi −
aik1 +
k

i=1
X

zj ≥ h
ˆyj + zi −
bj, xi −
zj ≥ h
bik1 ≤
L
k
DC L) = L ˆDn(

s.t. 




Further, ˆDn(

DC1).

ˆyi

xji

Xi=n/2+1
aj, xi −
xji

(5)

(i)
(ii)
(iii)

i, j
i, j
i

[n]
[n]
[n]

∈
∈
∈

Proof. By Proposition 2, conditions (i) and (ii) are neces-
sary and sufﬁcient for the existence of a DC function that
takes the values ˆyi at xi. Thus, these ﬁrst two conditions
allow exploration over all values a DC function can take.
Further, by the second part of Proposition 2 if a DC func-
tion interpolates this data, then so does a PLDC function,
with ai and bi serving as the gradients of the max-afﬁne
parts of the function. Thus, by Proposition 1, the condition
(iii) is necessary and sufﬁcient for the DC function implied
by the ﬁrst to conditions to have seminorm bounded by L.
It follows that the conditions allow exploration of all values
a
, at which point
DCL function may take at the given
the claim follows.

xi}

{

Now, notice that if we multiply each of the decision vari-
ables in the above program by L, the value of the program
is multiplied by a factor of L, while the constraints (i), (ii)
remain unchanged. On the other hand, the constraint (iii)
bik ≤
1. Thus, the resulting pro-
is modiﬁed to
k
gram is L times the program computing ˆDn(
DC1), ergo
ˆDn(
DC1).

aik
DCL) = L ˆDn(

+

k

Piecewise Linear Regression via a Difference of Convex Functions

3.2. Structural Risk Minimisation

To perform the SRM, we again utilize the structural re-
sult of Proposition 1 to determine the values that the op-
timal estimate takes at each of the xi. The choice of
the values is penalised by the seminorm as λL, where
λ = 24M ˆDn(
DC1), which may be computed using the
program (5). Note that the logarithmic term in the gen-
eralisation bound (4) is typically small, and is thus omit-
ted in the following. This also has the beneﬁt of render-
ing the objective function convex, as it avoids the log L
term that would instead enter.
If desired, an convex up-
per bound may be obtained for the same, for instance
by noting that
. This
has the effect of bumping up the value of λ required by
O(M 2/√n). However, theoretically this term is strongly
dominated by the ˆDn (see §4), while practically even the
value λ = ˆDn(
DC1) tends to produce overly smoothened
solutions (see §5).

max(1, log2 k

1 +

p

≤

k

k

k

f

f

)

With the appropriate choice of λ, this yields the following
convex optimisation problem,

n

min
ˆyi,zi,ai,bi,L

yi)2 + λL

( ˆyi −

(6)

i=1
X
ˆyj + zi −
zj ≥ h
k

zj ≥ h
bj, xi −
bik1 ≤
L

ˆyi −
s.t. 
zi −

aik1 +
k


aj, xi −
xji

xji

i, j
i, j
i

[n]
[n]
[n]

∈
∈
∈

Once again, in the above constraints, the ﬁrst two are nec-
essary and sufﬁcient to ensure that a DC function taking
the values ˆyi at xi exists, with the vectors ai, bi serving as
witnesses for the subgradients of the convex parts of such a
function at the xi, and the third constraint enforces that the
function has seminorm at most L. Notice that the third con-
dition effectively imposes ℓ1-regularisation on the weight
vectors ai, bi. This causes these weights to be sparse.

negative parts of each of their components separately. Over-
all, this renders the program (5) as a linear programming
problem over 2n(2d + 1) variables, and with n2 non-trivial
constraints. Note that in our setting, one typically requires
that n
d - that one has more samples than dimensions.
Via interior point methods, this problem may be solved in
O(n5) time.

≥

−

ˆy)2, the sec-
For the least squares loss ℓ(y, ˆy) = (y
ond program (6) is a convex quadratic program when the
1-norm constraints are rewritten as above. The decision
variables are the same as the ﬁrst problem, with the addi-
tion of the single L variable, and the constraints remain
identical. Again, via interior point methods, these pro-
grams can be solved in O(d2n5) time (see Ch. 11 of
Boyd & Vandenberghe (2004)). The latter term dominates
this complexity analysis. We note that in practice, these
problems can be solved signiﬁcantly faster than the above
bounds suggest.

Speeding up training via a GPU implementation an it-
erative solver for a modiﬁed version of the SRM in pro-
gram (6) is given in Algorithm (1) in the Appx E via the
ADMM method (Boyd et al., 2011). Each iteration of this
algorithm can be distributed to O(n2 + nd) parallel pro-
cessors. A python implementation is given in our GitHub
repository 2 which is compatible with GPU’s. We note that
a similar algorithm for that of Lipschitz convex regression
is provided in (Bal´azs, 2016; Mazumder et al., 2019) how-
ever not all the ADMM blocks are solved in closed form
and require additional optimization in each iteration.

bi, xii

ai, xii

and zi − h

Prediction By appending a 1 to the input, and the constants
to ai and bi, we can re-
yi + zi − h
duce the inference time problem to solving two maximum
inner product search problems over n vectors in Rd+1. This
is a well developed and fast primitive, e.g. Shrivastava & Li
(2014) provide a locality sensitive hashing based scheme
that solves the problem in time that is sublinear in n.

Finally, we may use the witnessing values ˆyi, zi and ai, bi,
to construct, in the manner of Proposition 2, the following
PLDC function, which interpolates the values ˆyi to the en-
tirety of the domain. Notice that since this function has the
same loss as any DC function that satisﬁes f (xi) = yi, this
construction enjoys risk bounds constructed above.

4. Analysis

We note that this analysis makes extensive use of the work
of Bal´azs et al. (2015); Bal´azs (2016) on convex and max-
afﬁne regression, with emphasis on the latter thesis, which
contains certain reﬁnements of the former paper.

ˆf (x) , max
i∈[n]h

ai, x

xii
−
bi, x

+ ˆyi + zi

(7)

+ zi

xii

−

max
i∈[n]h

−

3.3. Computational Complexity

In this section, we assume that the true model y = f (x)+εi
holds for a f that is a DC function, and that we have explicit
L. Also recall our assumption that
knowledge that
the distribution is supported on a set that is contained in the
ℓ∞ ball of radius R. We begin with a few preliminaries

k ≤

k

f

Training First, we note that we may replace the constraints
on the 1-norms of the vectors ai, bi in the above by linear
constraints via the standard trick of writing the positive and

A lower bound on risk The minimax risk of estima-
tion under the squared loss is Ω(n−2/d+2). This follows
by setting p = 1 (i.e., Lipschitzness) in Theorem 3.2 of

Piecewise Linear Regression via a Difference of Convex Functions

(Gy¨orﬁ et al., 2006), which can be done since the standard
constructions of obstructions to estimators used in showing
such lower bounds all have regularly varying derivatives,
and thus ﬁnite DC seminorms.

k

φ

−

φPLk ≤

PLDC solutions are not lossy Lemma 5.2 of (Bal´azs,
2016) argues that for every convex L-Lipschitz functions
φ with Lipschitz constant1, supx k∇∗φ
k1 bounded by L,
there exists a Lipschitz max-afﬁne function φPL with max-
36LRn−2/d.
imum over n pieces such that
Recall that PLDC functions can be represented as differ-
ences of max-afﬁne functions, and DC functions as differ-
ences of convex functions. Since the DC seminorm con-
trols the 1-norm of the subgradients, which dominates the
DCL function f , there ex-
2-norm, it follows that for every
fPLk∞ = O(n−2/d).
ists a pl-
DC2L function fPL with
f
k
Note that the resulting excess risk in the squared error
due to using PLDC functions can thus be bounded as
O(n−4/d), which is o(n−2/d+2), i.e., it is dominated by
minimax risk.

−

4.1. Statistical risk

The bound (4) provides a instance speciﬁc control on the
generalisation error of an estimate via the empirical max-
imum discrepancy ˆDn. This section is devoted to provid-
ing generic bounds on the same under the assumption of
i.i.d. sampled data. We adapt the analysis of (Bal´azs, 2016)
for convex regression in order to do so. The principal result
is as follows.

Theorem 3. For distributions supported on a compact do-
, with n
main Ω

d, it holds that

R

x

⊂ {k

k∞ ≤

}

ˆDn(

DCL)

≤

60LR

d
n

1 + 2

(cid:18)

log(n/d)
d + 4

.

(cid:19)

(cid:19)
Further, if the ground truth f
at least 1

(cid:18)

∈ DCL, then with probability
δ over the data, the estimator ˆf of (7) satisﬁes

≥
2/d+4

−
ˆf (x)
|

E[

|

2]

E[
Y
|
+ O((n/d)−2/d+4 log(n/d)) + O(

f (x)
|

2]

−

−

≤

Y

log(1/δ)/n).

p

Proof. Assume f
∈ DCL. Note that for any convex repre-
ψ2, we may instead construct a repre-
sentation f = ψ1 −
φ2 + c for a constant c so that the result-
sentation f = φ1 −
ing convex function φ1 and φ2 are uniformly bounded on
the domain - indeed, this may be done by setting c = f (0),
ψk(0). The φs retain the bound on their
and φk = ψk −
Lipschitz constants, and thus are uniformly bounded by
LR over the domain. Thus, we may represent the class
DCL functions as the sum of a constant, and a
of
DCL
1Bal´azs (2016) presents an argument with the 2-norm of sub-
gradients bounded, but this can be easily modiﬁed to the case of
bounded 1-norm under bounds on kxk∞.

DCL,0).

function whose convex parts are bounded. Call the latter
DCL,0. Importantly, since the constants are cancelled
class
in the computation of empirical maximum discrepancy, we
DCL) = ˆDn(
can observe that ˆDn(
The principle advantage of the above exercise is that the em-
pirical discrepancy for DC functions with bounded convex
parts can be controlled via the metric entropy of bounded
Lipschitz convex functions, which have been extensively
analysed by Bal´azs (2016). This is summarised in the fol-
lowing pair of lemmata. The ﬁrst argues that the discrep-
ancy of
DCL,0 functions is controlled by that of bounded
Lipschitz convex functions.

Lemma 1. Let
are L-Lipschitz and bounded by LR. Then ˆDn(
2 ˆDn(

CL,LR be the set of convex functions that
≤

DCL,0)

CL,LR).

The proof of the above is left to Appx C. The second lemma,
due to Dudley, is a generic method to allow control on the
discrepancy. We state this for

CL,LR

Lemma 2. Let
CL,LR under the sup-metric d(f, g) =
class
g
Then the empirical maximum discrepancy is bounded as

CL,LR, ǫ) be the metric entropy of the
k∞.

H∞(

−

k

f

ˆDn(

CL,LR)

inf
ǫ>0  

≤

ǫ + LR

2 H∞(

CL,LR, ǫ)
n

r

.

!

Finally, we invoke the control on metric entropy
of bounded Lipschitz convex functions provided by
(Bal´azs et al., 2015; Bal´azs, 2016)

Theorem (Bal´azs et al., 2015; Bal´azs, 2016). For ǫ
(0, 60LR],

∈

H∞(

CL,LR, ǫ)

≤

3d

40LR
ǫ

(cid:18)

d/2

(cid:19)

log

60LR
ǫ

.

(cid:19)

(cid:18)

Using the above in the bound of Lemma 2, and choosing
ǫ = (60LR)(d/n)2/d+4 yields the claim.

Control on the excess risk of the estimator follows readily.
For any λ
DC1), we have have, with high prob-
ability,

24M ˆDn(

≥

E[( ˆf
ˆE[( ˆf
ˆE[(f
E[(f
E[(f

−

−

−

−

−

≤

≤

≤

≤

ˆf
k

+ 2λ + O(1/√n)

Y )2]
Y )2] + λ
k
Y )2] + λ
f
k
Y )2] + 2λ
k
Y )2] + 2λ(L + 1) + O(1/√n)

+ 2λ + O(1/√n)

+ 2λ + O(1/√n),

k
f

k

where the ﬁrst and last inequalities utilise (4), while the
second inequality is because ˆf is the SRM solution. The
claim follows on incorporating the bound on ˆDn developed
above, and since we choose λ proportional to the same.

Piecewise Linear Regression via a Difference of Convex Functions

On adaptivity Notice that the argument for showing the
excess risk bound proceeds by controlling ˆDn. This allows
the argument to adapt to the dimensionality of the data. In-
deed, if the true law is supported on some low dimensional
manifold, then the empirical discrepancy, which only de-
pends on the observed data, grows only with this lower di-
mension. More formally, due to the empirical discrepancy
being an empirical object, we can replace the metric en-
tropy control over DC functions in Rd by metric entropy of
DC functions supported on the manifold in which the data
lies, which in turn grows only with the (doubling) dimen-
sion of this manifold.

On suboptimality Comparing to the lower bound, we
ﬁnd that the above rate is close to being minimax, although
the (multiplicative) gap is about n4/d2
and diverges polyno-
mially with n. In analogy with the observations of Bal´azs
(2016) for max-afﬁne regression, we suspect that this sta-
tistical suboptimality can be ameliorated by restricting the
PLDC estimate to have some (precisely chosen) Kn < n
hyperplanes instead of the full n. However, as discussed in
§1.1, such restrictions lead to increase in the computational
costs of training, and thus, we do not pursue them here.

5. Experiments

In this section we apply our method to both synthetic
and real datasets for regression and multi-class classiﬁca-
tion. The datasets were chosen to ﬁt in the regime of
102 as described in the introduction. All
n
results are averaged over 100 runs and are reported with
the 95% conﬁdence interval.

103, d

≤

≤

For the DC function ﬁtting procedure, we note that that the
theoretical value for the regularization weight tends to over-
smooth the estimators. This behaviour is expected since
the bound (4) is designed for the worst case. For all the
subsequent experiments we make two modiﬁcations - since
none of the values in the datasets observed are very large,
we simply set 12M = 1, and further, we choose the weight,
i.e. λ in (6), by cross validation over the set 2−j ˆDn(
DC1)
for j
8 : 1]. Fig. 1 presents both these estimates in a
simple setting where one can visually observe the improved
ﬁt. Note that this tuning is still minimal - the empirical
discrepancy of
DC1 ﬁxes a rough upper bound on the λ
necessary, and we explore only a few different scales.

−

∈

[

|

yi −

For the regression task we use the L1 empirical loss in our
algorithm, instead of L2. That is, the objective in (6) is
replaced by
. This change allows us to imple-
ment the ﬁtting program as a linear programming problem
and signiﬁcantly speeds up computation. However, in the
following we will only report the L2 error of the solutions
obtained thi way. We compare our method to a multi-layer
perceptron (neural network) with variable number of hid-

ˆyi|

P

Figure1. Top A two dimensional function along with the sampled
points used for estimating the function; Bottom learned DC func-
tion via L1 regression using only λ = 2 ˆDn (left); using cross
validation over λ (right).

den layers chosen from 1 : 10 by 5-fold cross validation,
Multivariate Adaptive Regression Splines (MARS) and K-
nearest neighbour(K-NN) where Best value of K was cho-
sen by 5-fold cross validation from 1 : 10.

For the multi-class classiﬁcation task we adopt the multi-
class hinge loss to train our model, i.e. the loss

m

i
X

Xj6=yi

max(fj(xi)

−

fyi(xi) + 1, 0),

where m is the number of classes and fj’s are DC functions.
We compare with the same MLP as above but trained with
the cross entropy Loss, KNN and a one versus all SVM.

In both cases, we have used MATLAB Statistics and Ma-
chine learning Library for their implementation of MLP,
KNN and SVM. For MARS we used the open source imple-
mentation in ARESLab toolbox implemented in MATLAB.
Our code along with the other algorithms is available in our
GitHub repository2
.

In each of the following tasks, we observe that our method
performs competitively to all considered alternatives in al-
most every dataset, and often outperforms them, across the
variation in dimensionality and dataset sizes.

2https://github.com/Siahkamari/Piecewise-linear-regression

Piecewise Linear Regression via a Difference of Convex Functions

Regression on Synthetic Data We generated data from
the function,

y = f (x) + ε

f (x) = sin

π
√d

(cid:0)

d

xj

+

j=1
X

(cid:1)

(cid:0)

1
√d

d

xj

2

,

j=1
X

(cid:1)

where the x is sampled from a standard Gaussian, while ε
is a centred Gaussian noise with standard deviation of 0.25.

We generate 50 points for training. For testing we estimate
the Mean Squared Error based on a test set of 5000 points
without the added noise. We normalize the MSE by vari-
ance of the values of test data and multiply by 100. Results
are presented in Fig. 2. Observe that our algorithm con-
sistently outperforms the competing methods, especially as
we increase the dimension of the data. Furthermore our
algorithm has lower error variance across the runs.

DC
MLP
MARS
KNN

E
S
M
d
e
z

i
l

a
m
r
o
N

300

250

200

150

100

50

0

Rock

  Acetylene   Moore

  Reaction   Car Small

Cereal

 BostonHousing   ForestFires

Figure3. Normalized Mean Squared Error in regression tasks.

Fig. 4. We observe to perform comparably to other algo-
rithms on some datasets and outperform in others.

DC
MLP
MARS
KNN

250

200

150

100

50

E
S
M
d
e
z

i
l

a
m
r
o
N

%
n
o
i
t
a
c
i
f
i
s
s
a
c
-
s
M

i

l

DC
MLP
SVM
KNN

50

45

40

35

30

25

20

15

10

5

0

0

1

3

5

7
9
dimension

11

13

15

Figure4. Miss-Classiﬁcation on UCI data sets.

BPD

Iris

 Balance Scale 

Ecoli

  Wine

 Heart Disease  Ionosphere  

Figure2. Mean Squared Error in a regression task vs dimension
of the data in the synthetic experiements. Note that both the value
and the size of the error bars are consistently better than the com-
peting methods

Regression on Real Data We apply the stated methods to
various moderately sized regression datasets that are avail-
able in the MATLAB statistical machine learning library.
The results are presented in Fig. 3.

In the plot, the datasets are arranged so that the dimension
increases from left to right. We observe that we do com-
parably to the other methods for some datasets and outper-
form in others. See Appx. D for a description of each of
the datasets studied.

Multi-class classiﬁcation We used popular UCI classiﬁ-
cation datasets for testing our classiﬁcation algorithm. We
repeated the experiments 100 times We present the mean
and 95% C.I.s on a 2-fold random cross validation set, in

6. Discussion

The paper proposes an algorithm to learn piecewise linear
functions using difference of max-afﬁne functions. Our
model results in linear or convex programs which can be
solved efﬁciently even in high dimensions. We have shown
several theoretical results on expressivity of our model, as
well as its statistical properties, including good risk de-
cay and adaptivity to low dimensional structure, and have
demonstrated practicality of our resulting model under re-
gression and classiﬁcation tasks.

Wider context Non-parametric methods are most often
utilised in settings with limited data in moderate dimen-
sions. Within this context, along with strong accuracy, it
is often desired that the method is fast, and is interpretable,
especially in relatively large dimensions.

In settings with large datasets, accuracy is relatively easy to
achieve, and speed at inference is more important. In low

 
 
 
 
 
Piecewise Linear Regression via a Difference of Convex Functions

dimensional settings, this makes methods such as MARS
attractive, due to their fast inference time, while in high-
dimensions MLPs are practically preferred. In settings with
low dimensionality and small datasets, interpretability and
speed take a backseat due to the small number of features,
while accuracy become the critical requirement, promoting
use of kernelised or nearest neighbour methods.

On the other hand, for small datasets in moderate to high
dimensions, interpretability gains an increased emphasis.
Within this context, our method results in a piecewise linear
ﬁt for which it is easy to characterise the locally important
features, and further does so with relatively sparse weights
via the ℓ1 regularisation on ai, bi. Further, since the subop-
timality of our statistical risk is controlled as nO(1/d2), as
the dimension increases, our method gets closer to the opti-
mal accuracy. This thus represents the natural niche where
application of DC function regression is appropriate.

7. Acknowledgment

This research was supported by NSF CAREER Award
1559558, CCF-2007350 (VS), CCF-2022446 (VS), CCF-
1955981 (VS) and the Data Science Faculty Fellowship
from the Raﬁk B. Hariri Institute.

References

Baˇc´ak, M. and Borwein, J. M. On difference convexity of
locally lipschitz functions. Optimization, 60(8-9):961–
978, 2011.

Bal´azs, G. Convex Regression: Theory, Practice, and Ap-

plications. PhD thesis, University of Alberta, 2016.

Bal´azs, G., Gy¨orgy, A., and Szepesv´ari, C. Near-optimal
max-afﬁne estimators for convex regression. In AISTATS,
2015.

Bartlett, P. L. and Mendelson, S. Rademacher and gaussian
complexities: Risk bounds and structural results. Jour-
nal of Machine Learning Research, 3(Nov):463–482,
2002.

Boyd, S. and Vandenberghe, L. Convex optimization. Cam-

bridge university press, 2004.

Boyd, S., Parikh, N., and Chu, E. Distributed optimiza-
tion and statistical learning via the alternating direction
method of multipliers. Now Publishers Inc, 2011.

Cui, Y., Pang, J.-S., and Sen, B. Composite difference-
max programs for modern statistical estimation prob-
lems. SIAM Journal on Optimization, 28(4):3344–3374,
2018.

Friedman, J. H. et al. Multivariate adaptive regression

splines. The annals of statistics, 19(1):1–67, 1991.

Gasso, G., Rakotomamonjy, A., and Canu, S. Recovering
sparse signals with a certain family of nonconvex penal-
ties and dc programming. IEEE Transactions on Signal
Processing, 57(12):4686–4698, 2009.

Ghosh, A., Pananjady, A., Guntuboyina, A., and Ramchan-
dran, K. Max-afﬁne regression: Provable, tractable,
and near-optimal statistical estimation. arXiv preprint
arXiv:1906.09255, 2019.

Gy¨orﬁ, L., Kohler, M., Krzyzak, A., and Walk, H.
A distribution-free theory of nonparametric regression.
Springer Science & Business Media, 2006.

Hannah, L. A. and Dunson, D. B. Multivariate convex re-
gression with adaptive partitioning. The Journal of Ma-
chine Learning Research, 14(1):3261–3294, 2013.

Hartman, P. et al. On functions representable as a difference
of convex functions. Paciﬁc Journal of Mathematics, 9
(3):707–713, 1959.

Hildreth, C. Point estimates of ordinates of concave func-
tions. Journal of the American Statistical Association,
49(267):598–619, 1954.

Holloway, C. A. On the estimation of convex functions.

Operations Research, 27(2):401–407, 1979.

Holmes, C. and Mallick, B. Bayesian regression with mul-
tivariate linear splines. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 63(1):3–17,
2001.

Horst, R. and Thoai, N. V. Dc programming: overview.
Journal of Optimization Theory and Applications, 103
(1):1–43, 1999.

Kripfganz, A. and Schulze, R. Piecewise afﬁne functions
as a difference of two convex functions. Optimization,
18(1):23–29, 1987.

Magnani, A. and Boyd, S. P. Convex piecewise-linear ﬁt-
ting. Optimization and Engineering, 10(1):1–17, 2009.

Mazumder, R., Choudhury, A., Iyengar, G., and Sen, B. A
computational framework for multivariate convex regres-
sion and its variants. Journal of the American Statistical
Association, 114(525):318–331, 2019.

Ovchinnikov, S. Max-min representation of piecewise lin-
ear functions. Contributions to Algebra and Geometry,
43(1):297–302, 2002.

Piot, B., Geist, M., and Pietquin, O. Difference of convex
functions programming for reinforcement learning.
In
Advances in Neural Information Processing Systems, pp.
2519–2527, 2014.

Piecewise Linear Regression via a Difference of Convex Functions

Shalev-Shwartz, S. and Ben-David, S. Understanding ma-
chine learning: From theory to algorithms. Cambridge
university press, 2014.

Shrivastava, A. and Li, P. Asymmetric lsh (alsh) for sub-
In
linear time maximum inner product search (mips).
Ghahramani, Z., Welling, M., Cortes, C., Lawrence,
N. D., and Weinberger, K. Q. (eds.), Advances in Neu-
ral Information Processing Systems 27, pp. 2321–2329.
Curran Associates, Inc., 2014.

Toriello, A. and Vielma, J. P. Fitting piecewise linear con-
tinuous functions. European Journal of Operational Re-
search, 219(1):86–95, 2012.

Wahba, G.

Spline models for observational data, vol-

ume 59. Siam, 1990.

Yuille, A. L. and Rangarajan, A. The concave-convex pro-
cedure (cccp). In Advances in neural information pro-
cessing systems, pp. 1033–1040, 2002.

Appendix to ‘Piecewise Linear Regression via a Difference of
Convex Functions’

A. Connection to L1-regularised splines

The following proposition argues that in the univariate case ﬁtting a DC function is equivalent to ﬁtting an L1-regularised
spline. Note that in the bivariate case, L1 splines are regularised by the Frobenius 1-norm of the Hessian of the matrix,
while in the multivariate case it is similarly regularised by a higher derivative tensor. Thus, our method is an alternate
generalisation of the L1-spline in the univaraite case.
Proposition 6. In the univariate setting, solving regression with the L1-spline objective,

is equivalent to regression with difference of convex functions penalized by their DC seminorm, with a free linear term

n

ℓ(f (xi), yi) + λ

1

0 |

Z

′′

f

ds.

(s)
|

inf
f ∈C 2

i=1
X

min
φ1,φ2 convex,a

n

i=1
X

ℓ(φ1(xi)

−

φ2(xi) + ax, yi) + 2λ sup

x∈[0,1] |

φ′
1(x)
|

+

φ′
2(x)
|

|

Proof. Suppose f is the solution to (8) and φ1 −

φ2 + ax = f , where φ1 and φ2 are convex. Hence for some a1, a2 ∈

φ′
1(x) = a1 +

φ′
2(x) = a2 +

a = a2 −

x

0
Z

x

f ′′+(s) + g(s) ds

f ′′−(s) + g(s) ds

0
Z
a1 + f ′(0),

(8)

(9)

R:

where g(s)
0 otherwise the second differential of φ1(s) or φ2(s) would be negative which contradicts convexity. Since
the error term in (9) is invariant to choices of a1, a2 and g, the minimization in (9) seeks to minimize only the regularization
term. Thus,

≥

min
φ1,φ2 convex
φ1−φ2+ax=f

sup
x (cid:18)

φ′
1(x)
|

|

+

φ′
2(x)
|

|

(cid:19)

x

f ′′+(s) + g(s)ds

x

f ′′−(s) + g(s)ds

+

a2 +
(cid:12)
(cid:12)
f ′′+(s) + g(s)ds
(cid:12)

0
Z

(cid:12)
(cid:12)
(cid:12)

1

1

(cid:12)
(cid:12)
f ′′−(s) + g(s)ds
(cid:12)

(cid:12)
(cid:12)
(cid:12)

0
Z

+

a2 +
(cid:12)
(cid:12)
(cid:12)
(cid:19)

(cid:19)

(cid:12)
(cid:12)
(cid:12)

f ′′−(s) + g(s)ds

1

0
Z

(cid:19)

(cid:12)
(cid:12)
(cid:12)

= min
g≥0

min
a1,a2

∗= min
g≥0

min
a1,a2

0
Z

a1 +

sup
x (cid:18)(cid:12)
(cid:12)
(cid:12)
max
(cid:18)(cid:12)
(cid:12)
(cid:12)
(cid:12)
f ′′+(s) + g(s)ds
(cid:12)
(cid:12)

a2
(cid:12)
(cid:12)
(cid:12)

a1

+

(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)

1

,

∗∗= min
g≥0

= min
g≥0

=

1
2

0 |
Z

1
2

1
2

(cid:18)

(cid:18)
1

0
Z
1

(cid:12)
(cid:12)
(cid:12)

0 |
Z
f ′′(s)
|

ds.

f ′′(s)
|

ds +

1

0
Z

a1 +

+

(cid:12)
(cid:12)
(cid:12)

g(s)ds

0
Z
1
2

(cid:12)
(cid:12)
(cid:12)
(cid:19)

Therefore (8) and (9) are equivalent as they have the same objective.

) is true since the expressions inside the absolute value are convex and monotonic in terms of x, which causes the inner

(
∗
maximization to take place at endpoints of x, i.e. x = 0 or x = 1. To show (

) consider,

∗∗

c1 =

c2 =

1

0
Z

1

0
Z

f ′′+(s) + g(s)ds

f ′′−(s) + g(s)ds.

(10)

|c1|+|c2|
2

. To show

≤

Piecewise Linear Regression via a Difference of Convex Functions

It’s sufﬁcient to show

C = min
a1,a2

max

|
(cid:16)

+

a1|

,

a2|

|

a1 + c1|

|

+

a2 + c2|

|

c1|

= |

c2|

|

.

+
2

(cid:17)

by choosing a1 =
this upper bound is tight, assume C < |c1|+|c2|

c1
2 and a2 =

−

−

2

c2
2 the value on the right hand side of (10) is achieved. Thus C

, hence both options for the max player should be smaller than this, e.g:

However this in turn causes:

a1|

|

+

a2|

|

c1|

< |

c2|

|

.

+
2

C

a1 + c1|

≥ |

+

|

a2 + c2| ≥ |
> |

a1|
c1| − |
+
c2|
c1|
|
2

+

c2| − |

a2|

|

,

which contradicts the assumption C < |c1|+|c2|

2

.

B. Proofs Omitted from §2

B.1. Proof that the DC seminorm is indeed a seminorm

Proposition 7. The functional

f

k

k

:= min

φ1,φ2 convex
φ1−φ2=f

sup
x

sup
vi∈∂∗φi(x) k

v1k1 +

v2k1

k

is a seminorm over the class

DC

(Rd). Further, if DC functions are identiﬁed up to a constant, it is a norm.

aφ2 is a DC representation if a > 0, and f =

Proof.
Homogenity: Since non-negative scalings of convex functions are convex, if f = φ1 −
aφ1 −
then av
k · k1 is a norm, it holds that
= min

aφ1) is a DC representation if a
(
−
av
k1. Thus,
k1 =
v2k1

|
|k
v1k1 +

∂∗(aφi) and since

aφ2 −
k

sup
vi∈∂∗φi(x) k

sup
x

af

−

∈

v

a

k

k

k

φ1,φ2 convex
φ1−φ2=af

φ2, then for any real a, af =
0. Trivially, if v
∂∗φi

≤

∈

sup
x

sup
vi∈∂∗φi(x) k

(
±

a)v1k1 +

(
±

a)v2k1

k

sup
x

sup
vi∈∂∗φi(x) |

a

(
k

v1k1 +

|

v2k1)

k

= min

φ1,φ2 convex
φ1−φ2=f

= min

φ1,φ2 convex
φ1−φ2=f

=

a

f

.

k

|k

|

Triangle Inequality: For a DC function f , let Cf be the set of pairs of continuous convex functions whose difference is f ,
i.e.

Cf :=

(φ1, φ2) cont., convex : φ1 −

{

φ2 = f

.

}

We ﬁrst argue that for DC functions f, g,

Cf +g =

(ψ1, ψ2) :

{

(φ1, φ2)

∃

∈

Cf , (

φ1,

φ2)

∈

Cg s.t. ψ1 = φ1 +

φ1, ψ2 = φ2 +

φ2}
e

e
ψ2 = f + g, and φ1 −
This follows because any decomposition ψ1 −
and vice versa, so Cf +g lies within the right hand side above. On the other hand, for any pair (φ1, φ2)
Cf +g, so the right hand side lies within the Cf +g.
φ1,
(

φ2 = f gives decomposition (ψ1 + φ2, ψ2 + φ1)

Cg, clearly (φ1 +

φ1, φ2 +

φ2)

φ2)

∈

e

e

Cg
∈
Cf and

∈

∈

e

e

e

e

Piecewise Linear Regression via a Difference of Convex Functions

As a consequence,

k
=

f + g

k
min
(ψ1,ψ2)∈Cf +g

sup
x

sup
ui∈∂∗ψi(x) k

= min

(φ1,φ2)∈Cf
e
e
φ2)∈Cg
φ1,
(

sup
x

sup
vi∈∂∗φi(x)
evi∈∂∗ ˜φi(x)

v1 +

k

min
(φ1,φ2)∈Cf
e
e
φ2)∈Cg
φ1,
(

sup
x

sup
vi∈∂∗φi(x)
evi∈∂∗ ˜φi(x)

u1k1 +

u2k1

k
v1k1 +

v2 +

k

v2k1

e
v1k1 +

k

v1k1 +

k

e
v2k1 +

k

v2k1

k

≤

≤

=

min
(φ1,φ2)∈Cf

sup
x

sup
vi∈∂∗φi(x) k

v1k1 +

k

+

f

k

k

g

.

k

k

e
v2k1 + min

e
φ1,

(

e
φ2)∈Cg

e
sup
x

sup
evi∈∂∗ ˜φi(x) k

v1k1 +

v2k1

k

e

e

Positive Deﬁnite up to constants: We note that if
f
k
implies that there exists a DC representation f = φ1 −
norm, ergo
∇
k∇
= 0. Thus, the norm is zero iff the function is a constant. Lastly, note that for any f and a constant c,
c

= 0, then f is a constant function. Indeed, the norm being zero
φ2 such that for all x, the biggest subgradients in ∂∗φi(x) have 0
f = 0 everywhere. Conversely, clearly for every constant function,
.

k
k1 = 0, and thus,

k1 =

φ1(x)

φ2(x)

f + c

k∇

=

f

k

k
Thus, equating DC functions that differ only by a constant turns the above seminorm into a norm.

k

k

k

k

B.2. ReLU networks and PLDC functions

B.2.1. PROOF OF PROPOSITION 4

Proof. Recall that a fully connected neural network with ReLU activations, and D layers with weight matrices
W 1, . . . , W D, is a function, here f , taking the form

f (x) =

wD+1
j

aD
j

j
X

al+1
i = max(

wl+1

i,j al

j, 0), D > l

j
X

a1
i = max(

w1

i,jxj , 0),

j
X

1

≥

Our proof of the statement is constructed by induction over the layers of the network, using the following relations:

max(a, b) + max(c, d) = max(a + b, a + d, b + c, b + d).

max(max(a, 0)

−

max(b, 0), 0) = max(a, b, 0)

max(b, 0)

−

=

max
z∈{0,1},t∈{0,1}

(tza + t(1

z)b)

max
t∈{0,1}

tb.

−

−

(11)

(12)

We will prove each node al

i is a DC function of the form:

al
i = max

k∈[Kl]h

αl

i,k, x

max
k∈[Kl]h

βl

i,k, x

i

i −

such that

max
k

(
k

αl+1
i,k k

,

βl+1
i,k k

)

k

≤

j
X

wi,j|

|

max
k

(
k

αl

j,kk

,

k

βl

j,kk

)

The base of induction is trivial by looking at the deﬁnition for a1
we assume the claim holds al

j for all j, we induce the results for al+1

i which is max of linear terms and therefore convex. Now
. Assume layer l has h hidden units. For clarity we

i

Piecewise Linear Regression via a Difference of Convex Functions

, al

j and Kl. We further deﬁne s , al+1

i

.

drop the index l from wl+1

j

s = max

wjaj, 0

(cid:18) Xj∈[h]

(cid:19)

w−

j aj, 0

−

(cid:19)

Xj∈[h]
j αj,k, x

w+

= max

= max

w+
j aj −

(cid:18) Xj∈[h]

max
k∈[K]

D

(cid:18) Xj∈[h]

(11)
= max

max
k1,k2∈[h]K

(cid:18)

D Xj∈[h]

(12)
=

max
k1,k2∈[h]K,z∈{0,1},t∈{0,1}

t

D

max
k1,k2∈[h]K,t∈{0,1}

−

t

Xj∈[h]
w−

−

Xj∈[h]
Now consider z = 1 and note that for each term of max function:

D

E

j αj,k1(j) + w+

j βj,k2(j), x

+ max
k∈[K]

−

D
E
w−
w+
j αj,k1(j) −

w−

j βj,k, x

−

E

Xj∈[h]

max
k∈[K]

−

D

j βj,k2(j), x

max
k1,k2∈[h]K

−

E

w−

j αj,k, x

max
k∈[K]

−

w+

j βj,k, x

, 0

E

D
j αj,k1(j) + w+

w−

j βj,k2(j), x

(cid:19)

E

, 0

(cid:19)

E

z(w+

j αj,k1(j) −

w−

j βj,k2(j)) + (1

−

w−

j αj,k1(j) + w+

j βj,k2(j)), x

E

−

D Xj∈[h]
z)(

−

w+
j αj,k1(j) −

w−
j βj,k2(j)k ≤

k

Xj∈[h]

Xj∈[h]

wj |

|

max(
k

αj,k1(j)k

,

βj,k2(j)k

)

k

wj |

|

(
max
k
k

αj,kk

,

βj,kk

)

k

≤

Xj∈[h]

The proof is analogous for z = 0.

B.2.2. PROOF OF PROPOSITION 5

Proof. For convenience, we extend the input dimension by 1 and append a constant 1 to the end, so that the effecive inputs
are (x, 1). This allows us to avoid writing constants in the following.

Let the PLDC function in question be

f (x) = max

ak, x

k∈[K]h

max
k∈[K]h

bk, x

.
i

i −

We give a construction below to compute maxk∈[K]h
width of at most
would then constitute the required construction.

≤

hidden layers, and
2K if K is a power of 2. Note that repeating this construction parallelly with the as replaced by bs

via a ReLU net that has at most 2

log2 K

ak, x

⌈

⌉

i

By adding dummy values of ak, bk (i.e., equal to 0), we may increase the K to a power of two without affecting the depth
2⌈log2 K⌉
in the claim. The resulting width will be bounded as 2
4K, and will yield the bound claimed. This avoids us
having to introduce further dummy nodes later, and simpliﬁes the argument.

≤

·

To begin with, we set the ﬁrst hidden layer to be of 2K nodes, labelled (k, +) and (k,

) for k

−

∈

[K], with the outputs

h1
ak, x
k,+ = max(
h
h1
k,− = max(

, 0)
i
ak, x

h−

, 0)
i

Thus, the outputs of the ﬁrst layer encode the inner products
of nodes into 3 nodes, indexed as (j, 0), (j, 1), (j, 0) for j

ak, x
h
[K] such that the outputs are

i

in pairs. In the next layer, we we collate two

pairs

±

j,0 = max(h1
h2
j,1 = max(h1
h2
h2
j,1 = max(

−

∈
h1
2j,+ + h1
h1
2j−1,+ −
2j−1,− −
h1
a2j, x
2j,−, 0) = max(
2j,+ −
h
2j,+ + h1
h1
2j,−, 0) = max(

, 0)
i
a2j, x

h−

, 0)
i

a2j−1 −
2j,−, 0) = max(
h

a2j, x

, 0)
i

Piecewise Linear Regression via a Difference of Convex Functions

In the next hidden layer, we may merge these three nodes for each j into two, giving a total width of
as (ℓ, +) and (ℓ,

). This utilises the simple relation

−

K + 1, represented

≤

easily shown by some casework. Let us deﬁne the outputs

max(a, b) = max(a

b, 0) + max(b, 0)

max(

−

−

b, 0),

−

ℓ,+ = max(h2
h3
h3
ℓ,− = max(

ℓ,0 + h2
h2
ℓ,0 −

h2
a2ℓ−1, x
a2ℓ, x
ℓ,2, 0) = max(max(
,
ℓ,1 −
i
h
h
h2
ℓ,1 + h2
a2ℓ−1, x
max(
ℓ,2, 0) = max(
h

), 0)
i
a2ℓ, x
h

,
i

−

−

), 0)
i

But now note that the outputs of the third hidden layer are analogous to those of the ﬁrst hidden layer, but with some
of the maximisations merged, and thus the circuit above can be iterated. This is exploiting the iterative property that
max(a, b, c, d) = max(max(a, b), max(c, d)). Thus, we may apply this construction log2 K times in total, reducing the
width of every odd numbered hidden layered by 2 each time, and using two hidden layers for each step. At the ﬁnal step,
, 0) and
by the iterative property of max, we will have access to a 2-node hidden layer with outputs max(maxk∈[K]h
i
max(

ak, x
, 0), and the ﬁnal layer may simply report the difference of these, with no nonlinearity.
i

maxk∈[K]h

ak, x

−

We note that more efﬁcient constructions are possible, but are harder to explain. In particular, the above construction can
be condensed into one with

3K width and

depth.

≤

log2 K

⌈

⌉

C. Proof of Theorem 1

M
L :=

DC

For succinctness, we let
. We note that this section makes extensive use of the work
M
of Bartlett & Mendelson (2002), and assumes some familiarity with the arguments presented there. We begin a restricted
version of Theorem 1.
Lemma 3. Given n i.i.d. samples S =
probability at least 1

from data yi = f (xi) + εi, where both f and ε are bounded by M , with

δ, it holds uniformly over all f

supx∈Ω |

(xi, yi)
}

DCL ∩ {

f (x)

≤

{

}

M
L that

∈ DC

−

E[(f (x)

|

y)2]

−

−

ˆES[(f (x)

y)2]

|

−

+ 12M ˆDn(

DCL) + 45M 2

4 + 8 log(2/δ)
n

r

We prove this lemma after the main text, and ﬁrst show how this is utilised to produce the bound of Theorem 1. The key
M
L′. This allows a doubling trick over
insight is that the classes
L′ for any L
M
L . The technique is standard, see, e.g., Thm. 26.14 of
the Ls to bootstrap the above bound to all of
(Shalev-Shwartz & Ben-David, 2014).

M
L ⊂ DC
L≥1 DC

M
L are nested, in that

DC
M =

DC

DC

≤

For naturals j
simultaneously that

≥

1, let Lj = 2j and δj = δ2−j = δ/Lj. With probability at least 1

δj, it holds for all f

−

M
Lj \DC

M
Lj−1

∈ DC

S

E[(f (x)

|

y)2]

−

−

ˆES[(f (x)

y)2]

−

| ≤

12M ˆDn(

DCLj ) + 45M 2

4 + 8 log Lj + 8 log(2/δ)
n

r

since
value of j as desired. Taking jf = max(1,

δj = δ, the union bound then extends this bound to all DC functions, whilst allowing us to use the appropriate
), we ﬁnd that for any f

M ,

f

P

∈ DC

⌈

log2 k
ˆES[(f (x)

k⌉

y)2]

|

−

−
DC2jf ) + 45M 2
r
DC2kf k+2) + 45M 2

E[(f (x)

y)2]

−
12M ˆDn(

12M ˆDn(

|

≤

≤

4 + 8 log 2jf + 8 log(2/δ)
n

4 + 8 ln(2) max(1, log2 k

n

) + 8 log(2/δ)

f

k

r

Proof of Lemma 3. We ﬁrst recall Theorem 8 of (Bartlett & Mendelson, 2002), which says that if ℓ is a loss function that
is uniformly bounded by B, then for any class of functions
of size n, with probability
δ/2, it holds uniformly over all f
at least 1

and an iid sample S =

(xi, yi)
}

that

F

{

−

∈ F

E[ℓ(f (x), y)]

≤

ˆES[ℓ(f (x), y)] +

ℜn(ℓ

◦ F

) + B

r

8 log(4/δ)
n

,

Piecewise Linear Regression via a Difference of Convex Functions

where

ℜn(ℓ

◦ F

In our case we are interested in

) is the Rademacher complexity of the class

ℓ(f (x), y)

=

F

DC

M
L and ℓ = (

· − ·

{
)2. Since

.

ℓ(0, y)
}
y

−
M and

|

| ≤

2M, we may take B = 9M 2.

f

|

| ≤

Next, we use properties of Theorem 12, part 4 of (Bartlett & Mendelson, 2002), which says that for L-Lipschitz ℓ which
takes the value ℓ(0, 0) = 0,
). Since our assumptions enforce that the argument to the squared loss
is bounded by 3M, we note that the loss is 6M -Lipschitz on this class. Thus, we conclude that with n samples, uniformly
over all f

ℜn(
F

ℜn(ℓ

◦ F

2L

≤

)

M
L

∈ DC

E[(f (x)

y)2]

−

≤

ˆES[(f (x)

−

y)2] + 12M

ℜn(

DC

M
L ) + 9M 2

8 log(4/δ)
n

.

r

Next we utilise Lemma 3 of (Bartlett & Mendelson, 2002), and that
1

δ/2,

M

L ⊂ DCL to conclude that with probability at least

DC

−

ℜn(

DC

M
L )

≤

ˆDn(

DC

M
L ) + 3M

2
n ≤

r

ˆDn(

DC L) + 3M

r

4 + 8 log(4/δ))
n

.

The claim then follows by using the union bound.

D. Proof of Lemma 1

Proof. For any function f , let

Note that ∆(f + g) = ∆(f ) + ∆(g). Then

∆(f ) ,

n/2

i=1
X

f (xi)

−

n

f (xi).

Xi=n/2+1

ˆDn(

∆(f )

f ∈DCL,0

DCL,0) = sup
sup
=
f ∈DCL,0
φ1,φ2∈CL,LR
φ1−φ2=f

∆(φ1)

sup
φ1,φ2∈CL,LR

≤

∆(φ1)

∆(φ2)

∆(φ2)

−

−

CL,LR)
DCL,0 as a difference of
where the ﬁrst inequality holds because of the representation of
the main text, and the second inequality holds since the supremum is non-negative, as 0

sup
φ1∈CL,LR

∆(φ1) + sup

∆(φ2)

φ2∈CL,LR

≤
= 2 ˆDn(

CL,LR functions, as discussed in
∈ CL,LR.

Piecewise Linear Regression via a Difference of Convex Functions

E. Algorithms via ADMM
This section provides a parallel ADMM optimizer for the piece-wise linear regression problem.

Algorithm 1 Piecewise Linear Regression via a Difference of Convex Functions

Input: data
Initialize:

{

(xi, yi)

xi ∈

|

RD, yi ∈

R

}i∈[n], ρ = 0.01, λ, T

ˆyi = zi = si,j = ti,j = αi,j = βi,j = 0
L = ai = bi = pi = qi = ui = γi = ηi = ζi = 0D×1,

Λi =

nxixT

i −

(cid:16)

xi(

xi)T

i
X

−

(

xi)xT

i +

xjxT

j + I

i
X

j
X

−1

(cid:17)

i, j
i

i

∈
∈

∈

[n]
[n]

[n]

for t = 1 to T do

Ai =

j
X

αj,i −

αi,j + sj,i −

si,j +

ai + aj, xi −
h

xji

+ 2yj

Bi =

ˆyi =

βj,i −
j
X
2
2 + nρ
1
2 + nρ
−
ai = Λi(pi −

zi =

bi = Λi(qi −
1
n

(
−

Ld =

βi,j + tj,i −
ρ
2(2 + nρ)

ti,j +

bi + bj, xi −
h

xji

Ai −

ρ
2(2 + nρ)

Bi

yi +

yi +

ηi +

1
2n(2 + nρ)

Ai +

1 + nρ
2n(2 + nρ)

Bi

(αi,j + si,j + ˆyi −

ˆyj + zi −

zj)(xi −

xj))

j
X

ζi +

j
X

(βi,j + ti,j + zi −

zj)(xi −

xj))

λd/ρ +

γi,d +

pi,d|

|

+

qi,d|

|

+ ui,d)

i
X

−

−

qi,d =

pi,d =

sign(ηi,d + ai,d)(
ηi,d + ai,d|
|
ζi,d + bi,d|
sign(ζi,d + bi,d)(
|
+ Ld)+
qi,d|
zi + zj +
bi, xi −
h
ˆyj + zi −
zj − h
+
|

1
2
1
2
ui,d = (
γi,d − |
pi,d| − |
si,j = (
αi,j −
ˆyi + ˆyj −
ti,j = (
βi,j −
zi + zj +
αi,j = αi,j + si,j + ˆyi −
βi,j = βi,j + ti,j + zi −
γi,d = γi,d + ui,d +
pi,d|
|
pi,d
ηi,d = ηi,d + ai,d −
qi,d
ζi,d = ζi,d + bi,d −

bi, xi −
Ld
qi,d| −

−

+ Ld −
+ Ld −

ui,d − |
ui,d − |

qi,d| −
pi,d| −

γi,d)+

γi,d)+

xji

ai, xi −
h
)+
xji
ai, xi −
zj − h
xji

)+

xji

[n]

[n]

[n]

[n]

[n]

[n]

i

i

i

i

i

i

∈

∈

∈

∈

∈

∈

i

i

i

∈

∈

∈

d

[n], d

[n], d

[n], d

i, j

i, j

i, j
i, j

[n], d

[n], d
[n], d

i

i
i

∈

∈
∈

[D]

[D]

[D]

[D]

[n]

[n]

[n]
[n]

[D]

[D]
[D]

∈

∈

∈

∈

∈

∈

∈
∈

∈

∈
∈

end for
Output:

ˆf (x) , maxi∈[n]h

ai, x

xii

+ ˆyi + zi −

−

maxi∈[n]h

bi, x

xii

−

+ zi

Piecewise Linear Regression via a Difference of Convex Functions

E.1. Algorithm Derivation

For the derivation of the ADMM algorithm, we start with a modiﬁed version of the empirical risk minimization in (6);
where we use a components wise regularization:

D

f

k

k

, inf
φ1,φ2

sup
x

sup
vi,d∈ ∂∗
Xd=1
∂xd
s.t. φ1, φ2 are convex, φ1 −

v1,d|

φi(x) |

+

v2,d|

|

φ2 = f,

Hence the new empirical risk minimization becomes:

n

min
ˆyi,zi,ai,bi,L

( ˆyi −

i=1
X
ˆyj + zi −
zj ≥ h
+
|

bi,d| ≤

zj ≥ h
bj , xi −
Ld

ˆyi −
zi −
ai,d|
|

s.t. 


D

yi)2 + λ

Ld

Xd=1
xji

aj, xi −
xji

i, j
i, j
i

∈
∈
∈

[n], d
[n]
[n], d

∈

∈

[D]

[D]

First we change the inequality constraint into equality constraints. We further introduce some auxiliary variables to ease
the solving of subsequent ADMM blocks.



n

D

yi)2 + λ

Ld

(13)

min
ˆyi,zi,ai,bi,L
pi,qi,si,j ,ti,j ,ui,d

(ˆyi −

i=1
X
ˆyj + zi −
zj − h

+ ui,d −

bi, xi −

Xd=1
ai, xi −
zj − h
xji
= 0
Ld = 0

xji

qi,d|

si,j + ˆyi −
ti,j + zi −
+
pi,d|
|
|
ai,d = pi,d
bi,d = qi,d
si,j, ti,j, ui,d ≥

0

s.t.






= 0 i, j
i, j
i
∈
i
∈
i
∈
i, j

[n]
∈
[n]
∈
[n], d
[n], d
[n], d

∈
∈
∈
[n], d

∈

[D]
[D]
[D]

[D]

∈

Then we write the augmented Lagrangian of ADMM as bellow.

min ℓ(ˆyi, zi, ai, bi, pi, qi, L, si,j, ti,j, ui, αi,j, βi,j, γi, ηi, ζi)

αi,j

si,j + ˆyi −

ˆyj + zi −

zj − h

(cid:16)
ti,j + zi −

zj − h

bi, xi −

si,j + ˆyi −

ˆyj + zi −

zj − h

ai, xi −

2

xji
(cid:17)

ρ
2

ai, xi −
ρ
2

+

+

xji
(cid:17)
(cid:16)
ti,j + zi −
(cid:16)
ui,d −

Ld +

|

xji
(cid:17)
ρ
2

+

2

xji
(cid:17)

zj − h

bi, xi −

pi,d|

+

qi,d|

|

2

(cid:17)

=

+

+

+

+

+

n

i=1
X

yi)2 + λL

(ˆyi −

i
X

j
X

i
X

j
X

i
X

Xd

i
X

Xd

i
X

Xd

βi,j

γi,d

ηi,d

ζi,d

(cid:16)
ui,d −
(cid:16)
ai,d −

(cid:16)
bi,d −

(cid:16)

(cid:16)

2

(cid:17)
pi,d

Ld +

|

+

|

qi,d|

pi,d|
ρ
2

+

pi,d

(cid:17)

qi,d

+

(cid:17)

ai,d −

ρ
2

(cid:16)
bi,d −
(cid:16)

qi,d

(cid:17)
2

(cid:17)

Taking gradients with respect to the augmented Lagrangian and solving for the roots provides the parallel algorithm in
Appx E.

Piecewise Linear Regression via a Difference of Convex Functions

F. Datasets

Table1. Datasets used for the regression task. The entries in the ﬁrst columns are linked to repositry copies of the same. The ﬁnal
column indicates if the DC function based method outperforms all competing methods or not.

DATA SET

LABELS

FEATURES DID DC DO BETTER?

ROCK SAMPLES
ACETYLENE
MOORE
REACTION
CAR SMALL
CEREAL
BOSTON HOUSING
FOREST FIRES

16
16
20
13
100
77
506
517

3
3
5
3
6
12
13
12

✓
✓
✓
✓
✗
✓
✗
✓

Table2. Datasets used for the multi-class classiﬁcation task. The entries in the ﬁrst columns are linked to the UCI machine learning
repositry copies of the same. The ﬁnal column indicates if the DC function based method outperforms all competing methods or not.

DATA SET

LABELS

FEATURES DID DC DO BETTER?

BPD
IRIS
BALANCE SCALE
ECOLI
WINE
HEART DISEASE
IONOSPHERE

223
150
625
337
178
313
351

1
4
4
7
13
13
34

✓
✓
✓
✓
✗
✗
✓

