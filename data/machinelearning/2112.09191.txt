Submitted to the Annals of Statistics

ANALYSIS OF GENERALIZED BREGMAN SURROGATE ALGORITHMS
FOR NONSMOOTH NONCONVEX STATISTICAL LEARNING

BY YIYUAN SHE, ZHIFENG WANG AND JIUWU JIN

Department of Statistics, Florida State University

Modern statistical applications often involve minimizing an objective
function that may be nonsmooth and/or nonconvex. This paper focuses on
a broad Bregman-surrogate algorithm framework including the local linear
approximation, mirror descent, iterative thresholding, DC programming and
many others as particular instances. The recharacterization via generalized
Bregman functions enables us to construct suitable error measures and es-
tablish global convergence rates for nonconvex and nonsmooth objectives in
possibly high dimensions. For sparse learning problems with a composite ob-
jective, under some regularity conditions, the obtained estimators as the sur-
rogate’s ﬁxed points, though not necessarily local minimizers, enjoy provable
statistical guarantees, and the sequence of iterates can be shown to approach
the statistical truth within the desired accuracy geometrically fast. The paper
also studies how to design adaptive momentum based accelerations without
assuming convexity or smoothness by carefully controlling stepsize and re-
laxation parameters.

1. Introduction. Many statistical learning problems can be formulated as minimizing a
certain objective function. In shrinkage estimation, the objective can often be represented as
the sum of a loss function and a penalty function, neither of which is necessarily smooth or
convex. For example, when the number of variables is much larger than the number of obser-
vations (p
n), sparsity-inducing penalties come into play and result in nondifferentiability.
Furthermore, many popular penalties are nonconvex [22, 19, 65], making the computation
and analysis more challenging. Although in low dimensions there are ways to tackle nons-
mooth nonconvex optimization, statisticians often prefer easy-to-implement algorithms that
scale well in big data applications. Therefore, ﬁrst-order methods, gradient-descent type al-
gorithms in particular, have recently attracted a great deal of attention due to their lower
complexity per iteration and better numerical stability than Newton-type algorithms.

≫

In this work, we study a class of algorithms in a Bregman surrogate framework. The idea
is that instead of solving the original problem minβ f (β), one constructs a surrogate function
g(β; β−) = f (β) + ∆ψ(β, β−),

(1)

and generates a sequence of iterates according to

1
2
0
2

c
e
D
6
1

]

C
O
.
h
t
a
m

[

1
v
1
9
1
9
0
.
2
1
1
2
:
v
i
X
r
a

β(t+1)

g(β; β(t)).

(2)

∈

arg min
β
The generalized Bregman function ∆ψ will be rigourously deﬁned in Section 2.1, and we
will call g a (generalized) Bregman surrogate. Note that ∆ψ is not necessarily the standard
Bregman divergence [9] because we do not restrict ψ to be smooth or strictly convex or even
convex. Bregman divergence does not seem to have been widely used in the statistics com-
munity, but see [64]. The generalized Bregman surrogate framework has a close connection

MSC2020 subject classiﬁcations: Primary 90C26, 49J52, 68Q25.
Keywords and phrases: nonconvex optimization, nonsmooth optimization, MM algorithms, Bregman diver-

gence, statistical algorithmic analysis, momentum-based acceleration.

1

 
 
 
 
 
 
2

Y. SHE ET AL.

to the majorization-minimization (MM) principle [28, 29]. But the surrogate here as a func-
tion of β matches f (β) to a higher order when β− is set to β (cf. Lemma 4) and we do not
always invoke the majorization condition g(β; β−)
f (β); the beneﬁts will be seen in step
size control and acceleration.

≥

A variety of algorithms can be recharacterized by Bregman surrogates, including DC pro-
gramming [55], local linear approximation (LLA) [67] and iterative thresholding [8, 47]. In
contrast to the large body of literature in convex optimization, little research has been done
on the rate of convergence of nonconvex optimization algorithms when p > n, and there is a
lack of universal methodologies. Instead of proving local convergence results for some care-
fully chosen initial points, this work aims to establish global convergence rates regardless
of the speciﬁc choice of the starting point, where a crucial element is the error measure. We
will see that the most natural measures are unsurprisingly problem-dependent, but can be
conveniently constructed via generalized Bregman functions.

Another perhaps more intriguing question to statisticians is how the statistical accuracy
improves or deteriorates as the cycles progress, and whether the ﬁnally obtained estimators
can enjoy provable guarantees in a statistical sense. See, for example, [1, 20, 63]; in partic-
ular, [36], one of the main motivations of our work, showed that for a composite objective
composed of a loss and a regularizer that enforces sparsity, the sequence of iterates β(t) gen-
erated by gradient-descent type algorithms can approach a minimizer βo at a linear rate even
when p > n, if the problem under consideration satisﬁes some regularity conditions. This
article reveals broader conclusions when using generalized Bregman surrogate algorithms in
the composite setting: the more straightforward statistical error between the t-th iterate β(t)
and the statistical truth β∗ enjoys fast convergence, and the convergent ﬁxed points, though
not necessarily local minimizers, let alone global minimizers, possess the desired statistical
accuracy in a minimax sense. The studies support the practice of avoiding unnecessary over-
optimization in high-dimensional sparse learning tasks. Our theory will make heavy use of
the calculus of generalized Bregman functions—in fact, the proofs become readily on hand
with some nice properties of ∆ established. Again, a wise choice of the discrepancy measure
can facilitate theoretical analysis and lead to less restrictive regularity conditions.

Finally, we would like to study and extend Nesterov’s ﬁrst and second accelerations
[39, 40]. Accelerated gradient algorithms [4, 57, 32] have lately gained popularity in high-
dimensional convex programming because they can attain the optimal rates of convergence
among ﬁrst-order methods. However, since convexity is indispensable to these theories, how
to adapt the momentum techniques to nonsmooth nonconvex programming is largely un-
known. Ghadimi and Lan [24] studied how to accelerate gradient descent type algorithms
when the objective function is nonconvex but strongly smooth; the obtained convergence
rate is of the same order as gradient descent for nonconvex problems. We are interested in
more general Bregman surrogates with a possible lack of smoothness and convexity, most
notably in high-dimensional nonconvex sparse learning. This work will come up with two
momentum-based schemes to accelerate Bregman-surrogate algorithms by carefully control-
ling the sequences of relaxation parameters and step sizes.

Overall, this paper aims to provide a universal tool of generalized Bregman functions in the
interplay between optimization and statistics, and to demonstrate its active roles in construct-
ing error measures, formulating less restrictive regularity conditions, characterizing strong
convexity, deriving the so-called basic inequalities in nonasymptotic statistical analysis, de-
vising line search and momentum-based updates, and so on. The rest of this paper is orga-
nized as follows. In Section 2, we introduce the generalized Bregman surrogate framework
and present some examples. Section 3 gives the main theoretical results on computational
accuracy and statistical accuracy. Section 4 proposes and analyzes two acceleration schemes.
We conclude in Section 5. Simulation studies and all technical details are provided in the
Appendices.

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

3

C

Notation. Throughout the paper, we use C, c to denote positive constants. They are not
necessarily the same at each occurrence. The class of continuously differentiable functions is
1. Given any matrix A, we denote its (i, j)-th element by Aij . The spectral norm
denoted by
and the Frobenius norm of A are denoted by
kF , respectively. The Hadamard
k2 and
B and their inner
product of two matrices A and B of the same dimension is denoted by A
A, B
B.
product is
−
i
h
to denote the submatrix of A formed by the
[p], we use A
Let [p] :=
, p
1,
}
{
Rn, we use A◦, ri(A), A to denote its interior, relative
columns indexed by
J
interior, and closure, respectively [45]. When f is an extended real-valued function from
Rp to R
.
D
∪ {
Let R+ = [0, +
∞

, its effective domain is deﬁned as dom(f ) =

B is positive semi-deﬁnite, we also write A

}
J ⊂
. Given a set A

A⊤B
{
. Given

Rp : f (β) < +

+
∞}
).

. If A

A
k

A
k

β
{

= tr

∞}

· · ·

(cid:23)

⊂

⊂

∈

◦

J

2. Basics of generalized Bregman surrogates.

2.1. Generalized Bregman functions. Bregman divergence [9], typically deﬁned for con-
tinuously differentiable and strictly convex functions, plays an important role in convex anal-
ysis. An extension of it based on “right-hand” Gateaux differentials helps to handle nons-
mooth nonconvex optimization problems. We begin with one-sided directional derivative.

of ψ at β

∈

(3)

DEFINITION 1. Let ψ : D

Rp

R be a function. The one-sided directional derivative

D with increment h is deﬁned as

⊂

→

δψ(β; h) = lim
0+

ǫ

→

ψ(β)

ψ(β + ǫh)
ǫ
D for sufﬁciently small ǫ : 0 < ǫ < ǫ0.

−

,

provided h is admissible in the sense that β + ǫh
When ψ : D

Rn is a vector function, δψ is deﬁned componentwise.

∈

→

In the following, ψ is called (one-sided) directionally differentiable at β if δψ(β; h) as
D, we say

deﬁned in (3) exists and is ﬁnite for all admissible h, and if this holds for all β
that ψ is directionally differentiable.

∈

When a > 0, δψ(β; ah) = aδψ(β; h), but δψ is not necessarily a linear operator with re-
spect to h. Deﬁnition 1 is a relaxed version of the standard Gateaux differential which studies
0. In high-dimensional sparse problems where nonsmooth regularizers
the limit when ǫ
and/or losses are widely used, (3) is more convenient and useful.

→

DEFINITION 2 (Generalized Bregman Function (GBF)). The generalized Bregman

function associated with a function ψ is deﬁned by

(4)

∆ψ(β, γ) = ψ(β)

ψ(γ)

δψ(γ; β

γ),

−
assuming β, γ
γ) is meaningful and ﬁnite. In particular, when
ψ is differentiable and strictly convex, the generalized Bregman function ∆ψ becomes the
standard Bregman divergence:

dom(ψ) and δψ(γ; β

−

−

−

∈

(5)
When ψ is a vector function, a vector version of ∆ is deﬁned componentwise.

Dψ(β, γ) := ψ(β)

ψ(γ), β

ψ(γ)

− h∇

.
i

−

−

γ

∇

When

ψ exists at β, δψ(β, h) reduces to

ψ(β), h

, which is linear in h. So if ψ

is the restriction of a function ϕ
∈
dom(ψ). For simplicity, all functions in our paper are assumed to be deﬁned on a whole vector
space (Rp, typically) unless otherwise mentioned, although most results can be formulated in
the case of extended real-valued functions under the convexity of their effective domains.

1 to a convex set, ∆ψ(β, γ) = ∆ϕ(β, γ) for all β, γ

∈ C

h∇

i

4

Y. SHE ET AL.

The generalized Bregman ∆ψ(
, γ) can be seen as the difference between the function ψ
·
and its radial approximations made at γ. A simple but important example is D2(β, γ) :=
2/2. In general, ∆ψ or Dψ may not be symmetric. The following
D
2
k
−
symmetrized version turns out to be useful:

2/2(β, γ) =

β
k

2
k·k

γ

(6)

¯∆ψ(β, γ) :=

(∆ψ+

1
2

r∆ψ)(β, γ) =

1
2 {

where

r∆(β, γ) denotes ∆(γ, β). If ψ is smooth, ¯∆ψ(β, γ) =

h∇
∆φ to denote ∆ψ(β, γ)
To simplify the notation, we use ∆ψ ≥
0,
≥

0 stands for ∆ψ(β, γ)

∀

and so ∆ψ ≥
follows.

∆ψ(β, γ) + ∆ψ(γ, β)
,
}
ψ(γ), β
.
∆φ(β, γ) for all β, γ,
≥
β, γ. Some basic properties of ∆ are given as

ψ(β)

− ∇

−

γ

i

LEMMA 1. Let ψ and ϕ be directionally differentiable functions. Then for any α, β, γ,

0; conversely, if ψ is direction-

Rn is continuous and directionally
∆ϕ(β, γ),
. Also, if
ψ(ϕ(γ))
i
h
Rn is linear, then ∆ψ
ϕ(β, γ) =
◦

∇

→

we have the following properties.
(i) ∆aψ+bϕ(β, γ) = a∆ψ(β, γ) + b∆ϕ(β, γ),
(ii) If ψ is convex, it is directionally differentiable and ∆ψ ≥
(iii) If ψ : Rn

0 then ψ is convex.
R is differentiable and ϕ : Rp

a, b

R.

∈

∀

→
R is directionally differentiable and ϕ : Rp

ϕ(β, γ) = ∆ψ(ϕ(β), ϕ(γ)) +
◦

ally differentiable and ∆ψ ≥
→
differentiable, then ∆ψ
ψ : Rn
→
∆ψ(ϕ(β), ϕ(γ)).
1
0

γ + t(β
δψ
γ) is integrable over t

γ); β

(iv) ∆ψ(β, γ) =

γ); β
[0, 1].

−
∈

γ

−

−

δψ(γ; β

−

γ)

dt, provided δψ(γ + t(β

−

−

(cid:2)

(cid:3)

(cid:0)

R

−

f , by (i) we can write ∆ψ = ρD2 −
2
2/2 is so for some ν
k · k

(cid:1)
The properties will be frequently used in the rest of the paper. For instance, for ψ =
∆f . Sometimes, though f is not necessar-
2
ρ
2/2
k · k
νD2, owing to (ii).
ily convex, f + ν
∈
For l(β) = l0(Xβ + α), commonly encountered in statistical applications, (iii) states that
∆l(β, γ) = ∆l0(Xβ + α, Xγ + α). For (iv), the integrability condition is met when the
directional derivative restricted to the interval [β, γ] is bounded by a constant (or more gen-
erally a Lebesgue integrable function); in particular, if ψ is L-strongly smooth, that is,
ψ
for any β, γ, where
exists and is Lipschitz continuous:
k∗ ≤
2/2 and for the Euclidean norm,
γ
is the dual norm of
k
LD2 results.

R, which means ∆f ≥ −

ψ(γ)
− ∇
β
L
k
≤

ψ(β)
k∇
, ∆ψ(β, γ)

β
k

k · k

∇

−

−

L

γ

k

Moreover, the GBF operator satisﬁes some interesting “idempotence” properties under
some mild assumptions, which is extremely helpful in studying iterative optimization algo-
rithms.

LEMMA 2.

(i) When ψ is convex, ∆∆ψ(
∆ψ(β, γ) for all α, β, γ.
·
(ii) When ψ is directionally differentiable, for all α = (1

,α)(β, γ)

,α)(β, γ)

≤

≥

∆ψ(β, γ), and when ψ is concave,

θ)γ + θβ with θ

(0, 1),

6∈

−

,α)(β, γ) = ∆ψ(β, γ) and in particular,
,β)(β, γ) = ∆∆ψ(
·

∆∆ψ(
·

,γ)(β, γ) = ∆ψ(β, γ).
γ) is bounded in a neighborhood of α and has restricted radial
α],

γ) = δψ(α; β

γ) for any h

[β

−
0+ δψ(α + ǫh; β
→

−

) has restricted linearity δψ(α; h) =
·

−
g(α), h
i
h

α, γ
∈
for some g and all h

−

−
[β

∈

−

(iii) When δψ(
; β
·
continuity at α: limǫ
or when δψ(α;
α, γ

α], we have

−

(8)

,α)(β, γ) = ∆ψ(β, γ).
In particular, (8) holds when ψ is differentiable at α or δψ(
; β
·

∆∆ψ(
·

γ) is continuous at α.

−

k · k∗
∆ψ ≤

∆∆ψ(
·
∆∆ψ(
·
(7)

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

5

We refer to (ii) as the weak idempotence property and (iii) as the strong idempotence
property. When ∆ψ becomes a legitimate Bregman divergence, (8) can be rephrased into the
three-point property Dψ(β, γ) = Dψ(β, α) + Dψ(α, γ)
[14].
It is worth mentioning that although from (iii), differentiability can be used to gain strong
idempotence, the weak idempotence (7) is often what we need, which always holds under
just directional differentiability.

ψ(α))
i

ψ(γ)

− ∇

− h

α,

∇

−

β

n

Rn has density pη(
) = exp
·

At the end of the subsection, we give some important facts of GBFs for canonical gener-
alized linear models (GLMs) that are widely used in statistics modeling. Here, the response
variable y
with respect
(
∈ Y
h·
{
n (typically the counting measure or Lebesgue measure), where
to measure ν0 deﬁned on
Rn represents the systematic component of interest, and σ is the scale parameter; see
η
[30]. Since σ is not the parameter of interest, it is more convenient to deﬁne the density
) with a slight abuse of notation) with respect
(still written as pη(
exp
·
to the base measure dν = exp(

, σ2)
c(
}
·

b(η))/σ2

b(η))/σ2

(
h·
{

i −

i −

, η

, η

−

⊂

Y

∈

}

−

, σ2)) dν0. The loss for η can be written as
c(
·
l0(η; y) =

y, η

/σ2.
+ b(η)
}

i

{−h

(9)

−h

/σ2 +
i

2/(2σ2).
2
k

That is, l0 corresponds to a distribution in the exponential dispersion family with cumu-
), dispersion σ2 and natural parameter η. In the Gaussian case, l0(η) =
lant function b(
·
y
k

η, y
Following [62], we deﬁne the natural parameter space Ω = dom(b) =
(always assumed to be nonempty) and the mean parameter space

Rn : b(η) <
Rn : µ =
M
∈
∞}
Ey, where y
, and call pη minimal
n with respect to ν implies a = 0. When Ω is open,
a, z
if
h
pη is called regular, and b can be shown to be differentiable to any order and convex, but
not necessarily strictly convex; if, in addition, pη is minimal, b is strictly convex and the
canonical link g = (
M◦. These can all be derived from, say, the
propositions in [62].

p for some density p deﬁned on

= c for almost every z

1 is well-deﬁned on

n with respect to ν

∈
µ
{

η
{
=

∈ Y

b)−

∇

∼

Y

}

i

LEMMA 3. Assume the exponential dispersion family setup with the associated loss de-

ﬁned in (9). (i) If Ω is an open set or pη is regular, then
l0(η; z) = ∆b(η, ∂b∗(z))/σ2

(10)

b∗(z)/σ2

−

∈

ri(

Ω, z

for all η
), where b∗ is the Fenchel conjugate of b, and ∂b∗(z) can take any
subgradient of b∗ at z. If pη is also minimal, ∆b becomes Db, ∂b∗(z) becomes g(z) (which
M◦. (ii) As long as Ω is open,
is unique), and ri(

) becomes

M

M

∈

(11)

for all η
any η1 ∈
GBF of l0 or b by

Ω, z
ri(
∈
∈
Ω◦ and η2 ∈

l0(η; z) = ∆b∗(z,
). If pη is also minimal, ∆b∗ = Db∗ and ri(

b∗(z)/σ2

b(η))/σ2

∇

−

M◦. (iii) Given
M
Ω, the Kullback Leibler (KL) divergence of pη2 from pη1 relates to the

) =

M

(12)

KL(pη1, pη2) = ∆l0(η2, η1) = ∆b(η2, η1)/σ2.

Property (i) shows the importance of GBF in maximum likelihood estimation. A Breg-
man version of Property (ii) was ﬁrst described in [3], while our conclusions based on
∆b, ∆b∗ are more general, as they do not require the strict convexity of b or the differen-
tiability of b∗. Consider for instance the multinomial GLM under a symmetric parametriza-
exp(ηk) or
tion: for [y1, . . . , ym]
k
yk = 1
}
Eyk = exp(ηk)/
exp(ηk), and thus b∗(µ) takes
µk log µk for

, 1
0, 1
yk ∈ {
}
{
exp(ηk) gives b = log

(n = 1), Eyk ∝

∈ Y

m,

≤

≤

=

P

P

P

P

(14)

(i) If

(15)

6

Y. SHE ET AL.

=

∈ M

[µk] :
{

and +
otherwise. Clearly, b∗ is not dif-
µk = 1, µk ≥
[µ1, . . . , µm]
0
}
log z + t1 : t
), ∂b∗(z) =
ferentiable (given any z
), but nicely our two GBF
{
representations still hold. In addition, if the right-hand side of (10) or (11), as a function of
z, is continuous on
, which is the case for Bernoulli, multinomial and Poisson, (i) and (ii)
hold for any z

M
from [62, Theorem 3.4].

M
P

ri(

∞

R

∈

∈

}

Property (iii) (notice the exchange of η1 and η2 in the generalized Bregman expressions)
can be used to formulate and verify model regularity conditions in minimax studies of sparse
GLMs, which are of great interest in high-dimensional statistical learning [58]. More con-
cretely, consider a general signal class

∈ M

Rp :

∈

p, 0

β∗
{

B
M

(s∗, M ) =

(13)
. Some applications limit the magnitude of the coefﬁcients βj via
where s∗ ≤
∞
) be any nondecreasing function with
a constraint or a penalty, resulting in a ﬁnite M . Let I(
·
c. Recall the regular
I(0) = 0, I
exponential dispersion family with systematic component η = Xβ and loss l(β) = l0(η)
deﬁned by (9).

0. Some particular examples are I(t) = t and I(t) = 1t

β∗
k

β∗
k

k0 ≤

k∞ ≤

,
}

s∗,

M

≤

6≡

≤

+

≥

THEOREM 1.

In the regular exponential dispersion family setup (with dom(b) a

nonempty open set), assume p

2, 1

s∗ ≤

≤

≥

p/2. Let

P (s∗) = s∗ log(ep/s∗).

∆l0(0, Xβ)σ2

κD2(0, β),

≤

β

∀

∈ B

(s∗, M )

) only, such that
where κ > 0, there exist positive constants c, ˜c, depending on I(
·

inf
ˆβ

sup

E

I

β∗

(s∗,M )

D2(β∗, ˆβ)/[˜c min
σ2P (s∗)/κ, M 2s∗
{

]
}

c > 0,

≥

(cid:1)(cid:9)

∈B
where ˆβ denotes any estimator of β∗.

(cid:8)

(cid:0)

(ii) If

(16)

where κ, κ

inf
ˆβ

β∗

κD2(β1, β2)
≤
∆l0(0, Xβ1)σ2

(

D2(Xβ1, Xβ2)
κD2(0, β1),

≤

βi ∈ B

∀

(s∗, M )

) only such that
0, then there exist positive constants c, ˜c depending on I(
·

D2(Xβ∗, X ˆβ)/[c min

(κ/κ)σ2P (s∗), κM 2s∗
{

]
}

c > 0.

≥

(cid:1)(cid:9)

≥
sup

(s∗,M )

∈B

E

I

(cid:8)

(cid:0)

The GBF-form conditions (15), (16) can be viewed as an extension of restricted isome-
try [11], and are often easy to check using the Hessian. For example, from Lemma 1, we
2
immediately know that if l0 is L-strongly smooth, (15) is satisﬁed with κ = L
2 even
k
when M = +
. This is the case for regression and logistic regression, and accordingly, no
estimation algorithms can beat the minimax rate s∗ log(ep/s∗) (ignoring trivial factors). The
optimal lower bounds provide useful guidance in establishing sharp statistical error upper
bounds of Bregman-surrogate algorithms in Section 3.2.

X
k

∞

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

7

2.2. Examples of Bregman surrogates.

EXAMPLE 1.

(Gradient descent and mirror descent). Gradient descent is a simple
1 which may be nonconvex. Starting with

ﬁrst-order method to minimize a function f
β(0), the algorithm proceeds as follows:

∈ C

(17)

β(t+1) = β(t)

α

f (β(t)),

−
where α > 0 is a step size parameter. Its rationale can be seen by formulating a Bregman-
surrogate algorithm using ∆ψ = ρD2 −
(18a)

β(t+1) = arg min

∆f )(β, β(t))

∆f :

∇

g(β; β(t)) = f (β) + (ρD2 −

β

(18b)

= β(t)

1
ρ ∇

−

f (β(t)),

−

∆f (
, β(t)) gives a linear approximation of f and 1/ρ amounts to the step
where f (
)
·
·
size. We call ρ the inverse step size parameter. (The generalized Bregman surrogate in (18a)
extends the class of algorithms to a directionally differentiable f , with the update given by
β(t+1) = β(t) + (0
denotes the negative part (t

δf (β(t); h◦))h◦/ρ and h◦

k2=1[δf (β(t); h)]

, where [ ]

arg max

t)/2).)

∨ −

∈

−

−

h

More generally, we can use a strictly convex ϕ

t
= (
|

| −

−

(19)

∈ C
g(β; β(t)) = f (β) + (ρDϕ −

k
1 to construct
∆f )(β, β(t)),

Minimizing (19) with respect to β gives the renowned mirror descent [38]: β(t+1) =
ϕ. Mirror descent is
(
∇
widely used in convex programming, but this work does not restrict f to be convex.

f (β(t))/ρ), where (

1 is the inverse of

ϕ(β(t))

− ∇

ϕ)−

ϕ)−

1(

∇

∇

∇

EXAMPLE 2.

(Iterative thresholding). Sparsity-inducing penalties are widely used in
high-dimensional problems; see, for example, ℓ0, ℓ1 [56], bridge penalties [22], SCAD [19],
capped-ℓ1 [66] and MCP [65]. There is a universal connection between thresholding rules and
penalty functions [48], and the mapping from penalties to thresholdings is many-to-one. This
makes it possible to apply an iterative thresholding algorithm to solve a general penalized
problem of the form minβ l(β) +
j P (̺βj; λ) [8, 47]:

(20)

β(t+1) = Θ(̺β(t)
P

l(β(t))/̺; λ)/̺,

− ∇
where Θ is a thresholding function inducing P , and ̺ > 0 is an algorithm parameter for
the sake of scaling and convergence control. This class of iterative algorithms is called the
Thresholding-based Iterative Selection Procedures (TISP) in [47] and is scalable in computa-
tion. For the rigorous deﬁnition of Θ and the Θ-P coupling formula, see Section 3.1 for detail.
>λ, which
Some examples of Θ include: (i) soft-thresholding ΘS(t; λ) = sgn(t)(
λ)1
t
|
|
induces the ℓ1 penalty, (ii) hard-thresholding ΘH(t; λ) = t1
>λ, which is associated with
|
(inﬁnitely) many penalties, with the capped-ℓ1 penalty, (55), and the discrete ℓ0 penalty as
particular instances. The nonconvex SCAD and MCP penalties also have their corresponding
thresholding rules. In this sense, thresholdings extend proximity operators. One can regard
(20) as an outcome of minimizing the following Bregman surrogate

| −

t
|

t
|

(21)

g(β; β(t)) = l(β) +

P (̺βj; λ) + (̺2D2 −

∆l)(β, β(t)).

Here, we linearize l only, as minβ g(β; β(t)) has (20) as its globally optimal solution. Inter-
estingly, the set of ﬁxed points under the g-mapping enjoys provable guarantees that may not

X

8

Y. SHE ET AL.

hold for the set of local minimizers to the original objective (Section 3.2.1). This is particu-
larly the case when Θ has discontinuities and P (t; λ) is given by PΘ(t; λ) + q(t; λ), where
R and q(t; λ) = 0 if
PΘ is deﬁned by (48) and q is a function satisfying q(t; λ)
R [49].
t = Θ(s; λ) for some s

0 for all t

≥

∈

− ∇

l(β(t))/̺2; q) for the sake of feature screening: min l(β) s.t.

A closely related iterative quantile-thresholding procedure [48, 52] proceeds by β(t+1) =
Θ#(β(t)
q, and uses
∆l)(β, β(t)). Here, the quantile thresholding
a similar surrogate g(β; β(t)) = l(β)+ (̺2D2 −
Θ#(α; q), as an outcome of min g(β; β−), keeps the top q elements of αj after ordering them
, and zero out the rest. To avoid ambiguity, we assume no
in magnitude,
α(p)|
α(1)| ≥ · · · ≥ |
|
ties occur in performing Θ#(α; q) throughout the paper, that is,

k0 ≤

β
k

>

.

∈

α(q)|
|

α(q+1)|
|

EXAMPLE 3.

(Nonnegative matrix factorization). Nonnegative Matrix Factorization
(NMF) [34] provides an effective tool for feature extraction and ﬁnds widespread appli-
cations in computer vision, text mining and many other areas. NMF approximates a non-
negative data matrix X
by the product of two nonnegative low-rank matrices
∈
p
Rr
Rn
W
+ . The KL divergence is often used to make a cost function, that
×
×
+
∈
Xij + (W H)ij],
i,j[Xij log(Xij/(W H)ij)
KL(X, W H) :=
is, minW
which gives a nonconvex optimization problem. The following multiplicative update rule
(MUR) shows good scalability in big data applications [15]:

∈
Rr×p
+

and H

Rn
+

+ ,H

Rn×r

P

−

×

∈

∈

p

r

(22)

(23)

H (t+1)

kj = H (t)

kj exp

−

h

W (t+1)
ik

= W (t)

ik exp

−

h

1
ρ

1
ρ

Wik −

WikXij
(W H (t))ij

Xi (cid:16)

Hkj −

Xj (cid:16)

HkjXij
(W (t)H)ij

,

(cid:17)i

.

(cid:17)i

The update formulas can be explained from a Bregman surrogate perspective. Since the prob-
lem is symmetric in W and H, ∆KL(X, W H) = ∆KL(X ⊤, H ⊤W ⊤), we take (22) for
instance to illustrate the point. Noticing that the criterion is separable in the column vec-
tors of H, it sufﬁces to look at minh
−
∈
xi + (W h)i], where x can be any column of X. Then it is easy to verify that the following
Bregman surrogate,

+ f (h) = KL(x, W h) =
Rr

i[xi log(xi/(W h)i)

P

(24)

g(h; h(t)) = f (h) + (ρDϕ −

Df )(h, h(t)), ϕ(h) =

(hi log hi −

hi),

leads to the multiplicative update formulas.

X

EXAMPLE 4.

(DC programming). DC programming [55] is capable of tackling a large
class of nonsmooth nonconvex optimization problems; see, for example, [23, 43]. A “dif-
ference of convex” (DC) function f is deﬁned by f (β) = d1(β)
d2(β), where d1 and d2
are both closed convex functions. To minimize f (β), a standard DC algorithm generates two
sequences

that obey

and

−

β(t)
{

}

γ(t)
{

}
γ(t)

(25)

∂d2(β(t)), β(t+1)

∂d∗1(γ(t)),

∈

∈

) at β, and d∗1(
where ∂d(β) is the subdifferential of d(
).
) is the Fenchel conjugate of d1(
·
·
·
(As before, d1, d2 are assumed to be real-valued functions deﬁned on Rp, so the sequences
are well-deﬁned and ﬁnite.) This elegant algorithm does not involve any line search and
guarantees global convergence given any initial point. Many popular nonconvex algorithms
can be derived from (25) [2].

Focusing on the β-update, we know that β(t+1) must be a solution to minβ d1(β)
β(t), γ(t)

. Due to the convexity of d2,

or minβ d1(β)

β(t), γ(t)

β

β
h

−

−
i ≤

− h

−

i

β, γ(t)
h

i

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

9

i

−

β
supγ
∂d2(β(t))h
∈
minβ d1(β)
β
− h
Choosing β(t+1)
β(t))(β(t+1)
−
man surrogate

β(t), γ
β(t), γ(t)
arg min d1(β)
β(t)

−
∈
β(t+1)
β(t))/
k

−

i

= δd2(β(t); β

β(t)) for all γ(t)

∂d2(β(t)), β

−

∈

should be no lower than minβ d1(β)

∈
δd2(β(t); β

Rp. Thus
β(t)).

δd2(β(t); β
−
−
2
2 ensures (25), which simply amounts to using a Breg-
k

−
β(t)) and γ(t) = δd2(β(t); β(t+1)

−

−

(26)
For the γ-updates, a Bregman surrogate g(γ; γ(t)) = (d∗2 −
similarly constructed.

g(β; β(t)) = f (β) + ∆d2(β, β(t)).

d∗1)(γ) + ∆d∗

1 (γ, γ(t)) can be

EXAMPLE 5.

(Local linear approximation). Zou and Li [67] proposed an effective
local linear approximation (LLA) technique to minimize penalized negative log-likelihoods.
In their paper, the loss function is assumed to be convex and smooth, and the penalty is
concave on R+. We give a new characterization of LLA by use of a Bregman surrogate.

Let l be a directionally differentiable loss function but not necessarily continuously differ-
), and satisﬁes
j P (βj). Using

entiable, and P be a function that is concave and differentiable over (0, +
R, P (0) = 0. Consider the problem minβ l(β) +
t) for any t
P (t) = P (
the generalized Bregman notation ∆

∞
k·k1(β, γ), or ∆1(β, γ) for short, deﬁne
P
∆P (βj, β(t)
αj∆1(βj , β(t)
P (βj) +
j )
j )

(27)

g(β; β(t)) = l(β) +

−

∈

.

−

In contrast to (21), (27) linearizes P instead of l. Simple calculation shows

X

(cid:3)

(28)

∆1(βj, β(t)

j ) =

βj | −
|
0,
(

sgn(β(t)

X (cid:2)
j )βj, β(t)
= 0
j
β(t)
j = 0,

(29)

j ) =

∆P (βj, β(t)

P ′(β(t)
P (β(t)
j )
−
,
βj |
P ′+(0)
|
) is the sign function and P ′+(β) denotes the right derivative of P (
) at β. Inter-
where sgn(
·
·
, the ∆1-based surrogate (27) can be shown to be
estingly, with αj =

j ), β(t)
β(t)
= 0
j
β(t)
j = 0,

P (βj )
P (βj )

j )(βj −

(

−

−

P ′+(β(t)
j )
|
|

l(β) +

β(t)
P (
j
|

β(t)
) + P ′+(
j
|
|

βj | − |
)(
|
|

β(t)
j

,

)
|

Xj

(cid:2)

(cid:3)

which is exactly the surrogate constructed by Zou and Li. To the best of our knowledge, the
generalized Bregman formulation is new.

LLA requires solving a weighted lasso problem at each step. We can further linearize l
as in Example 2 to improve its scalability. LLA is popular among statisticians, but to our
knowledge, there is a lack of global convergence-rate studies in large-p applications. We will
see that reformulating LLA from the generalized Bregman surrogate perspective leads to a
convenient choice of the convergence measure in analyzing the algorithm.

EXAMPLE 6.

(Sigmoidal regression). We use the univariate-response sigmoidal regres-
sion to illustrate this type of nonconvex problems that is commonly seen in artiﬁcial neural
networks. The formulation carries over to multilayered networks and recurrent networks [51].
, yn]⊤ be the re-
sponse vector. Deﬁne π(ν) = eν /(1 + eν); if ν is replaced by a vector, π is deﬁned compo-
nentwise. The sigmoidal regression solves

Let X = [x1, x2, . . . , xn]⊤ ∈

p be the data matrix, and y = [y1,

Rn

· · ·

×

(30)

min
β

f (β) =

1
2

n

Xi=1

(yi −

π(x⊤i β))2.

6
6
10

Y. SHE ET AL.

2f (β) =
Then
π(x⊤i β). Because µi ∈
vates a Bregman surrogate

n
i=1[(

P

∇

i + 3µ2

2µ3
[0, 1], we get

−

i −
2f (β)
∇

µi)yi + (3µ4
X ⊤diag

5µ3
i −
0.1yi|
{|

i + 2µ2
i )]xix⊤i , where µi =
n
i=1X, which moti-
+ 0.08
}

(cid:22)

f (β, β(t)), ψ(β) =

g(β; β(t)) = f (β) + Dψ

1
2
Solving minβ g(β; β(t)) yields β(t+1) = β(t) + B−
u(t)),
(y
where B = X ⊤diag
denotes the Hadamard
0.1yi|
{|
product. This type of surrogate functions is closely related to proximal Newton-type methods
[46] and signomial programming [33].

−
i=1X, u(t) = π(X ⊤β(t)) and
n
+ 0.08
}

0.1yi|
{|
u(t)

Xβ.
+ 0.08
}

β⊤X ⊤diag

1X ⊤(u(t)

u(t))

−

◦

◦

◦

−

3. Bregman-surrogate algorithm analysis. Motivated by the examples in Section 2,
we study a generalized Bregman-surrogate algorithm family for solving minβ f (β), with the
sequence of iterates deﬁned by

(31)

β(t+1)

arg min
β

∈

g(β; β(t)) := f (β) + ∆ψ(β, β(t)), t

0

≥

The objective function f and the auxiliary function ψ are assumed to be directionally differ-
entiable but need not be smooth or convex. ψ has ﬂexible options as seen from the previous
examples.

Equation (31) does not necessarily give an MM procedure, as the majorization condition
f (β) may not hold. But we have the following zeroth-order and ﬁrst-order de-
g(β; β−)
generacies when β− = β, which provides rationality of investigating the accuracy of ﬁxed
points under the g-mapping (31).

≥

LEMMA 4. Let g(β; β−) = f (β) + ∆ψ(β, β−) with f and ψ directionally differ-
β, h, where

entiable. Then (i) g(β; β) = f (β), and (ii) δg(β; β−, h)
δg(β; β−, h) is the directional derivative of g(

|β−=β = δf (β; h),
; β−) at β with increment h.

∀

·

The lemma relates the set of ﬁxed points of the algorithm mapping,

(32)

β : β
{

∈

arg min

β

g(β; β−)

,
|β−=β}

which we will call the ﬁxed points of g for short, to the set of directional stationary points of
f (under directional differentiability),

≥

(33)

0 for any admissible h
,
}

β : δf (β; h)
{
1. The link is general for any gener-
which becomes the set of stationary points when f
alized Bregman surrogate in (31) regardless of the speciﬁc form of ψ. An important impli-
cation is that in studying convergence it is legitimate to measure how β(t+1) and β(t) differ,
as widely used in practice. Later we will see that it is indeed possible to provide provable
guarantees for the ﬁxed points of this type of surrogates. In contrast, a general MM algo-
rithm does not always have the ﬁrst-order degeneracy and so attaining β(t+1) = β(t) does not
necessarily ensure a good-quality solution, especially in nonconvex scenarios.

∈ C

3.1. Computational accuracy. We ﬁrst study the optimization error of (31), then turn to
its statistical error in Section 3.2. This subsection aims to derive universal rates of conver-
gence under no regularity conditions.

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

11

•

General setting.

In this part, the objective f (β) does not have any known structure. To
better connect with some conventional results in convex optimization, we ﬁrst present two
propositions for (31) on the function-value convergence and iterate convergence. While the
resultant rates are encouraging, the error bounds are most informative under certain smooth-
ness and convexity assumptions. This suggests the necessity of choosing a proper conver-
gence measure in order to avoid stringent or awkward technical conditions in nonconvex
optimization.

PROPOSITION 1. Given an arbitrary initial point β(0), let β(t) be the sequence gener-

ated according to (31) where ψ is differentiable. Then

f (β(t+1))

(34)

avg
T
t
≤
for any ¯β satisfying

≤

0

f (¯β)

−

1
T + 1

≤

[∆ψ(¯β, β(0))

−

∆ψ(¯β, β(T +1))]

(35)

∆ψ(β(t+1), β(t)) + ∆f (¯β, β(t+1))

0, 0

t

≤

≤

T.

≥

Here, avg0
≤

t
≤

T f (β(t+1)) denotes the average of f (β(1)), . . . , f (β(T +1)).

In particular, if both f and ψ are convex, then f (β(t)) is nonincreasing and

(36)

f (β(T +1))

f (β)

−

∆ψ(β, β(0))
T + 1

,

≤

β.

∀

Equation (34) shows a convergence rate of

control. For example, for ∆ψ = ρDϕ −
sufﬁciently large, which in turns gives a small stepsize 1/ρ:

(1/T ) under (35) that amounts to step size
∆f in mirror descent, (35) shows that ρ should be

O

(∆f (β(t+1), β(t))

ρ

≥

−

∆f (¯β, β(t+1)))/Dϕ(β(t+1), β(t)),

≥

∆f (β(t+1), β(t))/Dϕ(β(t+1), β(t)) when f is convex. In nonconvex scenarios, the
or ρ
condition may be hard to verify, but one has reason to believe that with a properly small step
size, a generalized Bregman-surrogate algorithm should not be much slower than gradient
descent.

Actually, a faster rate of convergence may be obtained under some GBF comparison con-
ditions, (37) and (39) below, which can be viewed as substitutes for conventional strong
convexity in a more general sense. (The corresponding geometric decay of the errors is moti-
vating in high dimensional statistical learning, in light of the “restricted” strongly convexity
often possessed by such a type of problems [36].)

PROPOSITION 2. Consider the iterative algorithm deﬁned by (31) starting at an arbi-
trary point β(0) with ψ differentiable, and let βo be a minimizer of f (β). (i) If for some
κ > 1, ∆φ = ∆ψ + ∆f satisﬁes

(37)

then for any T

≥

¯∆φ ≥

κ

∆ψ,

κ

−

1

0, we have

(38)

¯∆φ(βo, β(T +1))

(ii) Alternatively, if

(39)

κ
1
−
κ + 1

(cid:17)

≤

(cid:16)

T +1

¯∆φ(βo, β(0))

κ
2

−

min
T
t
0
≤
≤

∆ψ(β(t+1), β(t)).

2 ¯∆f ≥

ε∆ψ

12

Y. SHE ET AL.

for some ε > 0, then

(40) ∆ψ(βo, β(T +1))

for any T

0.

≥

T +1

1
1 + ε

(cid:17)

≤

(cid:16)

∆ψ(βo, β(0))

1
ε

−

min
T
t
0
≤
≤

∆ψ(β(t+1), β(t))

REMARK 1. We give an illustration of (i) and (ii) to compare their assumptions and

conclusions. In gradient descent with ∆φ = ρD2, (37) becomes ρD2 ≥
1) or ∆f ≥
Then (38) reads

−
(ρ/κ)D2 and when f is µ-strongly convex and ρ-strongly smooth, κ = ρ/µ.

(ρD2 −

∆f )κ/(κ

T +1

(41)

D2(βo, β(T +1))

ρ
µ
−
ρ + µ
The D2-form bound is classical for problems with strong convexity; see, for example, Theo-
rem 2.1.15 in [41]. Yet it is worth mentioning that our Bregman comparison conditions do not
require ψ to be strongly convex to attain the linear rate. (40) gives a linear convergence re-
sult, too, in terms of yet another measure. In the same setup, (39) holds for ε : ερ/(2 + ε) = µ
and similarly

D2(βo, β(0)).

≤

(cid:16)

(cid:17)

(42)

∆ψ(βo, β(T +1))

≤

T +1

∆ψ(βo, β(0)).

ρ
µ
−
ρ + µ

(cid:16)

(cid:17)

A careful examination of the proof in Section A.8 shows that (39) is applied once, while (37)
is applied twice on both sides of (A.13), and so (ii) appears less technically demanding.
Picking a suitable error function can assist analysis and relax regularity assumptions. The
same ∆ψ will be used in studying the statistical error convergence in Theorem 5.

Instead of naively comparing f (β(t)) with f o, or β(t) with βo, which may be unattainable
or nonunique in nonconvex optimization, one can measure the algorithm convergence in a
wiser manner. Ben-Tal and Nemirovski [5] pointed out that with an inappropriate measure of
discrepancy, the convergence rate of gradient descent for minimizing a nonconvex objective
can be arbitrarily slow, and a common choice is to bound

(43)

min
t
≤

T k∇

2.
f (β(t))
k

This is reasonable since when
stationary point. (43) can be rewritten as ρ2 times

∇

f (β(t)) = 0, gradient descent stops iterating and delivers a

(44)

D2(β(t+1), β(t))

min
T
t
≤

β(t) =

as β(t+1)
f (β(t))/ρ. The idea of checking stationarity by the difference between
two successive iterates generalizes, thanks to Lemma 4, and eventually leads to an error
bound that can get rid of condition (35).

−∇

−

THEOREM 2. Any generalized Bregman surrogate algorithm deﬁned by (31) satisﬁes the

following bound for all T

1,

≥

(45)

avg
T
t
≤

≤

0

(2 ¯∆ψ + ∆f )(β(t), β(t+1))

1
T + 1

≤

f (β(0))

−

f (β(T +1))

.

(cid:2)

(cid:3)

(45) obtains the same rate of convergence as Proposition 1, but is free of any conditions
other than directional differentiability, because only the weak idempotence is needed to derive

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

13

the bound. A proper stepsize control can often make the GBF error nonnegative (e.g., (50)).
But even when β(t) diverges, (45) still applies.

Notice the factor ‘2’ proceeding the symmetrized Bregman ¯∆ψ on the left-hand side of
∆f

(45). This gives a relaxed stepsize control than MM. We use mirror descent ∆ψ = ρDϕ −
to exemplify the point without requiring f to be convex, cf. Example 1.

COROLLARY 1.

pose that ∆f ≤
is taken such that ρ > L/2. Then any accumulation point of β(t) is a ﬁxed point of g and

In the mirror descent setup with a possibly nonconvex objective, sup-
0, and the inverse stepsize parameter ρ

L ¯Dϕ for some L > 0, infβ f (β)

≥

(46)

avg
T
t
≤

≤

0

¯Dϕ(β(t), β(t+1))

f (β(0))
(T + 1)(2ρ

−

≤

.

L)

f (β(t))
2
Hence in the special case of gradient descent, (46) recovers min0
2 =
k
(1/T ) [5] when ρ > L/2. In comparison, MM algorithms always require ∆ψ ≥
0, or ρ
O
L. A smaller value of ρ means a larger step size with which the algorithm converges faster.

T k∇

t
≤

≥

≤

×

•

∈

Rn

Composite setting. High-dimensional statistical learning often has an additive objective
p is the predictor or feature matrix, l0(
f (β) = l0(Xβ) + P (̺β; λ), where X
) is the
·
loss deﬁned on Xβ (and so l(β) = l0(Xβ)), P (
; λ) is a sparsity-inducing regularizer and
·
X
k2 to match the scale. Unless otherwise
̺ is a controllable parameter, typically taking
k
mentioned, P (β; λ) denotes
j P (βj; λ) with a little abuse of notation.
Such a composite setup is widely assumed in convex optimization [57, 18]. But among the
abundant choices of l0 and P in the literature, many of them are nonconvex. The good news
is that the main theorem proved in the previous subsection adapts to the composite setting
and we give some results for iterative thresholding and LLA as an illustration (cf. Examples
2, 5).

P

Iterative thresholding. Many popularly used penalty functions are associated with threshold-
ings rigorously deﬁned as follows.

DEFINITION 3 (Thresholding function). A threshold function is a real-valued function
Θ(t; λ); (ii)
λ <
∞
≤
.
t <
Θ(t; λ) =

Θ(t; λ) deﬁned for
Θ(t; λ)

< t <
∞
t′; (iii) limt

such that (i) Θ(

−
Θ(t; λ)

−
t for 0

; (iv) 0

t; λ) =

and 0

−∞
Θ(t′; λ) for t
≤

≤

→∞

∞

≤
1 can be introduced such that dΘ−

≤

≤

∞
1(u; λ) du

1

Given Θ, a critical concavity number
− LΘ for almost every u
(47)

0, or
≥
LΘ = 1

−

ess inf

LΘ ≤

dΘ−
{

1(u; λ)/du : u
,
0
≥
}
1(u; λ) := sup
t : Θ(t; λ)
{
λ)1
|

t
|

with ess inf the essential inﬁmum and Θ−
u > 0. For
,
u
∀
}
the widely used soft-thresholding ΘS(t; λ) = sgn(t)(
>λ and hard-thresholding
t
|
LΘ > 0, the penalty in-
ΘH(t; λ) = t1
|
duced by Θ via (48) is nonconvex, and
LΘ gives a concavity measure of it according to
Lemma A.3. The Bregman surrogate characterization of iterative thresholding in (21) yields
a general conclusion for any Θ in possibly high dimensions.

LΘ equals 0 and 1, respectively. In fact, when

>λ,

| −

≤

t
|

≥

), con-
PROPOSITION 3. Given any thresholding Θ and directionally differentiable l(
·
l(β(t))/̺; λ)/̺ with

sider the iterative thresholding procedure (20): β(t+1) = Θ(̺β(t)
̺ > 0. Construct

− ∇

(48)

PΘ(t; λ) =

t
|

|

0
Z

(Θ−

1(u; λ)

u) du,

−

R,

t

∀

∈

14

Y. SHE ET AL.

and deﬁne f (β) = l(β) + PΘ(̺β; λ), g(β, β−) = l(β) + PΘ(̺β; λ) + (̺2D2 −
1)) and for all T
Then β(t)

arg minβ g(β, β(t
−

∆l)(β, β−).

∈

(49)

avg
T
t
≤

≤

0

(̺2(2

− LΘ)D2−

When the loss satisﬁes ∆l ≤

(50)

1
≥
r∆l)(β(t), β(t+1))

1
T + 1

≤

f (β(0))

−

f (β(T +1))

.

LD2, a reasonable choice of ̺ is

(cid:2)

̺2 > L/(2

− LΘ).

(cid:3)

LΘ > 0, the step size upper bound will be smaller than that as

LΘ = 0. This is often
So when
the price to pay for nonconvex optimization. On the other hand, (49) still ensures the universal
rate of convergence of

(1/T ), in spite of the high dimensionality and nonconvexity.

O

Local linear approximation. Next, we study the computational convergence of LLA for solv-
ing the penalized estimation problem min f (β) = l(β) + P (̺β), assuming l is directionally
differentiable, P (0) = 0, P ′+(0) < +
0 and P (t) is differentiable for any
, P (t) = P (
t > 0. Recall its Bregman form surrogate

∞

≥

−

t)

(51)

g(t)
LLA(β; β(t)) = l(β) + P (̺β) + ∆
j ] with α(t)

j =

)(̺β, ̺β(t)),

k

α(t) ◦(
·

)
k1−

P (
·
p. We abbreviate ∆

P ′+(β(t)
, 1
j )
|
|
LLA, which does not satisfy strong idempotence. By combining ¯∆(t)

where α(t) = [α(t)
) to
∆(t)
LLA and ∆f to evaluate
LLA’s optimization error, we obtain a convergence result without any additional assumptions.

α(t) ◦(
·
k

P (
·

)
1
k

≤

≤

−

j

PROPOSITION 4. Given any starting point β(0), the LLA iterates satisfy the following

bound for all T

1:

≥
[2 ¯∆(t)
LLA(̺β(t), ̺β(t+1)) + ∆f (β(t), β(t+1))]

avg
T
t
≤

≤

0

1
T + 1

≤

[f (β(0))

−

f (β(T +1))].

Ignoring the cost difference per iteration, the convergence rate of LLA is no slower
than that of gradient descent. If l is a negative log-likelihood function associated with a
log-concave density and P is concave on R+, as assumed in [67], 2 ¯∆(t)
LLA(̺β, ̺β′) +
¯∆1(̺βj, ̺β′j)
∆f (β, β′) = ∆l(β, β′) + ∆
β, β′. But Propo-
P (̺β′, ̺β) + 2
≥
−
sition 4 holds even when P is nonconcave on R+ and l is nonconvex.

P
The global convergence-rate results presented in this subsection are free of any regularity
conditions on sparsity, sample size, initial point and design incoherence. High-dimensional
learning algorithms may however show a better convergence rate when the problems under
consideration are “regular” in a certain sense.

j α(t)

0,

∀

j

3.2. Statistical accuracy. To statisticians, the statistical accuracy of Bregman-surrogate
algorithms with respect to a statistical truth (denoted by β∗) is perhaps more meaningful than
the optimization error to a certain local or global minimizer, since real world data are always
noisy. Section 3.2.1 and Section 3.2.2 will study the statistical error of the ﬁnal estimate ˆβ
and the t-th iterate β(t), respectively, where combining the generalized Bregman calculus
and the empirical process theory eases the treatment of a nonquadratic loss.

The techniques based on GBFs apply to a general problem (see, e.g., Theorem A.1 in
Section A.18), but here we focus on the aforementioned sparse learning in the composite
setting: minβ l(β) + PΘ(̺β; λ), where l(β) = l0(η) = l0(Xβ) is directionally differentiable
; λ) is induced by a thresholding Θ via (48). Since l0 is placed on Xβ, we include
and PΘ(
·
k2) in the penalty; this will yield a universal choice of
here a scaling parameter ̺ (often

X
k

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

15

the regularization parameter λ that does not vary with the sample size. Throughout Section
k2. Note that neither the loss nor the penalty needs to
3.2, we assume that ̺ satisﬁes ̺
be convex or smooth.

≥ k

X

Give any directionally differentiable ψ, the sequence of iterates is generated by

(52)

β(t+1)

arg min
β

∈

g(β; β(t)) := l(β) + PΘ(̺β; λ) + ∆ψ(β, β(t)).

Nonconvex iterative thresholding and LLA are particular instances.

First, we must characterize the notion of noise in this nonlikelihood setting, to take into
account the randomness of samples. Assume l0 is differentiable at point Xβ∗ (but not nec-
essarily differentiable on all of Rn) and deﬁne the effective noise by

(53)

ǫ =

−∇

l0(Xβ∗).

(An alternative assumption is that δl0(Xβ∗; h) is a sub-Gaussian random variable with mean
0 and scale bounded by cσ for any unit vector h, but we will not pursue further in the current
paper.)

Typically, E[ǫ] should be 0, and so

E[l0(Xβ∗)]
= 0 assuming the differentiation and
}
expectation are exchangeable, which means the statistical truth makes the gradient of its risk
n) following a distribution in the exponential family
vanish. For a GLM with yi (1
i
≤
1, the loss is then l(β) =
that has cumulant function b and canonical link function g = (b′)−
l0(Xβ) =

y, Xβ

∇{

≤

+

−h

1, b(Xβ)
i
h

i

(54)

ǫ = y

(cf. (9) with σ = 1), and so
E(y).

1(Xβ∗) = y

g−

−

−

Our effective noise, as a joint outcome of the loss and the response, does not depend on
the regularizer, and may differ from the raw noise. For example, under y = Xβ∗ + ǫraw,
aσ r2
Xβ [27], sim-
l(β) = lHuber(r) =
>aσ(a
i:
|
ple calculation gives ǫi = ǫraw
>aσ, which is bounded by aσ, thereby
aσ + aσ1
i
|
sub-Gaussian, no matter what distribution the raw noise follows. This nonparametricness is
apparent for any l0 that is (globally) Lipschitz, for example, the logistic deviance and hinge
loss for classiﬁcation.

a2σ2/2) with r = y

i /2 +
1
ǫraw
i
|

ri| −
|
ǫraw
i

P
|≤

P

−

|≤

ri

ri

i:

|

|

|

In this section, we assume that ǫ is a sub-Gaussian random vector with mean zero and scale
bounded by σ, cf. Deﬁnition A.1, where ǫi are not required to be independent. Examples
include Gaussian random variables and bounded random variables such as Bernoulli.
(β) =

The support of β is denoted by
(β)
|

, and its cardinality is J(β) =
j : βj 6
{
k0. We abbreviate J(β∗) to J ∗ and J(ˆβ) to ˆJ . In sparse learning, J ∗ ≪
p is
|J
typically true. The sparsity suggests the possibility of obtaining a fast rate of convergence in
statistical error. The following penalty induced by the hard-thresholding ΘH(t; λ) = t1
|
by (48) turns out to play a key role in the analysis

= 0
}

β
k

≪

=

>λ

J

t
|

n

(55)

PH(t; λ) = (

t
|
R and any thresholding rule Θ. This
PH (t; λ) for any t
An important fact is that PΘ(t; λ)
is simply because in shrinkage estimation, any Θ(t; λ) with λ as the threshold is identical to
zero as t

[0, λ) and is bounded above by the identity line for t

<λ + (λ2/2)1
|

t2/2 + λ

)1
|
|

t
|≥

λ.

λ.

−

≥

∈

t
|

∈

≥

3.2.1. Statistical accuracy of ﬁxed-point solutions. The ﬁnally obtained solutions from a

Bregman surrogate algorithm can be described as the ﬁxed points of g (recall (32)),

(56)

ˆβ

arg min
β

∈

g(β; ˆβ).

, and call such solutions the F -estimators. When the objective function
We denote the set by
is convex, an F-estimator is necessarily a globally optimal solution to the original problem by

F

(57)

≤

where λ = Aσ

p

(58)

(59)

16

Y. SHE ET AL.

Lemma 4, thus an M-estimator. In general, however, the lack of convexity and smoothness
may make ˆβ neither an M-estimator nor a Z-estimator [60], which poses new and intriguing
challenges to statistical algorithmic analysis. It is also worth mentioning that another impor-
tant class of “A-estimators” that have alternative optimality, typically arising from block co-
ordinate descent (BCD) algorithms like in Example 3, can often be converted to F-estimators;
see Section A.17.

Nicely, if the problem is regular, all F-estimators deﬁned through g can achieve essentially
the best statistical precision in possibly high dimensions. This is nontrivial since even f ’s
locally optimal solutions do not all have the provable guarantee (cf. Remark 4). Theorem
3 and Theorem 4 below only make use of the weak idempotence property; another notable
feature is that the conditions and conclusions below are regardless of the form of ∆ψ.

THEOREM 3.

Suppose there exist δ > 0, ϑ > 0 and large enough K

following inequality holds for any β

Rp:

0 so that the

≥

∈
LΘD2(β, β∗) + δD2(Xβ, Xβ∗) + ϑPH (̺(β
l(β, β∗) + PΘ(̺β; λ) + Kλ2J(β∗),

̺2

2 ¯∆

β∗); λ) + PΘ(̺β∗; λ)

−

log(ep)/

(δ

ϑ)ϑ with A a sufﬁciently large constant. Then

∧
p
D2(X ˆβ, Xβ∗)

σ2J ∗ log(ep),

2KA2

≤

(δ

∧

ϑ)δϑ

4KA2

PH (̺(ˆβ

β∗); λ)

ϑ)ϑ2 σ2J ∗ log(ep),

(δ

≤

−
cA2, where C, c are positive constants.

∧

with probability at least 1

−

Cp−

Moreover, an oracle inequality [17, 31] can be built to justify the estimators even when β∗
is not exactly sparse. Toward this goal, recall the notion of a pseudo-metric d (cf. Deﬁnition
A.2), that is, d is nonnegative, symmetric, and satisﬁes the triangle inequality, and suppose
without loss of generality that

αd2(η, η′)

≤

∆l0(η, η′)
L

α

≤

≤

+

∞

≤

Ld2(η, η′),

η, η′

∀

. For regression l(β) = l0(η) =

y
k

−

for some pseudo-metric d with
η

2
2/2, α = L = 1 > 0.
k
THEOREM 4. Assume for given β

−∞ ≤

ϑ, and a large enough K

(60)

(61)

E∆l(ˆβ, β∗)

Rp, there exist r: 0

r < 1, αr/L

≤

≥

0, positive δ,

γ); λ) + PΘ(̺β; λ)

−

≥

0 so that

∈
LΘD2(β, γ) + δD2(Xβ, Xγ) + ϑPH (̺(β
r)∆l(β, γ) + PΘ(̺γ; λ) + Kλ2J(β)

̺2

(1 +

α
L

≤
Rp, where λ = Aσ

for any γ
The oracle inequality below holds for some constant C > 0,

log(ep)/

(δ

∈

∧

ϑ)ϑ with A a sufﬁciently large constant.

p

p

E

n(cid:16)

2

1 + r
r
1

−

(cid:17)

≤

∆l(β, β∗) +

(1

−

(1 + r)KA2
r)(2ϑ

δ)ϑ

∧

σ2J(β) log(ep)

+

o

(1

−

C(1 + r)
r)(2ϑ

σ2.

δ)

∧

Compared with (57) which ﬁxes γ at β∗, (60) has (1 + α

L r)∆l in place of 2 ¯∆l as the ﬁrst
L r)∆l to dom-
LΘD2 in a restricted sense; Remark 2 argues that (60) is not technically demanding

term on the right-hand side. Nonrigorously, these conditions ask 2 ¯∆l or (1 + α
inate ̺2
compared with many other regularity conditions in the literature.

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

17

When r = 0, the multiplicative constant proceeding ∆l(β, β∗) in (61) is as small as 1,
resulting in a sharp oracle inequality [31]. If one sets β = β∗ in (61), the Bregman error
∆l(ˆβ, β∗) is of the order σ2J ∗ log(ep) for any thresholding (when δ, ϑ, K are treated as
constants). But the bias term ∆l(β, β∗) or ∆l0(Xβ, Xβ∗) helps to handle approximately
sparse signals: when β∗ contains a number of small nonzero elements, rather than taking β =
β∗, a reference β with a reduced support will yield an even smaller error bound beneﬁting
from the bias-variance tradeoff.

Unlike the optimization error bounds, the statistical error bounds never vanish (unless
0). We can similarly analyze the set of global minimizers, in which case the term
σ
→
LΘD2(β, β∗) is dropped from the regularity conditions, but the error bounds remain of
̺2
the same order (cf. Remark A.1 in Section A.12). In fact, for sparse GLMs, by Theorem 1,
the rate σ2J ∗ log(ep) is essentially minimax optimal (thus unbeatable) up to a logarithmic
factor.

∈
c; λ).

=
J
PΘ(̺(β′

(β), J =
β)
J
; λ) + ϑPH(̺γ

REMARK 2 (Regularity condition comparison). The GBF-based regularity conditions
(57), (60) are no more demanding than some commonly used regularity conditions. As-
PΘ(t) + PΘ(s), which holds when it is con-
sume that PΘ is subadditive: PΘ(t + s)
cave on R+. Let
, γ = β′
|
PΘ(̺β
; λ)
≤
J
plied by PΘ(̺γ
J
Kλ2J + PΘ(̺γ
2
(2
2/2
δ
k

β. Then, from PΘ(̺β′
|J
−
−
J
c; λ), (60) is im-
; λ) and PΘ(̺β′
c; λ) = PΘ(̺(β′
β)
−
J
ε)∆l(β, β′) +
LΘD2(̺β, ̺β′) + δ
J
2
Xγ
2/2
(2
; λ) +
≤
−
k
k
LΘD2(̺β, ̺β′) +
J
c; λ), or (1 + ϑ)PΘ(̺γ
ϑPH(̺γ
; λ) +
c; λ)
ε)∆l(β, β′) + Kλ2J + (1
J
c; λ) since PH ≤
ϑ)PΘ(̺γ
−
2
2/2. Then the above condition simpliﬁes to
y
−
k
c; λ) with
Xγ
ε′)
k

Xβ
k
(1 + ϑ)PΘ(̺γ
(2
≤
ε′ = ε + δ, or the following sufﬁcient condition (with K redeﬁned) for all γ

Xγ
k
To get more intuition, let l(β) =
; λ) +

J
2/2 + Kλ2J + (1
2
k

ϑ)PΘ(̺γ
Rp:
J

2
2/2
k

LΘk

≤
(β)

J
−

PΘ.

J
−

; λ)

̺γ

−

−

−

≤

J

J

(62)

(1 + ϑ)PΘ(̺γ

; λ) + LΘ
2 k
−
For lasso, where PΘ(β; λ) = λ
k1, there is a rich collection of regularity conditions in the
β
k
LΘ = 0 and ̺ can be arbitrarily large. (62) reduces to (with ϑ
literature. In this convex case,
and K redeﬁned and λ canceled)

K√Jλ
k

k2 + (1

ϑ)PΘ(̺γ

2
2 ≤

Xγ

̺γ

k

J

J

c

c

J

k

k

k

γ

γ

≥

Xγ

(63)

(1 + ϑ)̺

k2 + ̺

J k1 ≤
0, ϑ > 0. Taking ̺ = c
X
k

K√J
γ
k1,
∀
for some K
k2 results in scale invariance with respect to X. Let’s
compare (63) with the restricted eigenvalue (RE) condition and the compatibility condition
, the two conditions assume that there exist positive numbers κ, ϑRE such
[7, 59]. For given
J
2
2
2
1 (compatibility) or more restrictively,
2 (RE), for
that J
γ
Xγ
κ
2 ≥
J k
J k
k
k
k
k1. Therefore, (1 + ϑ)̺
all γ : (1 + ϑRE)
γ
γ
γ
γ
̺
J k1 ≥ k
k1
k2 ∨
k
k
k
with K = (1 + ϑRE)/(̺√κ), ϑ = ϑRE . That is, the RE-type conditions are more demanding
than (63) (and (60)). Another popular set of regularity conditions is based on restricted
strong convexity (RSC). Under a version of RSC condition (and assuming f is differen-
2 has a bound of order σ2(J ∗ log p)/n
2
tiable), [36, Theorem 1] showed that
k
for any stationary point ˜β. In the lasso case, the condition becomes
2
Xγ
2 −
k
k
0, from which it follows that for any
γ
τ log p
k
τ (2 + ϑRE)2 log p
2
γ
γ : (1 + ϑRE)
γ
γ
J k1 ≥ k
2 −
n k
k
k
J
τ (2 + ϑRE)2 J log p
τ (2 + ϑRE)2(J log p/n). Therefore,
2
γ
2 ≥
n k
J k
J log p, RSC implies RE and so is more restrictive than (63). See Remark A.1 in
when n
Section A.12 for an extension to general penalties.

2
1/n for some constant α > 0 and τ
≥
k
2
k1,
γ
Xγ
α
2 −
k
k
k
2
2, where κ′ = α
γ
κ′k
−
J k

2
γ
κ
2 ≥
k
k
K√J
Xγ
k

Xγ
k
J k1 ≤

2
1 ≥
J k

2
2 ≥
k

2
2 ≥
k

γ
α
k

γ
α
k

˜β
k

β∗

≫

−

J

J

c

c

18

Y. SHE ET AL.

REMARK 3 (Technical treatment). A big difference between our work and [36] is that the
latter enforces an ℓ1-type side constraint, for example,
R, in addition to the sparsity-
inducing penalty P . The use of the constraint is a necessary ingredient of the proofs and the
constraint parameter R appears in the minimum sample size condition and the error bounds
implicitly. However, few practically used algorithms seem to include such an additional ℓ1
constraint.

k1 ≤

β
k

Our analysis does not need any side constraint, and the resulting error bounds and the
oracle inequality hold with no minimum sample size requirement. In fact, in dealing with a
general penalty that may be nonconvex, our treatment of the stochastic term is distinctive
from the conventional “ℓ1 fashion” via H¨older’s inequality:
k1 (see,
k∞k
e.g., [10, 7, 37]). More concretely, applying the union bound to
will lead to a
2
2 + P (β; λ) up to multiplicative factors [36], while we can bound
further upper bound
k
2
2/a and a light penalty PH(β; λ)/b for any a, b > 0, with a
Xβ
ǫ, Xβ
k
k
h
proper choice of λ.

β
k
by the sum of

i ≤ k
X ⊤ǫ
k

ǫ, Xβ
h

X ⊤ǫ

k∞

β

i

REMARK 4 (Fixed points vs. local minimizers). Targeting at the ﬁxed points of the Breg-
man surrogate instead of the local minimizers of the original objective seems more reason-
able from a statistical perspective. Certainly, if f is smooth,
contains more valid solutions
(cf. Lemma 4). But a more important reason is that
can adaptively exclude bad local solu-
tions for some statistical learning problems with severe nonsmoothness and nonconvexity.

F

F

≤

For instance, each bridge ℓq-penalty (q : 0

q < 1) [22] determines a thresholding Θq,
which is however the solution for inﬁnitely many penalties; picking the particular one con-
structed from (48) that is the lowest and directionally differentiable [49], one can repeat
the analysis in Theorems 3, 4 to show provable guarantees for all the ﬁxed points of the
iterative Θq procedure. In contrast, as pointed out by [36], the original optimization prob-
lem may contain “faulty” local minimizers. In fact, when q = 0, the ℓ0-penalized problem
k0 (not directionally differentiable) always has 0 as a local
minβ
minimizer which is however a poor estimator as β∗ is large. Switching to the surrogate’s
ﬁxed points successfully addresses the issue: ˆβ = 0 is a valid ﬁxed point only when X ⊤y is
properly small:
λ, or the true signal is inconsequential relative to the maximum
noise level.

2/2 + (λ2/2)
2
β
k
k

X ⊤y
k

Xβ
k

k∞ ≤

−

y

3.2.2. Statistical analysis of the iterates from Bregman surrogates. We show a nice result
for (52) in the composite setting: under a regularity condition similar to those in Section 3.2.1,
with high probability, the t-th iterate can approach the statistical target within the desired
precision geometrically fast, even when p > n. Speciﬁcally, we add a mild multiple of ∆ψ to
the left-hand side of (57) and assume that for some δ > 0, ε > 0, ϑ > 0 and large K

0,

(64)

ε∆ψ(β∗, β) + δD2(Xβ, Xβ∗) + ϑPH (̺(β
(2 ¯∆

̺2

β∗); λ) + PΘ(̺β∗; λ)

−

LΘD2)(β, β∗) + PΘ(̺β; λ) + Kλ2J(β∗),
and ψ is differentiable for simplicity. Recall that (39) in Proposition 2 requires 2 ¯∆f to dom-
inate ε∆ψ; (64) gives a large-p extension of it.

l −

≤

β

∀

≥

THEOREM 5. Under the above regularity condition, for λ = Aσ

log(ep)/

(δ

with A sufﬁciently large and κ = 1/(1 + ε), we have

ϑ)ϑ

∧

(65)

∆ψ(β∗, β(t))

κt∆ψ(β∗, β(0)) +

≤

(Kλ2J ∗

for any t
stants.

≥

1 with probability at least 1

−

κ

1

κ

−
Cp−

p
∆ψ(β(s), β(s

−

p
1)))

−

min
s
1
≤
≤

t

cA2, where C, c are universal positive con-

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

19

The error measure ∆ψ(β∗, β(t)) in (65) has β∗ as its ﬁrst argument and differs from the

≤

∆l(ˆβ, β∗) used in (61). According to the proof, (64) only needs to hold for β = β(s) (0
≤
t), and so different starting values may give different values of κ. With ∆ψ ≥
0 (which
s
(σ2J ∗ log(ep))
can be realized by stepsize control), the fast converging statistical error to
implies that over-optimization may be unnecessary. As an example, consider the iterative
thresholding procedures with ∆l ≤
β∗

LD2 and ̺2 > L. Then (65) yields
̺2

λ2J ∗.

β(0)

β(t)

β∗

κt

O

k

−

2
2 ≤

k

2
2 +

k

(1

2κK
κ)(̺2

−

L)

−

̺2

−

L k

−

So it is possible to terminate the iterative algorithm before full computational convergence
without sacriﬁcing much statistical accuracy. The simulations in Section C.2 support this
point.

REMARK 5. Theorem 5 reveals the fast decay of the direct statistical error between β(t)
and β∗. [1] and [36] argued a similar point for gradient descent type algorithms, in a some-
how indirect manner: (i) β(t) can approach any globally optimal solution ˜β geometrically
fast in computation under a combination of an RSC condition and an RSM condition, and
(ii) under some regularity conditions, every local minimum point is close enough to the au-
thentic β∗. In the RSC condition for (i), the factor proceeding the dominant term ¯∆l is 1
(there are two different sets of RSC conditions used in Theorem 1 and Theorem 3 of [36],
the factor α1 in the second set corresponding to half of the α1 used in the ﬁrst set). But (64)
allows it to be 2. Moreover, Theorem 5 does not need the extra RSM condition and applies to
a broader class of algorithms. For example, we can show that the statistical error of the LLA
algorithm reduces at a linear rate to the desired precision under some regularity conditions;
see Proposition 5 and Lemma A.7 in Section A.16.

O

(1/t) to

4. Two acceleration schemes for generalized Bregman surrogates. How to acceler-
ate ﬁrst-order algorithms without incurring much additional cost per iteration has lately at-
tracted lots of attention in big data applications. In convex optimization, Nesterov’s momen-
tum techniques prove to be quite effective in that the rate of convergence can be improved
(1/t2), which is optimal when using ﬁrst-order methods on smooth prob-
from
lems [41, 57, 4, 32]. This section attempts to extend Nesterov’s ﬁrst and second accelerations
[39, 40] to Bregman-surrogate algorithms. With a possible lack of smoothness or convexity,
carefully choosing the relaxation parameters and step sizes is the key, and we will see the
beneﬁt of maximizing a quantity Rt/(θ2
t ρt) at the t-th iteration, with Rt appropriately de-
ﬁned via generalized Bregman notation. We consider the following two broad scenarios to
devise the acceleration schemes.

O

∆ψ0(β, γ) + ρD2(β, γ). This surrogate family includes
Scenario 1. g(β; γ) = f (β)
gradient descent type algorithms. Often, if minβ f (β) + ∆ψ(β, γ) is easy to solve, so is
minβ f (β) + ∆ψ(β, γ) + ρD2(β, γ), in which case ψ0 =
Scenario 2. g(β; γ) = f (β)
than the ﬁrst one.

ψ.
∆ψ0(β, γ) + ρ∆φ(β, γ). This gives a more general class

−

−

−

, γ), ∆ψ0(
This section assumes that f , ψ0, φ, ∆ψ0(
, γ) are directionally differentiable
·
·

given any γ. We introduce a convenient notation Cψ deﬁned for any ψ as follows

(66)

Cψ(α, β, θ) = θψ(α) + (1
1. Like ∆, C is a linear operator of ψ and its nonnegativity means convexity.

ψ(θα + (1

θ)ψ(β)

θ)β),

−

−

−

where 0
Some connections between ∆ and C are given below.

≤

≤

θ

20

Y. SHE ET AL.

LEMMA 5. Let ψ be directionally differentiable. (i) Cψ(α, β, θ) = (1

θ)∆ψ(β, α)

∆ψ(θα + (1
at α.

θ)β, α) for any α, β and θ

∈

−

[0, 1]. (ii) C∆ψ(
·

−
−
,α) = Cψ if ψ is differentiable

An acceleration scheme of the second kind. Scenario 2 is of our primary interest since it
applies more broadly. Below, we modify the surrogate and deﬁne an iterative algorithm (not
a descent method) that involves three sequences α(t), β(t), γ(t) starting at α(0) = β(0):

(67a)

(67b)

γ(t) = (1

θt)β(t) + θtα(t)

−
α(t+1)= argmin f (β)

∆ψ0(β, γ(t))+µ0∆φ(β, γ(t))+θtρt∆φ(β, α(t))

−
θt)β(t) + θtα(t+1),

β(t+1) = (1
0, θt ∈

(67c)
for some µ0 ≥
0), to be chosen later. Notice the extra GBF term
≥
µ0∆φ(
, γ(t)) in (67b) in addition to ∆φ(
, α(t)). The design of relaxation parameters θt and
·
·
inverse step size parameters ρt, µ0 holds the key to acceleration. Let

(0, 1], ρt > 0 (

−

∀

t

(68)

¯ψ0 = ψ0 −
We advocate the following line search criterion

µ0φ.

(69a)

Rt := θ2

t ρt∆φ(α(t+1), α(t))
+ C

−
,γ (t))(α(t+1), β(t), θt)

∆ ¯ψ0(β(t+1), γ(t)) + (1

0,

f (
·

∆ ¯ψ0 (
·
1θt
ρt
The update of the relaxation parameter involves ρ and µ as well.

)
−
1(ρt

1 + µ0)

(69b)

θ2
t

, t

θt

θt

≥

≥

=

−

1.

1

−

−

−

θt)∆ ¯ψ0(β(t), γ(t))

−

Theorem 6 presents two error bounds without assuming convexity or smoothness, and

shows in general the reasonability of (69a).

THEOREM 6. Let ρt be any positive sequence. Consider the algorithm deﬁned by (67a)–
φ(
f (
·
·

,γ (t))(β, α(t+1))+(µ0∆∆φ(
·

∆ψ0 (
·

,γ (t))

)
−

−

)

(67c) and (69b). Let
+ θtρt∆∆φ(
·

Et(β) = ∆ ¯ψ0(β, γ(t))+∆
))(β, α(t+1)).
φ(
−
·
(i) When µ0 = 0, for any β and T

,α(t))

0,

(70)

(71)

≥

f (β(T +1))
−
θ2
T ρT

f (β)

+ T

·

avg
T
t
≤

≤

0

Et(β)
θtρt

+ T

∆φ(β, α(0))

≤
−
(ii) Moreover, given any µ0 ≥

∆φ(β, α(T +1)) +

0,

1
θ0
−
θ2
0ρ0

(cid:2)

·

0

avg
T
t
≤
≤
f (β(0))

Rt
θ2
t ρt

f (β)

.

−

(cid:3)

f (β(T +1))

f (β) + θ2

T (ρT +

)∆φ(β, α(T +1))

−
ΠT

s=t+1(1

θs)

−

µ0
θT
(Rt + θtEt(β))

+ ΣT

t=0

(cid:0)
T
t=1(1

θt)

(1

(cid:1)

θ0)(f (β(0))

−

−

f (β)) + θ2

0ρ0∆φ(β, β(0))

−

≤
for all β and T

(cid:16)Q
≥

(cid:17)(cid:2)
0, where by convention,

u
s=l as = 1 as l > u.

(cid:3)

Q

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

21

some σ > 0. With the additional knowledge that f (
)
·
L ¯ψ0

First, we make a discussion of the results for convex optimization. Assume ∆φ ≥
D2 for some L ¯ψ0 ≥
θ2
t (ρt −

σD2 for
, γ(t)) is convex and ∆ ¯ψ0 ≤
∆ ¯ψ0(
·

L ¯ψ0/σ)∆φ(α(t+1), α(t)) + (1

θt)∆ ¯ψ0(β(t), γ(t))

0, (69a) is implied by

(72)

≥

−

−

0.

t −

So when f is convex, criterion (69) is satisﬁed by ρt = ρ
L ¯ψ0/σ, ψ0 = f, µ0 = 0 and
θ2
t + 4θ2
θ4
t )/2, degenerating to Nesterov’s second method [40, 57], and the
θt+1 = (
(1/T 2) according to (70) and (75). The second conclusion tells
convergence rate is of order
p
more when strong convexity (or restricted strong convexity) arises. Given a convex f satisfy-
∆f ≤
ing µDφ ≤
L and φ differentiable, taking ψ0 = f , µ0 = µ, and ρt =
Et(β) = ∆f
0
L
≥
−
L)σD2(α(t+1), α(t)) =
t ρtDφ(α(t+1), α(t))
θ2
and Rt ≥
0. According to (69b), the following choice

∆f (
·
∆ ¯ψ0(β(t+1), γ(t))
≥

,γ(t))(β, α(t+1))
θ2
t (ρt + µ0 −

≤
µφ(β, γ(t)) + ∆

LDφ with 0 < µ

µφ(β, γ(t))

µ ensures

∆f

f (
·

)
−

O

≥

≥

−

−

−

(73)

θt = θ0 =

√4κ

with κ = L/µ

3 + 1

sufﬁces, and the optimization problem to solve in (67b) becomes

2

−

−

(74)

min f (γ(t)) + δf (β; β

γ(t)) + µDφ(β, γ(t))+

From (71), both f (β(T +1))
parameter √4κ
√4κ

2(L
√4κ
f (β) and Dφ(β, α(T +1)) enjoy a linear convergence with rate
(√κ log(1/ǫ)), signiﬁcantly faster than
(κ log(1/ǫ)) in Proposition 2. Hence (67), (69) can achieve rate-optimality in various con-
O
vex scenarios. To the best of our knowledge, this is the ﬁrst “all-in-one” form of the second
acceleration that adapts.

, or an iteration complexity of

Dφ(β, α(t)).

µ)
3 + 1

−
−

3
1
−
3+1

−
−

O

−

The proposed algorithm can even go beyond convexity. As a demonstration, let us apply
the acceleration to the iterative quantile-thresholding procedure (cf. Example 2) for solving
the feature screening problem: min l(β) =
y
q, which is noncon-
k
2
2/2, µ0 = 0
vex. Here, q is bounded above by p but may be larger than n. Take φ =
k · k
p and X, deﬁne the restricted
and ψ0(β) = l(β)
0. Given any s
≤
L ≥
− L
β
Xβ
isometry number ρ+(s) [12] that satisﬁes
s, which can
ρ+(s)
k0 ≤
k
k
2
be much smaller than
2 as s is small.
k

2
2/2 s.t.
k

φ(β) for some

2
2 ≤
k

k0 ≤

X
k

2
2,
k

β
k

β
k

Xβ

β :

−

∀

COROLLARY 2. Assume q is set larger than the target

r. Then for any
thereby θt+1 = (
according to (67a)–(67c) satisﬁes l(β(T +1))
all T

ρ+(2q)/√r, there exists a universal ρt (ρt = ρ+(2q)(1
L ≥
t + 4θ2
θ4

k0 with the ratio denoted by
1/√r), say),
−
θ2
t )/2, such that the accelerated iterative quantile-thresholding
A/T 2 for
t
≤

0, where A is independent of T .

T ∆ψ0(β∗, γ(t))

l(β∗) + min0

β∗
k

t −

p

≤

−

≤

≥

The proof of the corollary shows the power of an accumulative Rt-control, and applies
more generally: if the objective function f (β), possibly nonconvex, can be written as the
LD2 and a function P (β) that can be lifted:
sum of a convex function l(β) with ∆l ≤
∆P +
L0D2 ≥
2
0, then one can utilize a ψ0 as l
2 and a
0 for some ﬁnite
L0k · k
0 in (70) (although not every Rt is necessarily
universal ρt to fulﬁll T
t ρt)
avgt
≤
nonnegative) so as to attain an

L0 ≥
T Rt/(θ2
(1/T 2) error bound. See Remark A.3 in Section A.14.

0.6

≥

−

·

Of course, a time-varying ρt can provide ﬁner control, and the theorem does not limit ρt to
1)2 (t
1)
(a+2)/(t+b)

be constant. In fact, under µ0 = 0, as long as ρt/ρt
for some constants a, b: a >

1
(at + ab + 1)/(t + b
−
a+1, induction based on (69b) gives θt ≤

1 ≥

2, b

−

≥

≥

−

−

O

(T + c1)2/(a + 2)2 + c2 (with constants ci dependent on a, b) for any

22

Y. SHE ET AL.

T
t=0 ρT /(ρtθt)

≥

1, from which it follows that
P

and
t

≥
(75)

θ2
T =

O

(1/T 2) and T

avg0

t
≤

T (1/(ρtθt))

≤
0, (70) gives f (β(T +1))

≥ O

(T 2/ρT ).

·
t ρt)

T
t=0 Rt/(θ2

≥

P

0 or just

Now, under Rt ≥
(ρT /T 2) for any β. Typically, (69a) involves a line search. If the condition fails for the
≤ O
current value of ρt, one can set ρt = αρt for some α > 1 and recalculate θt, γ(t), α(t+1) and
β(t+1) according to (69b) and (67) to verify it again. In implementation, it is wise to limit the
number of searches at each iteration (denoted by M ) to control the per-iteration complexity.
If (69a) does not hold after m times of search, we simply pick the ρt that gives the largest
Rt/(θ2
t ρt) based on Theorem 6. Some details are in Algorithm B.1. In simulation studies,
letting M = 3, α = 2 already shows excellent performance; see Figure C.5 and Figure C.6.

f (β)+min0

t
≤

−

≤

T Et(β)

An acceleration scheme of the ﬁrst kind. For the algorithms falling into Scenario 1, we can
alternatively consider two sequences of iterates generated by

(76a)

(76b)

γ(t) = β(t) +

ρt
{

−

1θt(1

β(t+1) = arg min f (β)

−

−

θt

1)/(ρt

(β(t)
1 + µ0)
}
∆ψ0(β, γ(t))+µ0D2(β, γ(t))+ρtD2(β, γ(t)),

1)),

β(t

1θt

−

−

−

−

−

0, θt ∈

(0, 1], ρt > 0 for all t

0, and we force γ(0) = β(0). (76a), (76b)
for some µ0 ≥
give a new ﬁrst type acceleration, and notably, the novel update of γ(t) involves ρt
1. When
−
β(t+1) = γ(t) one stops the algorithm and obtains a ﬁxed point with provable statistical guar-
antees as shown in Section 3.2.1.
Similar to (68), let ¯ψ0 = ψ0 −

µ0k · k

≥

2
2/2. Deﬁne the line search criterion
θt)∆ ¯ψ0(β(t), γ(t))

(77a)

0,

≥

Rt := (ρtD2 −
θ2
1(ρt
t
=

θt

−

∆ ¯ψ0)(β(t+1), γ(t)) + (1
1 + µ0)

−

1θt
ρt

1

(77b)

, θt ≥
Note that Rt is deﬁned differently from (69a). The following theorem reveals the importance
of maximizing Rt in each iteration step when performing possibly nonconvex optimization.

0, ρt > 0, t

θt

≥

−

1.

−

−

0), consider the algorithm deﬁned by (76a), (76b),
,γ (t))(β, β(t), θt) + ∆

,γ (t))

C
{

f (
·

)
−

∆ψ0 (
·

f (
·

)
−

∆ψ0 (
·

THEOREM 7. Given any ρt > 0 (t

and (77b). Let
(θtβ + (1

≥
Et(β) = ∆ ¯ψ0(β, γ(t)) +
θt)β(t), β(t+1))
/θt.
}
(i) When µ0 = 0, we have
f (β(T +1))
−
θ2
T ρT

f (β)

+ T

−

·

D2(β, β(0)) +

≤
(ii) Moreover, given any µ0 ≥
f (β(T +1))

f (β) + θ2

θ0
1
−
θ2
0ρ0

−
ΠT

+ ΣT

t=0

s=t+1(1

θs)

0

avg
T
t
≤
≤
f (β(0))

Et(β)
θtρt

+ T

·

Rt
θ2
t ρt

avg
T
t
≤

≤

0

f (β)

for any β and T

0.

≥

−

(cid:2)
0, for all β and T

(cid:3)

0,

T (ρT +

µ0
θT

≥
)D2(β, (γ(T +1)

(Rt + θtEt(β))

−
(cid:1)
θ0)(f (β(0))

θT +1)β(T +1))/θT +1)

(1

−

−

≤

T
t=1(1

(cid:16)Q

(cid:0)
−

θt)

(1

(cid:17)(cid:2)

−

f (β)) + θ2

0ρ0D2(β, β(0))

.

−

(cid:3)

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

23

Again, the new proposal of the iterate and parameter updates adapts to various situations,
with µ0 (which can be a sequence µt, cf. Remark A.2) measuring the degree of convexity
(or restricted convexity in a nonconvex composite problem). For example, when f is con-
θ2
vex and L-strongly smooth, µ0 = 0, ρt = L, ψ0 = f , and θt+1 = (
t )/2 make
(77) hold, corresponding to Nesterov’s ﬁrst method. Interestingly, if f is µ-strongly con-
p
vex, the associated standard momentum update γ(t) = β(t) + θt(θ−
1))
t
−
1/κ (κ = L/µ) (cf. Remark A.4), showing no theoretical
only attains a linear rate at 1
µ,
advantage over the plain gradient descent. (76) ﬁxes the issue: with µ0 = µ, ρt = L
θt = 2/(√4κ
1)/
3
(√4κ
3/(4κ)). (When µ0 is unknown, (76b) based on the split L = ρt + µt
is still advantageous over the classical acceleration with ρt = L.) We proved these error
bounds by use of GBFs, which is perhaps more straightforward than Nesterov’s ingenious
proof based on the notion of estimate sequence, and more importantly, (76), (77) provide a
universal “all-in-one” form, instead of separate schemes in different situations [41].

3 + 1), an accelerated linear rate parameter is obtained as (√4κ

t −
1)(β(t)

t + 4θ2
θ4

−
3 + 1)(

1
1 −

−
−

β(t

p

−

−

−

−

−

≤

1

−

Xβ

y
k

Theorem 7 accommodates diverse choices of the parameters ψ0, µ0, ρt, θt and is motivat-
2
ing in the nonconvex composite setup. Consider, for example, min f (β) =
2/2 +
k
−
PΘ(̺β; λ). Because the objective is nonconvex when p > n and
LΘ > 0, how to acceler-
ate the associated iterative thresholding procedure is an unconventional problem. From the
studies in Section 3.2, we have learned that a sparsity-inducing penalty with a properly large
threshold to suppress the noise can result in strong convexity in a restricted sense. We can then
use a surrogate f (β) + (ρD2 −
∆ψ0)(β, β−) where ψ0(β) =
2
2
y
β
2/2
2/2
LΘk
k
k
k
−
∆ψ0(β, γ(t)). More-
∆ψ0(
Et(β)
, γ) is convex (cf. Lemma A.3),
and µ0 = 0. Since f (
)
·
·
over, thanks to the sparsity in β(t), and thus γ(t), X(β(t)
γ(t)) involves just a small number
of features. So with an incoherent design, a properly small ̺ can make ∆ψ0(β(t), γ(t))
0.
X
Now, taking a constant ρt as large as, for instance,
LΘ, may yield a convergence
k
(1/t2). (Actually, linear convergence may result from the restricted strong
rate of order
convexity under some regularity conditions.) More generally, different ρt’s are allowed in the
(t + 3)/(t + 1)2. A line search can
theorem: (75) is still secured with just, say, ρt/ρt
be used to determine a proper sequence ρt; see Algorithm B.2 for more details.

2
2 −
k

1 ≥

Xβ

̺2

̺2

O

≥

−

≥

−

−

−

1

−

The proposed accelerations of the ﬁrst kind and of the second kind can be utilized in a
wide range of problems. Because they are momentum based, the original algorithms need
not be substantially modiﬁed to have an improved iteration complexity, and the two theo-
rems proved in this section apply in any dimensions with no design coherence restrictions.
Another delightful fact is that our “all-in-one” forms update the iterates adaptively according
0, which can be relaxed to a sequence of local measures µt
to the degree of convexity µ0 ≥
(Remark A.2). With a line search to get properly large µt, this could be helpful in high di-
mensional sparse learning problems which may or may not have restricted strong convexity
(the associated parameter often hard to determine in theory).

5. Summary. This paper studied the class of iterative algorithms derived from GBF-
deﬁned surrogates with a possible lack of convexity and/or smoothness. These surrogates
differ from the MM surrogates frequently used in statistical computation, in that they gain
additional ﬁrst-order degeneracy and may drop the majorization requirement. GBFs have
interesting connections to the densities in the exponential family and possess some idempo-
tence properties that are useful for studying iterative algorithms.

The GBF calculus built by the lemmas not only facilitates optimization error analysis but
can be bound to the empirical process theory for nonasymptotic statistical analysis (cf. Sec-
tions 3.2 and A.18). In addition to obtaining some insightful results in the realm of convex
optimization, we were able to build universal global convergence rates for a broad class of

24

Y. SHE ET AL.

Bregman-surrogate algorithms for nonsmooth nonconvex optimization. Moreover, in the non-
convex composite setting that is of great interest in high dimensional statistics, we found that
the sequence of iterates generated by Bregman surrogates can approach the statistical truth
at a linear rate even when p > n, and the obtained ﬁxed points enjoy oracle inequalities with
essentially the optimal order of statistical accuracy, under some regularity conditions less de-
manding than those used in the literature. Finally, we devised two “all-in-one” acceleration
schemes with novel updates of the iterates and relaxation and stepsize parameters, and some
sharp theoretical bounds were shown without assuming smoothness or convexity.

APPENDIX A: PROOFS

We list some notation and symbols that are used in the proofs. Given a directionally differ-
γ), ¯∆ψ(β, γ) = (∆ψ(β, γ) +
entiable function ψ, ∆ψ(β, γ) = ψ(β)
δψ(γ; β
r∆ψ(β, γ) = ∆ψ(γ, β). We occasionally denote ∆ψ(β, γ) by ∆(β, γ)
∆ψ(γ, β))/2, and
when there is no ambiguity. The classes of continuous functions and continuously differen-
1, respectively. Recall that all functions are assumed
tiable functions are denoted by
to be deﬁned on a vector space unless otherwise mentioned.

0 and

ψ(γ)

−

−

−

C

C

constants C, c > 0 such that P
deﬁned by σ(ξ) = inf
Gaussian random vector with scale bounded by σ if all one-dimensional marginals
are sub-Gaussian satisfying
σ

DEFINITION A.1. We call ξ a sub-Gaussian random variable if and only if there exist
ct2,
t > 0. The scale (or ψ2-norm) of ξ is
Ce−
Rp is called a sub-
2
≤
}
ξ, α
i
h
k2,

α
k
DEFINITION A.2. We call d a pseudo-metric if it satisﬁes d(η1, η2) = d(η2, η1)

ξ
{|
σ > 0 : E exp(ξ2/σ2)
{

. More generally, ξ

ikψ2 ≤

ξ, α

Rp.

} ≤

| ≥

kh

α

∈

∈

∀

∀

t

0 and

≥

d(η1, η2)

d(η1, η3) + d(η2, η3), for all η1, η2, η3.

≤

We state a ﬁrst-order optimality condition satisﬁed by all local minimizers of f that is
directionally differentiable. The result is basic and we omit the proof. It holds the key to
deriving the so-called “basic inequality” in a variety of statistical learning problems.

LEMMA A.1. Let f : Rp

Rp be a convex set.
Suppose that f is directionally differentiable at βo that is a local minimizer to the problem
∆f (β, βo) for all
minβ
β

→
C f (β). Then δf (βo; h)
∈
C.

R be a real-valued function and C

⊂
f (βo)

βo or f (β)

0 with h = β

≥

−

−

≥

∈

(i) This property is straightforward by deﬁnition:

A.1. Proof of Lemma 1.
∆aψ+bϕ(β, γ)
= (aψ + bϕ)(β)

(aψ + bϕ)(γ)

ψ(β)

−
δψ(γ; β
ψ(γ)
= a
= a∆ψ(β, γ) + b∆ϕ(β, γ).

−

−

(cid:2)

−
γ)

δ(aψ + bϕ)(γ; β

γ)

−

+ b

ϕ(β)

ϕ(γ)

−

−

δϕ(γ; β

γ)

−

−

(cid:3)

(cid:2)

(cid:3)

(ii) From [45, Theorem 23.1], the convexity of ψ implies the directional differentiability
) for any given β, and we can write
·

of ψ and the positively homogenous convexity of δψ(β;

(A.1)

δψ(β; h) = inf
ǫ>0

ψ(β + ǫh)
ǫ

−

ψ(β)

.

Putting ǫ = 1 and h = γ
0.

−

β in (A.1) gives δψ(β; γ

β)

−

≤

ψ(γ)

−

ψ(β), thus ∆ψ(γ, β)

≥

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

25

Conversely, suppose that ψ deﬁned on Rn is directionally differentiable (δψ exists and
δψ(β; h), sγ :

0. For any sβ :

sβ, h
h

i ≤

is ﬁnite), thus radially continuous, and ∆ψ ≥
h,
sγ, h
h
(A.2)

δψ(γ; h)

ψ(β)

ψ(γ)

i ≤

sγ , β

∀

i ≥

γ

β

−

−

∆ψ(β, γ)
∆ψ(γ, β)

0,

≥

0.

−

−
sβ
h

− h

− h
sγ , β

(A.3)

ψ(γ)

ψ(β)

sβ, γ

Adding them together gives
−
the Clarke-Rockafellar subdifferential of ψ, thereby its convexity according to [16].
(iii) To show the ﬁrst result, notice that ∆ψ

i ≥

−

γ

ϕ(β, γ)
◦
γ)))
/ǫ =
}

h∇

∆ψ(ϕ(β), ϕ(γ)) = limǫ
δ(ψ

ϕ(γ)

ψ(ϕ(γ)), ϕ(β)

−

−

i−

ψ(ϕ(γ)

0+{
ϕ)(γ; β

−

→
◦

i ≥
0. This indicates the monotone property of

≥

+ǫ(ϕ(β)
−
γ). From ψ

ϕ(γ)))

ψ(ϕ(γ +ǫ(β
0,

−
1 and ϕ

−

∈ C
ϕ)(γ; β

∈ C
γ) = lim
ǫ

0+{

−

δ(ψ

◦

ψ(ϕ(γ) + (ϕ(γ + ǫ(β

ψ(ϕ(γ)), ϕ(γ + ǫ(β

γ))

γ))

−

−

−

−

ϕ(γ)))

ψ(ϕ(γ))
/ǫ
}

−

ϕ(γ)/ǫ

i

→
= lim
ǫ
→

0+h∇

=

ψ(ϕ(γ)), δϕ(γ; β

h∇

γ)
.
i

−

Next, we prove the second result. Let ϕ : Rp

Using the deﬁnition of ∆ϕ(β, γ) (the componentwise extension), we obtain the conclusion.
Rn be the linear function ϕ(β) = Xβ + α
p. By deﬁnition, ∆ψ
ϕ(β, γ) =
◦

with its Jacobian matrix Dϕ(β) := [ Djϕi(x)] = X
ψ(ϕ(β))
δ(ψ

ψ(ϕ(γ))

ϕ)(γ; β

γ) and

Rn

→

∈

×

−
δ(ψ

−

ϕ)(γ; β

◦

−

−

◦
γ) = lim
ǫ

0+{

→
= lim
ǫ
→

0+{

ψ(ϕ(γ + ǫ(β

γ)))

ψ(ϕ(γ))
/ǫ
}

−

−

ψ(ϕ(γ) + ǫ Dϕ(γ)(β

γ)))

ψ(ϕ(γ))
/ǫ
}

−

−

= δψ(ϕ(γ); Dϕ(γ)(β

γ)) = δψ(ϕ(γ); ϕ(β)

−

ϕ(γ)),

−

from which it follows that ∆ψ

ϕ(β, γ) = ∆ψ(ϕ(β), ϕ(γ)).
◦

(iv) From Theorem 11 in [25], for any continuous function f with ﬁnite Dini derivative

D+f (x) := lim supǫ
−
b
a D+f (x)dx. By deﬁnition, ψ is continuous when restricted to the line segment
f (a) =
[β, γ] (radial continuity). It follows that
R

f (x))/ǫ, if D+f (x) is integrable over [a, b], f (b)

0+(f (x + ǫ)

−

→

ψ(β)

−

ψ(γ) = ψ

γ + t(β

−

γ)

1

t=0

(cid:1)(cid:12)
(cid:12)
(cid:12)
γ + (t + ǫ)(β

=

=

=

1
ǫ

lim
0+
→

ǫ

1
ǫ

lim
0+
→

(cid:0)
1

0

Z

0

Z

1

1

ǫ

δψ

0

Z

ψ

h

(cid:0)

h
(cid:0)
γ + t(β

γ)

−

−

ψ

γ + t(β

−

γ)

dt

(cid:1)i

ψ

γ + t(β

(cid:1)
γ) + ǫ(β

(cid:0)
γ)

−

−

−

ψ

γ + t(β

−

γ)

dt

(cid:1)i

(cid:1)

(cid:0)

γ); β

−

−

γ

dt.

(cid:1)

Hence, ∆ψ can be formulated by

(cid:0)

∆ψ(β, γ) =

1

δψ

γ + t(β

0

Z

h

(cid:0)

γ); β

γ

−

−

−

δψ(γ; β

−

(cid:1)

γ)

dt.

i

26

Y. SHE ET AL.

A.2. Proof of Lemma 2.

(i) First, if δψ(α;

α) is directionally differentiable, then

∆ψ(β, γ)

(A.4)
−
for any α, β, γ. In fact, ∆ψ(β, γ)
(β, γ) = ∆

α)(β, γ).

δψ(α;

∆∆ψ(
·
∆∆ψ(
·

−

· −
,α)(β, γ) = ∆
,α)(β, γ) = ∆

δψ(α;

·−

α)(β, γ)

ψ(
·

)
−

∆ψ(
·

,α)(β, γ) = ∆

ψ(α)+δψ(α;

α)

·−

Accordingly, when ψ is convex, which means δψ(α;

α) is convex as well (cf. Section
0 by Lemma 1. The result under concavity can be similarly proved.

· −

A.1), ∆

·−

δψ(α;

α)(β, γ)

·−

≥

(ii) Let

(A.5)

; α) = δψ(α;
q(
·

α).

· −
0 or θ

We want to show for α = θβ + (1
;α)(β, γ) is well-deﬁned
and equals 0. This is intuitive due to the linearity of q when restricted to [β, γ], assuming
β

α are positively collinear.

θ)γ with θ

α and γ

q(
·

≥

−

≤

1, ∆

−
−
To verify it, by deﬁnition,

; α)(γ; β
δq(
·

−

γ) = lim
0+
ǫ

[q(γ + ǫ(β

γ); α)

−

−

q(γ; α)]/ǫ

→
= lim
0+
ǫ
→
= lim
0+
ǫ
→
= lim
0+
ǫ
→

[δψ(α; γ + ǫ(β

γ)

−

−

α)

−

δψ(α; γ

α)]/ǫ

−

[δψ(α; (θ

[δψ(α; (ǫ

ǫ)(γ

θ)(β

β))

γ))

−

−

−

−

−

−

δψ(α; θ(γ

β))]/ǫ

−

δψ(α; (

−

θ)(β

−

γ))]/ǫ,

and so with θ > 0,

; α)(γ; β
δq(
·

−

γ) = lim
0+
ǫ

[(θ

→

ǫ)δψ(α; γ

β)

−

−

θδψ(α; γ

β)]/ǫ

−

−

and with θ

0,

≤
; α)(γ; β
δq(
·

δψ(α; γ

=

−

β),

−

γ) = lim
0+
ǫ

→

[(ǫ

−

−

θ)δψ(α; (β

γ))

(
−

−

−

θ)δψ(α; (β

γ))]/ǫ

−

γ).

= δψ(α; β
The above derivation also guarantees the existence of ∆∆ψ(
·
; α)(γ; β
β
δq(
h
·
−
δψ(α; γ
β
α, γ
−
i ≤
−
δψ(α; β
γ) = δψ(α; β
; α)(γ; β
δq(
·
(iii) By deﬁnition, we have

q(γ; α)
β
0,
≤
h
δψ(α; γ

−
β) = 0. As θ
α)

−
i ≥
α) + δψ(α; γ

0 and so q(β; α)

−
−
α)

α, γ

−

−

−

−

−

−

−

β

−

,α)(β, γ). Now, as θ
γ) = δψ(α; β

−
0 and q(β; α)
γ) = 0.

−

≥
α)
−
q(γ; α)

1,

−
−

; α)(z; β
δq(
·

−

γ)

0+

q(z + ǫ2(β

= lim
ǫ2→

1
ǫ2 {
1
ǫ2 {
Under the restricted linearity condition δψ(α; h) =

δψ(α; z + ǫ2(β

= lim
ǫ2→

γ); α)

γ)

−

−

−

−

0+

q(z; α)
}

z = γ + t(β

γ) with t

[0, 1),

∈

−

α)

δψ(α; z

α)
.
}

−

−

g(α), h
h

,
i

∀

h

∈

[β

−

α, γ

−

α], for

; α)(z; β
δq(
·

−

g(α), z + ǫ2(β

γ)

α

z + α
i

−

−

−

1
ǫ2 h

0+

γ) = lim
ǫ2→
g(α), β
h

=

γ

.
i

−

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

27

Under the restricted continuity condition limǫ
γ) with t
α], for z = γ + t(β
[β

α, γ

−

−

−

0+ δψ(α + ǫh; β
[0, 1),

−

γ) = δψ(α; β

γ),

h

∀

∈

−

→
∈

; α)(z; β
δq(
·

−

γ)

= lim
ǫ2→

0+

1
ǫ2 (cid:26)

1
ǫ1

lim
ǫ1→

0+

= lim
ǫ2→

0+

lim
ǫ1→

0+

= lim
ǫ2→

0+

lim
ǫ1→

0+

= lim
ǫ2→

0+

0+

lim
ǫ1→
1

h

1
ǫ1ǫ2 Z
δψ

1

0

0
Z

0+

= lim
ǫ2→
0
Z
= δψ(α; β

0+

lim
ǫ1→
γ),

−

ψ

α + ǫ1[z + ǫ2(β

γ)

−

−

α]

−

(cid:0)
ψ

α + ǫ1(z

(cid:1)

ψ(α)

α)

−

−

i(cid:27)

(cid:1)
ǫ1ǫ2γ + ǫ1ǫ2β

ǫ1)α + ǫ1z

−

h

1
ǫ1

ψ

h
(1

(cid:0)

−

−

0+

lim
ǫ1→
1
ǫ1ǫ2

ψ(α)

i

ψ

(1

−

−

ǫ1)α + ǫ1z

(cid:0)
1

δψ

(1

−

ǫ1)α + ǫ1z + ǫ1ǫ2s(β

(cid:0)
(cid:1)
γ); ǫ1ǫ2(β

−

γ)

ds

−

(cid:1)

(cid:1)i

(cid:0)

−

(1

(cid:0)

ǫ1)α + ǫ1z + ǫ1ǫ2s(β

γ); β

−

−

γ

ds

δψ

α + ǫ1(z + ǫ2s(β

γ)

−

−

α); β

γ

−

(cid:0)

(cid:1)
ds

(cid:1)

where we used the positive homogeneity of δψ(α;
) and the dominated convergence theo-
·
rem. (The integral is well-deﬁned due to the boundedness and Lebesgue measurability of the
integrand.)

The two sets of conditions are not equivalent in multiple dimensions. But in either case,
; α)(z; β
δq(
·
∆∆q(
·

γ) is a term independent of z. Hence by Lemma 1 (iv),

; α)(γ; β
δq(
·

; α)
δq(
·

,α)(β, γ) =

γ + t(β

dt = 0.

γ); β

γ)

−

−

−

−

−

γ

1

0
Z

h

(cid:0)

(cid:1)

i

A.3. Proof of Lemma 3.
makes a conjugate pair and so
i
notation (∂ϕ(z), z), we represent it as

(i) Let ϕ = b∗. Then for all subgradient g

∂ϕ(z), (g, z)
= b(g) + ϕ(z) (see, e.g., [45]). Using the shorthand
= b(∂ϕ(z)) + ϕ(z). Therefore,

g, z
h

∈

σ2l0(η; z) + b∗(z) =
=

∂ϕ(z), z
h
z, η

i
+ b(η) + ϕ(z)

−h

i

z, η

−h
= b(η)

−

+ b(η) +

i
b(∂ϕ(z))

b(∂ϕ(z))

= b(η)
−
= ∆b(η, ∂ϕ(z)).

b(∂ϕ(z))

z, ∂ϕ(z)
h

z, η

i −
∂ϕ(z)
i
b(∂ϕ(z)), η

−

− h

− h∇

∂ϕ(z)
i

−

When pη is minimal,
deﬁned on
clude the dispersion parameter), and so (g(z),
pair.

1 is well-
is full-dimensional and the canonical link g = (
M◦ (Proposition 3.1 and Proposition 3.2 in [62] can be slightly modiﬁed to in-
b(g(z))) or (g(z), z) makes a conjugate

b)−

M

∇

∇

b(η) or µ for brevity. It follows that η

∂ϕ(µ) and so

(ii) Let µ(η) =

z, η

−h

i

∇
+ b(η) + b∗(z) =

∈

ϕ(µ) + ϕ(z)

i −
ϕ(µ) + ϕ(z)

z, η

+

µ, η
i
h
µ, η

−h

=

z

−h

−
δϕ(µ; z

i −
µ)

≥ −

−

ϕ(µ) + ϕ(z) = ∆ϕ(z, µ),

−

28

Y. SHE ET AL.

where the inequality is due to [45, Theorem 23.2]. We claim that the inequality is actually an
equality.

Indeed, if there exist η1, η2 ∈
b(η1), η2 −
∇
vector y following pη in the exponential family, where η = tη1 + (1

−
= 0 and so ∆b(η2, η1) = 0 since b is convex. Therefore, for any random

∂ϕ(µ) with η1 6

= η2, then ¯∆b(η2, η1) =

t)η2, t

b(η2)

η1i

(0, 1),

h∇

−

∈

V ar((η2 −

η1)T y) = 0,

which can be obtained from Proposition 3.1 of [62]. Because exp((
η1, z
η2 −
for any η
h
is not minimal). It follows that

= c for almost every z

Ω, we have

∈ Y

∈

i

, η

b(η))/σ2) > 0
n with respect to ν (i.e., pη

i −

h·

Finally, from δϕ(µ; h) = sup

z
h
g, h

−
: g

{h

i

µ, η1 −
∂ϕ(µ
∈

= 0.

η2i
[45, Theorem 23.4], the claim is true.
}

on

In the case that pη is also minimal, ϕ can be shown to be strictly convex and differentiable
M◦ [45, Theorem 26.4].
(iii) Let dPη = pη dν0. By deﬁnition,

KL(pη1, pη2) =

log( dPη1/ dPη2) dPη1 =

pη1 log(pη1/ dpη2) dν0

Z

Z

and so

KL(pη1, pη2) =

log

y,η1i−
e(
h
{

b(η1))/σ2

c(y,σ2)/e(
y,η2i−
h

−

b(η2))/σ2

−

c(y,σ2)

dPη1

}

Z
1
σ2
1
σ2 {
1
σ2 {

=

=

=

Z
b(η2)

b(η2)

y, η1 −
h

η2i −

b(η1) + b(η2) dPη1

b(η1) +

Z

y, η1 −
h

η2i

dPη1}

b(η1) +

b(η1), η1 −

η2i}

h∇

−

−

= ∆b(η2, η1)/σ2,
where the third equality is due to Ey
y =
from Proposition 3.1 of [62]). Moreover, from Lemma 1, σ2∆l0(η2, η1) = ∆b(η2, η1).

b(η1) under η1 ∈

Ω◦ (which can be derived

pη1

∇

∼

A.4. Proof of Lemma 4.

In this proof, all directional derivatives are with respect with
β. The result of (i) is trivial from the construction of g. For (ii), by deﬁnition, we have
β−). It
δg(β; β−, h) = δf (β; h) + δψ(β; h)
follows from q(β; β−) = limǫ

δq(β; β−, h) with q(β; β−) = δψ(β−; β

−
0+[ψ(β− + ǫ(β

ψ(β−)]/ǫ that

β−))

−

→
[q(β + ǫ′h; β−)

−
q(β; β−)]/ǫ′

−

−

(1/ǫ′) lim
0+

ǫ

→

[ψ(β− + ǫ(β + ǫ′h

δψ(β−; β

β−)/ǫ′.

β−))

−

−

ψ(β−)]/ǫ

(cid:9)

δq(β; β−, h) = lim
0+
→
= lim
ǫ′
0+
→

ǫ′

−
When β− = β, δψ(β−; β

(cid:8)
lim
ǫ′
0+
→

−
β−) = 0 and so
−
δq(β; β−, h)
|β−=β = lim
→
= lim
ǫ′′
→
= δψ(β; h).

0+

0+

ǫ′

ǫ

[ψ(β + ǫ(ǫ′h))

ψ(β)]/(ǫǫ′)

lim
0+
→
[ψ(β + ǫ′′h)

−
ψ(β)]/ǫ′′

−

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

29

The above argument also guarantees the existence of δg(β; β−, h)
δg(β; β−, h)

|β−=β = δf (β; h) for any β and h.

|β−=β. Therefore,

A.5. Proof of Lemma 5. All results in Lemma 1 and Lemma 2 can be formulated for C.
µD2 implies
θ)D2(α, β), and so on. To show

0, Caφ+bϕ = aCφ + bCϕ, ∆ψ ≥

For example, ψ is convex if and only if Cψ ≥
µC2 since C2(α, β, θ) := C
Cψ ≥
(i), we have

2/2(α, β, θ) = θ(1

2
k·k

−

Cψ(α, β, θ) + ∆ψ(θα + (1

θ)β, α)

−
ψ(θα + (1

θ)β)

−

= θψ(α) + (1

−
+ ψ(θα + (1

θ)ψ(β)

−

= (θ

= (1

= (1

−

−

−

1)ψ(α) + (1

θ)ψ(β)
(1
θ)∆ψ(β, α).

−

−

−

θ)β)

ψ(α)

δψ(α; θα + (1

−

−
θ)ψ(β)

−
δψ(α; (1

α)

−

θ)β

α))

θ)ψ(α)

−

(1

−

−

θ)(β

−
θ)δψ(α; β

−

−

α)

−

; α) = δψ(α;
Similar to the proof of Lemma 2, let q(
·

C∆ψ(
·

,α)(β, γ, θ)

Cψ(β, γ, θ) = C

ψ(α)+q(
−
·
; α). We can show analogous results
without requiring the directional differentiability of q(
·
to Lemma 2. For example, for any convex ψ, from the positively homogenous convexity of
q,

q(
·

;α)(β, γ, θ),

· −

α). Then
;α)(β, γ, θ) = C

C∆ψ(
·

,α) ≤
θ)γ + θβ with θ

Cψ

holds for any α, and for α = (1

(0, 1),
,α)(β, γ) = Cψ(β, γ)
follows from the restricted linearity of q. In particular, when
C

−
C∆ψ(
·

0 which gives the result in (ii).

6∈

∇

ψ(α) exists, q is linear and so

q(
·

;α) ≡
A.6. Proof of Theorem 1. The theorem can be proved based on Theorem 6.1 of [37] and
property (iii) of Lemma 3 in Section 2.1. We give some details for the second conclusion; the
proof of the ﬁrst follows similar lines and is easier. Consider a signal subclass
β : βj ∈ {
{

k0 ≤

0, τ R

β
k

1 =

,
}

,
}

s∗

B

where

R = [σ(log(ep/s∗))1/2/κ1/2]

M

∧

and 1 > τ > 0 is a small constant to be chosen later. Clearly,
approximation, log
stant c.

s∗ log(p/s∗)

log

p
s∗

≥

≥

1

|B
| ≥
β1 −
k
in [44], there exists a subset

Let ρ(β1, β2) =

(cid:1)

(cid:0)
10

β2k0, the Hamming distance between β1 and β2. By Lemma A.3
10 and
B

1 such that 0

∈ B

log

10

|B

| ≥

⊂ B
c1s∗ log(ep/s∗), ρ(β1, β2)

c2s∗,

β1, β2 ∈ B

∀

≥

10, β1 6

= β2

1

(s∗, M ). By Stirling’s
cs∗ log(ep/s∗) for some universal con-

∈ B

B

for some universal constants c1, c2 > 0. Then
2 = κτ 2R2ρ(β1, β2)
2
β2k

Xβ1 −
k

β1 −
κ
k

Xβ2k

(A.6)

c2κτ 2R2s∗

≥

for any β1, β2 ∈ B

10, β1 6

2
2 ≥
= β2.

30

Y. SHE ET AL.

By Lemma 3 (iii), since Ω is open, for any β

10, we have

KL(pβ, p0) = ∆l0(0, Xβ)

τ 2κR2s∗/(2σ2).

∈ B

≤

Therefore,

(A.7)

1
10

|B

| −

1

KL(pβ, p0)

≤

τ 2s∗ log(ep/s∗).

}

10

Xβ
\{
∈B

0

Combining (A.6) and (A.7) and choosing a sufﬁciently small value for τ , we can apply

Theorem 2.7 of [58] to get the desired lower bound.

A.7. Proof of Proposition 1. We ﬁrst introduce a lemma.

β(t)
{

LEMMA A.2. For the sequence of iterates
deﬁned by (31) starting from an arbi-
trary point β(0), if f (
; β(t)) are directionally differentiable, the following inequality
) and g(
·
·
holds for any β and t
0
f (β) + ∆ψ(β, β(t))
f (β(t+1)) + ∆ψ(β(t+1), β(t)) + (∆∆ψ(
·

;β(t)) + ∆f )(β, β(t+1)).

(A.8)

≥

≥

}

It can be proved by Lemma A.1 and Lemma 1 (details omitted). Rearranging (A.8) gives

f (β) + ∆ψ(β(t+1), β(t)) + ∆f (β, β(t+1))

f (β(t+1))

−
∆ψ(β, β(t))

∆∆ψ(
·
Under ∆ψ(β(t+1), β(t)) + ∆f (β, β(t+1))

−

≤

;β(t))(β, β(t+1)).

0, we have

≥
∆ψ(β, β(t))

f (β(t+1))

(A.9)
By Lemma 2, when ψ is differentiable, ∆∆ψ(
·
up the corresponding inequality for t = 0, 1, . . . , T leads to

f (β)

−

−

≤

∆∆ψ(
·

;β(t))(β, β(t+1)).

;β(t)) is well-deﬁned and equals ∆ψ. Adding

T

t=0
X

Therefore,

[f (β(t+1))

f (β)]

−

≤

∆ψ(β, β(0))

−

∆ψ(β, β(T +1)).

f (β(t+1))

f (β)

−

1
T + 1

≤

[∆ψ(β, β(0))

−

∆ψ(β, β(T +1))].

avg
T
t
≤

≤

0

Note that under just the directional differentiability of ∆ψ(
; β(t)), (35) can be replaced by
·
∆ψ(β(t+1), β(t)) + (∆f + ∆∆ψ(
0, 0
≥
·

In the speciﬁc case that both f and ψ are convex, (35) is always satisﬁed by Lemma 1 and

∆ψ)(β, β(t+1))

;β(t)) −

T .

≤

≤

t

letting β = β(t) in (A.9) gives

Hence f (β(T +1))
proof is complete.

−

f (β) = min0

≤

t
≤

f (β(t+1))

−

f (β(t))

∆ψ(β(t), β(t+1))

0.

≤

≤ −
T f (β(t+1))

f (β)

avg0

≤

t
≤

≤

−

T f (β(t+1))

−

f (β). The

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

31

A.8. Proof of Proposition 2. Substituting βo for β in Lemma A.2 gives

(A.10)

f (βo) + ∆ψ(β(t+1), β(t)) + ∆∆ψ(
·

;β(t))(βo, β(t+1))

f (β(t+1))

−
∆ψ(βo, β(t))

≤

∆f (βo, β(t+1)).

−

By Lemma A.1, we get

(A.11)

f (β(t+1))

f (βo)

−

≥

∆f (β(t+1), βo).

Combining (A.10) and (A.11) yields

(A.12)

(2 ¯∆f + ∆∆ψ(

·

;β(t)))(βo, β(t+1)) + ∆ψ(β(t+1), β(t))

∆ψ(βo, β(t)).

≤

(A.13)

It follows from the strong idempotence property that
(2 ¯∆f + ∆ψ)(βo, β(t+1))

−
T , and so (40) can be obtained under 2 ¯∆f ≥

min
T
t
0
≤
≤
ε∆ψ.
To show the ﬁrst result, since ∆φ = ∆ψ + ∆f , (A.13) becomes
(2 ¯∆f + ∆φ −

∆f )(βo, β(t+1))

∆f )(βo, β(t))

(∆φ −

∆ψ(βo, β(t))

for any 0

≤

≤

≤

−

≤

t

∆ψ(β(t+1), β(t)),

∆ψ(β(t+1), β(t)).

min
T
t
0
≤
≤

Because κ > 1, (37) implies that

∆f ≥
Applying the inequality twice, we obtain ((κ+1)/κ) ¯∆φ(βo, β(t+1))
β(t))

(κ + 1) ¯∆φ/κ

−

min0

r∆φ.

−

(A.14)

t
≤

T ∆ψ(β(t+1), β(t)), or
κ
1
−
κ + 1

≤
¯∆φ(βo, β(t+1))

≤

¯∆φ(βo, β(t))

κ
κ + 1

−

min
T
t
0
≤
≤

∆ψ(β(t+1), β(t)).

(2

−

≤

(κ+1)/κ) ¯∆φ(βo,

The ﬁnal conclusion can be obtained by applying (A.14) iteratively for t = 0, 1, . . . , T .

A.9. Proofs of Theorem 2 and Corollary 1. The proof of the theorem follows from

Section A.7. In fact, setting β = β(t) in (A.8) gives

r∆ψ +∆∆ψ(
(

·

;β(t)) + ∆f )(β(t), β(t+1))

f (β(t))

−

≤

f (β(t+1)),

which, by the weak idempotence property (with α = β(t)), reduces to
(2 ¯∆ψ + ∆f )(β(t), β(t+1))

−
Summing up (A.15) over t = 0, 1, . . . , T gives the conclusion.

f (β(t))

(A.15)

f (β(t+1)).

≤

Next, we prove a result slightly more general than Corollary 1. Recall the surrogate

g(β; β−) = f (β) + (ρDϕ −

∆f )(β, β−)

where ϕ
tiable but not necessarily convex or differentiable. Denote arg min g(β; β−) by

1 is a strictly convex function, and f is continuous and directionally differen-

∈ C

(β−).

T

COROLLARY 1’.

Suppose that ∆f ≤
rameter ρ satisﬁes ρ > L/2. Then avg0
t
≤
≤
f (β(0))
¯Dϕ(β(t), β(t+1))
L) .
avg0
(T +1)(2ρ

T

t
≤

≤

≤
Moreover, for any accumulation point of β(t) at which

−

point of

T

. This is particularly true when f

1.

∈ C

L ¯Dϕ for some L > 0 and the inverse stepsize pa-
r∆f )(β(t), β(t+1))
f (β(0))
T (2ρ ¯Dϕ −
(T +1) and so

≤

is continuous, it must be a ﬁxed

T

32

Y. SHE ET AL.

PROOF. Observe from (A.15) that

f (β(t))

−

f (β(t+1))

≥

≥

≥

The error bounds can be obtained.

(2ρ ¯Dϕ −
(2ρ ¯Dϕ −
(2ρ
−

2 ¯∆f + ∆f )(β(t), β(t+1))
∆f )(β(t+1), β(t))

L) ¯Dϕ(β(t+1), β(t))

0.

≥

Let βo be the limit point of some subsequence βtl as l

monotonically to liml

→∞

f (βtl) = f (βo). It follows that

. Hence f (β(t)) converges

→ ∞

¯Dϕ(β(t+1), β(t)) = 0.

lim
+
t
→

∞

is a well-deﬁned function because of the strict convexity of the g-optimization problem.

T
From the continuity assumptions,

0 = lim
+
l
(βo) = βo, i.e., βo is a ﬁxed point of

∞

→

¯Dϕ(β(tl+1), β(tl)) = ¯Dϕ(
T

.

T

and thus

T

(βo), βo)

A.10. Proof of Proposition 3. First, we show a result when using the Bregman surrogate
g(β; β−) = l(β) + P (̺β) + ∆ψ(β, β−) for solving minβ f (β) = l(β) + P (̺β) where l and
P directionally differentiable and can be nonconvex. Deﬁne
D2 ≥

LP := inf

R : ∆P +

(A.16)

,
0
}

{L ∈

L

which provides an index to characterize the degree of nonconvexity of P , c.f. [36]. Assume
arg minβ g(β; β(t)), the following inequality holds for all
LP >
1
T
≥

. Then for β(t+1)

−∞

∈

(2 ¯∆ψ + ∆l −

̺2

LP D2)(β(t), β(t+1))

≤

1
T + 1

f (β(0))

−

f (β(T +1))

.

(cid:2)
The result can be proved from Theorem 2, noticing the fact that ∆f (β, β−) = ∆l(β, β−) +
∆P (̺β, ̺β−)
LP D2(β, β−) for any
− LP D2(̺β, ̺β−) = ∆l(β, β−)
≥
β, β−. The details are omitted.

∆l(β, β−)

̺2

−

(cid:3)

It sufﬁces to proving the following lemma to complete the proof of Proposition 3.

LEMMA A.3. Given any thresholding function Θ satisfying Deﬁnition 3, let PΘ be the
LΘ as deﬁned in (47) equals

LPΘ that is given in (A.16).

Θ-induced penalty in (48). Then

j

PROOF. Since ∆PΘ(β, γ) =

ate case. Recall that Θ−
γ
1(u; λ)
|0 (Θ−
|
−
1(u; λ)
u for u
Θ−
R
−
) a.e., and so
u
s′(u) = s′(
|
|

∆PΘ(βj , γj), it sufﬁces to show the result in the univari-
1(u; λ) := sup
t : Θ(t; λ)
) =
{
|
0 without loss of generality. We deﬁne s(u; λ) =
u) du, we assume γ
≥
) to (
0, and extend s(
s(u), u > 0. Clearly,
u) =
·
. By deﬁnition,
s′(u; λ) : u
−LΘ = ess inf
{

u > 0. Since PΘ(γ) = PΘ(
γ
|

, 0) by s(
= 0
}

,
u
}

−∞

P

≤

−

−

≥

∀

δPΘ(γ; β

γ) =

−

γ),

s(γ)(β
β
s(0)
|

−
,
|

(

if γ
0,
≥
if γ = 0.

avg
T
t
≤

≤

0

6
ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

33

When β

0 and γ

≥

= 0, we get
(∆PΘ +

D2)(β, γ)
L
PΘ(γ)

−

−

= PΘ(β)

δPΘ(γ; β

γ) +

D2(β, γ)

L

−
1
2 L

s(u) du

s(γ)(β

−

−

γ) +

γ)2

(β

−

s(u) du

β

−

γ

Z

s(γ) du +

β

(u

γ) du

−

L

γ
Z

β

β

β

γ

Z

γ

Z

γ

Z

=

=

=

=

s(u)

−

s(γ) +

(u

L

−

γ)

du

(cid:2)

β

u

(cid:3)

s′(v) +

dv du.

L

(cid:3)

(cid:2)

When β < 0 and γ

= 0,

γ Z

Z

γ

(∆PΘ +

= PΘ(β)

D2)(β, γ)
L
PΘ(γ)

−

−

δPΘ(γ; β

γ) +

L

−

D2(β, γ)

β

−

β

−

γ

Z

γ

Z

−

β

−

γ

Z

−

β

−

=

=

=

=

s(u) du

s(γ)(β

γ) +

−

−

1
2 L

(β

−

γ)2

s(u) du

β

−

−

γ

Z

−

γ) du +

s(

−

β

−

L

γ

Z

−

(u + γ) du

s(u)

s(

−

−

γ) +

L

(u + γ)

du

(cid:2)

u

(cid:3)

s′(v) +

L

γ

dv du.

Z

−

γ Z

−

(cid:2)
Similarly, when γ = 0, (∆PΘ +
verify that

LΘ =

LPΘ.

(cid:3)
D2)(β, 0) =

L

β
|0
|

R

u
0 [s(v) +
R

L

] dv du. It is then easy to

A.11. Proof of Proposition 4. Let f (β) = l(β) + P (̺β) and recall

LLA(β; β(t)) = f (β) + ∆(t)
g(t)

= f (β) +

∆P )(̺βj , ̺β(t)

j ).

LLA(̺β, ̺β(t))
(α(t)
∆1 −
j

Xj

The proof is similar to that of Theorem 2 and we give some details for complete-
LLA(β; β(t)) as shown in Example 5 implies
ness. The important fact β(t+1)
∆
;β(t))(β(t), β(t+1))

LLA(̺β(t), ̺β(t))

f (β(t+1))+∆(t)

arg minβ g(t)

LLA(̺β(t+1), ̺β(t))

∈
f (β(t))

∆(t)

≤

−

−

g(t)
LLA(
·

6
6
34

or

f (β(t))

−

f (β(t+1))

Y. SHE ET AL.

∆f (β(t), β(t+1)) + ∆

≥
= ∆f (β(t), β(t+1)) + ∆∆(t)

∆(t)

LLA(̺
·

,̺β(t))(β(t), β(t+1)) + ∆(t)
,̺β(t))(̺β(t), ̺β(t+1)) + ∆(t)

LLA(̺β(t+1), ̺β(t))

LLA(̺β(t+1), ̺β(t))

= ∆f (β(t), β(t+1)) + ∆(t)
= ∆f (β(t), β(t+1)) + 2 ¯∆(t)

LLA(
·
LLA(̺β(t), ̺β(t+1)) + ∆(t)
LLA(̺β(t), ̺β(t+1)).

LLA(̺β(t+1), ̺β(t)).

The conclusion follows from summing up this inequality for t = 0, 1, . . . , T .

A.12. Proofs of Theorem 3 and Theorem 4. Let f (β) = l(β) + PΘ(̺β; λ) and recall

g(β; β−) = f (β) + ∆ψ(β, β−). We ﬁrst introduce a lemma.

. Then for any β

Rp, we have the following inequality regard-

∈

LEMMA A.4. Let ˆβ
∈ F
less of the speciﬁc form of ψ
(∆l −
∆l(β, β∗) +

(A.17)

̺2

≤

LΘD2)(β, ˆβ) + ∆l(ˆβ, β∗) + PΘ(̺ˆβ; λ)
i

+ PΘ(̺β; λ).

ǫ, X ˆβ
h

Xβ

−

PROOF. Denote ˆg(β) := g(β; ˆβ) = l(β)+PΘ(̺β; λ)+∆ψ(β, ˆβ). Since ˆβ is a minimizer

ˆg(ˆβ). On the one hand,

), Lemma A.1 shows that for any β, ∆ˆg(β, ˆβ)
of ˆg(
·
ˆg(β)

ˆg(ˆβ)

ˆg(β)

−

≤

−

= l(β)

−

l(ˆβ) + PΘ(̺β; λ)

PΘ(̺ˆβ; λ) + ∆ψ(β, ˆβ)

−

= l(β)

l(β∗)

−
= ∆l(β, β∗) +

+ PΘ(̺β; λ)

−

h∇

−
l(β∗), β
β∗
PΘ(̺ˆβ; λ) + ∆ψ(β, ˆβ)

i −

−

−

−
∆l(ˆβ, β∗) +

= ∆l(β, β∗)

+ PΘ(̺β; λ)
On the other hand, by Lemma 1, Lemma 2, and Lemma A.3,
∆ˆg(β, ˆβ) = ∆l(β, ˆβ) + ∆

ǫ, X ˆβ
h

Xβ

−

−

i

(l(ˆβ)

l(β∗)) + PΘ(̺β; λ)

PΘ(̺ˆβ; λ) + ∆ψ(β, ˆβ)

(∆l(ˆβ, β∗) +

l(β∗), ˆβ

β∗

)
i

−

h∇

= ∆l(β, ˆβ) + ∆
∆l(β, ˆβ)
̺2

−

≥

, ˆβ)(β, ˆβ)

PΘ(̺
·

)(β, ˆβ) + ∆∆ψ(
)(β, ˆβ) + ∆ψ(β, ˆβ)
LΘD2(β, ˆβ) + ∆ψ(β, ˆβ).

PΘ(̺
·

·

The conclusion follows.

PΘ(̺ˆβ; λ) + ∆ψ(β, ˆβ).

−

To handle the stochastic term

LEMMA A.5. Let X
∈
bounded by σ, and λo = σ
constants A0, C, c > 0 such that for any a

Rn

i

−

Xβ

in (A.17), we introduce the following result.

ǫ, X ˆβ
h
p, ǫ be a sub-Gaussian random vector with mean 0 and scale
k2. Then there exist universal
A0, the following event

×
log(ep). Suppose that ̺

≥ k

p
ǫ, Xβ
2
h

i −

1
a k

Xβ

sup
Rp
β

X
2b > 0 and A1 ≥
1
b

≥
2
2 −
k

[PH (̺β; √abA1λo)]

aσ2t

≥

o

n
occurs with probability at most C exp(

∈

ct)p−

cA2
1.

−

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

35

The lemma can be proved by Lemma 4 of [49] based on a scaling argument.
Let R = supβ, ˆβ

2b [PH (̺(ˆβ

ǫ, X ˆβ

X ˆβ

Xβ

Xβ

1

β); √abAλo)]
}

−

with λo = σ

p

(A.18)

−

−

i −

1
2a k

Rp{h
∈

2
2 −
k
log(ep). Plugging the bound into (A.17) gives
(∆l −
∆l(β, β∗) +

LΘD2)(β, ˆβ) + ∆l(ˆβ, β∗) + PΘ(̺ˆβ; λ)
2
2 +
k

[PH(̺(ˆβ

1
2a k

1
2b

X ˆβ

Xβ

̺2

≤
aσ2t)

with P(2R

≥

≤

C exp(

ct)p−

−

−
cA2 for any a

−
2b > 0 and A large.

≥

To prove Theorem 3, substitute β∗ for β in (A.18) and combine it with the regularity

condition (57), resulting in

1
a

δ

−

D2(X ˆβ, Xβ∗) +

ϑ

1
2b

−

PH(̺(ˆβ

β∗; λ)

−

≤

Kλ2J(β∗) + R

(cid:0)

(cid:1)

2b > 0, and A
(2ϑ)), b = 1/(2ϑ) or a = 2/((2δ)

where λ = √abAλo, a
2/(δ
β∗); λ). Finally, by Lemma A.5, P(R

≥

∧

∧
0)

(cid:0)

For Theorem 4, we combine (A.18) and (60) with γ = ˆβ:

1

≥

−

≤

Cp−

cA2.

(cid:1)
A0 with A0 given in Lemma A.5. Setting a =

≥
ϑ), b = 1/ϑ bounds D2(X ˆβ, Xβ∗) or PH(̺(ˆβ

PΘ(̺β; λ)

−
β); √abAλo)] + R

r∆l(β, ˆβ) + ∆l(β, β∗) + Kλ2J(β) + R.

∆l(ˆβ, β∗) +

α
L
−
≤
Take the same choice for λ and set a = 2/(δ

D2(Xβ, X ˆβ)

1
a

(cid:0)

(cid:1)

δ

Case (i): αr/L = 0. The conclusion follows easily, and does not need any restriction on α

(2ϑ)), b = 1/(2ϑ).

∧

or L.

Case (ii): αr/L > 0. Then 0 < r < 1 and α/L > 0. If α < 0, (αr/L)∆l0(Xβ, X ˆβ)

αrd2(Xβ, X ˆβ)

0, reducing to the ﬁrst case. Assume α > 0. Then

−

≤

≤
r∆l(β, ˆβ) =

α
L

r∆l0(Xβ, X ˆβ)

α
L
αr(d(Xβ, Xβ∗) + d(X ˆβ, Xβ∗))2

αrd2(Xβ, X ˆβ)

≤

αr(1 + 1/M )d2(Xβ, Xβ∗) + αr(1 + M )d2(X ˆβ, Xβ∗)
r(1 + 1/M )∆l(β, β∗) + r(1 + M )∆l(ˆβ, β∗),

≤

≤

≤

for any M > 0. Take M = (1

r)/(1 + r). Then

1
1/
{

−

So we obtain

−
r(1 + M )
}

= 1 + r(1 + 1/M ) = (1 + r)/(1

r).

−

(

1
r
−
1 + r

∆l0 +

δ
2

D2)(X ˆβ, Xβ∗)

1 + r
1
r
Finally, from P(2R
ct)p−
oracle inequality is proved. In fact, we also get

∆l0(Xβ, Xβ∗) +

C exp(

aσ2t)

−

≤

−

≤

≥

KA2

((2ϑ)

δ)ϑ

∧

σ2J(β) log(ep) + R.

cA2

C exp(

−

≤

ct), we have ER

Caσ2. The

≤

E[D2(X ˆβ, Xβ∗)]

2(1 + r)
r)δ
(1

−

≤

E[∆l0(Xβ, Xβ∗)]

σ2J(β) log(ep) +

C

∧

(ϑ

σ2

δ)δ

2KA2

δ)ϑδ

(ϑ

∧

+

under the same condition.

36

Y. SHE ET AL.

REMARK A.1. Recall J ∗ = J(β∗),

j P (βj; λ). When ˆβ
is a global minimizer, applying the bound of the stochastic term proved in Lemma A.5 gives
the same conclusions (58), (59), under

(β∗) and P (β; λ) =

J ∗ =

P

J

≤

(A.19)

β∗); λ) + PΘ(̺β∗; λ)

−
δD2)(Xβ, Xβ∗) + PΘ(̺β; λ) + Kλ2J ∗,

ϑPH (̺(β
(2 ¯∆l0 −
for some δ > 0, ϑ > 0 and large enough K
the arguments in Remark 2 to show that (A.19) is implied by (1 + ϑ)PΘ(̺(β
≤
−
(2 ¯∆l0 −
∗c; λ). Furthermore, when
−
l0 is µ-strongly convex as in regression, one can take δ = µ and the regularity condition is
implied by

0. Assuming PΘ is subadditive, we can follow

δD2)(Xβ, Xβ∗) + Kλ2J ∗ + (1

ϑ)PΘ(̺(β

β∗)
J

β∗)
J

∗; λ)

−

≥

β

∀

(A.20)

(1 + ¯ϑ)PΘ(̺γ

J

∗; λ)

¯Kλ√J ∗k

Xγ

k2 + PΘ(̺γ

J

∗c; λ),

γ

∀

for some ¯ϑ (= 2ϑ
−
[PΘ(̺γ

(A.21)

ϑ ) > 0 and ¯K
∗; λ)]2

1

≤
J
P 2
Θ(̺γj; λ)

(A.22)

∗

Xj
∈J

≤
√2(2µ
1

−
ϑ

=

δ)K

˜Kλ2J ∗
(cid:0)

˜Kλ2

≤

−
Xγ
k
Xγ
k

2
2,
k
2
2,
k

≥
γ : PΘ(̺γ

(cid:1)
∀

γ : PΘ(̺γ

∀

0, or the constrained forms

∗c; λ)

∗c; λ)

J

J

≤

≤

(1 + ¯ϑ)PΘ(̺γ

(1 + ¯ϑ)PΘ(̺γ

∗; λ)

∗; λ)

J

J

for some ¯ϑ > 0 and ˜K
0. The conclusions and conditions can also be formulated in the
oracle inequality setup of Theorem 4. (A.20), (A.21) and (A.22) extend the comparison con-
dition (62), compatibility condition and RE condition to a more general penalty.

≥

A.13. Proof of Theorem 5. We prove the result under a more relaxed assumption: l, g

are merely directionally differentiable, and (64) is replaced by

LΘD2 + ε∆ψ)(β∗, β) + (∆ψ −

(̺2
(2 ¯∆l0 −
≤
for some δ > 0, ε > 0, ϑ > 0 and large K

∆∆ψ(
·

δD2)(Xβ, Xβ∗) + PΘ(̺β; λ) + Kλ2J(β∗),

β, α

∀

0.

≥

;α))(β∗, β) + ϑPH(̺(β

β∗); λ) + PΘ(̺β∗; λ)

−

Recall the objective function f (β) = l(β) + PΘ(̺β; λ) and the surrogate function

g(β; β−) = f (β) + ∆ψ(β, β−). From Lemma A.2 and Lemma A.3, we obtain

f (β) + ∆ψ(β, β(t))
f (β(t+1)) + ∆ψ(β(t+1), β(t)) + (∆∆ψ(
·
f (β(t+1)) + ∆ψ(β(t+1), β(t)) + (∆∆ψ(
·

≥

≥

Substituting β∗ for β yields

;β(t)) + ∆l + ∆
;β(t)) + ∆l −
̺2

PΘ(̺
·

))(β, β(t+1))
LΘD2)(β, β(t+1)).

∆ψ(β∗, β(t))
∆ψ(β(t+1), β(t)) + (∆∆ψ(
·
l(β∗) + PΘ(̺β(t+1); λ)
+ l(β(t+1))

≥

−
= ∆ψ(β(t+1), β(t)) + ∆∆ψ(
·
ǫ, Xβ(t+1)

2 ¯∆l(β∗, β(t+1))

− h

;β(t)) + ∆l)(β∗, β(t+1))

− LΘD2(̺β∗, ̺β(t+1))

PΘ(̺β∗; λ)

−

;β(t))(β∗, β(t+1))

− LΘD2(̺β∗, ̺β(t+1))

Xβ∗

i

−

+ PΘ(̺β(t+1); λ)

PΘ(̺β∗; λ).

−

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

37

From the above regularity condition,

ε∆ψ(β∗, β(t+1)) +
+ ϑPH(̺(β(t+1)

−

LΘD2(̺β∗, ̺β(t+1)) + δD2(Xβ∗, Xβ(t+1))
β∗); λ) + PΘ(̺β∗

; λ) + (∆ψ −

∆∆ψ(
·

;β(t)))(β∗, β(t+1))

2 ¯∆l(β∗, β(t+1)) + PΘ(̺β(t+1); λ) + Kλ2J(β∗),

≤

and so we obtain

(A.23)

(ε + 1)∆ψ(β∗, β(t+1)) + ∆ψ(β(t+1), β(t))
+ δD2(Xβ∗, Xβ(t+1)) + ϑPH (̺(β(t+1)
∆ψ(β∗, β(t)) +

Xβ∗

ǫ, Xβ(t+1)
h

−
+ Kλ2J ∗.

β∗); λ)

According to Lemma A.5, as long as A

≤

−
A0/√2, choosing λ = √2abAλo, b =
ϑ) guarantees that the probability of the following inequality occurring

≥

i

1/(2ϑ), a = 1/(δ
for all t,

∧

ǫ, Xβ(t+1)
h

−
is no greater than Cp−

Xβ∗

δD2(Xβ∗, Xβ(t+1))

ϑPH(̺(β(t+1)

−

i −

cA2. Together with (A.23), with probability 1

β∗); λ)

−
Cp−

cA2

0,

≥

(A.24)

∆ψ(β∗, β(t+1))

1
ε + 1

≤

[∆ψ(β∗, β(t))

−

−
∆(β(t+1), β(t)) + Kλ2J ∗]

for all t. The desired inequality can be shown by iteratively applying (A.24) for t = 0, 1, 2, . . ..

A.14. Proofs of Theorem 6 and Corollary 2. First we prove Theorem 6. Note that
(67b), (69b) have additional terms involving µ0. The ﬁrst result for µ0 = 0 can be shown
based on a GBF translation of the proof of Proposition 1 in [57]. For convenience, let ht(β)
∆ψ0(β, γ(t)) + µ0∆φ(β, γ(t)). Applying Lemma A.2 to
= f (β)
−
(67b) yields (∆f −
ht(α(t+1))

∆ ¯ψ0(β, γ(t)) = f (β)
∆∆ ¯ψ0 (
θtρt∆φ(α(t+1), α(t))

,γ(t)) + θtρt∆∆φ(
·

ht(β) + θtρt∆φ(β, α(t))

,α(t)))(β, α(t+1))

β, or

≤

−

−

·

−

∀

(A.25)

≤

ht(α(t+1))
θtρt∆φ(β, α(t))

−

ht(β) + θtρt∆φ(α(t+1), α(t))

By deﬁnition, Cht(α(t+1), β(t), θt) = θtht(α(t+1)) + (1
it to (A.25) multiplied by θt gives

ht(β(t+1))

(1

−

−

θt)ht(β(t))

θtht(β)

−

(θtρt∆∆φ(
·

−

,α(t)) + ∆

,γ (t)))(β, α(t+1)).

∆ ¯ψ0 (
f (
)
·
·
−
θt)ht(β(t))

−

ht(β(t+1)); adding

−

+ θ2

+ θ2

t ρt∆φ(α(t+1), α(t)) + Cht(α(t+1), β(t), θt)
t ρt∆∆φ(
,α(t))
·
t ρt∆φ(β, α(t))
θ2

)(β, α(t+1))
φ(
·
−
t ρt∆φ + θt∆
(θ2

f (
·

)
−

∆ ¯ψ0 (
·

−

,γ (t)))(β, α(t+1)),

f (β(t+1))

f (β)

−

(∆

+ θt{
∆ ¯ψ0 (
f (
·
·
t ρt(∆φ(β, α(t))
θ2

)
−

−

(1

θt)[f (β(t))
−
,γ (t)) + θtρt∆∆φ(
·
∆φ(β, α(t+1))),

−

≤

−

f (β)] + θt∆ ¯ψ0(β, γ(t))
))(β, α(t+1))
}

φ(
·

+ Rt

,α(t))

t

∀

≥

−
0

≤

and so

(A.26)

38

Y. SHE ET AL.

where Rt is given by θ2
C

,γ (t))(α(t+1), β(t), θt).

f (
·
Under µ0 = 0, (A.26) implies that

∆ ¯ψ0 (
·

)
−

t ρt∆φ(α(t+1), α(t))

∆ ¯ψ0(β(t+1), γ(t)) + (1

θt)∆ ¯ψ0(β(t), γ(t)) +

−

−

(A.27)

≤

f (β(t+1))

1
θ2
t ρt
(cid:2)
∆φ(β, α(t))

−

−

f (β)

θt
1
−
θ2
t ρt
−
(cid:3)
∆φ(β, α(t+1)).
θt)/θ2

f (β(t))

f (β)

−

+ Et(β)
θtρt

+

Rt
θ2
t ρt

(cid:2)

(cid:3)

Since in this case (69b) gives (1
conclusion

−

t ρt = 1/θ2
t
−

1ρt
−

1 for any t

≥

1, we obtain the ﬁrst

1
θ2
T ρT

f (β(T +1))

f (β)

−

(cid:2)
∆φ(β, α(0))

(cid:3)

∆φ(β, α(T +1)).

≤

−

1
θ0
−
θ2
0ρ0

−

f (β(0))

−

(cid:2)

T

f (β)

+

t=0 (cid:16)
X

(cid:3)

Et(β)
θtρt

+

Rt
θ2
t ρt

(cid:17)

On the other hand, given µ0 ≥

0, (A.26) can be written as

−

−

(1

f (β(t+1))
f (β)
+ Rt + θt∆ ¯ψ0(β, γ(t)) + θt∆
+ θt(µ0∆∆φ(
·

,γ (t))

−

θt)[f (β(t))

f (β)]

−
∆ψ0 (
·

,γ (t))(β, α(t+1))

))(β, α(t+1))

f (
)
·
−
) + θtρt∆∆φ(
·
µ0
θt

φ(
·
−
θ2
t (ρt +

,α(t))

φ(
·
)∆φ(β, α(t+1)).

−

(A.28)

t ρt∆φ(β, α(t))
θ2

≤

−

Therefore, we have

f (β(t+1))

f (β) + θ2

t (ρt +

−
θt)[f (β(t))

f (β)] + θ2

−

µ0
θt
t ρt∆φ(β, α(t)),

)∆φ(β, α(t+1)) + θtEt(β) + Rt
t

0

∀

≥

(1

≤
and from (69b),

−

f (β(t+1))

−

f (β) + θ2

t (ρt +

(1

≤

−

θt)[f (β(t))

f (β) + θ2
t
−

−

−

µ0
θt

)∆φ(β, α(t+1)) + θtEt(β) + Rt
1(ρt
t

)∆φ(β, α(t))],

1 +

1

∀

≥

µ0
θt

−

1

The second conclusion can be obtained by a recursive argument and RT + θT ET (β) + (1
T
θT )(RT
s=t+1(1
(1
1ET
· · ·
θs))(Rt + θtEt(β)).

θ1)(R0 + θ0E0(β)) =

1(β)) +

1 + θT

T
t=0(

θT )

· · ·

(1

−

−

−

−

−

−
−

P
’, (71) still holds when ∆φ ≥

Q

REMARK A.2. With the ‘=’ in (69b) replaced by ‘

0 (or
φ is convex), and (70) still holds if we set β to be a minimizer of f . But the equality form of
(69b) makes our conclusions applicable to say the noise-free statistical truth β = β∗, which
may not be a minimizer of the sample-based objective. The same comment applies to Theorem
7.

≤

Also, it is trivial to see that the conclusions extend to a varying sequence of µt. (Concretely,
the µ0 in (67b), (68), and
1, µT ,
respectively.) One can add backtracking for µt in the algorithm to further reduce its iteration
complexity.

Et(β) becomes µt, and the µ0 in (69b), (71) becomes µt

−

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

39

Finally, we prove Corollary 2’ which implies Corollary 2 and applies to any convex l0 in
(A.29) below. The proof is based on an accumulative Rt bound that can be derived in a more
general setup; see (A.32) in Remark A.3.

Here, the optimization problem of interest in “variable screening” is

(A.29)

β
k
to estimate the target β∗ satisfying the strict inequality
φ(β) for some
ψ0(β) = l(β)

min l(β) = l0(Xβ) s.t.

k0 ≤
β∗
k

k0 < q. Take µ0 = 0, φ =
p, we extend the notion of restricted isometry numbers ρ+, ρ

L ≥

0.

Given l0, X, and s

− L

q,

2
2/2,
k · k

[12]:

≤

(A.30)

ρ+(s)D2(β, γ),
∀
(The dependence on X and l0 is dropped for the sake of brevity.)

∆l0(Xβ, Xγ)

(s)D2(β, γ)

≤

≤

ρ

−

β, γ :

β
k

−

γ

k0 ≤

−
s.

COROLLARY 2’. Let l0 be any convex function and q be any nonnegative integer no more
ρ+(2q)/√r, there must exist a universal

than p. As long as r = q/
β∗
k
t, e.g.,
ρt = ρ0,

k0 > 1, for any

L ≥

∀

ρ0 = (1

1/√r)ρ+(2q),

−

p

θ2
and thus θt+1 = (
t )/2, so that the accelerated iterative quantile-thresholding
according to (67a)–(67c) satisﬁes

t + 4θ2
θ4

t −

l(β(T +1))
θ2
T

l(β∗)

−

+ T

·

avg
T
t
≤

≤

0

∆ψ0(β∗, γ(t))
θt

+ L

θT )

(1

−
θ3
T

D2(β(T +1), β(T ))

or l(β(T +1))

l(β∗) + min0

≤

t
≤

−

T ∆ψ0(β∗, γ(t))

≤

ρ0D2(β∗, α(0)), for all T

≥
≤
A/T 2 with A independent of T .

0,

Seen from the error measure, r should be appropriately large (but cannot be too large from

the perspective of statistical accuracy.)

To prove the corollary, we ﬁrst introduce a useful result [50, Lemma 9].

p, ˆβ = Θ#(y; q) is a globally optimal solution to minβ

Rp l(β) =
∈

(β), ˆ
J

=

J

(ˆβ) and assume J(ˆβ) = q. Then, for any

LEMMA A.6. Given q

β
y
k
β with J(β)

2
2/2 s.t.
k

−

≤
q. Let
β
k0 ≤
k
s = q/r and r

≤

=

J

J
1,
≥
l(ˆβ)

−

l(β)

where

(
J

L

, ˆ
J

) = (

ˆ
ˆ
/
J \J |
|
J |

)1/2

≤

|J \

1
≥ {

D2(ˆβ, β),
)
(
}
J
1/2.
(s/q)1/2 = r−

, ˆ
J

− L

α(t+1)
k

Set β = β∗ in the previous proof, and apply, instead of Lemma A.2, Lemma A.6 (where
k0 = q due to the no-tie-occurring assumption) to (67b). (A.25) is then replaced by
l(α(t+1))

l(β∗) + ∆ψ0(β∗, γ(t)) + θtρtD2(α(t+1), α(t))

∆ψ0(α(t+1), γ(t))

−

−

θtρtD2(β∗, α(t))

≤

(θtρt +

)(1

L

−

−

Accordingly, (A.26) becomes

1
√r

)D2(β∗, α(t+1)).

l(β(t+1))

l(β∗)

(1

−

−

−

θt)(l(β(t))

−

+ θt

∆ψ0(β∗, γ(t)) +

(1

1
√r

)

L

−
−
D2(β∗, α(t+1))),

(cid:2)

n

t ρt(D2(β∗, α(t))
θ2

≤

−

l(β∗)) + Rt

θtρt
√r

(cid:3)

D2(β∗, α(t+1))

o

40

Y. SHE ET AL.

where Rt is the same as before. Based on Lemma 1, Lemma 2, Lemma 5 (together with some
results in its proof), (A.30), and the following facts

β(t)

−
β(t+1)

γ(t) = θt(β(t)

α(t)) = θt(1

−

γ(t) = θt(α(t+1)

α(t)),

∀

−

−

−
t

≥

θt

−
0

1)(β(t
−

1)

α(t)),

1

t

∀

≥

−

we obtain for all t

1,
≥
t ρtD2(α(t+1), α(t))
θ2

Rt ≥

∆ψ0(β(t+1), γ(t)) + (1

θt)∆ψ0(β(t), γ(t))

−

−
θt)D2(α(t+1), β(t))

−
)D2(α(t+1), α(t))

θt)D2(α(t+1), β(t))

−
)D2(α(t+1), α(t))

+

θt(1

L
t (ρt +

= θ2

+

L{
t (ρt +

= θ2

L
θt(1

L
θt(1

+

L{
θ2
t (ρt +

θt)D2(α(t+1), β(t))

−
)D2(α(t+1), α(t))

−

−

(1

∆l0(Xβ(t+1), Xγ(t)) + (1
θt)D2(β(t), γ(t))
}
∆l0(Xβ(t+1), Xγ(t)) + (1
θt)θ2
t (1
−
t ρ+(2q)D2(α(t+1), α(t))
θ2

(1

θt

−

−

−

−

−

−

−

−

θt)∆l(β(t), γ(t))

θt)∆l(β(t), γ(t))

1)2D2(α(t), β(t
−

1))
}

and Rt ≥

≥

+

L
θt(1

−

L
ρt +

θ2
t {
Rt
θ2
t ρt ≥

T

t=0
X

θt(1

D2(α(t+1), β(t))
θt)
{
−
D2(α(t+1), α(t)) as t = 0. It follows that
ρ+(2q)
}
ρ+(2q)

1)2D2(α(t), β(t
−

ρt +

θt

−

−

L −
T

D2(α(t+1), α(t)) +

1
θT
−
θT ρT

L

L −
ρt

1))
,
}

D2(α(T +1), β(T ))

t=0
X

L

+

T

T

1

−

t=0 n
X
ρt +

=

t=0
X

1
θt
−
θtρt −

(1

−

θt)2(1
ρt+1

−

θt+1)

D2(α(t+1), β(t))

o

L −
ρt

ρ+(2q)

D2(α(t+1), α(t)) + L

θT )

(1
−
θ3
T ρT

D2(β(T +1), β(T ))

T

1

−

θt
1
−
θtρt

+

L

1

(1

−

−

θt)θt+1

D2(α(t+1), β(t)).

θt+1
θt

o

t=0
X
Therefore, choosing a universal ρt = ρ0 ≥

n

ρ+(2q)

− L

(which implies θt ↓

) ensures

T
t=0 Rt/(θ2
Moreover,

0.
t ρt)
≥
Et(β∗)
≥

1/√r)

P
or L(1
−
satisfying

∆ψ0(β∗, γ(t)) holds under

D2(β∗, α(t+1))
L
≥
ρt/√r. It is easy to see that as long as r > 1, there exist positive

1
√r )

θtρt
√r

(1

−

−

(cid:8)

(cid:9)

0
, ρ0

} ≥
L

(A.31)

(√r
L
ρ0 ≥
(

1)
−
≥
ρ+(2q)

ρ0

.

− L

Furthermore, for any
rest of the proof proceeds as before.

L ≥

ρ+(2q)/√r, we can always choose ρ0 = (1

1/√r)ρ+(2q). The

−

REMARK A.3. The idea of controlling the overall

can be extended with a
proper choice of ψ0 to a general problem min f (β) that may be nonconvex. In fact, if f (β)
0 for some
can be decomposed as l(β) + P (β) with 0

LD2 and ∆P +

t
≤

P

T

Rt
θ2
t ρt

∆l ≤

≤

L0D2 ≥

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

41

ﬁnite
arguments, we obtain

L0 ≥

0, then setting µ0 = 0, ψ0 = l

2
2/2 with
− Lk · k

L ≥ L0 and repeating the previous

T

Rt
θ2
t ρt ≥

T

t=0
X

t=0
X
1
θ2
t ρt

T

1

−

t=0
X
1
T
−

(A.32)

+

+

1
ρt

(ρt∆φ +

D2 −

L

LD2)(α(t+1), α(t))

∆l(β(t+1), γ(t+1)) +

L − L0)(1
(
θ3
T ρT

θT )

−

D2(β(T +1), β(T ))

1
θt
−
θtρt

L

L − L0
L

(1

−

−

θt)θt+1

θt+1
θt

o

D2(α(t+1), β(t)).

n

t=0
X
With ρt = ρ0, it can be shown that (1
at t = 3 and so
further.) Therefore, under ∆φ ≥
so that for any β,

L ≥

1.2

θt)θt+1

θt+1
θt

−

achieves the maximum value 0.1612

L0 makes the last term nonnegative. (A varying ρt may reduce
1.2

σD2, we can choose any ρ0 ≥

)/σ and

L ≥

− L

(L

L
L0

f (β(T +1))
θ2
T

−

f (β)

+ T

·

avg
T
t
≤

≤

0

Et(β)
θt

+

L − L0)(1
(
θ3
T

θT )

−

D2(β(T +1), β(T ))

Hence for some A independent of T , f (β(T +1))
∆ψ0(β, γ(t))+ ∆
f (β(T +1))
φ is differentiable.

f (β)+ min0

T {

t
≤

−

≤

−
)
f (
−
·

ρ0D2(β, α(0)),

≤
f (β) + min0
∆ψ0 (
·

t
≤
≤
,γ (t))(β, α(t+1))

∀
T Et(β)
} ≤

T

0.

≥

A/T 2, or
≤
A/T 2 when

A.15. Proof of Theorem 7. The construction of the new acceleration scheme and the
proof are motivated by Proposition 2 of [57], with the use of GBF calculus. First, from
Lemma A.2, given any β′t,
f (β(t+1))
(ρtD2 −

∆ ¯ψ0)(β(t+1), γ(t)) + ∆f (β′t, β(t+1))

−
∆ ¯ψ0)(β′t, γ(t))

f (β′t) + (ρtD2 −

,γ(t)))(β′t, β(t+1))

(ρtD2 −

∆∆ ¯ψ0 (

(A.33)

≤

·

−
θt)β(t) with θt to be determined. Deﬁne ht(
)
) = f (
·
·

∆ ¯ψ0(
, γ(t)). By
·

−

Let β′t = θtβ + (1
−
the deﬁnition of C,

f (β′t) = θt∆ ¯ψ0(β, γ(t)) + (1

−

θtf (β)

(1

−

−

−

−

θt)∆ ¯ψ0(β(t), γ(t))
θt)f (β(t)) + Cht(β, β(t), θt).

−

∆ ¯ψ0(β′t, γ(t))

Plugging the last equality into (A.33) yields

−

−

−

(1

f (β))

f (β(t+1))
θt)(f (β(t))
f (β)
−
∆ ¯ψ0)(β(t+1), γ(t)) + Cht(β, β(t), θt)
+ (ρtD2 −
+ θt∆ ¯ψ0(β, γ(t)) + (1
−
(ρtD2 −
∆ ¯ψ0)(β′t, γ(t))
+ ∆ ¯ψ0(β′t, γ(t))
= ρt[D2(β′t, γ(t))

θt)∆ ¯ψ0(β(t), γ(t))
(ρtD2 −
∆∆ ¯ψ0 (
∆f (β′t, β(t+1))
−
D2(β′t, β(t+1))]

−

≤

·

−

∆ht(β′t, β(t+1))

−

,γ(t)))(β′t, β(t+1))

42

Y. SHE ET AL.

and based on the deﬁnition of ¯ψ0 and Rt,

f (β) + Rt

f (β(t+1))
−
+ ρtD2(β′t, β(t+1)) + C
+ θt∆ ¯ψ0(β, γ(t)) + C
θt)(f (β(t))

(1

≤

−
From Section A.5, C
and so

−

µ0D2(
·

,γ (t))(β, β(t), θt) + µ0D2(β′t, β(t+1))

µ0D2(
·

)
−

f (
·

,γ (t))(β, β(t), θt) + ∆

∆ψ0 (
·
f (β)) + ρtD2(β′t, γ(t)).
,γ (t))(β, β(t), θt) = µ0C2(β, β(t), θt) = µ0θt(1

∆ψ0 (
·

f (
·

)
−

,γ (t))(β′t, β(t+1))

θt)D2(β, β(t))

−

(A.34)

f (β(t+1))

f (β) + Rt + θtEt(β)

−

+ (ρt + µ0)D2(β′t, β(t+1)) + µ0θt(1

θt)D2(β, β(t))

−

(1

≤

−

θt)(f (β(t))

−

f (β)) + ρtD2(β′t, γ(t)).

We would like to write (ρt + µ0)D2(θtβ + (1

θt)D2(β, β(t))
into the form of a multiple of D2(β, ν(t+1)) for some ν(t+1). This can be done by solving
the gradient equation with respect to β:

θt)β(t), β(t+1)) + µ0θt(1

−

−

(A.35)

ν(t+1) =

On the other hand,

ρtD2(θtβ + (1

∇

−

(A.36)

ν(t) =

(ρt + µ0)θtβ(t+1)
ρtθ2

ρtθt(1

θt)β(t)

.

−

−
t + µ0θt
θt)β(t), γ(t)) = 0 gives
γ(t)
θt −

β(t).

−
θt

θt

1

Combining (A.35) and (A.36) results in

(A.37)

ρt
−
ρt
as in (76a). Therefore, (A.34) becomes

γ(t) = β(t) +

1θt(1
1θt

1)

θt
−
−
1 + µ0

(β(t)

−

β(t

−

1)),

−

−

(A.38)

f (β(t+1))

f (β) + (θ2

−
θt)(f (β(t))

(1

t ρt + µ0θt)D2(β, ν(t+1)) + Rt + θtEt(β)

f (β)) + θ2

t ρtD2(β, ν(t)).

−
Let µ0 = 0. It follows from (A.38) that

≤

−

1
θ2
t ρt
(1

f (β(t+1))

f (β)

−

+ D2(β, ν(t+1)) + Et(β)
θtρt

+

Rt
θ2
t ρt

(A.39)

≤

Under (77b), we have

(cid:2)
θt)

−
θ2
t ρt

f (β(t))

−

(cid:2)

(cid:3)
f (β)

(cid:3)

+ D2(β, ν(t)),

0.

t

∀

≥

(A.40)

≤

1
θ2
t ρt
(cid:2)
1
1ρt
−

θ2
t
−

f (β(t+1))

f (β)

−

+ D2(β, ν(t+1)) + Et(β)
θtρt

+

Rt
θ2
t ρt

f (β(t))

−

1

(cid:2)

(cid:3)
f (β)

(cid:3)

+ D2(β, ν(t)),

1.

t

∀

≥

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

43

Summing (A.40) for t = T, . . . , 1 and (A.39) for t = 0 gives

1
θ2
T ρT

[f (β(T +1))

−

T

f (β)] +

Et(β)
θtρt

+

Rt
θ2
t ρt

θ0
1
−
θ2
0ρ0

≤

(cid:2)

f (β(0))

f (β)

−

(cid:3)

t=0 (cid:16)
X
+ D2(β, ν(0))

(cid:17)
D2(β, ν(T +1)),

−

and so the ﬁrst bound noticing that ν(0) = γ(0) = β(0).

Moreover, given any µ0 ≥
f (β(t+1))

(A.41)

0, from (77b), (A.38) implies for any t

1,

≥

−
θt)[f (β(t))

f (β) + (θ2

t ρt + µ0θt)D2(β, ν(t+1)) + Rt + θtEt(β)

f (β) + (θ2
t
−

1ρt

−

−

1 + µ0θt

−

1)D2(β, ν(t))].

(1

≤

−

Similar to the proof of Theorem 6, a recursive argument using (A.41) and (A.38) gives the
second bound.

REMARK A.4. Compared with the proof of Theorem 6, the proof here needs to perform
a ﬁner analysis of Cht (the proof of Corollary 2’ uses a similar treatment). Otherwise one
would get γ(t) = β(t) + θt(θ−
β(t
1 + µ0) in
t
−
place of (76a), (77b), respectively. Following the same proof, we can show that the resultant
algorithm does result in a linear rate when µ0 = µ > 0, but offers no acceleration (θ0 = 1/κ)
in strongly smooth and convex optimization.

θt) = θ2
t
−

1)) and ρtθ2

1)(β(t)

1
1 −

t /(1

1(ρt

−

−

−

−

Finally, Remark A.2 still applies. For example, the second conclusion holds when the ‘=’
’, and it is straightforward to see that µ0 can be similarly replaced

in (77b) is replaced by ‘
by a sequence of varying µt to speed the convergence.

≤

A.16. Statistical accuracy of LLA iterates.

P (̺β), l(β) = l0(Xβ), P (̺β) =
P ′+(0) < +
(0, +

t)
−
LLA = ∆

). Recall ∆(t)

, P (t) = P (

∞

≥
α(t) ◦(
·

k

∞

P

In this subsection, assume f (β) = l(β) +
j P (̺βj) (by a slight abuse of notation), P (0) = 0,
0, P (t) is differentiable for any t > 0, and P is concave on

)
k1−

) which does not satisfy the strong idempotence.

P (
·

ASSUMPTION
A
that the following inequality holds

(ε, δ, ϑ, K, α, β) Given X, α, β, there exist ε > 0, δ > 0, ϑ > 0, K

0 such

≥

(1 + ε)∆
α◦(
·
2 ¯∆l(β∗, β) + P (̺β; λ)

)
k1−

P (
·

k

≤

P (̺β∗; λ) + Kλ2J ∗.

−

)(̺β∗, ̺β) + δD2(Xβ∗, Xβ) + ϑPH(̺(β

β∗); λ)

−

PROPOSITION 5. Assume that for any given T

1,

(ε, δ, ϑ, K, α(t), β(t)) (1

T )
t
≤
ϑ)ϑ. Then the

≤

is satisﬁed for some ε > 0, δ > 0, ϑ > 0, K
following inequality holds with probability at least 1

≥

∆(T )

LLA(̺β∗, ̺β(T ))

≤

κT ∆(0)

≥
A
0. Let λ = Aσ
Cp−

cA2
p
−
LLA(̺β∗, ̺β(0)) +

log(ep)/

(δ

∧

p
Kλ2J ∗,

κ

−

κ

1

where κ = 1/(1 + ε) and C, c are universal positive constants.

PROOF. From the proof of Proposition 4, for any β,

∆f (β, β(t+1)) + ∆∆(t)

LLA(
·
f (β(t+1)) + ∆(t)
LLA(̺β, ̺β(t))

,̺β(t))(̺β, ̺β(t+1))
∆(t)

f (β)

≤

−

−

LLA(̺β(t+1), ̺β(t)).

44

Y. SHE ET AL.

Using the deﬁnition of ∆(t)

LLA, we have
∆P (̺β, ̺β(t+1))

−
+ ∆l(β, β(t+1)) +

(A.42)

∆∆

P (·,̺β(t) )

(̺β, ̺β(t+1))

α(t)
j

∆∆1(

·

j )(̺βj , ̺β(t+1)

j

,̺β(t)

)

Xj

f (β)

f (β(t+1)) + ∆(t)

LLA(̺β, ̺β(t)),

≤
−
where we used ∆(t)
LLA(̺β(t+1), ̺β(t))

) is concave on (0, +
0 since P (
·

∞

).

≥

) which is differentiable on (0, +
LEMMA A.7. For any P (
·
, we have ∆∆P (
·

0, P (0) = 0 and P ′+(0) < +

∞
,α)(β, γ) = ∆P (β, γ)

) and satisﬁes P (t) =
P ′+(0)∆1(β,

t)

−

P (
γ)1α=0 for any α, β, γ

−

≥

,α)(β, γ) = ∆1(β, γ)1α

=0.

R. In particular, ∆∆1(
·

∞

∈

The result can be shown from the proof of Lemma 2. Indeed, from (29),

∆P (
, α) =
·

P (α)
P ′+(0)
= 0, by Lemma 1 and Lemma 2, ∆∆P (
·
,α)(β, γ) = ∆P (β, γ)

P ′(α)(
−
,
| · |
,α)(β, γ) = ∆P (β, γ)

)
P (
·
)
P (
·

α), α

−
−

· −

(

When α
= ∆P (β, γ). When α = 0, ∆∆P (
·
two cases gives

−

= 0
α = 0.

∆

−

P (α)+P ′(α)(

α)(β, γ)

P ′+(0)∆1(β, γ). Combining the

·−

When P (β) =

β
k

∆∆P (
·
k1, ∆∆1(

·

,α)(β, γ) = ∆P (β, γ)
−
,α)(β, γ) = ∆1(β, γ)

P ′+(0)∆1(β, γ)1α=0.
∆1(β, γ)1α=0 = ∆1(β, γ)1α

=0.

−

From Lemma A.7,

∆P (̺β, ̺β(t+1))

−

∆∆

P (·,̺β(t) )

(̺β, ̺β(t+1)) =

P ′+(0)∆1(̺βj, ̺β(t+1)

j

)

Xj:β(t)

j =0

and

α(t)
j

∆∆1(

j )(̺βj, ̺β(t+1)

j

,̺β(t)

) =

α(t)
j

∆1(̺βj, ̺β(t+1)

j

).

j =0 P ′+(0)∆1(̺βj, ̺β(t+1)

j

) +

f (β(t+1)) + ∆(t)

P

j:β(t)
LLA(̺β, ̺β(t)).

Xj

Xj:β(t)
Plugging these into (A.42) gives ∆l(β, β(t+1)) +

j

=0

·

∆1(̺βj, ̺β(t+1)

=0 α(t)
j:β(t)
j
j
Together with α(t)

f (β)

−

j

)
≤
P ′+(β(t)
j )
|

| ≤

j =

P

(A.43)

∆l(β, β(t+1)) +

α(t)
j

∆1(̺βj, ̺β(t+1)

j

)

P ′+(0), we have

Xj

Letting β = β∗ and using the deﬁnition of ǫ, we obtain
2 ¯∆l(β∗, β(t+1)) + ∆

∆(t)

LLA(̺β∗, ̺β(t)) +

≤
From the regularity condition,

f (β)

−

≤

f (β(t+1)) + ∆(t)

LLA(̺β, ̺β(t)).

)

α(t)◦(
k
·
ǫ, Xβ(t+1)
h

k1 (̺β∗, ̺β(t+1)) + P (̺β(t+1); λ)

Xβ∗

i

−

+ P (̺β∗; λ).

(1 + ε)∆(t+1)

LLA (̺β∗, ̺β(t+1)) + δD2(Xβ∗, Xβ(t+1)) + ϑPH (̺(β(t+1)

β∗); λ)

−

2 ¯∆l(β∗, β(t+1)) + ∆
k

α(t) ◦(
·

k1 (̺β∗, ̺β(t+1)) + P (̺β(t+1); λ)

)

≤

−

P (̺β∗; λ) + Kλ2J ∗

6
6
6
6
6
6
ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

45

t

for 1
then applying a similar probabilistic argument as in Theorem 5.

T . The ﬁnal conclusion can be proved by combining the last two inequalities and

≤

≤

A.17. A-estimators as F-estimators.

In this part, we show that an important class of
A-estimators that has alternative optimality, typically arising from block coordinate descent
(BCD) algorithms, can often be converted to F-estimators, and analyzed in a similar way. Let
β = [βT
k] to denote the
subvector after removing the kth block. Assume

[K]]T where β[k] is the kth block, 1

K, and we use β[

[1], . . . , βT

≤

≤

k

−

f = l + P

where l is differentiable, and P is separable: P (β) =
of β[k] only, f is denoted by f (β[k]; β[
estimator if

−

Pk(β[k]). When viewed as a function
k]). We say ˆβ has alternative optimality or is an A-

P

(A.44)

ˆβ[k] ∈

arg min
β[k]

f (β[k]; ˆβ[

k]), 1

k

≤

≤

K.

−

LEMMA A.8. Let ˆβ be an A-estimator of min f (β). Construct a surrogate function:

(A.45)

gρ(β; β−) = f (β)

∆l(β, β−) +

ρkD2(β[k], β−[k])

−

where ρ = (ρ1, . . . , ρK ) with ρk ≥

(i) If Pk are directionally differentiable and

0.

X

(A.46)

for some

(A.47)

Lk ≥

∆Pk +

LkD2 ≥
0
0, then for any ρk ≥ Lk, ˆβ must satisfy
gρ(β; β−)
∈

arg min

ˆβ

β

(ii) If l as a function of βk satisﬁes ∆
l(
·
tively,

;β[−k]) ≤

(A.48)

∆ˆlk(

·

)(β[k], ˆβ[k])

where ˆlk(β[k]) denotes lk(β[k]; ˆβ[
if ˆβ[k] is the unique solution to (A.44), then ρk ≥

−

|β−= ˆβ.
β[

∀

LkD2,

Lk sufﬁces.

k], 1

k

≤

≤

K, or less restric-

−

LkD2(β[k], ˆβ[k]),

β[k]

≤

∀
k]), then for any ρk > Lk, (A.47) still holds. In addition,

Overall, (A.47) provides a useful joint optimization form that can be used as the so-called
“basic inequality” in empirical process theory, and so with the lemma, A-estimators can an-
alyzed like F-estimators. Moreover, the quality of the initial point can be incorporated in the
analysis; see [54].

PROOF. (i) The condition (A.46) means that gρ is convex in, β[k]. By Lemma 4 and
K, we immediately know that ˆβ

k

Lemma 1, and the fact that gρ is separable in β[k], 1
is necessarily a solution to minβ gρ(β; ˆβ).

≤

≤

(ii) We use a shorthand notation ˆgk(β[k]) to denote gρ(β; ˆβ) as a function of β[k] when
arg min ˆgk(β[k]). It sufﬁces to show ˜β[k] = ˆβ[k]. Because of the
k] = ˆβ[
β[
separability of gρ,

k]. Let ˜β[k] ∈

−

−

f (˜β[k]; ˆβ[

k]) + (ρk −

≤
Lk)D2(ˆβ[k], ˜β[k]) = 0. The conclusion follows.

−

Lk)D2(ˆβ[k], ˜β[k])

f (ˆβ[k]; ˆβ[

k])

−

and so (ρk −

46

Y. SHE ET AL.

Some conclusions like (i) can be extended to functions deﬁned on Riemannian manifolds.
It is also worth mentioning that in the regression setup, which is of primary interest in many
statistical applications, we can use some surrogates with ρk = 1, regardless of the design or
penalty, to convert alternative optimality to joint optimality. The following lemma exempliﬁes
the point in matrix regression, and is condition free.

LEMMA A.9. Let l0(A; Y ) =
(i) Let A =

Y
k

X kBk with B = (B1, . . . , BK), where the dependence of B (and X k) is

A

2
F /2, and A be deﬁned differently as follows.
k

−

dropped for simplicity. Consider the problem

P

(A.49)

min
B1,...,BK

l0(A; Y ) +

Pk(Bk) s.t. A =

X kBk.

Then the set of A-estimators of (A.49) is exactly the set of F-estimators associated with the
following surrogate

X

X

(A.50)

g(B, B−) = l0(A; Y )

−

Dl0(A, A−) +

Pk(Bk) +

D2(X kBk, X kB−k ).

(ii) Let A = XB1 · · ·

X k) is dropped for simplicity. Consider

BK with B = (B1, . . . , BK), where the dependence of B (and

X

X

(A.51)

min
B1,...,BK

l0(A; Y ) +

Pk(Bk) s.t. A = XB1 · · ·

BK.

X
Redeﬁne l0 as a function ¯l0 of B and introduce a discrepancy measure d2 as follows

l0(A; Y ) = ¯l0(B; X, Y )

d2(B, B−) =

1
2

K

Xk=1

XB−1 · · ·
k

B−k
−

1(Bk −

B−k )B−k+1 · · ·

B−K k

2
F .

Then the set of A-estimators of (A.51) is exactly the set of F-estimators associated with the
surrogate

(A.52)

g(B, B−) = ¯l0(B)

−

∆¯l0(B, B−) +

Pk(Bk) + d2(B, B−)

=

A−

1
2 k
K

Y

2
F +
k

−

X
XB−1 · · ·
h

B−K −

Y ,

XB−1 · · ·

B−k
−

1(Bk −

B−k )B−k+1 · · ·

B−Ki

+

Pk(Bk)

X

XB−1 · · ·
k

B−k
−

1(Bk −

B−k )B−k+1 · · ·

B−Kk

2
F .

Xk=1
K
1
2

Xk=1

The lemma can be directly proved by the deﬁnition of GBF and matrix differentiation and
its proof is omitted. For the application of the ﬁrst result (i), see [50] for example. The second
result can be used to study bilinear problems or NMF like matrix decomposition problems.
One could show a statistical accuracy result in terms of d1 (which satisﬁes d1

Kd2),

≤

d1(B, B−) =

1
2

K

Xk=1

(cid:13)
(cid:13)

XB−1 · · ·

B−k
−

1(Bk −

B−k )B−k+1 · · ·

B−K

2
F ,

(cid:13)
(cid:13)

under a proper regularity condition involving d2; see, for example, [53].

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

47

A.18. Statistical error analysis of a general optimal solution. This part demonstrates
that using the statistical notions and Bregman calculus developed earlier can perform statis-
tical analysis of a general optimization problem that may not be in the MLE setup:

(A.53)

f (β) s.t. β

min
β

∈ S

where f is directionally differentiable and
straints Aβ = α, sparsity constraints

Rp can be formulated by linear equality con-
s, nonnegativity constraints β
0, and so on.
Statistically, we would like to study how a target parameter can be recovered from solving
(A.53) in the present of data noise. Following (53), let β∗ be a statistical truth and deﬁne the
associated effective noise by ǫ =

f (β∗), assuming f is differentiable at β∗.

S ⊂
k0 ≤

β
k

≥

Although β∗ in the above deﬁnition can be any point, a meaningful recovery must be under

−∇

some conditions satisﬁed the associated ǫ. Consider the following three scenarios:

(a) Statistical estimation often assumes a zero mean noise:

(A.54)

Eǫ = 0,
which essentially means that the statistical truth makes the gradient of the expectation of f
(so as to remove data randomness) vanish—see Section 3.2. Yet (A.54) alone does not always
guarantee a unique β∗.

(b) Stronger conclusions can be obtained for the β∗ that satisﬁes the no-model-ambiguity
ǫ, β∗ is

D = dom(f ) with the gradient

assumption: f is differentiable at β∗
a ﬁnite optimal solution to the Fenchel conjugate as ζ =

f (β∗) =

∇

ǫ:

−

∈

(A.55)

f ∗(ζ) = sup
β h

ζ, β

i −

−
f (β),

ǫ. This assumption sim-
and the extended real-valued convex function f ∗ is differentiable at
ǫ) makes a so-called “conjugate pair”. Note that f need not be overall
ply means that (β∗,
strictly convex, especially when D is compact according to Danskin’s min-max theorem [6].
(c) Another popular assumption in statistical learning is strong convexity in a restricted
α
s):
k
µD2)(β1, β2)

sense (especially when β = Aα with

(A.56)

k0 ≤

−

−

0,

β1, β2 ∈ S

∀

≥

(∆f −

for some µ > 0. The condition may hold even when the number of unknowns is much larger
than the sample size [12, 36].

The following theorem uses the GBF calculus to argue how the statistical accuracy of the
obtained solutions is determined by the (tail decay of) effective noise. Probabilistic arguments
can follow to bound the stochastic terms more explicitly.

THEOREM A.1. Let ˆβ be an optimal solution to (A.53).
(i) Under the zero mean assumption (A.54) and β∗

satisﬁes a Fenchel-Young form bound

, the risk of ˆβ in terms of ∆f

∈ S

(A.57)

E[f ∗(ǫ) + f (β∗)].
(ii) Under the no-model-ambiguity assumption in (b) and β∗

E∆f (ˆβ, β∗)

≤

(A.58)

∆f (ˆβ, β∗)

∆f ∗(ǫ,

≤

ǫ).

−

(iii) An oracle inequality holds for any δ > 0 and any reference β

, we have

∈ S

(A.59)

where Γ(β) =

(A.60)

(∆f −
θ
θ :
k
{

δD2)(ˆβ, β∗)

∆f (β, β∗) +

≤
β for some ¯β

k2 ≤
D2(ˆβ, β∗)

1, θ = ¯β
−
1
∆f (β, β∗) +
µ

≤

:

∈ S

1
2δ

ǫ, θ

[ sup
θ

Γ(β)h
∈

]2,
i

∈ S}
1
2µ2 [ sup

θ

Γ(β)h
∈

ǫ, θ

]2.
i

. In particular, under (A.56),

48

Y. SHE ET AL.

The ﬁrst two bounds reveal the important role of the Fenchel conjugate of the loss, and can
be made more explicit under proper Orlicz norm conditions of ǫ; the third conclusion, on the
basis of the supremum of an empirical process [60], demonstrates how modern probabilistic
tools can be used to derive ﬁnite-sample error bounds of ˆβ in a general noisy setup.

PROOF. First, by deﬁnition, f (ˆβ)

f (β∗), from which it follows that ∆f (ˆβ, β∗)

≤

≤

ǫ, ˆβ
h

β∗

i

−

. Deﬁne

h(δ) = ∆f (δ + β∗, β∗).

By assumption, f is a proper function and applying Fenchel-Young’s inequality gives

∆f (ˆβ, β∗)

ǫ, δ

≤ h

i|δ= ˆβ

−

β∗

≤

1
c

∆f (δ + β∗, β∗)

|δ= ˆβ

β∗ +

−

1
c

h∗(cǫ)

or (1

1/c)∆f (ˆβ, β∗)

On the other hand,

−

h∗(cǫ)/c for any c > 0.

≤

h∗(ζ) = sup
δ h

ζ, δ

i −

f (β∗ + δ) + f (β∗) +

f (β∗), δ

i

h∇

= sup
δ h

ζ +

= sup
δ h

ζ +

∇

∇

f (β∗), δ

i −

f (β∗ + δ) + f (β∗)

f (β∗), β∗ + δ

i −

f (β∗ + δ) + f (β∗)

ζ +

− h

∇

f (β∗), η∗

i

= f ∗(ζ +

∇

f (β∗)) + f (β∗)

ζ +

− h

∇

f (β∗), β∗

,
i

where f (β∗),

f (β∗) are known to be ﬁnite. Therefore we obtain

∇
∆f (ˆβ, β∗)

1

(A.61)

1
−
−
Taking c = 2 and using Eǫ = 0 gives the ∆f risk bound (A.57).

−

−

≤

c

[f ∗((c

1)ǫ) + f (β∗)

(c

ǫ, β∗
1)
h

],
i

∀

c > 0.

Next, we prove the second bound under the no-model-ambiguity assumption. Using the

optimality of β∗, we have further

h∗(ζ) = f ∗(ζ +

f (β∗))

f ∗(

ǫ)

∇
Moreover, from the assumption and deﬁnition (A.55), it is easy to show that β∗
and so

− h

−

−

∂f ∗(

−

∈

ǫ),

ζ, β∗

.
i

−

∇

f ∗(

ǫ) = β∗, from which it follows that
h∗(ζ) = ∆f ∗ (ζ

ǫ).
(A.62)
Taking c = 2 gives (A.58) (even though Eǫ may not be 0).
, f (ˆβ)

Finally, for any β

f (β) and so

ǫ,

−

−

∈ S
∆f (ˆβ, β∗)
We obtain a general result

≤

≤

∆f (β, β∗) +

ǫ, (ˆβ
h

ˆβ
β)/
k

−

−

β

k2)

ik

ˆβ

β

k2.

−

(∆f −

δD2)(ˆβ, β∗)

≤

∆f (β, β∗) +

1
2δ

(A.63)

for any δ > 0.

ǫ, θ

[ sup
θ

Γ(β)h
∈

]2,
i

Based on the regularity condition,

δ
2

D2(ˆβ, β∗)

≤

∆f (ˆβ, β∗)

δ
2

−

D2(ˆβ, β∗)

≤

∆f (β, β∗) +

1
δ

for any δ

≤

µ. Taking δ = µ gives the desired result.

ǫ, θ

( sup
θ

Γ(β)h
∈

)2
i

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

49

Algorithm B.1 Accelerated Bregman of the second kind

Input β(0): initial value; ρmin > 0, α > 0, M ∈ N, µ0 ≥ 0 (e.g., ρmin = 1, α = 2, M = 3)

ρt ← ρmin/α, s ← 0
repeat

1: θ0 ∈ (0, 1], t ← 0, α(0) ← β(0);
2: while not converged do
3:
4:
5:
6:
7:

8:
9:

10:
11:

−
ψ0 (β, γ(t))+µ0∆

s ← s + 1
ρt ← αρt
if t ≥ 1, then θt ← (pr2 + 4r − r)/2 with r = (ρt
γ(t) ← (1 − θt)β(t) + θtα(t)
α(t+1) ← arg minβ{f (β) − ∆
β(t+1) ← (1 − θt)β(t) + θtα(t+1)
Rt ← θ2
φ(α(t+1), α(t)) − ∆ ¯ψ0
(β(t), γ(t)) + C
f (
·

t ρt∆
+(1 − θt)∆ ¯ψ0
until Rt ≥ 0 or s > M
if s > M , pick (α(t+1), β(t+1), γ(t), ρt, θt) with the largest Rt/(θ2
t ← t + 1

(β(t+1), γ(t))

∆ ¯ψ0

)
−

(
·

t ρt)

,γ(t))(α(t+1), β(t), θt)

1θt

−

1 + µ0)θt

−

1/ρt

φ(β, γ(t)) + θtρt∆

φ(β, α(t))}

12:
13:
14:
15: end while
16: return β(t+1).

APPENDIX B: ALGORITHMS FOR ACCELERATIONS

For clarity, we give an outline of the algorithms for acceleration.

Algorithm B.2 Accelerated Bregman of the ﬁrst kind

Input β(0): initial value; ρmin > 0, α > 0, M ∈ N, µ0 ≥ 0 (ρmin = 1, α = 2, M = 3)

ρt ← ρmin/α, s ← 0
repeat

1: θ0 ∈ (0, 1], t ← 0;
2: while not converged do
3:
4:
5:
6:
7:

s ← s + 1
ρt ← αρt
if t ≥ 1, then θt ← (pr2 + 4r − r)/2 with r = (ρt
γ(t) ← β(t) + {ρt
1θt(1 − θt
−
−
if t ≥ 1 and β(t) if t = 0
ψ0 (β, γ(t)) + µ0D2(β, γ(t)) + ρtD2(β, γ(t))}
β(t+1) ← arg minβ{f (β) − ∆
Rt ← (ρtD2 − ∆ ¯ψ0

1 + µ0)θt
1/ρt
−
−
1))
1 + µ0)}(β(t) − β(t
−

)(β(t+1), γ(t)) + (1 − θt)∆ ¯ψ0

(β(t), γ(t))

1)/(ρt

1θt

1θt

−

−

−

until Rt ≥ 0 or s > M
if s > M , pick (β(t+1), γ(t), ρt, θt) with the largest associated Rt/(θ2
t ← t + 1

t ρt)

8:

9:

10:
11:
12:
13:
14: end while
15: return β(t+1).

APPENDIX C: EXPERIMENTS

This section performs some simulation studies to support the theoretical results.

C.1. Computational error.
solve two nonconvex problems.

In this part, we use mirror descent and DC programming to

50

Y. SHE ET AL.

•

Nonconvex mirror descent for IS divergence minimization.

In infrared astronomi-
cal satellite (IRAS) image reconstruction [13] and audio signal processing [21, 35], the
Itakura-Saito (IS) divergence (or the negative cross Burg entropy), IS(a, b) =
i(ai/bi −
1), is popularly used to measure the discrepancy between the observed data
log(ai/bi)
and the reconstructed data. Given X
+, the problem can be deﬁned by
+, which is nonconvex in β. To maintain the nonnegativity
min f (β) := IS(y, Xβ) s.t. β
constraint in updating β automatically, we develop a mirror descent algorithm. Concretely,
deﬁne g(β; β−) = f (β) + (ρDϕ −
βj. Then,
minimizing g(β; β(t)) with respect to β gives rise to a multiplicative rule

∆f )(β, β−), where ϕ(β) =

j βj log βj −

+ , and y
×

Rn

Rn

Rp

P

−

∈

∈

∈

p

P

(C.1)

β(t+1)
j

= β(t)

j exp

1
ρ

−

h

(Xβ(t))i −
(Xβ(t))2
i

yi

Xij

.

i

Xi

O

≤
×

the
(1/T ) rate of convergence holds for the optimization error
From Theorem 2,
T (2ρ ¯Dϕ −
∆f )(β(t+1), β(t)). To verify this, we generated a design matrix of size
avg0
t
≤
1000 with all elements drawn from U (0, 1), and set y = Xβ∗ + e with β∗j chosen uni-
1000
(0, σ2) with σ2 = 10. We ﬁxed 1/ρ = 0.01. Figure
formly from the interval (0, 5) and ei ∼ N
) for
C.1 shows how the logarithm of optimization error converges to
50 different β(0) with β(0)
randomly chosen from U (0, 1). Observe that all the error curves
j
in the log-log plot are bounded by a line with slope

(since log 0 =

−∞

−∞

1.

−

FIG C.1. Log-log plot of optimization error v.s. number of iterations: mirror descent for IS divergence
minimization with 50 random starting points. All error curves are bounded above by the dashed line
which has slope

1.

−

•

DC programming for capped-ℓ1 SVM. High-dimensional classiﬁcation with concur-
rent feature selection can be achieved by minimizing a composite objective function. Given
yix⊤i β)+ be the hinge
X = [x1, . . . , xn]⊤ ∈
×
, λ2/2) be the capped-
loss [61] that is nondifferentiable, and P (β; λ) =
ℓ1 penalty [66] which is nonsmooth and nonconvex. [42] proposed an effective DC algo-
rithm for solving minβ f (β) := l(β) + P (β; λ) based on the decomposition P (β; λ) =
d1(β; λ)

n
i=1(1
p
j=1 min(λ
P

n, let l(β) =
1, 1
}

d2(β; λ) with

p and y

−
βj |
|

∈ {−

Rn

P

−

(C.2)

d1(β; λ) = λ

β
k

k1, d2(β; λ) =

p

Xj=1

max(λ

βj | −
|

λ2/2, 0).

ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

51

As stated in Example 4, we can recharacterize DC as a Bregman-surrogate algorithm
d2 (β, β(t))

arg min f (β) + ∆

β(t+1)

(C.3a)

∈

(C.3b)

arg min
β

∈

n

Xi=1

max(0, 1

−

p

yix⊤i β) + λ

βj | −

βj1

β(t)
j

|

|≥

.

λ/2

|
Xj=1
(cid:0)
n
i=1 ξi +λ

p

(cid:1)
p
j=1 βj1
|
[n], j

1

λ

∈

∈

−

yix⊤i β for i

(C.3b) is equivalent to a linear program: minξ,ζ,β
λ/2
[p],
s.t. ξi ≥
which can be efﬁciently solved by standard linear programming (LP) solvers. For the con-
vergence of the DC algorithm, a similar result can be shown for the optimization error
r∆d2 +∆d1)(β(t), β(t+1)), following the lines of the proof of Proposition
avg0
4.

j=1 ζj −
[p] and ξi, ζj ≥
P

ζj for j
P

T (∆l+

0 for i
P

βj ≤

ζj ≤

|≥
∈

[n],

β(t)

t
≤

−

R400

We generated X

800 with each row following
|, β∗ =
N
(0, σ2) with σ2 = 10. We ﬁxed λ =
[15, 10, 0, . . . , 0]⊤, and y = sgn(Xβ∗ + e) where ei ∼ N
1 and ran the DC algorithm for 50 different starting points with each component randomly
drawn from U (0, 1). The corresponding optimization error curves are plotted in Figure C.2,
where the

(1/T ) rate of convergence is impressive.

(0, Σ) and Σij = 0.5|

∈

∈

×

≤

−

j

i

j

O

l

)
e
a
c
s

g
o
l
(

r
o
r
r
e

l

a
n
o
i
t
a
t
u
p
m
o
C

9

8

7

6

5

4

3

2

1

0

1

2

3

4

5

6

7

Iteration (log scale)

FIG C.2. Log-log plot of optimization error v.s. number of iterations: DC programming for capped-ℓ1
SVM (50 different initial points).

C.2. Statistical error.

In this part, we consider two algorithms for sparse regression:
LLA and iterative thresholding. The nonconvex “hard” penalty deﬁned by (55) is applied,
Rn,
p and y
which is constructed from the hard-thresholding rule via (48). Given X
p
we study the following regularized problem minβ f (β) := l(β) +
j=1 PH(̺βj ; λ), where
X
k2. The loss functions under consideration are the ordinary
l is the loss function and ̺ =
k
quadratic loss and a nonconvex loss which is resistant to gross outliers:

Rn

P

∈

∈

×

2
(i) ℓ2 loss: l(β) =
2/2;
k
(ii) Tukey’s biweight loss: l(β) =

y
k

Xβ

−

c
−
and 0 otherwise, where c = 4.685σ with σ a robust estimate of the standard deviation of
errors [26].

ψ(t)dt and ψ(t) = t[1

| ≤

t
|

P

−

R

|
0

|

(t/c)2]2 if

n
i=1

x⊤

i β

yi

Rn

p has i.i.d. rows drawn from

In either case, we have a nonconvex optimization problem. In simulations, the design matrix
X
|, the response is given by
N (0, σ2) and σ2 = 10, and the regularization parameter λ is set to
y = Xβ∗ + e with ei ∼
Aσ

log(ep). We set n = 800, p = 1000, β∗ = [12, 8, 0, . . . , 0]⊤ and A = 2.

(0, Σ) with Σij = 0.15|

N

∈

×

−

j

i

p

 
 
 
52

Y. SHE ET AL.

First, we tested the statistical accuracy of LLA (cf. Theorem 3 and Proposition 5). We

generated 15 initial points β(0) with each element following U (
0.5, 1, 1.5
}
and 5 for each. Figure C.3 shows how the statistical error varies as the cycles progress, with
each curve representing an average over 20 implementations of the same setting. Here, the
errors are plotted on a log scale for a better view of the convergence rate. Unlike Figure
C.1 and Figure C.2, the statistical errors can not reach 0 (or
in the log plot) due to the
existence of noise. But they all achieved essentially the same order of statistical precision,
which veriﬁes Theorem 3, and the statistical convergence of LLA was really fast.

a, a), with a

−∞

∈ {

−

l

)
e
a
c
s

g
o
l
(

r
o
r
r
e

l

a
c
i
t
s
i
t
a
t
S

14

13

12

11

10

9

8

7

6

5

0

l

)
e
a
c
s

g
o
l
(

r
o
r
r
e

l

a
c
i
t
s
i
t
a
t
S

14

13

12

11

10

9

8

7

6

5

0

5

10

15

Iteration

5

10

15

Iteration

FIG C.3. Log plot of the statistical accuracy of LLA iterates in PH -penalized sparse regression (left)
and robust regression (right).

On the other hand, the computational burden of LLA turned out to be pretty high, mainly
due to the cost of solving a weighted lasso problem at each iteration. We thus turned to
iterative thresholding because of its low per-iteration complexity. Figure C.4 shows some
analogous results. According to Figure C.4, all ﬁnal statistical errors were controlled within
the same order of precision. The convergence process seems to conform to the bound in The-
log(1/κ) t + log(∆ψ(β∗, β(0))), and when
orem 5: when t is small, log ∆ψ(β∗, β(t)) .
t is large, log ∆ψ(β∗, β(t)) . κt∆ψ(β∗, β(0)) + log(κKλ2J ∗/(1
κ)), demonstrating an
exponential decay.

−

−

C.3. Accelerations. We test the acceleration schemes in IS divergence minimization and

robust sparse regression in this subsection.

Figure C.5 shows the power of applying the (second) acceleration in IS divergence mini-

mization problem in Section C.1, where we used 50 starting points with β(0)
≤
p. With the acceleration, the number of iterations was brought down from 1000 to less
j
than 50 to reach the same value of the objective function, and the overall computational time
was saved by nearly 90%.

U (0, 1), 1

j ∼

≤

Figure C.6 shows the convergence of statistical error when applying the (ﬁrst) acceleration
scheme in iterative thresholding for the PH -penalized Tukey’s loss minimization problem as
mentioned in Section C.2. The simulation setting remains the same as before and we sampled
20 initial points with β(0)
p. A substantial reduction in the number of
≤
iterations was achieved. Of course, the line search causes some overhead in computation.
But the accelerated iterative thresholding still reduced the overall running time by more than
30%, and obtained slightly better statistical accuracy.

1, 1), 1

j ∼

U (

≤

−

j

 
 
 
 
 
 
ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

53

l

)
e
a
c
s

g
o
l
(

r
o
r
r
e

l

a
c
i
t
s
i
t

t

a
S

16

15

14

13

12

11

10

9

8

7

6

l

)
e
a
c
s

g
o
l
(

r
o
r
r
e

l

a
c
i
t
s
i
t

t

a
S

16

15

14

13

12

11

10

9

8

7

6

0

10

20

30

40

50
Iteration

60

70

80

90

100

0

10

20

30

40

50
Iteration

60

70

80

90

100

FIG C.4. Log plot of the statistical accuracy of iterative thresholding iterates in PH -penalized sparse
regression (left) and robust regression (right).

Mirror Descent
Accelerated Mirror Descent

l

)
e
a
c
s

g
o
l
(

e
u
a
v

l

n
o
i
t
c
n
u
F

10

8

6

4

2

0

-2

100

200

300

400

500
Iteration

600

700

800

900

1000

FIG C.5. Objective function value (shown on log scale) v.s. number of iterations for the plain and
accelerated exponentiated gradient descent algorithms in nonconvex Burg entropy optimization.

REFERENCES

[1] AGARWAL, A., NEGAHBAN, S. and WAINWRIGHT, M. J. (2012). Fast global convergence of gradient
methods for high-dimensional statistical recovery. The Annals of Statistics 40 2452–2482.
[2] AN, L. T. H. and TAO, P. D. (2005). The DC (difference of convex functions) programming and DCA revis-
ited with DC models of real world nonconvex optimization problems. Annals of Operations Research
133 23–46.

[3] BANERJEE, A., MERUGU, S., DHILLON, I. S. and GHOSH, J. (2005). Clustering with Bregman Diver-

gences. Journal of Machine Learning Research (JMLR) 6.

[4] BECK, A. and TEBOULLE, M. (2009). A fast iterative shrinkage-thresholding algorithm for linear inverse

problems. SIAM Journal on Imaging Sciences 2 183–202.

[5] BEN-TAL, A. and NEMIROVSKI, A. (2013). Optimization III: Convex Analysis, Nonlinear Programming

Theory, Standard Nonlinear Programming Algorithms. Lecture Notes.

[6] BERTSEKAS, D. P. (1999). Nonlinear Programming, 2nd ed. Athena Scientiﬁc.
[7] BICKEL, P. J., RITOV, Y. and TSYBAKOV, A. B. (2009). Simultaneous analysis of lasso and Dantzig selec-

tor. The Annals of Statistics 1705–1732.

[8] BLUMENSATH, T. and DAVIES, M. E. (2009). Iterative hard thresholding for compressed sensing. Applied

and Computational Harmonic Analysis 27 265–274.

[9] BREGMAN, L. M. (1967). The relaxation method of ﬁnding the common point of convex sets and its ap-
plication to the solution of problems in convex programming. USSR Computational Mathematics and
Mathematical Physics 7 200–217.

 
 
 
 
 
 
 
 
 
54

Y. SHE ET AL.

15

14

13

12

11

10

l

)
e
a
c
s

g
o
l
(

r
o
r
r
e

l

a
c
i
t
s
i
t
a
t
S

9

8

0

Iterative Thresholding
Accelerated Iterative Thresholding

50

100

150

Iteration

FIG C.6. Statistical error (shown on log scale) v.s. iteration number for iterative thresholding and
accelerated iterative thresholding in robust sparse regression.

[10] BUNEA, F., TSYBAKOV, A. and WEGKAMP, M. (2007). Sparsity oracle inequalities for the LASSO. Elec-

tronic Journal of Statistics 1 169–194.

[11] CANDES, E. and TAO, T. (2007). The Dantzig selector: Satistical estimation when p is much larger than n.

The Annals of Statistics 2313–2351.

[12] CANDES, E. J. and TAO, T. (2005). Decoding by Linear Programming. IEEE Trans. Inf. Theor. 51 4203–

4215.

[13] CAO, EGGERMONT, P. P. B. and TEREBEY, S. (1999). Cross Burg entropy maximization and its application

to ringing suppression in image reconstruction. IEEE Transactions on Image Processing 8 286–292.

[14] CHEN, G. and TEBOULLE, M. (1993). Convergence Analysis of a Proximal-Like Minimization Algorithm

Using Bregman Functions. SIAM Journal on Optimization 3 538–543.

[15] CICHOCKI, A., ICHI AMARI, S., ZDUNEK, R., KOMPASS, R., HORI, G. and HE, Z. (2006). Ex-
tended SMART algorithms for non-negative matrix factorization. In ICAISC (L. RUTKOWSKI,
R. TADEUSIEWICZ, L. A. ZADEH and J. M. ZURADA, eds.). Lecture Notes in Computer Science
4029 548-562. Springer.

[16] CORREA, R., JOFRE, A. and THIBAULT, T. (1994). Subdifferential Monotonicity as Characterization of

Convex Functions. Numerical Functional Analysis and Optimization 15 531–535.

[17] DONOHO, D. and JOHNSTONE, I. (1994). Ideal Spatial Adaptation via Wavelet Shrinkages. Biometrika 81

425–455.

[18] DUCHI, J. C., HAZAN, E. and SINGER, Y. (2011). Adaptive Subgradient Methods for Online Learning and

Stochastic Optimization. J. Mach. Learn. Res. 12 2121-2159.

[19] FAN, J. and LI, R. (2001). Variable selection via nonconcave penalized likelihood and its oracle properties.

Journal of the American Statistical Association 96 1348–1360.

[20] FAN, J., XUE, L. and ZOU, H. (2014). Strong oracle optimality of folded concave penalized estimation.

The Annals of Statistics 42 819–849.

[21] F ´EVOTTE, C., BERTIN, N. and DURRIEU, J.-L. (2009). Nonnegative Matrix Factorization with the Itakura-
saito Divergence: With Application to Music Analysis. Neural Computation 21 793–830.
[22] FRANK, I. E. and FRIEDMAN, J. H. (1993). A Statistical View of Some Chemometrics Regression Tools.

Technometrics 35 109-135.

[23] GASSO, G., RAKOTOMAMONJY, A. and CANU, S. (2009). Recovering sparse signals with a certain family
of nonconvex penalties and DC programming. IEEE Transactions on Signal Processing 57 4686-4698.
[24] GHADIMI, S. and LAN, G. (2016). Accelerated gradient methods for nonconvex nonlinear and stochastic

programming. Mathematical Programming 156 59–99.

[25] HAGOOD, J. W. and THOMSON, B. S. (2006). Recovering a function from a Dini derivative. The American

Mathematical Monthly 113 34-46.

[26] HAMPEL, F. R., RONCHETTI, E. M., ROUSSEEUW, P. J. and STAHEL, W. A. (2005). Robust Statistics.

John Wiley & Sons, New York.

[27] HUBER, P. J. (1981). Robust Statistics. John Wiley and Sons, New York.
[28] HUNTER, D. R. and LANGE, K. (2004). A tutorial on MM algorithms. The American Statistician 30–37.
[29] HUNTER, D. R. and LI, R. (2005). Variable selection using MM algorithms. The Annals of Statistics 33

1617–1642.

 
 
 
ANALYSIS OF BREGMAN-SURROGATE ALGORITHMS

55

[30] JØRGENSEN, B. (1987). Exponential Dispersion Models. Journal of the Royal Statistical Society. Series B

49 127-145.

[31] KOLTCHINSKII, V. (2011). Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Prob-

lems. Springer.

[32] KRICHENE, W., BAYEN, A. and BARTLETT, P. L. (2015). Accelerated Mirror Descent in Continuous and
Discrete Time. In Advances in Neural Information Processing Systems (C. CORTES, N. LAWRENCE,
D. LEE, M. SUGIYAMA and R. GARNETT, eds.) 28. Curran Associates, Inc.

[33] LANGE, K. and ZHOU, H. (2014). MM algorithms for geometric and signomial programming. Mathemati-

cal Programming 143 339–356.

[34] LEE, D. D. and SEUNG, H. S. (1999). Learning the parts of objects by nonnegative matrix factorization.

Nature 401 788–791.

[35] LEF `EVRE, A., BACH, F. R. and F ´EVOTTE, C. (2011). Online algorithms for nonnegative matrix factor-
ization with the Itakura-Saito divergence. In IEEE Workshop on Applications of Signal Processing to
Audio and Acoustics, WASPAA 2011 313–316.

[36] LOH, P.-L. and WAINWRIGHT, M. J. (2015). Regularized M-estimators with nonconvexity: statistical and

algorithmic theory for local optima. Journal Machine Learning Research 16 559–616.

[37] LOUNICI, K., PONTIL, M., TSYBAKOV, A. B. and VAN DE GEER, S. (2011). Oracle Inequalities and

Optimal Inference under Group Sparsity. Annals of Statistics 39 2164-2204.

[38] NEMIROVSKI, A. and YUDIN, D. (1983). Problem complexity and method efﬁciency in optimization. Wiley-

Interscience series in discrete mathematics. Wiley, Chichester, New York.

[39] NESTEROV, Y. (1983). A method of solving a convex programming problem with convergence rate

O(1/k2). Soviet Mathematics Doklady 27 372–376.

[40] NESTEROV, Y. (1988). On an approach to the construction of optimal methods of minimization of smooth

convex functions. Ekonom. i. Mat. Metody (In Russian) 24 509–517.

[41] NESTEROV, Y. (2004). Introductory Lectures on Convex Optimization: A Basic Course. Applied optimiza-

tion. Kluwer Academic Publ., Boston, Dordrecht, London.

[42] ONG, C. S. and AN, L. T. H. (2013). Learning sparse classiﬁers with difference of convex functions algo-

rithms. Optimization Methods and Software 28 830-854.

[43] PAN, W., SHEN, X. and LIU, B. (2013). Cluster analysis: unsupervised learning via supervised learning

with a non-convex penalty. Journal of Machine Learning Research 14 1865–1889.

[44] RIGOLLET, P. and TSYBAKOV, A. (2011). Exponential Screening and optimal rates of sparse estimation.

Annals of Statistics 39 731–771.

[45] ROCKAFELLAR, R. T. (1970). Convex Analysis. Princeton University Press, Princeton, NJ.
[46] SCHMIDT, M. (2010). Graphical Model Structure Learning with ℓ1-Regularization, PhD thesis, University

of British Columbia.

[47] SHE, Y. (2009). Thresholding-based Iterative Selection Procedures for Model Selection and Shrinkage.

Electronic Journal of Statistics 3 384-415.

[48] SHE, Y. (2012). An iterative algorithm for ﬁtting nonconvex penalized generalized linear models with

grouped predictors. Computational Statistics and Data Analysis 9 2976-2990.

[49] SHE, Y. (2016). On the ﬁnite-sample analysis of Θ-estimators. Electronic Journal of Statistics 10 1874–

1895.

[50] SHE, Y. and CHEN, K. (2017). Robust reduced-rank regression. Biometrika 104 633-647.
[51] SHE, Y., HE, Y. and WU, D. (2014). Learning Topology and Dynamics of Large Recurrent Neural Net-

works. IEEE Transactions on Signal Processing 62 5881-5891.

[52] SHE, Y., LI, H., WANG, J. and WU, D. (2013). Grouped Iterative Spectrum Thresholding for Super-

Resolution Sparse Spectrum Selection. IEEE Transactions on Signal Processing 61 6371-6386.

[53] SHE, Y., SHEN, J. and ZHANG, C. Supervised Multivariate Learning with Simultaneous Feature Auto-
grouping and Dimension Reduction. Journal of the Royal Statistical Society: Series B (Statistical
Methodology). To appear.

[54] SHE, Y., WANG, Z. and SHEN, J. Gaining Outlier Resistance with Progressive Quantiles: Fast Algorithms

and Theoretical Studies. Journal of the American Statistical Association. To appear.

[55] TAO, P. D. and SOUAD, E. B. (1986). Algorithms for solving a class of nonconvex optimization problems.

Methods of subgradients. North-Holland Mathematics Studies 129 249-271.

[56] TIBSHIRANI, R. (1996). Regression Shrinkage and Selection via the Lasso. Journal of the Royal Statistical

Society (Series B) 58 267-288.

[57] TSENG, P. (2008). On accelerated proximal gradient methods for convex-concave optimization. Technical

report, Department of Mathematics, University of Washington.

[58] TSYBAKOV, A. B. (2008). Introduction to Nonparametric Estimation. Springer, New York, NY.
[59] VAN DE GEER, S. A. and B ¨UHLMANN, P. (2009). On the conditions used to prove oracle results for the

Lasso. Electronic Journal of Statistics 3 1360–1392.

56

Y. SHE ET AL.

[60] VAN DER VAART, A. W. and WELLNER, J. (1996). Weak Convergence and Empirical Processes: With

Applications to Statistics. Springer.

[61] VAPNIK, V. N. (1995). The Nature of Statistical Learning Theory. Springer-Verlag, New York, NY, USA.
[62] WAINWRIGHT, M. J. and JORDAN, M. I. (2008). Graphical Models, Exponential Families, and Variational

Inference. Foundations and Trends in Machine Learning 1 1-305.

[63] WANG, Z., LIU, H. and ZHANG, T. (2014). Optimal computational and statistical rates of convergence for

sparse nonconvex learning problems. The Annals of Statistics 42 2164–2201.

[64] ZHANG, C., JIANG, Y. and CHAI, Y. (2010). Penalized Bregman divergence for large-dimensional regres-

sion and classiﬁcation. Biometrika 97 551–566.

[65] ZHANG, C. H. (2010). Nearly unbiased variable selection under minimax concave penalty. The Annals of

Statistics 38 894-942.

[66] ZHANG, T. (2010). Analysis of Multi-stage Convex Relaxation for Sparse Regularization. Journal of Ma-

chine Learning Research 11 1081–1107.

[67] ZOU, H. and LI, R. (2008). One-step Sparse Estimates in Nonconcave Penalized Likelihood Models. Annals

of Statistics 36 1509–1533.

