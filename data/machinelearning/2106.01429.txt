2
2
0
2

p
e
S
0
3

]
L
M

.
t
a
t
s
[

2
v
9
2
4
1
0
.
6
0
1
2
:
v
i
X
r
a

Smooth Bilevel Programming
for Sparse Regularization

Clarice Poon∗, Gabriel Peyré†

October 3, 2022

Abstract

Iteratively reweighted least square (IRLS) is a popular approach to solve
sparsity-enforcing regression problems in machine learning. State of the
art approaches are more eﬃcient but typically rely on speciﬁc coordinate
pruning schemes. In this work, we show how a surprisingly simple re-
parametrization of IRLS, coupled with a bilevel resolution (instead of an
alternating scheme) is able to achieve top performances on a wide range
of sparsity (such as Lasso, group Lasso and trace norm regularizations),
regularization strength (including hard constraints), and design matrices
(ranging from correlated designs to diﬀerential operators). Similarly to
IRLS, our method only involves linear systems resolutions, but in sharp
contrast, corresponds to the minimization of a smooth function. Despite
being non-convex, we show that there are no spurious minima and that
saddle points are “ridable”, so that there always exists a descent direction.
We thus advocate for the use of a BFGS quasi-Newton solver, which
makes our approach simple, robust and eﬃcient. We perform a numerical
benchmark of the convergence speed of our algorithm against state of the
art solvers for Lasso, group Lasso, trace norm and linearly constrained
problems. These results highlight the versatility of our approach, removing
the need to use diﬀerent solvers depending on the speciﬁcity of the ML
problem under study.

1

Introduction

Regularized empirical risk minimization is a workhorse of supervised learning,

and for a linear model, it reads

min
β∈Rn

R(β) +

1
λ

L(Xβ, y)

(Pλ)

where X ∈ Rm×n is the design matrix (n being the number of samples and
m the number of features), L : Rm × Rm → [0, ∞) is the loss function, and

∗Department of Mathematical Sciences, University of Bath, Bath BA2 7AY, UK
†CNRS and DMA, Ecole Normale Supérieure, PSL University,

cmhsp20@bath.ac.uk
45 rue d’Ulm, F-75230 PARIS cedex 05, FRANCE, gabriel.peyre@ens.fr

1

 
 
 
 
 
 
R : Rn → [0, ∞) the regularizer. Here λ (cid:62) 0 is the regularisation parameter
which is typically tuned by cross-validation, and in the limit case λ = 0, (P0) is
a constraint problem minβ R(β) under the constraint L(Xβ, y) = 0.

In this work, we focus our attention to sparsity enforcing penalties, which
induce some form of structure on the solution of (Pλ), the most celebrated
examples (reviewed in Section 2) being the Lasso, group-Lasso and trace norm
regularizers. All these regularizers, and much more (as detailed in Section), can
be conveniently re-written as an inﬁmum of quadratic functions. While Section 2
reviews more general formulations, this so-called “quadratic variational form” is
especially simple in the case of block-separable functionals (such as Lasso and
group-Lasso), where one has

R(β) = min
η∈Rk
+

(cid:88)

1
2

||βg||2
2
ηg

1
2

+

h(η),

(1)

g∈G
where G is a partition of {1, . . . , n}, k = |G| is the number of groups and h :
+ → [0, ∞). An important example is the group-Lasso, where R(β) = (cid:80)
Rk
g ||βg||2
is a group-(cid:96)1 norm, in which case h(η) = (cid:80)
i ηi. The special case of the Lasso,
corresponding to the (cid:96)1 norm is obtained when g = {i} for i = 1, . . . , n and k = n.
This quadratic variational form (1) is at the heart of the Iterative Reweighted
Least Squares (IRLS) approach, reviewed in Section 1.1. We refer to Section 2
for an in-depth exposition of these formulations.

Sparsity regularized problems (Pλ) are notoriously diﬃcult to solve, especially
for small λ, because R is a non-smooth function. It is the non-smoothness of R
which forces the solutions of (Pλ) to belong to low-dimensional spaces (or more
generally manifolds), the canonical example being spaces of sparse vectors when
solving a Lasso problem. We refer to [3] for an overview of sparsity-enforcing
regularization methods. The core idea of our algorithm is that a simple re-
parameterization of (1) combined with a bi-level programming (i.e. solving two
nested optimization problems) can turn (Pλ) into a smooth program which is
much better conditioned, and can be tackled using standard but highly eﬃcient
optimization techniques such as quasi-Newton (L-BFGS). Indeed, by doing the
√
change of variable (vg, ug) (cid:44) (

ηg) in (1), (Pλ) is equivalent to

ηg, βg/

√

f (v) where

min
v∈Rk

f (v) (cid:44) min
u∈Rn

G(u, v)

G(u, v) (cid:44) 1
2

h(v (cid:12) v) +

1
2

||u||2 +

1
λ

L(X(v (cid:12)G u), y).

(2)

(3)

Throughout, we deﬁne (cid:12) to be the standard Hadamard product and for v ∈
Rk and u ∈ Rn, we deﬁne v (cid:12)G u ∈ Rn to be such that (v (cid:12)G u)g = vgug.
Provided that v (cid:55)→ h(v (cid:12) v) is diﬀerentiable and L(·, y) is a convex, proper,
lower semicontinuous function, the inner minimisation problem has a unique
solution and f is diﬀerentiable. Moreover, in the case of the quadratic loss
, the gradient of f can be computed in closed form, by solving
L(z, y) (cid:44) 1
a linear system of dimension m or n. This paper is thus devoted to study the
theoretical and algorithmic implications of this simple twist on the celebrated
IRLS approach.

2 ||z − y||2

2

2

A
T
S
I

o
r
P
-
x
v
c
n
o
N

Figure 1: Evolution of 20 coeﬃcients via ISTA and gradient descent of f .

Comparison with proximal gradient To provide some intuition about the
proposed approach, Figure 1 contrasts the iterations of gradient descent on
f and of the iterative soft thresholding algorithm (ISTA) on the Lasso. We
consider a random Gaussian matrix X ∈ R10×20 with λ = ||X (cid:62)y||∞/10. The
ISTA trajectory is non-smooth when some feature crosses 0. In particular, if a
coeﬃcient (such as the red one on the ﬁgure) is initialized with the wrong sign,
it takes many iterations for ISTA to ﬂip sign. In sharp contrast, the gradient
ﬂow of f does not exhibit such a singularity and exhibits smooth geometric
convergence. We refer to the appendix for an analysis of this phenomenon.

Contributions Our main contribution is a new versatile algorithm for sparse
regularization, which applies standard smooth optimization methods to minimize
the function f in (2). We ﬁrst propose in Section 2 a generic class of regularizers
R that enjoy a quadratic variational form. This section recaps existing results
under a common umbrella and shows the generality of our approach. Section 3.1
then gathers the theoretical analysis of the method, and in particular the proof
that while being non-convex, the function f has no local minimum and only
“ridable” saddle points. As a result, one can guarantee convergence to a global
minimum for many optimisation schemes, such as gradient descent with random
perturbations [36, 34] or trust region type methods [46]. Furthermore, for the
case of the group Lasso, we show that f is an inﬁnitely diﬀerentiable function with
uniformly bounded Hessian. Consequently, standard solvers such as Newton’s
method/BFGS can be applied and with a superlinear convergence guarantee.
Section 4 performs a detailed numerical study of the method and benchmarks it
against several popular competing algorithms for Lasso, group-Lasso and trace
norm regularization. Our method is consistently amongst the best performers,
and is in particular very eﬃcient for small values of λ, and can even cope with
the constrained case λ = 0.

1.1 Related works

State of the art solvers for sparse optimisation Popular approaches to
nonsmooth sparse optimisation include proximal based methods. The simplest

3

instance is the Forward-Backward algorithm [37, 18] which handles the case
where L is a smooth function. There are many related inertial based acceleration
schemes, such as FISTA [6] and particularly eﬀective techniques that leads to
substantial speedups are the adaptive use of stepsizes and restarting strategies
[45]. Other related approaches are proximal quasi-Newton and variable metric
methods [17, 7], which incorporate variable metrics into the quadratic term of
the proximal operator. Another popular approach, particularly for the Lasso-like
problems are coordinate descent schemes [23]. These schemes are typically
combined with support pruning schemes [26, 42, 39]. In the case where both the
regularisation and loss terms are nonsmooth (e.g. in the basis pursuit setting),
typical solvers are the primal-dual [14], ADMM [11] and Douglas-Rachford
algorithms [21]. Although these schemes are very popular due to their relatively
low per iteration complexity, these methods have sublinear convergence rates in
general, with linear convergence under strong convexity.

The quadratic variational formulation and IRLS Quadratic variational
formulations such as (1) have been exploited in many early computer vision
works, such as [24] and [25]. One of the ﬁrst theoretical results can be found
in [25], see also [10] which lists many examples. Our result in Theorem 1 can
be seen as a generalisation of these early works. Norm regularizers of this
form were introduced in [40], and further studied in the monograph [3] under
the name of subquadratic norms. The main algorithmic consequence of the
quadratic variational formulation in the literature is the iterative reweighted
least squares (IRLS) algorithm. When L(z, y) = 1
is the quadratic loss,
a natural optimisation strategy is alternating minimising. However, due to the
1/ηg term, one needs to introduce some regularisation to ensure convergence.
One popular approach is to add ε
to the formulation (1) which leads
g η−1
g
2
to the IRLS algorithm [19]. The ε-term can be seen as a barrier to keep ηg
positive which is reminiscent of interior point methods. The idea of IRLS is to
do alternating minimisation over β and η, where the minimisation with respect
to β is a least squares problem, and the minimisation with respect to η admits a
closed form solution. A nuclear norm version of IRLS has been used in [1] where
an alternating minimisation algorithm was introduced. Finally, we remark that
although nonconvex formulations for various low complexity regularizers have
appeared in the literature, see for instance [50, 31, 38, 32] for the case of the (cid:96)1
and nuclear norms, they are typically associated with alternating minimisation
algorithms.

2 ||z − y||2

(cid:80)

2

IRLS methods are
Variable projection/reduced gradient approaches
quite slow because the resulting minimization problem is poorly conditioned.
Adding the smoothing term ε
only partly alleviates this, and also breaks
2
the sparsity enforcing property of the regularizer R. We avoid both issues in (2)
by solving a “reduced” problem which is much better conditioned and smooth.
This idea of solving a bi-variate problem by re-casting it as a bilevel program is
classical, we refer in particular to [51, Chap. 10] for some general theoretical

g η−1
g

(cid:80)

4

results on reduced gradients, and also Danskin’s theorem (although this is
restricted to the convex setting) in [8]. Our formulation falls directly into the
framework of variable projection [52, 28], introduced initially for solving nonlinear
least squares problems. Properties and advantages of variable projection have
been studied in [52], we refer also to [33, 59] for more recent studies. Nonsmooth
variable projection is studied in [57], although the present work is in the classical
setting of variable projection due to our smooth reparametrization. Reduced
gradients have also been associated with the quadratic variational formulation
in several works [3, 48, 49]. The idea is to apply descent methods over g(η) =
. Although the function over η and β is discontinuous,
minβ R0(η, β)+ 1
the function g over η is smooth and one can apply ﬁrst order methods, such
as proximal gradient descent to minimise g under positivity constraints. While
quasi-Newton methods can be applied in this setting with bound constraints,
we show in Section 4.1 that this approach is typically less eﬀective than our
nonconvex bilevel approach. In the setting of the trace norm, the optimisation
problem is constrained on the set of positive semideﬁnite matrices, so one is
restricted to using ﬁrst order methods [48].

2 ||Xβ −y||2

2

2 Quadratic variational formulations

We describe in this section some general results about when a regulariser
has a quadratic variational form. Our ﬁrst result brings together results which
are scattered in the literature: it is closely related to Theorem 1 in [25], but
their proof was only for strictly concave diﬀerentiable functions and did not
explicitly connect to convex conjugates, while the setting for norms have been
characterized in the monograph [3] under the name of subquadratic norms.

Theorem 1. Let R : Rn → R. The following are equivalent:

(i) R(β) = ϕ(β (cid:12) β) where ϕ is proper, concave and upper semi-continuous,

with domain Rd
+

.

(ii) There exists a convex function ψ for which R(β) = inf z∈Rn

1
2

+

(cid:80)n

i=1 ziβ2

i +

ψ(z).
Furthermore, ψ(z) = (−ϕ)∗(−z/2) is deﬁned via the convex conjugate (−ϕ)∗ of
−ϕ, leading to (1) using the change of variable η ← 1/z and h(η) = 2ψ(1/η).
When R is a norm, the function h can be written in terms of the dual norm R∗ as
i ηi. Moreover, R(β)2 = inf η∈Rn
h(η) = maxR∗(w)(cid:54)1
See Appendix B for the proof to this Theorem. Some additional properties of
ψ are derived in Lemma 1 of Appendix B, one property is that if R is coercive,
then lim||z||→0 ψ(z) = +∞, so the function f is coercive (see also remark 2).

i η−1
i

i w2

i β2

(cid:8)(cid:80)

\ h(η) (cid:54) 1(cid:9).

(cid:80)

+

2.1 Examples

Let us ﬁrst give some simple examples of both convex and non-convex norms:
– Euclidean norms: for R = || · ||2, making use of R∗ as stated in Theorem 1,

one has h = || · ||∞.

5

(cid:80)
(cid:80)

– Group norms: for G is a partition of {1, . . . , n}, the group norm is R(β) =
g∈G ||βg||. Using the previous result for the Euclidean norm, one has h(η) =
g∈G ||(ηi)i∈g||∞. This expression can be further simpliﬁed to obtain (1) with
in place of Rn by noticing that the optimal η is

a reduced vector η in R|G|
+
constant in each group g.

– (cid:96)q (quasi) norms: For R(β) = |β|q where q ∈ (0, 2), one has ϕ(u) = uq/2 and
2−q where Cq = (2 − q)qq/(2−q). Note that for
, v (cid:55)→ h(v2) = vγ for γ > 1 is diﬀerentiable. Analysis and numerics for

one veriﬁes that h(η) = Cqη
q > 2
3
this nonconvex setting can be found in Appendix F.

q

Matrix regularizer The extension of Theorem 1 to the case where β = B is
a matrix can be found in the appendix. When R = ϕ(BB(cid:62)) is a function on
matrices, the analogous quadratic variational formulation is

R(B) = min
Z∈Sn
+

min
B∈Rn×r

1
2

(cid:88)

g

tr(B(cid:62)Z −1B) +

1
2

h(Z),

(4)

where Sn
denotes the set of symmetric positive semideﬁnite matrices and h(Z) =
+
2(−ϕ)∗(−Z −1/2). Letting U = Z −1/2B and V = Z 1/2, we have B = V U and
the equivalence

min
B

R(B) +

1
2λ

||A(B) − y||2
2

= min

V ∈Rn×n

f (V ) (cid:44) min

U ∈Rn×r

1
2

||U ||2

F +

1
2

h(V (cid:62)V ) +

1
2λ

||A(V U ) − y||2
2.

(5)

(6)

where A : Rn×r → Rm is a linear operator. Again, provided that V (cid:55)→ h(V (cid:62)V )
is diﬀerentiable, f is a diﬀerentiable function with ∇f (V ) = V ∂h(V (cid:62)V ) +
λ A∗(A(V U ) − y))U (cid:62) and U such that λU + V (cid:62)A∗(A(V U ) − y) = 0. For the
1
B(cid:62)B), we have h(Z) = tr(Z) and ∂h(Z) = Id. Note
trace norm, R(B) = tr(
that, just in the vectorial case, one could write the inner minimisation problem
over the dual variable α ∈ Rm and handle the case of λ = 0.

√

3 Theoretical analysis

Our ﬁrst result shows the equivalence between (Pλ) and a smooth bilevel

problem.

Theorem 2. Denote Ly (cid:44) L(·, y) and let L∗
denote the convex conjugate of Ly.
y
Assume that Ly is a convex, proper, lower semicontinuous function and R takes
the form (1). The problem (Pλ) is equivalent to

min
v∈Rk

f (v) (cid:44) min
u∈Rn

= max
α∈Rm

1
2
1
2

h(v (cid:12) v) +

||u||2 +

1
λ

Ly (X(v (cid:12)G u)) .

h(v (cid:12) v) −

L∗

y (λα) −

1
2

||v (cid:12)G (X (cid:62)α)||2
2.

1
2
1
λ

6

(7)

(8)

where v (cid:12)G u (cid:44) (ugvg)g∈G. The minimiser β to (Pλ) and the minimiser v to
(7) are related by β = v (cid:12)G u = −v2 (cid:12)G X (cid:62)α. Provided that v (cid:55)→ h(v (cid:12) v) is
diﬀerentiable, the function f is diﬀerentiable with gradient

(cid:0)||X (cid:62)

g α||2(cid:1)

∇f (v) = v(cid:12)∂h(v2)−v(cid:12)G

||v(cid:12)G(X (cid:62) ˜α)||2
2.
(9)
Note that ∇f is uniquely deﬁned even if α is not unique. If λ > 0 and Ly is
diﬀerentiable, then we have the additional formula, with u ∈ argmin˜u
2 +
λ Ly(X(v (cid:12)G ˜u)),
1

where α ∈ argmax ˜α −L∗

2 ||˜u||2

y(˜α)−

1

g

1
2

∇f (v) = v (cid:12) ∂h(v2) +

1
λ

(cid:0)(cid:104)ug, X (cid:62)

g ∂Ly(X(v (cid:12)G u)(cid:105)(cid:1)

.

g∈G

(10)

Proof. The equivalence between (Pλ) and (7) is simply due to the quadratic
variational form of R, and the change of variable vg =
ηg.
The equivalence to (8) follows by convex duality on the inner minimisation
problem, that is

ηg, and ug = βg/

√

√

f (v) = min

u

= min
u,z

h(v2) +

G1(v, u) (cid:44) 1
2
1
1
||u||2 +
2
2

h(v2) +

1
λ

1
λ

||u||2 +

1
2
Ly(z) where

Ly(X(v (cid:12)G u))

z = X(v (cid:12)G u)

= min
u,z

max
α

= max

α

min
u

1
2
1
2

h(v2) +

h(v2) +

1
2
1
2

||u||2 +

1
λ

Ly(z) − (cid:104)α, z(cid:105) + (cid:104)α, X(v (cid:12)G u)(cid:105)

||u||2 + (cid:104)α, X(v (cid:12)G u)(cid:105) −

1
λ

L∗

y(λα).

Using the optimality condition over u, we obtain u = −v (cid:12)G X (cid:62)α and hence,

f (v) = max

α

G2(v, α) (cid:44) 1
2

h(v2) −

1
2

||v (cid:12)G X (cid:62)α||2

2 −

1
λ

L∗

y(λα).

By [51, Theorem 10.58], if the following set

S(v) (cid:44) (cid:8)∂vG2(v, α) = v − v (cid:12)G (||X (cid:62)

g α||2

2)g∈G \ α ∈ argminα G2(v, α)(cid:9)

is singled valued, then f is diﬀerentiable with ∇f (v) = ∂vG2(v, α) for α ∈
argminα G2(v, α). Observe that even if argminα G2(v, α) is not single valued,
since G2 is strongly convex for v (cid:12)G X (cid:62)α, S(v) is single-valued and hence, f is
a diﬀerentiable function.

In the case where Ly is diﬀerentiable, we can again apply [51, Theorem 10.58],
to obtain ∇f (v) = v (cid:12) ∂h(v2) + ∂vG1(v, y) with u = argminu G1(v, u) (noting
that G1(v, ·) is strongly convex and has a unique minimiser) which is precisely
the gradient formula (10).

For the Lasso and basis pursuit setting, the gradient of f can be computed

in closed form:

7

, then
Corollary 1. If R has a quadratic variational form and Ly(z) = 1
∂Ly(z) = z − y, L∗
and the gradient of f can be written
2 ||α||2
2
as in (9) and additionally (10) when λ > 0. Furthermore, α ∈ Rm in (9) and
u ∈ Rn in (10) solves

y(α) = (cid:104)y, α(cid:105) + 1

2 ||z − y||2

2

(X diag(¯v2)X (cid:62)+λId)α = −y,

and (diag(¯v)X (cid:62)X diag(¯v)+λId)u = v(cid:12)G(X (cid:62)y),

(11)
where ¯v ∈ Rn is deﬁned as ¯v (cid:12) u = (vgug)g∈G for all u ∈ Rn and Id denotes the
identity matrix.

This shows that our method caters for the case λ = 0 with the same algorithm
in a seamless manner. This is unlike most existing approach which work well for
λ > 0 (and typically do not require matrix inversion) but fails when λ is small,
whereas solvers dedicated for λ = 0 might require inverting a linear system, see
Section 4.4 for an illustrative example.

3.1 Properties of the projected function f

In this section, we analyse the case of the group Lasso. The following theorem
ensures that the projected function f has only strict saddle points or global
minima. We say that v is a second order stationary point if ∇f (v) = 0 and
∇2f (v) (cid:23) 0. We say that v is a strict saddle point (often called “ridable”) if
it is a stationary point but not a second order stationary point. One can thus
always ﬁnd a direction of descent outside the set of global minimum. This can
be exploited to derive convergence guarantees to second order stationary points
for trust region methods [46] and gradient descent methods [36, 34].

Theorem 3. In the case h(z) = (cid:80)
, the projected
function f is inﬁnitely continuously diﬀerentiable and for v ∈ Rk, ∇f (v) =
v (cid:12) (cid:0)1 − |ξ|2(cid:1) where ξg = 1
g (X(u (cid:12) v) − y) and u solves the inner least squares
problem for v. Let J denote the support of v, by rearranging the columns and
rows, the Hessian of f can be written as the following block diagonal matrix

i zi and L(z, y) = 1

2 ||z − y||2

λ X (cid:62)

2

∇2f (v) =

(cid:18)diag(1 − ||ξg||2

2)g∈J + 4U (cid:62)W U
0

0

diag(1 − ||ξg||2

2)g∈J c

(cid:19)

(12)

g Xhvh)g,h∈J + λIdJ

where W (cid:44) Id − λ (cid:0)(vgX (cid:62)
(cid:1)−1 and U is the block diagonal
matrix with blocks (ξg)g∈J , with maxg∈G ||ξg||2 (cid:54) C and ||∇2f (v)|| (cid:54) 1 + 3C 2
where C (cid:44) ||y||2 maxg∈G ||Xg||/λ. Moreover, all stationary points of f are either
global minima or strict saddles. At stationary points, the eigenvalues of the
Hessian of f are at most 4 and is at least

(cid:18)

min

4(1 − λ/(λ + ˆσ)), min
g(cid:54)∈J

(1 − ||ξg||2)

(cid:19)

where ˆσ is the smallest eigenvalue of (vgX (cid:62)

g Xhvh)g,h∈J .

8

The proof can be found in Appendix C. We simply mention here that by
examining the ﬁrst order condition of (Pλ), we see that β is a minimizer if and
only if ξ satisﬁes −ξg = βg
for all g ∈ Supp(β) and ||ξg||2 (cid:54) 1 for all g ∈ G.
||βg||2
The ﬁrst condition on the support of β is always satisﬁed at stationary points of
the nonconvex function (2), and by examining (12), the second condition is also
satisﬁed unless the stationary point is strict.

Remark 1 (Example of strict saddle point for our f ). One can observe that
v = 0 is a strict saddle point, as the solution to the associated linear system
yields u = 0 and hence ∇f (v) = 0. If λ (cid:62) ||X (cid:62)y||∞, then u = v = 0 corresponds
to a global minimum, otherwise, it is clear to see that there exists g such that
1 − ||ξg||2 < 0 and v = 0 is a strict saddle point.
Remark 2. Since f ∈ C∞, it is Lipschitz smooth on any bounded domain. As
mentioned, f is coercive when R is coercive, and hence, its sublevel sets are
bounded. So, for any descent algorithm, we can apply results based on ∇kf
being Lipschitz smooth for all k.
Remark 3. The nondegeneracy condition that ||ξg||2 < 1 outside the support of v
and invertibility of (X (cid:62)
g Xh)g,h∈J is often used to derive consistency results [4, 62].
By Proposition 1, we see that this condition guarantees that the Hessian of f is
positive deﬁnite at the minimum, and hence, combining with the smoothness
properties of f explained in the previous remark, BFGS is guaranteed to converge
superlinearly for starting points suﬃciently close to the optimum [43, Theorem
6.6].

4 Numerical experiments

In this section, we use L-BFGS [13] to optimise our bilevel function f and we
denote the resulting algorithm “Noncvx-Pro”. Throughout, the inner problem is
solved exactly using either a full or a sparse Cholesky solver. One observation
from our numerics below is that although Noncvx-Pro is not always the best
performing, unlike other solvers, it is robust to a wide range of settings: for
example, our solver is mostly unaﬀected by the choice of λ while one can observe
in Figures 2 and 3 that this has a large impact on the proximal based methods
and coordinate descent. Moreover, Noncvx-Pro is simple to code and rely on
existing robust numerical routines (Cholesky/ conjugate gradient + BFGS)
which naturally handle sparse/implicit operators, and we thus inherit their nice
convergence properties. All numerics are conducted on 2.4 GHz Quad-Core Intel
Core i5 processor with 16GB RAM. The code to reproduce the results of this
article is available online1.

4.1 Lasso

We ﬁrst consider the Lasso problem where R(β) = (cid:80)n

i=1 |βi|.

1 https://github.com/gpeyre/2021-NonCvxPro

9

Datasets. We tested on 8 datasets from the Libsvm repository2. These
datasets are mean subtracted and normalised by m.

Solvers. We compare against the following 10 methods:
1. 0-mem SR1: a proximal quasi newton method [7].

2. FISTA w/ BB: FISTA with Barzilai–Borwein stepsize [5] and restarts [45].

3. SPG/SpaRSA: spectral projected gradient [58].

4. CGIST: an active set method with conjugate gradient iterative shrink-

age/thresholding [27].

5. Interior point method: from [35].

6. CELER: a coordinate descent method with support pruning [39].

7. Non-cvx-Alternating-min: alternating minimisation of u and v in G from (3)

[32].

8. Non-cvx-LBFGS: Apply L-BFGS to minimise the function (u, v) (cid:55)→ G(u, v)

in (3).

9. L-BFGS-B [13]: apply L-BFGS-B under positivity constraints to minu,v∈Rn

i vi + 1

2λ ||X(u − v) − y||2

(cid:80)
. This is the standard approach for applying L-
BFGS to (cid:96)1 minimisation and corresponds to splitting β into its positive and
negative parts.

2

(cid:80)

i ui+

+

10. Quad-variational: Based on our idea of Noncvx-Pro, another natural (and to
our knowledge novel) approach is to apply L-BFGS-B to the bilevel formulation
of (1) without nonconvex reparametrization. Indeed, by applying (1) and using
(cid:80)
convex duality, the Lasso can solved by minimizing g(η) (cid:44) maxα∈Rm
i ηi −
i ηi|(cid:104)xi, α(cid:105)|2 + (cid:104)α, y(cid:105). The gradient of g is g(η) = λ
λ
2 ||α||2 − 1
λ |X (cid:62)α|2
where | · |2 is in a pointwise sense and α maximises the inner problem, and
we apply L-BFGS-B with positivity constraints to minimise g.

2 − 1

(cid:80)

1
2

2

Experiments. The results are shown in Figure 2 (with further experiments in
the appendix). We show comparisons at diﬀerent regularisation strengths, with
λ∗ being the regularisation parameter found by 10 fold cross validation on the
mean squared error, and λmax = ||X (cid:62)y||∞ is the smallest parameter at which
the Lasso solution is guaranteed to be trivial.

4.2 Group Lasso

The multi-task Lasso [29] is the problem (7) where one minimises over
j=1 ||βj||2 with βj ∈ Rq
in terms

β ∈ Rn×q, the observed data is y ∈ Rm×q and R(β) = (cid:80)n
denotes the jth row of the matrix β and the loss function is 1
of the the Frobenius norm.

2 ||y − Xβ||2

F

2 https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/

10

a
i
m
e
k
u
e
l

)
9
2
1
7
,
8
3
(

)
3
8
6
,
0
0
0
0
6
(

t
s
i
n
m

λ∗

1
2 λmax

1
10 λmax

1
50 λmax

Figure 2: Lasso comparisons with diﬀerent regularisation parameters.

Datasets. We benchmark our proposed scheme on a joint MEG/EEG data
from [41]. X denotes the forward operator with n = 22494 source locations, and
is constructed from 301 MEG sensors and 59 EEG sensors, so that m = 360.
The observations y ∈ Rm×q represent time series measurements at m sensor with
q = 181 timepoints each. The β corresponds to the source locations which are
assumed to remain constant across time.

Solvers. We perform a comparison against FISTA with restarts and BB step,
the spectral projected gradient method SPG/SpaRSA, and CELER. Since n is
much larger than q and m, we use for NonCvx-Pro the saddle point formulation
(8) where the maximisation is over α ∈ Rm×q. Computation of α in ∇f (v)
involves solving (Idm + 1
λ X diag(v2)X (cid:62))α = y., that is, q linear systems each of
size m.

Experiments. Figure 3 displays the objective convergence plots against run-
ning time for diﬀerent λ: λ = λmax/r with λmax = maxi ||X (cid:62)
i y||2 for r =
10, 20, 50, 100. We observe substantial performance gains over the benchmarked
methods: In MEG/EEG problems, the design matrix tends to exhibit high
correlation of columns and proximal-based algorithms tend to perform poorly
here. Coordinate descent with pruning is known to perform well here when
the regularisation parameter large [39], but its performance deteriorates as λ
increases.

4.3 Trace norm

In multi-task feature learning [1], for each task t = 1, . . . , T , we aim to ﬁnd
ft : Rn → R given training examples (xt,i, yt,i) ∈ Rn × R for i = 1, . . . , mt.
One approach is to jointly solve these T regression problems by minimising (5)
where R is the trace norm (also called “nuclear norm”), r = T , y = (yt)T
,
t=1
A(B) = (XtBt)T
with Xt being the matrix with ith row as xt,i and Bt being

t=1

11

λ = 1

10 λmax

λ = 1

20 λmax

λ = 1

50 λmax

λ = 1

100 λmax

Figure 3: Comparisons for multitask Lasso on MEG/EEG data

the tth column of the matrix B. Note that letting ut ∈ Rn denote the tth
column of U , the computation of U in ∇f (V ) involves solving T linear systems
t yt. Here, the trace norm encourages the tasks
(λIdn + V (cid:62)X (cid:62)
to share a small number of linear features.

t XtV )ut = V (cid:62)X (cid:62)

Datasets. We consider the three datasets commonly considered in previous
works. The Schools dataset 3 from [1] consists of the scores of 15362 students
from 139 schools. There are therefore T = 139 tasks with 15362 = (cid:80)
t mt
data points in total, and the goal is to map n = 27 student attributes to exam
performance. The SARCOS dataset 4 [60, 61] has 7 tasks, each corresponding
to learning the dynamics of a SARCOS anthropomorphic robot arms. There are
n = 21 features and m = 48, 933 data points, which are shared across all tasks.
The Parkinsons dataset [56] 5 which is made up of m = 5875 datapoints from
T = 42 patients. The goal is to map n = 19 biomarkers to Parkinson’s disease
symptom scores for each patient.

Solvers. Figure 4 reports a comparison against FISTA with restarts and
IRLS. The IRLS algorithm for (5) is introduced in [1] (see also [3]), and applies
alternate minimisation after adding the regularisation term ελ tr(Z −1)/2 to
(4). The update of B is a simple least squares problem while the update for
Z is Z ← (BB(cid:62) + εId) 1
2 . Our nonconvex approach has the same per-iteration
complexity as IRLS, but one advantage is that we directly deal with (5) without
any ε regularisation.
Remark 4. Quad-variational mentioned in Section 4.1 does not extend to the
trace norm case, since the function g would be minimised over Sn
, for which the
+
application of L-BFGS-B is unclear. For this reason, bilevel formulations for the
trace norm [48] have been restricted to the use of ﬁrst order methods.

Experiments. For each dataset, we compute the regularisation parameter
λ∗ by 10-fold cross-validation on the RMSE averaged across 10 random splits.
Then, with this regularisation parameter, we compare Non-convex-pro, FISTA

3 https://home.ttic.edu/~argyriou/code/ 4 http://www.gaussianprocess.org/gpml/data/
5 http://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring

12

Schools

Parkinsons

SARCOS

Figure 4: Comparisons for multi-task feature learning, IRLS-d corresponds to
IRLS with ε = 10−d.

and IRLS with diﬀerent choices of ε. The convergence plots are show in Figure
4. We observe substantial computational gains for the Schools and Parkinson’s
dataset. For the SARCOS dataset, IRLS performed the best, and even though
Nonconvex-pro is comparatively less eﬀective here, although we remark that the
number of tasks is much smaller (T = 7) and the recorded times are much shorter
(less than 0.2s). Further numerical illustrations with synthetic data are shown in
the appendix – our method is typically less sensitive to problem variations.

4.4 Constraint Group Lasso and Optimal Transport

A salient feature of our method is that it can handle arbitrary small reg-
ularization parameter λ and can even cope with the constrained formulation,
when λ = 0, which cannot be tackled by most state of the art Lasso solvers.
To illustrate this, we consider the computation of an Optimal Transport (OT)
map, which has recently gained a lot of attention in ML [47]. We focus on
the Monge problem, where the ground cost is the geodesic distance on either a
graph or a surface (which extends original Monge’s problem where the cost is
the Euclidean distance). This type of OT problems has been used for instance
to analyze and process brain signals in M/EEG [30], for application in computer
graphics [55] and is now being applied to genomic datasets [54]. As explained for
instance in [53, Sec.4.2], the optimal transport between two probability measures
a and b on a surface can be computed by advecting the mass along a vector ﬁeld
v(x) ∈ R3 (tangent to the surface) with minimum vectorial L1 norm (cid:82) ||v(x)||dx
(where dx is the surface area measure) subject to the conservation of mass
div(v) = a − b. Once discretized on a 3-D mesh, this boils down to solving
a constrained group Lasso problem (P0) where βg ∈ R3 is a discretization of
v(zg) at some vertex zg of the mesh, X is a ﬁnite element discretization of the
divergence using ﬁnite elements and y = a − b. The same applies on a graph,
in which case the vector ﬁeld is aligned with the edge of the graph and the
divergence is the discrete divergence associated to the graph adjacency matrix,
see [47]. This formulation is often called “Beckmann problem”.

13

Figure 5: Resolution of Beckmann problem on a graph (left) and a 3-D mesh
(right). The probability distributions a and b are displayed in blue and red and
the optimal ﬂow β is represented as green segments (on the edge of the graph
and on the faces of the mesh). The convergence curves display the decay of
log10(||βt − β(cid:63)||1) during the iterations of the algorithms (DR=Douglas-Rachford,
PD=Primal-Dual) as a function of time t in second.

Datasets. We consider two experiments: (i) following [30] on a 3-D mesh of
the brain with n = 20000 vertices with localized Gaussian-like distributions a
and b (in blue and red), (ii) a 5-nearest neighbors graph in a 7-dimensional
space of gene expression (corresponding to 7 diﬀerent time steps) of baker’s
yeast, which is the dataset from [20]. The n = 614 nodes correspond to the most
active genes (maximum variance across time) and this results in a graph with
1927 edges. The distributions are synthetic data, where a is a localized source
whereas b is more delocalized.

Solvers. We test our method against two popular ﬁrst order schemes: the
Douglas-Rachford (DR) algorithm [37, 16] (DR) and the primal-dual scheme
of [14]. DR is used in its dual formulation (the Alternating Direction Method of
Multipliers – ADMM) but on a re-parameterized problem in [55]. The details of
these algorithms are given in Appendix E.

Experiments. Figure 5 shows the solution of this Beckmann problem in these
two settings. While DR has the same complexity per iteration as the computation
of the gradient of f (resolution of a linear system), a chief advantage of PD
is that it only involves the application of X and X (cid:62) et each iterations. Both
DR and PD have stepping size parameters (denoted µ and σ) which have been
tuned manually (the rightmost ﬁgure shows two other sub-optimal choices of
parameters). In contrast, our algorithm has no parameter to tune and is faster
than both DR and PD on these two problems.

5 Conclusion

Most existing approaches to sparse regularisation involve careful smoothing
of the nonsmooth term, either by proximal operators or explicit regularisation
as in IRLS. We propose a diﬀerent direction: a simple reparameterization leads
to a smooth optimisation problem, and allows for the use of standard numerical

14

5101520253035-3-2.5-2-1.5-1-0.500.5NonCvx-ProDR,=0.1PD,=0.150100150200250-2.5-2-1.5-1-0.500.51NonCvx-ProDR,=0.001DR,=0.01DR,=0.05PD,=0.1PD,=1PD,=10tools, such as BFGS. Our numerical results demonstrate that this approach is
versatile, eﬀective and can handle a wide range of problems.

We end by making some remarks on possible future research directions. The
application of our method to other loss functions requires the use of an inexact
solver for the inner problems, and controlling the impact of its approximation
is an interesting avenue for future work. Furthermore, it is possible that one
can obtain further acceleration by combining with screening rules or active set
techniques.

Acknowledgments

The work of G. Peyré was supported by the French government under
management of Agence Nationale de la Recherche as part of the “Investissements
d’avenir” program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Insti- tute) and
by the European Research Council (ERC project NORIA).

A Pseudocode for gradient descent implementa-

tion

For concreteness, we write down in Algorithm 1 the gradient descent algorithm

for solving

min
β∈Rn

1
2λ

||Xβ − y||2

2 + ||β||1,

where we recall that X ∈ Rm×n. The choice of λ = 0 corresponds to the
Basis-pursuit setting. Note that ∇f (βt) = gt is computed either as in line 5 or
line 9 of the algorithm and one can use these computations for any gradient
based algorithm (e.g. BFGS). Note also that this is simply gradient descent on
a smooth function, and one can apply typical methods to choosing the stepsize

15

γk, such as the Barzilai-Borwein stepsize [5].

Algorithm 1: Gradient descent implementation of Ncvx-Pro for solving
Lasso.
1 initialization v0 ∈ Rn (with no zero entries), stepsize γt > 0;

Result: βt

2 while not converged do
3

if n (cid:54) m and λ > 0 then

ut = − (cid:0)diag(vt)X (cid:62)X diag(vt) + λId(cid:1)−1 (cid:0)vt (cid:12) X (cid:62)y(cid:1);
λ ut (cid:12) X (cid:62)(Xut (cid:12) vt − y);
gt = vt (cid:12) vt + 1
βt = ut (cid:12) vt;

else

αt = − (cid:0)X diag(vt (cid:12) vt)X (cid:62) + λId(cid:1)−1
gt = vt (cid:12) vt − vt (cid:12) |X (cid:62)αt|2;
βt = −vt (cid:12) vt (cid:12) X (cid:62)αt;

y;

4

5

6

7

8

9

10

11

end
vt+1 = vt − γtgt

12
13 end

B Proofs and additional results for Section 2

Proof to Theorem 1. To show that i) implies ii), recall that a convex, proper
and lower semicontinuous function −ϕ can be written in terms of its convex
conjugate which has domain Rd
. By writing β2 (cid:44) β (cid:12) β, using the deﬁnition of
−
R, we have

−R(β2) = −ϕ(β2) = sup
v(cid:54)0

(cid:104)β2, v(cid:105) − (−ϕ)∗(v) = − inf
u(cid:62)0

(cid:104)β2, u(cid:105) + (−ϕ)∗(−u).

which is ii) with ψ(u) (cid:44) (−ϕ)∗(−u) as required.
Conversely, if R is of the form in ii), then

R(β) = inf
u∈Rn
+

(cid:104)u, β2(cid:105) + ψ(u) = − sup
u∈Rn
+

−(cid:104)u, β2(cid:105) − ψ(u),

so R(β) = −ψ∗(−β (cid:12) β) and −ψ∗(−·) is clearly a proper, upper semicontinuous,
concave function.

For the expression of ψ when R is a norm,from the above, we know that
ψ = (−ϕ)∗(−z), and recall that for any norm, R(β) = maxR∗(w)(cid:54)1(cid:104)w, β(cid:105). So,

ψ(z) = max
u(cid:62)0

(cid:104)−u, z(cid:105) + ϕ(u)

= max

(cid:104)−β2, z(cid:105) + ϕ(β2) = max

(cid:104)−β2, z(cid:105) + R(β)

β

= max

(cid:104)−z, β2(cid:105) + max

R∗(w)(cid:54)1

(cid:104)β, w(cid:105) = max

R∗(w)(cid:54)1

1
4

(cid:88)

i

w2
i
zi

,

β

β

16

where in the line, we swapped the two maximums and used the optimality condi-
i ηi.
tion over β which is 2β(cid:12)z = w. That is, h(η) = 2ψ(− 1

2η ) = maxR∗(w)(cid:54)1
To derive the identity for R(β)2, by the Cauchy-Schwarz inequality

i w2

(cid:80)

R(β)2 = sup

R∗(w)(cid:54)1

|(cid:104)β, w(cid:105)|2 (cid:54) sup

R∗(w)(cid:54)1

(cid:32)

(cid:88)

β2
i ηi

(cid:33) (cid:32)

(cid:88)

i

i

(cid:33)

w2
i
ηi

= 4ψ(η)

(cid:88)

β2
i ηi

i

for all η > 0. Therefore,

R(β)2 (cid:54)

inf
η(cid:62)0,ψ(η)(cid:54) 1
4

(cid:88)

β2
i ηi

i

= sup
λ>0

inf
η(cid:62)0

= sup
λ>0

−

λ
4

λ(ψ(η) −

+ λR(β/

1
4
√

(cid:88)

) +

β2
i ηi

i

λ) = sup
λ>0

−

√

λ
4

+

λR(β) = R(β)2.

where we used the identity λR(β/
one positive homogeneous.

√

λ) = (cid:80)

i β2

i ηi + λψ(η) and the fact that R is

We derive some properties of the function h:

Lemma 1. Consider the function ϕ and ψ from Theorem 1. If ϕ : [0, ∞) →
[L, U ], where L > −∞ and U ∈ R ∪ {+∞}, then ψ is an decreasing function
with domain contained in [0, ∞), taking values in [L, U ]. If R is coercive, then
lim||z||→0 ψ(z) = +∞.

Proof to Lemma 1. Let ϕ : [0, ∞) → [L, U ], where L > −∞ and U ∈ R ∪ {+∞}.
We describe the properties of the function ψ(z) = (−ϕ)∗(−z) = supu(cid:62)0(cid:104)z, −u(cid:105) +
ϕ(u).

(i) dom(ψ) ⊂ [0, ∞): since ϕ is bounded below, it is clear that for z < 0,

supu(cid:62)0(cid:104)z, −u(cid:105) + ϕ(u) = +∞.

(ii) ψ(0) = supu(cid:62)0 ϕ(u) = U .

(iii) Suppose M (cid:44) sup {v \ v ∈ ∂ϕ(u), u (cid:62) 0} < ∞. Then, for all z (cid:62) M ,
−z + ∂ϕ(u) < 0 for all u (cid:62) 0 and hence, ψ(z) = ϕ(0) (cid:62) L. Therefore,
ψ takes values in [L, U ]. So, if M is ﬁnite, then one can restrict the
optimisation over z to values in [0, M ].

(iv) ψ is decreasing: By Danskin’s theorem [8, Prop. B.25], for z ∈ {v \ v ∈ ∂ϕ(u), u (cid:62) 0},

∂ψ(z) = (cid:8)−u \ u ∈ argminu(cid:62)0(cid:104)u, −z(cid:105) + ϕ(u)(cid:9) ⊂ (−∞, 0).

17

B.1 Functions on matrices

We have the following result for matrix valued functions. Let Sn
+

set of symmetric positive semideﬁnite matrices.

denote the

Theorem 4. Let R : Rn×m → R. The following are equivalent

i) R(B) = ϕ(BB(cid:62)) where ϕ is a proper concave upper semi-continuous

function with domain Sn
+

.

ii) There exists a convex function ψ such that R(B) = minZ∈Sn

+

tr(B(cid:62)ZB) +

ψ(Z).

Moreover, we have ψ(Z) = (−ϕ)∗(−Z). If R is a norm, then ψ can be written
as

ψ(Z) = max

R∗(W )(cid:54)1

1
4

tr(W (cid:62)Z −1W ).

Moreover,

R(B)2 = inf
Z∈Sn
+

(cid:26)

tr(B(cid:62)ZB) \ ψ(Z) (cid:54) 1
4

(cid:27)

.

(13)

(14)

√

√

Nuclear norm R(W ) = tr(
the space of symmetric positive semideﬁnite matrices, ϕ(B) = tr(
and ψ(D) = 1
positive semideﬁnite matrices and ∂A tr(AB) = B.

4 tr(D−1), where we use ∂A tr(

W W (cid:62)) where
√

· is the matrix square root. On
√
B) is concave
A)−1 for all symmetric

A) = (2

√

(Nonconvex) spectral regularisation Given a symmetric psd matrix Z =
i )U (cid:62). For α ∈ (0, 1), consider
U diag(σi)U (cid:62) and α > 0, let Z α (cid:44) U diag(σα
R(W ) = tr((W W (cid:62))α/2) = (cid:80)
where σi are the singular values of W . Then,
given a symmetric psd matrix, ϕ(Z) = tr(Z α/2) which is concave [9, Thm 4.2.3]
and

i σα
i

ψ(Z) = min
V ∈Sd
+

− tr(V Z) + ϕ(V ) =

min
U ∈Od,σ∈Rd
+

− tr(diag(σ)U ZU (cid:62)) +

(cid:88)

σα/2
i

i

(cid:88)

ˆZiiσi +

(cid:88)

σα/2
i

where

ˆZ = U ZU (cid:62)

i
α
ˆZ
α−2
ii

i

where

ˆZ = U ZU (cid:62) and U ∈ Od

= min

U ∈Od,σ(cid:62)0

−

= Cα min
U ∈Od

(cid:88)

i
α
α−2 )

= Cα tr(Z

Therefore,

R(B) = inf
Z∈Sd
+

tr(B(cid:62)ZB) + Cα tr(Z

α
α−2 ).

Proof of Theorem 4. To derive (13),

ψ(Z) = max
U ∈Sn
+

−(cid:104)U, Z(cid:105) + ϕ(U ) = max
V ∈Rn

−(cid:104)V V (cid:62), Z(cid:105) + ϕ(V V (cid:62))

= max
V ∈Rn

−(cid:104)V V (cid:62), Z(cid:105) + R(V )

18

Then, (13) follows, since by convex duality and deﬁnition of R∗,

max
R∗(W )(cid:54)1

1
4

tr(W (cid:62)Z −1W ) = max

R∗(W )(cid:54)1

max
V

(cid:104)−Z, V V (cid:62)(cid:105) + (cid:104)V, W (cid:105) = max

(cid:104)−Z, V V (cid:62)(cid:105) + R(V ).

V

Finally, by the submultiplicative property of the Frobenius norms, for all

Z ∈ Sn
+

with Z (cid:31) 0,

R(B)2 = sup

R∗(W )(cid:54)1

|(cid:104)Z −1/2W, Z 1/2B(cid:105)|2 (cid:54) sup

R∗(W )(cid:54)1

tr(W (cid:62)Z −1W ) tr(B(cid:62)ZB)

= 4ψ(W ) tr(B(cid:62)ZB)

It follows that just as in the proof of Theorem 1 that

R(B)2 (cid:54) inf
Z∈Sn
+

tr(B(cid:62)ZB) where ψ(Z) (cid:54) 1
4

.

C Proof of Section 3

Proof of Proposition 3. Let

G(u, v) (cid:44) 1
2

||u||2 +

1
2

||v||2

2 +

1
2λ

||X(v (cid:12)G u) − y||2
2.

We know from Theorem 2 that f is diﬀerentiable with

∇f (v) = ∂vG(u, v) = v + λ−1u (cid:12) X (cid:62)(Xv (cid:12)G u − y)

where u = argminu G(u, v). In particular,

0 = ∂uG(u, v) = u + λ−1X (cid:62)(X(v (cid:12)G u) − y).

Since ∂uuG = λ−1(vgX (cid:62)
g Xhvh)g,h + Id is invertible, by the implicit function
theorem u is a smooth function of v with ∂vu = [∂uuG]−1∂vuG. In particular,

∇2f (v) = ∂vvG(u, v) + ∂uvG(u, v)∂vu.

So, the Hessian of f is the Schur complement of the Hessian of G (as also

observed in [52, 57]). We write ∇2G =

(cid:19)

(cid:18) A B
B(cid:62) D

where

A (cid:44) ∂vvG = λ−1 (cid:0)u(cid:62)
B (cid:44) ∂uvG = λ−1 (cid:0)(u(cid:62)
D (cid:44) ∂uuG = λ−1(vgX (cid:62)

g X (cid:62)
g,h
g X (cid:62)
g Xhvh)g,h
g Xhvh)g,h + Id

g Xhuh

(cid:1)

+ Id
(cid:1) + diag(ξ(cid:62)
g )

19

λ X (cid:62)(X(u (cid:12) v) − y). Then, ∇2f (t) = A − BD−1B(cid:62). Note that in
where ξ = 1
fact, u is inﬁnitely diﬀerentiable by the implicit function theorem, and so, f is
also inﬁnitely diﬀerentiable.

We now derive a formula for the Hessian of f . By permuting the rows and

columns of ∇2G, we can assume that, letting J denote the support of v,

(cid:18)λ−1 (cid:0)u(cid:62)

g X (cid:62)

A (cid:44)

B (cid:44) λ−1

(cid:18)(cid:0)(u(cid:62)

g X (cid:62)

(cid:19)

(cid:1)

g,h∈J

+ IdJ

g Xhuh
0

0
IdJ c
g Xhvh)g,h∈J + λ diag(ξ(cid:62)
0

g )g∈J

(cid:1)

(cid:19)

λ diag(ξ(cid:62)

0
g )g∈J c

D (cid:44)

(cid:18)λ−1(vgX (cid:62)

g Xhvh)g,h∈J + IdJ

0

(cid:19)

0
IdJ c

Note that A and D is positive deﬁnite. So, ∇2G is positive semideﬁnite if and
only if A − BD−1B(cid:62) is positive semideﬁnite. Note that A − BD−1B(cid:62) is a block
diagonal matrix, with the bottom right block as IdJ c − diag(||ξg||2

To work out the expression for the top left block of A − BD−1B(cid:62), let us
ﬁrst examine the top left block of the matrix B: Note that by deﬁnition of u,

2)g∈J c.

λ−1vgX (cid:62)

g (X(v (cid:12)G u) − y) + ug = 0 =⇒ ∀g ∈ Supp(v), ξg = −

ug
vg

.

Deﬁne the block diagonal matrix Uu/v = diag(ug/vg)g∈J , then U (cid:62)
diag(||ug||2/v2

g)g∈J . The top left block of B is

u/vUu,v =

λ−1(u(cid:62)
= U (cid:62)
u/v
= U (cid:62)
u/v

g X (cid:62)

g Xhvh)g,h∈J + diag(ξ(cid:62)
(cid:0)λ−1(vgX (cid:62)
(cid:0)λ−1(vgX (cid:62)
(cid:124)

g Xhvh)g,h∈J + IdJ
g Xhvh)g,h∈J + IdJ

(cid:125)

g )g∈J = λ−1U (cid:62)
(cid:1) − U (cid:62)
(cid:1)
−2U (cid:62)

u/v(vgX (cid:62)
g Xhvh)g,h∈J + diag(ξ(cid:62)
g )
u/v + diag(ξ(cid:62)
g )
u/vH − 2U (cid:62)
u/v = U (cid:62)

u/v.

(cid:123)(cid:122)
(cid:44)H

By our notation of H, the top left block of D−1 is H −1. So, the top left block
of BD−1B(cid:62) is

(U (cid:62)

u/vH−2U (cid:62)

u/v)(Uu/v−2H −1Uu/v) = U (cid:62)

u/vHUu/v−4U (cid:62)

u/vUu/v+4U (cid:62)

u/vH −1Uu/v.

and the top left block of A − BD−1B(cid:62) is

u/vUu/v + 4U (cid:62)

IdJ − U (cid:62)
= diag(1 − ||ξg||2

u/vUu/v − 4U (cid:62)

u/vH −1Uu/v

2)g∈J + 4U (cid:62)

u/vUu/v − 4U (cid:62)

u/vH −1Uu/v.

Note that ||Id − H −1|| (cid:54) 1, and given w ∈ R|G|,

(cid:104)∇2f (v)w, w(cid:105) =

(cid:88)

g∈G
(cid:54) (cid:88)
g∈G

(1 − ||ξg||2

2)w2

g + 4(cid:104)(Id − H −1)(w (cid:12)G ξ), w (cid:12)G ξ(cid:105)

(1 − ||ξg||2

2)w2

g + 4||ξg||2

2w2
g,

20

. We have a global Lipschitz
and it follows that ||∇2f (v)|| (cid:54) 1 + 3 maxg∈G ||ξg||2
2
bound on the gradient of f if ||ξg|| (cid:54) L for some L, which is true because for
each v, u minimises

min
u

1
2

||u||2

2 +

1
2λ

||X(v (cid:12)G u) − y||2
2

(cid:54) ||y||2
2
2λ

So, maxg∈G ||ξg||2 (cid:54) ||y||2 maxg∈G ||Xg||/λ, and ||∇2f (v)|| (cid:54) 1+3||y||2 maxg∈G ||Xg||2/λ2.

At stationary points, we also have

g X (cid:62)
u(cid:62)

g (X(v (cid:12)G u) − y) + λvg = 0 =⇒ ∀g ∈ Supp(v), u(cid:62)

g ξg = −vg.

Together, this means that at stationary points, ||ug||2 = v2
g
Therefore, the top left block of A − BD−1B(cid:62) becomes

and U (cid:62)

u/vUu/v = IdJ .

4IdJ − 4U (cid:62)
u/v

g Xhvh)g,h∈J + IdJ
g Xhvh)g,h∈J + IdJ (cid:23) (1+µ)Id, where µ = min Eig (cid:0)λ−1(vgX (cid:62)

since λ−1(vgX (cid:62)
Therefore, the smallest eigenvalue of A − BD−1B is at least

Uu/v (cid:23) 0

(cid:0)λ−1(vgX (cid:62)

(cid:1)−1

g Xhvh)g,h∈J

(cid:1).

(cid:18)

min

4µ/(1 + µ), min
g(cid:54)∈J

(cid:19)

(1 − ||ξg||2)

(cid:54) min
g(cid:54)∈J

(1 − ||ξg||2)

Moreover, if A − BD−1B (cid:23) 0, then ming(cid:54)∈J (1 − ||ξg||2) (cid:62) 0, which implies
that (u, v) deﬁnes a minimiser to the original group Lasso problem, hence, (u, v)
deﬁnes a global minimum. Therefore, every stationary point is either a global
minimum or a strict saddle point.

Remarks on the comparison with ISTA in the introduction To explain
the observed behavior, note that gradient descent for f with stepsize γ reads
vk+1 = vk − γ∇f (vk) = vk(1 − γ (cid:0)1 − |ξk|2(cid:1)) where ξk (cid:44) 1
λ X (cid:62)(Xvk (cid:12) uk − y)
(see Proposition 3). Note that if β∗ is a minimiser, then ξ∗ (cid:44) 1
λ X (cid:62)(Xβ∗ − y)
satisﬁes ||ξ∗||∞ (cid:54) 1 and the set {i \ |(ξ∗)i| = 1} is often called the extended
support and contains the support of β∗. It is clear that we can expect coeﬃcients
outside the extended support to (eventually) decay to 0 geometrically. Since ξk is
uniformly bounded (see Proposition 3), for γ suﬃciently small, vk never changes
sign and any sign change in the iterate βk (cid:44) vk (cid:12) uk is due to uk. In contrast,
the ISTA dynamics is βk+1 = sign(βk − γξk) max (|βk − γξk| − γ, 0). Due to the
thresholding operation, a coeﬃcient of βk is initialised with the wrong sign will
spend some iterations as 0 before correcting its sign.

D Supplementary to Section 4

D.1 Remarks on numerical experiments

Initialisation points We generated random initialisation point from the nor-
mal distribution. In our experiments, methods which are not reparameterized

21

T = 5

T = 10

T = 100

T = 500

Figure 6: Multitask feature learning (nuclear norm regularisation) with synthetic
data. We have T tasks, n = 30 features and m = 10, 000 samples in total. The
matrix Xt associated to each task has iid entries drawn uniformly at random
from [0, 1]. See the description in Section 4.3.

(e.g. the proximal methods), are given the same random initial point, while
reparameterized methods have their own random initialisation, since some of
these require positive starting points and some need double the number of vari-
ables. We ﬁnd that the comparisons are not much aﬀected by the choice of
initial points.

Inversion of linear systems As mentioned in Corollary (1), for the Lasso,
when computing the gradient of f , one can either invert a n × n linear system
or an m × m linear system. The same applies to Quad-variational, since the
solution to the inner maximisation problem is, by the Woodbury identity,

α = (λIdm + XηX (cid:62)

η )−1y =

1
λ

y −

1
λ

Xη(λIdn + X (cid:62)

η Xη)−1(X (cid:62)

η y)

where Xη = X diag(
out, we simply use backslash in MATLAB for the matrix inversion.

η), with the correspondence that β = η (cid:12) X (cid:62)α. Through-

√

Implementation details All numerics are done in Matlab with the exception
of CELER which is in Python:

• CELER are conducted in Python and we used the code https://mathurinm.

github.io/celer/ provided by the original paper [39]

• 0-mem SR1, FISTA w/ BB and SPG/SpaRSA use the Matlab code from

https://github.com/stephenbeckr/zeroSR1 of the paper [7].

• Interior point method uses the Matlab code https://web.stanford.edu/

~boyd/l1_ls/ of [35].

• CGIST uses the Matlab code http://tag7.web.rice.edu/CGIST.html

of [27].

• We had our own implementation of Non-cvx-Alternating-min and IRLS.

• Quad-variational, Non-cvx-LBFGS and Noncvx-Pro are written in Matlab
using the L-BFGS-B solver from https://github.com/stephenbeckr/

22

L-BFGS-B-C which is a Matlab wrapper for C code converted from the
well known Fortran implementation of [13].

D.2 Additional examples

Lasso In Figure 7, we show additional numerics for the Lasso, testing against
datasets from the Libsvm repository. The regularisation parameter λ associated
to each plots is found by cross validation on the mean squared error.

Group Lasso In Figure 8, we show additional numerics for the multitask
Lasso setup described in Section 4.2. We test on two synthetic datasets of size
(m, n, q) = (300, 1000, 100) with 5 relevant features and (m, n, q) = (50, 1200, 20)
with 10 relevant features. The data matrix X has entries drawn from a normal dis-
tribution. We also test on a MEG/EEG dataset with (m, n, q) = (305, 22494, 85)
from the MNE repository https://mne.tools/0.11/manual/datasets_index.
html. We display convergence plots for diﬀerent regularisation parameters.

Trace norm In Figure 6 we show additional numerics for the multifeature
learning setup described in Section 4.3. The data matrices Xt has entries drawn
uniformly at random from [0, 1]. We consider diﬀerent number of tasks T tasks,
n = 30 features and m = 10, 000 samples in total (the samples are split at
random across the diﬀerent tasks).

E Douglas-Rachford and Primal-Dual Algorithms

We consider the resolution of a constrained group Lasso problem

min
Xβ=y

||β||1,2 =

(cid:88)

g

||βg||2

which we write as the minimization of either F (β) + G(β) (for DR) or F (β) +
G0(Xβ) where F = || · ||1,2, G = ιC where the constraint set is C = {β \ Xβ = y}
and G0 = ι{y}. Here ιC is the convex indicator function of a closed convex set C.
DR and PD are generic algorithm to solve minimization of function of the
form F + G and F + G0 ◦ X when one is able to compute eﬃciently the so-called
proximal operator of the involved functionals, where the proximal operator of
some convex function H and some step size τ (cid:62) 0 is

Proxτ H (β) (cid:44) argmin

β(cid:48)

1
2

||β − β(cid:48)||2

2 + H(β(cid:48)).

In our special case, one has

Proxτ F (β) =

(cid:16)

max(||βg||−τ, 0)

(cid:17)

βg
||βg||

g

and Proxτ G0 (β) = y.

, Proxτ G(β) = β+X (cid:62)(XX (cid:62))−1(y−Xβ),

23

e
n
o
l
a
b
a

)
8
,
7
7
1
4
(

g
n
i
s
u
o
h

)
3
1
,
6
0
5
(

a
t
a
d
a
c

)
8
,
0
4
6
0
2
(

)
2
2
1
,
6
9
6
2
2
(

)
0
0
3
,
9
4
7
9
4
(

a
8
a

a
8
w

a
i
m
e
k
u
e
l

)
9
2
1
7
,
8
3
(

)
3
8
6
,
0
0
0
0
6
(

t
s
i
n
m

4
-
t
c
e
n
n
o
c

)
6
2
1
,
7
5
5
7
6
(

λ∗

1
2 λmax

1
10 λmax

1
50 λmax

Figure 7: Comparisons of Lasso with diﬀerent regularisation parameters on
datasets from Libsvm. The ﬁrst column shows the optimal regularisation pa-
rameter λ∗ found by cross validation. The second, third and fourth columns
correspond to diﬀerent fractions of λmax = ||X (cid:62)y||∞ which is the parameter for
which the Lasso solution is identically zero. The smaller this fraction, the less
sparse the solution.

24

)
0
0
1
,
0
0
0
1
,
0
0
3
(

)
0
2
,
0
0
2
1
,
0
5
(

)
5
8
,
4
9
4
2
2

,
5
0
3
(

λ = 1

10 λmax

λ = 1

20 λmax

λ = 1

50 λmax

λ = 1

100 λmax

Figure 8: Comparisons for multitask Lasso at diﬀerent regularisation strengths.
The problem sizes (m, n, q) are displayed on the left of each row. The top two
rows are synthetic datasets generated by random Gaussian variables with 5 and
10 active features respectively. The last row corresponds to a MEG/EEG dataset
from the MNE website

25

DR algorithm. We denoted the reﬂected proximal map as rProxτ H (β) =
2 Proxτ H (β) − β. For some step size µ > 0 and weight 0 < γ < 2 (which is set to
γ = 1 in our experiments), the iterates (βk)k of DR are βk (cid:44) ProxµG(zk) where
zk satisﬁes

zk+1 = (1 −

γ
2

)zk +

γ
2

rProxµF (rProxµG(zk)).

PD algorithm. Denoting G∗
of G0, the PD iterations read

0(u) = supβ(cid:104)β, u(cid:105)−G0(β) the Legendre transform

(wk + σX( ˜βk))

wk+1 = ProxσG∗
βk+1 = Proxτ F (βk − τ K (cid:62)(wk+1))
˜βk+1 = βk+1 + θ(βk+1 − βk).

0

0(u) = (cid:104)u, y(cid:105) so that ProxσG∗

In our case, one has G∗
(u) = u − τ y. Convergence
of the PD algorithm is ensure as long as τ σ||X||2 < 1 where ||X|| is the operator
In our
norm, and 0 < θ (cid:54) 1 (we use θ = 1 in the numerical simulation).
numerical simulation, we set τ σ||X||2 = 0.9 and tuned the value of the parameter
σ.

0

F Non-convex optimisation with (cid:96)q quasi-norms

As mentioned, for q ∈ (0, 2), R(β) (cid:44) ||β||q

j |βj|q has a quadratic
variational form. In the case where q > 2/3, we have the following bilevel smooth
formulation:

q = (cid:80)

Corollary 2. When q > 2/3, (Pλ) is equivalent to

inf
v∈Rn

f (v) (cid:44) inf
u∈Rn

1
2

||u||2

2 +

Cq
2

n
(cid:88)

j=1

|vj|

2q
2−q +

1
λ

L(X(u (cid:12) v), y)

(15)

where Cq = (2 − q)qq/(2−q). The function f is diﬀerentiable function provided
that q > 2/3. Its gradient can be computed as in Theorem 2.

Remark 5 (Existing approaches). Existing approaches to (cid:96)q minimisation are
typically iterative thresholding/proximal algorithms [12], IRLS [15, 18] or it-
Iterative thresholding algorithms are
erative reweighted (cid:96)1 algorithms [22].
applicable only for the case where the loss function is diﬀerentiable, and hence
not applicable for Basis pursuit problems which we describe below. Moreover,
computation of the proximal operation requires solving a nonlinear equation. For
iterative reweighted algorithms, they require gradually decreasing an additional
regularisation parameter ε > 0. This can be problematic in practice and for
ﬁnite ε, one does not solve the original optimisation problem.

26

Remark 6. Since we have a diﬀerentiable unconstrained problem, the problem
(15) can be handled using descent algorithms and convergence analysis is standard.
For example, since f is coercive, for any descent algorithm applied to f , we can
assume that the generated sequence vk is uniformly bounded and ∇f (vk) is also
uniformly bounded. So, by applying standard results [8, Proposition 1.2.1], we
can conclude that all limit points of sequences vk generated by descent methods
under line search on the stepsize are stationary points. In fact, since we have an
unconstrained minimisation problem with a continuously diﬀerentiable f which
is also semialgebraic (for rational q) and hence satisfy the KL inequality [2],
convergence of the full sequence by descent methods with line search can be
guaranteed [44].

F.1 Basis pursuit

In this section, we focus on the basis pursuit problem with q ∈ (2/3, 1),

min
β

||β||q
q

where Xβ = y.

The set of local minimums are all β for which Xβ = y and there exists α such
that (cid:0)X (cid:62)α(cid:1)
i = q|βi|q−1 sign(βi) on the support of β. When q > 2/3 and f is
diﬀerentiable with

∇f (v) = q

2

2−q |v|γ−1 sign(v) − v (cid:12) |X (cid:62)α|2, where γ (cid:44) 2q/(2 − q) > 1.

At a stationary point v, letting β = −v2 (cid:12) X (cid:62)α, we have Xβ = y and ∇f (v) = 0
implies that on the support of v, q|v|2 = |β|2−q and so,

X (cid:62)α = −v−2β = −q sign(β) (cid:12) |β|q−1,

which is precisely the optimality condition of the original problem.

In Figure 9, we show that gradient descent
Illustrations for Basis pursuit
dynamics for f in the case of the indicator function L(·, y) = ι{y} and a random
Gaussian matrix X ∈ R10×20, that is

vk+1 = vk − τ ∇f (vk) = vk − τ

(cid:16)
q

2

2−q |vk|

3q−2

2−q (cid:12) sign(v) − vk (cid:12) |X (cid:62)αk|2(cid:17)

where

X diag(vk (cid:12) vk)X (cid:62)αk = −y.
Observe that as q → 2/3, the evolution paths of vk becomes increasingly linear.
In Figure 10, we follow the experiment setup of [15] and generate 100 problem
instances ( ¯X, ¯y, ¯β). Each problem instance consist of a matrix ¯X ∈ Rm×n with
m = 140 rows and n = 256 columns whose entries are identical independent
distributed Gaussian random variable with mean 0 and variance, a vector ¯β of
size n with K = 40 entries uniformly distributed on {1, . . . , n} and whose nonzero
entries are iid Gaussian with mean 0 and variance 1 and ¯y (cid:44) ¯X ¯β. For each

27

k
β

k
v

q=0.7

q=0.8

q=1

Figure 9: Evolution of 20 coeﬃcients for Basis pursuit with (cid:96)q regularisation.
The same stepsize τ is used for all plots. Top row show the evolution of βk and
the bottom row show the evolution of vk.

Figure 10: Number of successful recovery by (cid:96)q minimisation.

problem, we carry out the following procedure. For each m ∈ {60, . . . , 140} ∩ 2N,
we let X be the matrix from the ﬁrst m rows of ¯X, and y be the ﬁrst m entries
of ¯y. We then compute β by minimising f for this X and y using BFGS with 10
randomly generated starting points and declare “success" if ||β − ¯β||2 (cid:54) 10−3 for
one of these starting points.

References

[1] Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. Convex

multi-task feature learning. Machine learning, 73(3):243–272, 2008.

[2] Hedy Attouch, Jérôme Bolte, and Benar Fux Svaiter. Convergence of
descent methods for semi-algebraic and tame problems: proximal algo-
rithms, forward–backward splitting, and regularized gauss–seidel methods.
Mathematical Programming, 137(1):91–129, 2013.

28

[3] Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozin-
arXiv preprint

ski. Optimization with sparsity-inducing penalties.
arXiv:1108.0775, 2011.

[4] Francis R Bach. Consistency of the group lasso and multiple kernel learning.

Journal of Machine Learning Research, 9(6), 2008.

[5] Jonathan Barzilai and Jonathan M Borwein. Two-point step size gradient

methods. IMA journal of numerical analysis, 8(1):141–148, 1988.

[6] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding
algorithm for linear inverse problems. SIAM journal on imaging sciences,
2(1):183–202, 2009.

[7] Stephen Becker, Jalal Fadili, and Peter Ochs. On quasi-newton forward-
backward splitting: Proximal calculus and convergence. SIAM Journal on
Optimization, 29(4):2445–2481, 2019.

[8] Dimitri P Bertsekas. Nonlinear programming. Journal of the Operational

Research Society, 48(3):334–334, 1997.

[9] Rajendra Bhatia. Positive deﬁnite matrices. Princeton university press,

2009.

[10] Michael J Black and Anand Rangarajan. On the uniﬁcation of line processes,
outlier rejection, and robust statistics with applications in early vision.
International journal of computer vision, 19(1):57–91, 1996.

[11] Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and
statistical learning via the alternating direction method of multipliers. Now
Publishers Inc, 2011.

[12] Kristian Bredies, Dirk A Lorenz, and Stefan Reiterer. Minimization of
non-smooth, non-convex functionals by iterative thresholding. Journal of
Optimization Theory and Applications, 165(1):78–112, 2015.

[13] Richard H Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou Zhu. A limited
memory algorithm for bound constrained optimization. SIAM Journal on
scientiﬁc computing, 16(5):1190–1208, 1995.

[14] Antonin Chambolle and Thomas Pock. A ﬁrst-order primal-dual algorithm
for convex problems with applications to imaging. Journal of mathematical
imaging and vision, 40(1):120–145, 2011.

[15] Rick Chartrand and Valentina Staneva. Restricted isometry properties and
nonconvex compressive sensing. Inverse Problems, 24(3):035020, 2008.

[16] Patrick L Combettes and Jean-Christophe Pesquet. A douglas–rachford
splitting approach to nonsmooth convex variational signal recovery. IEEE
Journal of Selected Topics in Signal Processing, 1(4):564–574, 2007.

29

[17] Patrick L Combettes and Băng C V˜u. Variable metric forward–backward
splitting with applications to monotone inclusions in duality. Optimization,
63(9):1289–1318, 2014.

[18] Ingrid Daubechies, Michel Defrise, and Christine De Mol. An iterative
thresholding algorithm for linear inverse problems with a sparsity constraint.
Communications on Pure and Applied Mathematics: A Journal Issued by
the Courant Institute of Mathematical Sciences, 57(11):1413–1457, 2004.

[19] Ingrid Daubechies, Ronald DeVore, Massimo Fornasier, and C Sinan Gün-
türk. Iteratively reweighted least squares minimization for sparse recovery.
Communications on Pure and Applied Mathematics: A Journal Issued by
the Courant Institute of Mathematical Sciences, 63(1):1–38, 2010.

[20] Joseph L DeRisi, Vishwanath R Iyer, and Patrick O Brown. Exploring
the metabolic and genetic control of gene expression on a genomic scale.
Science, 278(5338):680–686, 1997.

[21] Jim Douglas and Henry H Rachford. On the numerical solution of heat
conduction problems in two and three space variables. Transactions of the
American mathematical Society, 82(2):421–439, 1956.

[22] Simon Foucart and Ming-Jun Lai. Sparsest solutions of underdetermined
linear systems via (cid:96)q-minimization for 0 < q (cid:54) 1. Applied and Computational
Harmonic Analysis, 26(3):395–407, 2009.

[23] Jerome Friedman, Trevor Hastie, and Rob Tibshirani. Regularization paths
for generalized linear models via coordinate descent. Journal of statistical
software, 33(1):1, 2010.

[24] Davi Geiger and Alan Yuille. A common framework for image segmentation.

International Journal of Computer Vision, 6(3):227–243, 1991.

[25] Donald Geman and George Reynolds. Constrained restoration and the
recovery of discontinuities. IEEE Transactions on pattern analysis and
machine intelligence, 14(3):367–383, 1992.

[26] Laurent El Ghaoui, Vivian Viallon, and Tarek Rabbani. Safe feature
elimination for the lasso and sparse supervised learning problems. arXiv
preprint arXiv:1009.4219, 2010.

[27] Tom Goldstein and Simon Setzer. High-order methods for basis pursuit.

UCLA CAM Report, pages 10–41, 2010.

[28] Gene Golub and Victor Pereyra. Separable nonlinear least squares: the
variable projection method and its applications. Inverse problems, 19(2):R1,
2003.

[29] Alexandre Gramfort, Matthieu Kowalski, and Matti Hämäläinen. Mixed-
norm estimates for the m/eeg inverse problem using accelerated gradient
methods. Physics in Medicine & Biology, 57(7):1937, 2012.

30

[30] Alexandre Gramfort, Gabriel Peyré, and Marco Cuturi. Fast optimal
transport averaging of neuroimaging data. In International Conference on
Information Processing in Medical Imaging, pages 261–272. Springer, 2015.

[31] Trevor Hastie, Rahul Mazumder, Jason D Lee, and Reza Zadeh. Matrix
completion and low-rank svd via fast alternating least squares. The Journal
of Machine Learning Research, 16(1):3367–3402, 2015.

[32] Peter D Hoﬀ. Lasso, fractional norm and structured sparse estimation using
a hadamard product parametrization. Computational Statistics & Data
Analysis, 115:186–198, 2017.

[33] Je Hyeong Hong, Christopher Zach, and Andrew Fitzgibbon. Revisiting the
variable projection method for separable nonlinear least squares problems.
In 2017 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pages 5939–5947. IEEE, 2017.

[34] Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I
Jordan. How to escape saddle points eﬃciently. In International Conference
on Machine Learning, pages 1724–1732. PMLR, 2017.

[35] Kwangmoo Koh, Seung-Jean Kim, and Stephen Boyd. An interior-point
method for large-scale l1-regularized logistic regression. Journal of Machine
learning research, 8(Jul):1519–1555, 2007.

[36] Jason D Lee, Ioannis Panageas, Georgios Piliouras, Max Simchowitz,
Michael I Jordan, and Benjamin Recht. First-order methods almost always
avoid saddle points. arXiv preprint arXiv:1710.07406, 2017.

[37] Pierre-Louis Lions and Bertrand Mercier. Splitting algorithms for the
sum of two nonlinear operators. SIAM Journal on Numerical Analysis,
16(6):964–979, 1979.

[38] Morteza Mardani and Georgios B Giannakis. Estimating traﬃc and anomaly
maps via network tomography. IEEE/ACM transactions on networking,
24(3):1533–1547, 2015.

[39] Mathurin Massias, Alexandre Gramfort, and Joseph Salmon. Celer: a fast
solver for the lasso with dual extrapolation. In International Conference on
Machine Learning, pages 3315–3324. PMLR, 2018.

[40] Charles A Micchelli, Jean M Morales, and Massimiliano Pontil. Regularizers
for structured sparsity. Advances in Computational Mathematics, 38(3):455–
489, 2013.

[41] Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, and Joseph Salmon.
Gap safe screening rules for sparse multi-task and multi-class models. arXiv
preprint arXiv:1506.03736, 2015.

31

[42] Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, and Joseph Salmon.
Gap safe screening rules for sparsity enforcing penalties. The Journal of
Machine Learning Research, 18(1):4671–4703, 2017.

[43] Jorge Nocedal and Stephen Wright. Numerical optimization. Springer

Science & Business Media, 2006.

[44] Dominikus Noll and Aude Rondepierre. Convergence of linesearch and trust-
region methods using the kurdyka–łojasiewicz inequality. In Computational
and analytical mathematics, pages 593–611. Springer, 2013.

[45] Brendan O’donoghue and Emmanuel Candes. Adaptive restart for ac-
celerated gradient schemes. Foundations of computational mathematics,
15(3):715–732, 2015.

[46] Razvan Pascanu, Yann N Dauphin, Surya Ganguli, and Yoshua Bengio.
On the saddle point problem for non-convex optimization. arXiv preprint
arXiv:1405.4604, 2014.

[47] Gabriel Peyré, Marco Cuturi, et al. Computational optimal transport:
With applications to data science. Foundations and Trends® in Machine
Learning, 11(5-6):355–607, 2019.

[48] Ting Kei Pong, Paul Tseng, Shuiwang Ji, and Jieping Ye. Trace norm
regularization: Reformulations, algorithms, and multi-task learning. SIAM
Journal on Optimization, 20(6):3465–3489, 2010.

[49] Alain Rakotomamonjy, Francis Bach, Stéphane Canu, and Yves Grandvalet.
Simplemkl. Journal of Machine Learning Research, 9:2491–2521, 2008.

[50] Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factor-
ization for collaborative prediction. In Proceedings of the 22nd international
conference on Machine learning, pages 713–719, 2005.

[51] R Tyrrell Rockafellar and Roger J-B Wets. Variational analysis, volume

317. Springer Science & Business Media, 2009.

[52] Axel Ruhe and Per Åke Wedin. Algorithms for separable nonlinear least

squares problems. SIAM review, 22(3):318–337, 1980.

[53] Filippo Santambrogio. Optimal transport for applied mathematicians.

Birkäuser, NY, 55(58-63):94, 2015.

[54] Geoﬀrey Schiebinger, Jian Shu, Marcin Tabaka, Brian Cleary, Vidya Subra-
manian, Aryeh Solomon, Joshua Gould, Siyan Liu, Stacie Lin, Peter Berube,
et al. Optimal-transport analysis of single-cell gene expression identiﬁes
developmental trajectories in reprogramming. Cell, 176(4):928–943, 2019.

[55] Justin Solomon, Raif Rustamov, Leonidas Guibas, and Adrian Butscher.
Earth mover’s distances on discrete surfaces. ACM Transactions on Graphics
(TOG), 33(4):1–12, 2014.

32

[56] Athanasios Tsanas, Max Little, Patrick McSharry, and Lorraine Ramig.
Accurate telemonitoring of parkinson’s disease progression by non-invasive
speech tests. Nature Precedings, pages 1–1, 2009.

[57] Tristan van Leeuwen and Aleksandr Aravkin. Non-smooth variable projec-

tion. arXiv preprint arXiv:1601.05011, 2016.

[58] Stephen J Wright, Robert D Nowak, and Mário AT Figueiredo. Sparse
reconstruction by separable approximation. IEEE Transactions on signal
processing, 57(7):2479–2493, 2009.

[59] Christopher Zach and Guillaume Bourmaud. Descending, lifting or smooth-
ing: Secrets of robust cost optimization. In Proceedings of the European
Conference on Computer Vision (ECCV), pages 547–562, 2018.

[60] Yu Zhang and Qiang Yang. A survey on multi-task learning. IEEE Trans-

actions on Knowledge and Data Engineering, 2021.

[61] Yu Zhang and Dit-Yan Yeung. A convex formulation for learning task
relationships in multi-task learning. arXiv preprint arXiv:1203.3536, 2012.

[62] Peng Zhao and Bin Yu. On model selection consistency of lasso. The

Journal of Machine Learning Research, 7:2541–2563, 2006.

33

