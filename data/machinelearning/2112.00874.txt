1
2
0
2

c
e
D
1

]

G
L
.
s
c
[

1
v
4
7
8
0
0
.
2
1
1
2
:
v
i
X
r
a

NEURAL STOCHASTIC DUAL DYNAMIC PROGRAMMING

A PREPRINT

Hanjun Dai∗, Yuan Xue∗, Zia Syed, Dale Schuurmans, Bo Dai
Google
{hadai, yuanxue, zsyed, schuurmans, bodai}@google.com

ABSTRACT

Stochastic dual dynamic programming (SDDP) is a state-of-the-art method for solving multi-stage
stochastic optimization, widely used for modeling real-world process optimization tasks. Unfor-
tunately, SDDP has a worst-case complexity that scales exponentially in the number of decision
variables, which severely limits applicability to only low dimensional problems. To overcome this lim-
itation, we extend SDDP by introducing a trainable neural model that learns to map problem instances
to a piece-wise linear value function within intrinsic low-dimension space, which is architected
speciﬁcally to interact with a base SDDP solver, so that can accelerate optimization performance on
new instances. The proposed Neural Stochastic Dual Dynamic Programming (ν-SDDP) continually
self-improves by solving successive problems. An empirical investigation demonstrates that ν-SDDP
can signiﬁcantly reduce problem solving cost without sacriﬁcing solution quality over competitors
such as SDDP and reinforcement learning algorithms, across a range of synthetic and real-world
process optimization problems.

1

Introduction

Multi-stage stochastic optimization (MSSO) considers the problem of optimizing a sequence of decisions over a ﬁnite
number of stages in the presence of stochastic observations, minimizing an expected cost while ensuring stage-wise
action constraints are satisﬁed (Birge and Louveaux, 2011; Shapiro et al., 2014). Such a problem formulation captures a
diversity of real-world process optimization problems, such as asset allocation (Dantzig and Infanger, 1993), inventory
control (Shapiro et al., 2014; Nambiar et al., 2021), energy planning (Pereira and Pinto, 1991), and bio-chemical process
control (Bao et al., 2019), to name a few. Despite the importance and ubiquity of the problem, it has proved challenging
to develop algorithms that can cope with high-dimensional action spaces and long-horizon problems (Shapiro and
Nemirovski, 2005; Shapiro, 2006).

There have been a number of attempts to design scalable algorithms for MSSO, which generally attempt to exploit
scenarios-wise or stage-wise decompositions. An example of a scenario-wise approach is Rockafellar and Wets (1991),
which proposed a progressive hedging algorithm that decomposes the sample averaged approximation of the problem
into individual scenarios and applies an augmented Lagrangian method to achieve consistency in a ﬁnal solution.
Unfortunately, the number of subproblems and variables grows exponentially in the number of stages, known as
the “curse-of-horizon”. A similar proposal in Lan and Zhou (2020) considers a dynamic stochastic approximation,
a variant of stochastic gradient descent, but the computational cost also grows exponentially in the number of stages.
Alternatively, stochastic dual dynamic programming (SDDP) (Birge, 1985; Pereira and Pinto, 1991), considers a
stage-wise decomposition that breaks the curse of horizon (F¨ullner and Rebennack, 2021) and leads to an algorithm
that is often considered state-of-the-art. The method essentially applies an approximate cutting plane method that
successively builds a piecewise linear convex lower bound on the optimal cost-to-go function. Unfortunately, SDDP
can require an exponential number of iterations with respect to the number of decision variables (Lan, 2020), known as
the “curse-of-dimension” (Bal´azs et al., 2015).

Beyond the scaling challenges, current approaches share a common shortcoming that they treat each optimization
problem independently. It is actually quite common to solve a family of problems that share structure, which intuitively
should allow the overall computational cost to be reduced (Khalil et al., 2017; Chen et al., 2019). However, current

∗Equal contribution

 
 
 
 
 
 
Neural Stochastic Dual Dynamic Programming

A PREPRINT

methods, after solving each problem instance via intensive computation, discard all intermediate results, and tackle
all new problems from scratch. Such methods are destined to behave as a perpetual novice that never shows any
improvement with problem solving experience.

In this paper, we present a meta-learning approach, Neural Stochastic Dual Dynamic Programming (ν-SDDP), that,
with problem solving experience, learns to signiﬁcantly improve the efﬁciency of SDDP in high-dimensional and
long-horizon problems. In particular, ν-SDDP exploits a specially designed neural network architecture that produces
outputs interacting directly and conveniently with a base SDDP solver. The idea is to learn an operator that take
information about the speciﬁc problem instance, and map it to a piece-wise linear function in the intrinsic low-dimension
space for accurate value function approximation, so that can be plugged into a SDDP solver. The mapping is trainable,
leading to an overall algorithm that self-improves as it solves more problem instances. There are three primary beneﬁts
of the proposed approach with carefully designed components:

i) By adaptively generating a low-dimension projection for each problem instance, ν-SDDP reduces the curse-of-

dimension effect for SDDP.

ii) By producing a reasonable value function initialization given a description of the problem instance, ν-SDDP is
able to amortize its solution costs, and gain a signiﬁcant advantage over the initialization in standard SDDP on the
two benchmarks studied in the paper.

iii) By restricting value function approximations to a piece-wise afﬁne form, ν-SDDP can be seamlessly incorporated

into a base SDDP solver for further reﬁning solution, which allows solution time to be reduced.

Figure 1 provides an illustration of the overall ν-SDDP method developed in this paper.

Figure 1: Overall illustration of ν-SDDP. For training, the algorithm iterates N times to solve different problem instances.
For each instance, it repeats two passes: forward (solving LPs to estimate an optimal action sequence) and backward
(adding new afﬁne components to the value function estimate). Once a problem instance is solved, the optimal value
function and optimal actions are used for neural network training. During inference time for a new problem, it can
predict high-quality value function with little cost, which can be embedded into SDDP for further improvements.

The remainder of the paper is organized as follows. First, we provide the necessary background on MSSO and
SDDP in Section 2. Motivated by the difﬁculty of SDDP and shortcomings of existing learning-based approaches,
we then propose ν-SDDP in Section 3, with the design of the neural component and the learning algorithm described
in Section 3.1 and Section 3.2 respectively. We compare the proposed approach with existing algorithms that also
exploit supervised learning (SL) and reinforcement learning (RL) for MSSO problems in Section 4. Finally, in Section 5
we conduct an empirical comparison on synthetic and real-world problems and ﬁnd that ν-SDDP is able to effectively
exploit successful problem solving experiences to greatly accelerate the planning process while maintaining the quality
of the solutions found.

2 Preliminaries

We begin by formalizing the multi-stage stochastic optimization problem (MSSO), and introducing the stochastic dual
dynamic programming (SDDP) strategy we exploit in the subsequent algorithmic development. We emphasize the
connection and differences between MSSO and a Markov decision process (MDP), which shows the difﬁculties in
applying the advanced RL methods for MSSO.

2

Neural Stochastic Dual Dynamic Programming

A PREPRINT

Multi-Stage Stochastic Optimization (MSSO). Consider a multi-stage decision making problem with stages t =
1, . . . , T , where an observation ξt ∼ Pt(·) is drawn at each stage from a known observation distribution Pt. A full
observation history {ξt}T
t=1 forms a scenario, where the observations are assumed independent between stages. At
stage t, an action is speciﬁed by a vector xt. The goal is to choose a sequence of actions {xt}T
t=1 to minimize the
overall expected sum of linear costs (cid:80)T
t=1 ct(ξt)(cid:62)xt under a known cost function ct. This is particularly challenging
with feasible constraints on the action set. Particularly, the feasible action set χt at stage t is given by

χt(xt−1, ξt)
χ1 (ξ1)

(1)
(2)
where At, Bt and bt are known functions. Notably, the feasible set χt(xt−1, ξt) at stage t depends on the previous
action xt−1 and the current stochastic observation ξt. The MSSO problem can then be expressed as

:= {xt|At (ξt) xt = bt (ξt) − Bt−1 (ξt) xt−1, xt (cid:62) 0} ,
:= {x1|A1(ξ1)x1 = b1 (ξ1) , x1 (cid:62) 0} ,

∀t = 2, . . . , T,

(cid:40)

v :=

minx1 c1(ξ1)(cid:62)x1 + Eξ2
s.t.

(cid:104)

xt ∈ χt(xt−1, ξt) ∀t = {1, . . . , T } ,

minx2 c2 (ξ2)(cid:62) x2· · · + EξT

minxT cT (ξT )(cid:62) xT

(cid:104)

(cid:105)(cid:105)

,

(3)

where ξ1 and the problem context U = {ut}T
t=1 for ut := (Pt, ct, At, Bt, bt) are provided. Given the context U given,
the MSSO is speciﬁed. Since the elements in U are probability or functions, the deﬁnition of U is only conceptual.
In practice, we implement U with its sufﬁcient representations. We will demonstrate the instantiation of U in our
experiment section. MSSO is often used to formulate real-world inventory control and portfolio management problems;
we provide formulations for these speciﬁc problems in in Appendix B.1 and Appendix B.2.

Similar to MDPs, value functions provide a useful concept for capturing the structure of the optimal solution in terms of
a temporal recurrence. Following the convention in the MSSO literature (F¨ullner and Rebennack, 2021), let

Qt (xt−1, ξt) :=

min
xt∈χt(xt−1,ξt)

ct (ξt)(cid:62) xt + Eξt+1 [Qt+1 (xt, ξt+1)]
,
(cid:125)

(cid:124)

(cid:123)(cid:122)
Vt+1(xt)

∀t = 2, . . . , T,

(4)

which expresses a Bellman optimality condition over the feasible action set. Using this deﬁnition, the MSSO problem
(3) can then be rewritten as

v := (cid:8)minx1 c1(ξ1)(cid:62)x1 + V2 (x1) ,

s.t. x1 ∈ χ1(ξ1)(cid:9) .

(5)

Theorem 1 (informal, Theorem 1.1 and Corollary 1.2 in F ¨ullner and Rebennack (2021)) When
optimiza-
tion (5) almost surely has a feasible solution for every realized scenario, the value functions Qt (·, ξ) and Vt (·) are
piecewise linear and convex in xt for all t = 1, . . . , T .

the

t

t }m

Select J samples from uniform{1, ..., m}
for t = 1, . . . , T and j = 1, . . . , J do

(cid:9)T
t=1 , ξ1, n)
j=1 ∼ Pt (·) for t = 2, . . . , T

Algorithm 1 SDDP((cid:8)V 0
1: Sample {ξj
2: for i = 1, . . . , n do
3:
4:
5:

Stochastic Dual Dynamic Program-
ming (SDDP). Given the problem
speciﬁcation (3) we now consider solu-
tion strategies. Stochastic dual dynamic
programming (SDDP) (Shapiro et al.,
2014; F¨ullner and Rebennack, 2021)
is a state-of-the-art approach that ex-
ploits the key observation in Theorem 1
that the optimal V function can be ex-
pressed as a maximum over a ﬁnite
number of linear components. Given
this insight, SDDP applies Bender’s de-
composition to the sample averaged ap-
proximation of (3). In particular, it per-
forms two steps in each iteration: (i) in
a forward pass from t = 0, trial solu-
tions for each stage are generated by solving subproblems (4) using the current estimate of the future expected-cost-to-go
function Vt+1; (ii) in a backward pass from t = T , each of the V -functions are then updated by adding cutting planes
derived from the optimal actions xt−1 obtained in the forward pass. (Details for the cutting plan derivation and the
connection to TD-learning are given in Appendix C due to lack of space.) After each iteration, the current Vt+1 provides
a lower bound on the true optimal expected-cost-to-go function, which is being successively tightened; see Algorithm 1.

Calculate the dual variables of (6) for each ξj
Update V i

t in (29);
t with dual variables via (32) (Appendix C)

6:
7:
8:
9:
10:
11: end for

end for
for t = T, . . . , 1 do

t+1 (xt),
t−1,j, ξj
t )

(cid:46) minibatch
(cid:46) forward pass

(cid:26)arg min ct(ξj

s.t. xtj ∈ χt(xi

(cid:46) backward pass

t )(cid:62)xt + V i

end for

xi
tj ∈

(6)

(cid:27)

MSSO vs. MDPs. At the ﬁrst glance, the dynamics in MSSO (3) is describing markovian relationship on actions, i.e.,
the current action xt is determined by current state ξt and previous action xt−1, which is different from the MDPs on
markovian states. However, we can equivalently reformulate MSSO as a MDP with state-dependent feasible action
set, by deﬁning the t-th step state as st := (xt−1, ξt), and action as at := xt ∈ χt (xt−1, ξt). This indeed leads

3

Neural Stochastic Dual Dynamic Programming

A PREPRINT

to the markovian transition pset (st+1|st, xt) = 1 (xt ∈ χt (xt−1, ξt)) pt+1 (ξt+1), where 1 (xt ∈ χt (xt−1, ξt)) :=
{1 if xt ∈ χt (xt−1, ξt) , 0 otherwise}. Although we can represent MSSO equivalently in MDP, the MDP formulation
introduces extra difﬁculty in maintaining state-dependent feasible action and ignores the linear structure in feasibility,
which may lead to the inefﬁciency and infeasibility when applying RL algorithms (see Section 5). Instead MSSO take
these into consideration, especially the feasibility.

Unfortunately, MSSO and MDP comes from different communities, the notational conventions of the MSSO versus
MDP literature are directly contradictory: the Q-function in (4) corresponds to the state-value V -function in the MDP
literature, whereas the V -function in (4) is particular to the MSSO setting, integrating out of randomness in state-value
function, which has no standard correspondent in the MDP literature. In this paper, we will adopt the notational
convention of MSSO.

3 Neural Stochastic Dual Dynamic Programming

Although SDDP is a state-of-the-art approach that is widely deployed in practice, it does not scale well in the
dimensionality of the action space (Lan, 2020). That is, as the number of decision variables in xt increases, the number
of generated cutting planes in the Vt+1 approximations tends to grow exponentially, which severely limits the size of
problem instance that can be practically solved. To overcome this limitation, we develop a new approach to scaling up
SDDP by leveraging the generalization ability of deep neural networks across different MSSO instances in this section.

We ﬁrst formalize the learning task by introducing the contextual MSSO. Speciﬁcally, as discussed in Section 2, the
problem context U = {ut}T
t=1 with ut := (Pt, ct, At, Bt, bt) soly deﬁnes the MSSO problem, therefore, we denote
W (U ) as an instance of MSSO (3) with explicit dependence on U . We assume the MSSO samples can be instantiated
from contextual MSSO following some distribution, i.e., W(U ) ∼ P (W), or equivalently, U ∼ P (U ). Then, instead
of treating each MSSO independently from scratch, we can learn to amortize and generalize the optimization across
different MSSOs in P (W). We develop a meta-learning strategy where a model is trained to map the ut and t to a
piecewise linear convex Vt-approximator that can be directly used to initialize the SDDP solver in Algorithm 1. In
principle, if optimal value information can be successfully transferred between similar problem contexts, then the
immense computation expended to recover the optimal Vt functions for previous problem contexts can be leveraged to
shortcut the nearly identical computation of the optimal Vt functions for a novel but similar problem context. In fact, as
we will demonstrate below, such a transfer strategy proves to be remarkably effective.

3.1 Neural Architecture for Mapping to Value Function Representations

To begin the speciﬁc development, we consider the structure of the value functions, which are desired in the deep neural
approximator.

• Small approximation error and easy optimization over action: recall from Theorem 1 that the optimal Vt-function
must be a convex, piecewise linear function. Therefore, it is sufﬁcient for the output representation from the deep
neural model to express max-afﬁne function approximations for Vt, which conveniently are also directly usable in
the minimization (6) of the SDDP solver.

• Encode the instance-dependent information: to ensure the learned neural mapping can account for instance
speciﬁc structure when transferring between tasks, the output representation needs to encode the problem context
information, {(Pt, ct, At, Bt, bt)}T

t=1.

• Low-dimension representation of state and action: the complexity of subproblems in SDDP depends on the
dimension of the state and action exponentially, therefore, the output Vt-function approximations should only depend
on a low-dimension representation of x.

For the ﬁrst two requirements, we consider a deep neural representation for functions f (·, ut) ∈ MK for t = 1, . . . , T ,
where MK is the piece-wise function class with K components, i.e.,

(cid:26)

MK :=

φ (·) : X → R(cid:12)

(cid:12)φ (x) = max

k x + αk, βk ∈ Rd, αk ∈ R
β(cid:62)

.

(7)

k=1,...,K
That is, such a function f takes the problem context information u as input, and outputs a set of parameters ({αk},
{βk}) that deﬁne a max-afﬁne function φ. We emphasize that although we consider MK with ﬁxed number of linear
components, it is straightforward to generalize to the function class with context-dependent number of components via
introducing learnable K(ut) ∈ N. A key property of this output representation is that it always remains within the set
of valid V -functions, therefore, it can be naturally incorporated into SDDP as a warm start to reﬁne the solution. This
approach leaves design ﬂexibility around the featurization of the problem context U and actions x, while enabling the
use of neural networks for f (·, u), which can be trained end-to-end.

(cid:27)

4

Neural Stochastic Dual Dynamic Programming

A PREPRINT

To further achieve a low-dimensional dependence on the action space while retaining convexity of the value function
representations for the third desideratum, we incorporate a linear projection x = Gy with y ∈ Rp and p < d, such that
G = ψ (u) satisﬁes G(cid:62)G = I. With this constraint, the mapping f (·, u) will be in:

(cid:26)

MK

G :=

φ(·) : Y → R(cid:12)

(cid:12)φG (y) = max

k=1,...,K

k Gy + αk, βk ∈ Rd, G ∈ Rd×p, αk ∈ R
β(cid:62)

(cid:27)

.

(8)

t=1 , f, ψ, ξ1)

Algorithm 2 Fast-Inference({ut}T
1: Set G = ψ (U )
2: Projected problem instance {qt}T
{f (·, qt)}T
3:

We postpone the learning of f and ψ to Section 3.2 and ﬁrst illustrate the accelerated SDDP solver in the learned effective
dimension of the action space in Algorithm 2. Note that after we obtain the solution y in line 3 in Algorithm 2 of the
projected problem, we can recover x = Gy as a coarse solution for fast inference. If one wanted a more reﬁned solution,
the full SDDP could be run on the un-projected instance starting from the updated algorithm state after the fast call.
Practical representation details:
In
our implementation, we ﬁrst encode the
index of time step t by a positional en-
coding (Vaswani et al., 2017) and ex-
ploit sufﬁcient statistics to encode the
distribution P (ξ) (assuming in addition
that the Pt are stationary). As the func-
tions ct, At, Bt and bt are typically
static and problem speciﬁc, there struc-
tures will remain the same for different
Pt. In our paper we focus on the gen-
eralization within a problem type (e.g.,
the inventory management) and do not
expect the generalization across problems (e.g., train on portfolio management and deploy on inventory management).
Hence we can safely ignore these LP speciﬁcations which are typically of high dimensions. The full set of features
are vectorized, concatenated and input to a 2-layer MLP with 512-hidden relu neurons. The MLP outputs k linear
components, {αk} and {βk}, that form the piece-wise linear convex approximation for Vt. For the projection G, we
simply share one G across all tasks, although it is not hard to incorporate a neural network parametrized G. The overall
deep neural architecture is illustrated in Figure 5 in Appendix D.

ξj
t
t,j
/* Optional reﬁnement */
(cid:110)

forward pass in low-dimension space.
(cid:110)
(cid:110)

t=1 = {Gut}T
t=1 , ξ1, 1

(cid:46) we only need one

(cid:46) reﬁne solution.

(cid:46) fast inference

{f (·, ut)}T

t=1 , ξ1, n

t (ξj
x∗
t )

= SDDP

=SDDP

t=1,

G˜yt

(cid:17)(cid:111)

(cid:17)(cid:111)

(cid:17)(cid:111)

ξj
t

ξj
t

˜xt

˜yt

=

(cid:110)

(cid:111)

5:

4:

t,j

t,j

t,j

(cid:16)

(cid:16)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

,

,

3.2 Meta Self-Improved Learning

(cid:16)

(cid:110)

zi :=

t }T

U, {V ∗

t=1 , {x∗

t (ξj)}T,m

The above architecture will be trained using a meta-learning strategy, where we collect successful prior experience
Dn :=
U = {ut}T
obtained by SDDP for each stage.
Given the dataset Dn, the parameters of f and ψ, W := {Wf , Wψ}, can be learned by optimizing the following
objective via stochastic gradient descent (SGD):

by using SDDP to solve a set of training problem instances. Here,
t=1 and (cid:8)x∗
t }T

i=1
t=1 denotes a problem context, and {V ∗

are the optimal value functions and actions

(cid:9)T,m
t,j

t=1,j

(cid:111)n

(cid:17)

tj

i

(cid:88)

minW

(cid:96) (W ; z) :=

n
(cid:88)

T
(cid:88)

(cid:32)

−

m
(cid:88)

(xi∗

tj )(cid:62)Gi
ψ

z∈Dn
(cid:16)

Gi
ψ

(cid:17)(cid:62)

i=1

t=1

j

Gi

ψ = Ip,

∀i = 1, . . . , n

s.t.

(cid:17)(cid:62)

(cid:16)

Gi
ψ

xi∗
tj + EMD

(cid:16)

f (·; ui

t), V i∗

t (·)

(cid:33)

(cid:17)

+ λσ (W ) ,

(9)

where Gψ := ψ(u), EMD (f, V ) denotes the Earth Mover’s Distance between f and V , and σ (W ) denotes a convex
regularizer on W . Note that the loss function (9) is actually seeking to maximize (cid:80)n
ψ xi∗
tj
under orthonormality constraints, hence it seeks principle components of the action spaces to achieve dimensionality
reduction.
To explain the role of EMD, recall that f (x, u) outputs a convex piecewise linear function represented by (cid:8)(βf
αf
k
function, hence expressible by a maximum over afﬁne functions. Therefore, EMD (f, V ∗
distance between the sets (cid:8)βf
min
M ∈Ω(K,t)

k )(cid:62)x +
l=1 in SDDP is also a convex piecewise linear
t ) is used to calculate the

l }t
l=1, which can be recast as
+ |M 1 (cid:54) 1, M (cid:62)1 (cid:54) 1, 1(cid:62)M 1 = min(K, t)(cid:9) ,

(cid:9)K
k=1, while the optimal value function V ∗

(cid:104)M, D(cid:105) , Ω(K, t) = (cid:8)M ∈ RK×t

(cid:9)K
k=1 and {β∗

t (x) := (cid:8)(β∗

tj )(cid:62)GψG(cid:62)

l )(cid:62)x + α∗

l (ξ)(cid:9)t

j (xi∗

k , αf

l , α∗

(cid:80)m

(10)

(cid:80)t

t=1

i=1

k

where D ∈ RK×t denotes the pairwise distances between elements of the two sets. Due to space limits, please refer
to Figure 6 in Appendix D for an illustration of the overall training setup. The main reason we use EMD is due to the

5

Neural Stochastic Dual Dynamic Programming

A PREPRINT

Algorithm 3 ν-SDDP
1: Initialize dataset D0;
2: for epoch i = 1, . . . , n do
3:

4:

5:

6:

Sample a multi-stage stochastic decision problems U = {ut}T
Initial (cid:8)V 0
(cid:18)(cid:110)

(cid:9)T
t=1 = (1 − γ)0 + γ
= SDDP((cid:8)V 0

·, {ut}T

(cid:111)T,m

(cid:9)T
t=1 , ξ1, n);

x∗(ξj
t )

(cid:17)(cid:111)T

fW

t=1

t=0

(cid:19)

(cid:110)

(cid:16)

t

t

t,j=1

t=1 ∼ P (U );
with γ ∼ B (pi);

Collect solved optimization instance Di = Di−1 ∪
for iter = 1, . . . , b do
Sample zl ∼ Di;
Update parameters W with stochastic gradients: W = W − η∇W (cid:96) (W ; zl) ;

t=1,j

t=1 , {x∗

U, {V ∗

t (ξj)}T,m

t }T

(cid:16)

(cid:17)

;

7:
8:
9:
10:
11: end for

end for

fact that f and V ∗ are order invariant, and EMD provides an optimal transport comparison (Peyr´e et al., 2019) in terms
of the minimal cost over all pairings.

Remark (Alternative losses): One could argue that it sufﬁces to use the vanilla regression losses, such as the L2-square
loss (cid:107)f (·, u, ξ) − V ∗ (·, ξ)(cid:107)2
2, to ﬁt f to V ∗. However, there are several drawbacks with such a direct approach. First,
such a loss ignores the inherent structure of the functions. Second, to calculate the loss, the observations x are required,
and the optimal actions from SDDP are not sufﬁcient to achieve a robust solution. This approach would require an
additional sampling strategy that is not clear how to design (Defourny et al., 2012).

Training algorithm: The loss (9) pushes f to approximate the optimal value functions for the training contexts,
while also pushing the subspace Gψ to acquire principle components in the action space. The intent is to achieve
an effective approximator for the value function in a low-dimensional space that can be used to warm-start SDDP
inference Algorithm 2. Ideally, this should result in an efﬁcient optimization procedure with fewer optimization
variables that can solve a problem instance with fewer forward-backward passes. In an on-line deployment, the learned
components, f and ψ, can be continually improved from the results of previous solves. One can also optinally exploit
the learned component for the initialization of value function in SDDP by annealing with a mixture of zero function,
where the weight is sampled from a Bernolli distribution. Overall, this leads to the Meta Self-Improved SDDP algorithm,
ν-SDDP, shown in Algorithm 3.

4 Related Work

The importance of MSSO and the inherent difﬁculty of solving MSSO problems at a practical scale has motivated
research on hand-designed approximation algorithms, as discussed in Appendix A.

Learning-based MSSO approximations have attracted more attentions. Rachev and R¨omisch (2002); Høyland et al.
(2003); Hochreiter and Pﬂug (2007) learn a sampler for generating a small scenario tree while preserving statistical
properties. Recent advances in RL is also exploited. Defourny et al. (2012) imitate a parametrized policy that maps from
scenarios to actions from some SDDP solvers. Direct policy improvement from RL have also been considered. Ban
and Rudin (2019) parametrize a policy as a linear model in (25), but introducing large approximation errors. As an
extension, Bertsimas and Kallus (2020); Oroojlooyjadid et al. (2020) consider more complex function approximators
for the policy parameterization in (25). Oroojlooyjadid et al. (2021); Hubbs et al. (2020); Balaji et al. (2019); Barat
et al. (2019) directly apply deep RL methods. Avila et al. (2021) exploit off-policy RL tricks for accelerating the SDDP
Q-update. More detailed discussion about Avila et al. (2021) can be found in Appendix A. Overall, the majority of
methods are not able to easily balance MSSO problem structures and ﬂexibility while maintaining strict feasibility with
efﬁcient computation. They also tend to focus on learning a policy for a single problem, which does not necessarily
guarantee effective generalization to new cases, as we ﬁnd in the empirical evaluation.

Context-based meta-RL is also relevant, where the context-dependent policy (Hausman et al., 2018; Rakelly et al.,
2019; Lan et al., 2019) or context-dependent value function (Fakoor et al., 2019; Arnekvist et al., 2019; Raileanu
et al., 2020) is introduced. Besides the difference in MSSO vs. MDP in Section 2, the most signiﬁcant difference is
the parameterization and inference usage of context-dependent component. In ν-SDDP, we design the speciﬁc neural
architecture with the output as a piece-wise linear function, which takes the structure of MSSO into account and can be
seamlessly integrated with SDDP solvers for further solution reﬁnement with the feasibility guaranteed; while in the
vanilla context-based meta-RL methods, the context-dependent component with arbitrary neural architectures, which

6

Neural Stochastic Dual Dynamic Programming

A PREPRINT

will induce extra approximation error, and is unable to handle the constraints. Meanwhile, the design of the neural
component in ν-SDDP also leads to our particular learning objective and stochastic algorithm, which exploits the
inherent piece-wise linear structure of the functions, meanwhile bypasses the additional sampling strategy required for
alternatives.

5 Experiments

Problem Deﬁnition. We ﬁrst tested on inventory opti-
mization with the problem conﬁguration in Table. 1. We
break the problem contexts into two sets: 1) topology, pa-
rameterized via the number of suppliers S, inventories I,
and customers C; 2) decision horizon T . Note that in the
Mid-Lng setting there are 310 continuous action variables,
which is of magnitudes larger than the ones used in inven-
tory control literature (Graves and Willems, 2008) and benchmarks, e.g., ORL (Balaji et al., 2019) or meta-RL (Rakelly
et al., 2019) on MuJoCo (Todorov et al., 2012).

Small-size topology, Short horizon (Sml-Sht)
Mid-size topology, Long horizon (Mid-Lng)
Portfolio Optimization

Table 1: Problem Conﬁguration in Inventory Optimization.

2-2-4, 5
10-10-20, 10
T = 5

Conﬁguration (S-I-C, T )

Problem Setting

Within each problem setting, a problem instance is further captured by the problem context. In inventory optimization,
a forecast model is ususally used to produce continuous demand forecasts and requires re-optimization of the inventory
decisions based on the new distribution of the demand forecast, forming a group of closely related problem instances.
We treat the parameters of the demand forecast as the primary problem context. In the experiment, demand forecasts
are synthetically generated from a normal distribution: dt ∼ N (µd, σd). For both problem settings, the mean and
the standard deviation of the demand distribution are sampled from the meta uniform distributions: µd ∼ U(11, 20),
σd ∼ U(0, 5). Transport costs from inventories to customers are also subject to frequent changes. We model it via a
normal distribution: ct ∼ N (µc, σc) and use the distribution mean µc ∼ U(0.3, 0.7) as the secondary problem context
parameter with ﬁxed σc = 0.2. Thus in this case, the context for each problem instance that f (·, µt) needs to care about
is ut = (µd, σd, µc).

The second environment is portfolio optimization. A forecast model is ususally used to produce updated stock price
forcasts and requires re-optimization of asset allocation decisions based on the new distribution of the price forecast,
forming a group of closely related problem instances. We use an autoregressive process of order 2 to learn the price
forecast model based on the real daily stock prices in the past 5 years. The last two-day historical prices are used as
problem context parameters in our experiments. In this case the stock prices of ﬁrst two days are served as the context
U for f .

Due to the space limitation, we postpone the detailed description of problems and additional performances comparison
in Appendix E.

Baselines.

In the following experiments, we compare ν-SDDP with mainstream methodologies:

• SDDP-optimal: This is the SDDP solver that runs on each test problem instance until convergence, and is expected

to produce the best solution and serve as the ground-truth for comparison.

• SDDP-mean: It is trained once based on the mean values of the problem parameter distribution, and the resulting
V -function will be applied in all different test problem instances as a surrogate of the true V -function. This approach
enjoys the fast runtime time during inference, but would yield suboptimal results as it cannot adapt to the change of
the problem contexts.

• Model-free RL algorithms: Four RL algorithms, including DQN, DDPG, SAC, PPO, are directly trained online
on the test instances without the budget limit of number of samples. So this setup has more privileges compared to
typical meta-RL settings. We only report the best RL result in Table 2 and Table 3 due to the space limit. Detailed
hyperparameter tuning along with the other performance results are reported in Appendix E.

• ν-SDDP-fast: This is our algorithm where the the meta-trained neural-based V -function is directly evaluated on each
problem instance, which corresponds to Algorithm 2 without the last reﬁnement step. In this case, only one forward
pass of SDDP using the neural network predicted V -function is needed and the V -function will not be updated. The
only overhead compared to SDDP-mean is the feed-forward time of neural network, which can be ignored compared
to the expensive LP solving.

• ν-SDDP-accurate: It is our full algorithm presented in Algorithm 2 where the meta-trained neural-based V -function

is further reﬁned with 10 more iterations of vanilla SDDP algorithm.

7

Neural Stochastic Dual Dynamic Programming

A PREPRINT

Table 2: Average Error Ratio of Objective Value.

Task

Parameter Domain

SDDP-mean

ν-SDDP-fast

ν-SDDP-accurate

Best RL

t
h
S
-
l

m
S

g
n
o
L
-
d
i
M

demand mean (µd)
joint (µd & σd)

demand mean (µd)
joint (µd & σd)
joint (µd & σd & µc)

16.15 ± 18.61% 2.42 ± 1.84%

1.32 ± 1.13%

38.42 ± 17.78%

20.93 ± 22.31% 4.77 ± 3.80%

1.81 ± 2.19%

33.08 ± 8.05%

24.77 ± 27.04% 2.90 ± 1.11%

1.51 ± 1.08%

17.81 ± 10.26%

27.02 ± 29.04% 5.16 ± 3.22%

3.32 ± 3.06%

50.19 ± 5.57%

29.99 ± 32.33% 7.05 ± 3.60%

3.29 ± 3.23% 135.78 ± 17.12%

Table 3: Objective Value Variance.

Task

Parameter Domain

SDDP-optimal

SDDP-mean

ν-SDDP-fast

ν-SDDP-accurate

Best RL

t
h
S
-
l

m
S

g
n
o
L
-
d
i
M

demand mean (µd)
joint (µd & σd)

demand mean (µd)
joint (µd & σd)
joint (µd & σd & µc)

6.80 ± 7.45

14.83 ± 17.90

9.60 ± 3.35

10.12 ± 4.03

3.90± 8.39

10.79 ± 19.75

19.83 ± 22.02

11.04 ± 10.83

13.73 ± 16.64

1.183± 4.251

51.96 ± 14.90

73.39 ± 59.90

44.27 ± 9.00

33.42 ± 18.01

1.98± 2.65

54.89 ± 32.35

85.76 ± 77.62

45.53 ± 24.14

36.31 ± 20.49

205.51 ± 150.90

55.14 ± 38.93

86.26 ± 81.14

44.80 ± 28.57

36.19 ± 20.08

563.19 ± 114.03

5.1 Solution Quality Comparison

For each new problem instance, we evaluate the algorithm performance by solving and evaluating the optimization objec-
tive value using the trained V -function model over 50 randomly sampled trajectories. We record the mean(candidate)
and the standard derivation of these objective values produced by each candidate method outlined above. As SDDP-
optimal is expected to produce the best solution, we use its mean on each problem instance to normalize the difference
in solution quality. Speciﬁcally, error ratio of method candidate with respect to SDDP-optimal is:

φ =

mean(candidate) − mean(SDDP-optimal)
abs {mean(SDDP-optimal)}

(11)

Inventory optimization: We report the average optimalty ratio of each method on the held-out test problem set with
100 instances in Table 2. By comparison, ν-SDDP learns to adaptive to each problem instance, and thus is able to
outperform these baselines by a signiﬁcantly large margin. Also we show that by tuning the SDDP with the V -function
initialized with the neural network generated cutting planes for just 10 more steps, we can further boost the performance
(ν-SDDP-accurate). In addition, despite the recent reported promising results in applying deep RL algorithms in
small-scale inventory optimization problems (Bertsimas and Kallus, 2020; Oroojlooyjadid et al., 2020, 2021; Hubbs
et al., 2020; Balaji et al., 2019; Barat et al., 2019), it seems that these algorithms get worse results than SDDP and
ν-SDDP variants when the problem size increases.

We further report the average variance along with its standard deviation of different methods in Table 3. We ﬁnd
that generally our proposed ν-SDDP (both fast and accurate variants) can yield solutions with comparable variance
compared to SDDP-optimal. SDDP-mean gets higher variance, as its performance purely depends on how close the
sampled problem parameters are to their means.

Portfolio optimization: We evaluated the same metrics as above. We train a multi-dimensional second-order autore-
gressive model for the selected US stocks over last 5 years as the price forecast model, and use either synthetic (low)
or estimated (high) variance of the price to test different models. When the variance is high, the best policy found by
SDDP-optimal is to buy (with appropriate but different asset allocations at different days) and hold for each problem
instance. We found our ν-SDDP is able to rediscover this policy; when the variance is low, our model is also able to
achieve much lower error ratio than SDDP-mean. We provide study details in Appendix E.2.

5.2 Trade-off Between Running Time and Algorithm Performance

We study the trade-off between the runtime and the obtained solution quality in Figure 2 based on the problem instances
in the test problem set. In addition to ν-SDDP-fast and SDDP-mean, we plot the solution quality and its runtime
obtained after different number of iterations of SDDP (denoted as SDDP-n with n iterations). We observe that for the
small-scale problem domain, SDDP-mean runs the fastest but with very high variance over the performance. For the
large-scale problem domain, ν-SDDP-fast achieves almost the same runtime as SDDP-mean (which roughly equals to
the time for one round of SDDP forward pass). Also for large instances, SDDP would need to spend 1 or 2 magnitudes
of runtime to match the performance of ν-SDDP-fast. If we leverage ν-SDDP-accurate to further update the solution in

8

Neural Stochastic Dual Dynamic Programming

A PREPRINT

Sml-Sht-joint (µd & σd)

Mid-Lng-joint (µd & σd & µc)

Mid-Lng-joint (µd & σd & µc)

Figure 2: Time-solution trade-off. In the left two plots, each dot represents a problem instance with the runtime and
the solution quality obtained by corresponding algorithm. The right most plot shows how ν-SDDP-accurate improves
further when integrated into SDDP solver.

Sml-Sht-joint (µd & σd)

Mid-Lng-joint (µd & σd & µc)

Figure 3: ν-SDDP-fast with different # generated cutting planes.

Figure 4: Performance of ν-SDDP with
low-rank projection.

each test problem instance for just 10 iterations, we can further improve the solution quality. This suggests that our
proposed ν-SDDP achieves better time-solution trade-offs.

5.3 Study of Number of Generated Cutting Planes

In Figure 3 we show the performance of ν-SDDP-fast with respect to different model capacities, captured by the number
of cutting planes the neural network can generate. A general trend indicates that more generated cutting planes would
yield better solution quality. One exception lies in the Mid-Lng setting, where increasing the number of cutting planes
beyond 64 would yield worse results. As we use the cutting planes generated by last n iterations of SDDP solving in
training ν-SDDP-fast, our hypothesis is that the cutting planes generated by SDDP during the early stages in large
problem settings would be of high variance and low-quality, which in turn provides noisy supervision. A more careful
cutting plane pruning during the supervised learning stage would help resolve the problem.

5.4 Low-Dimension Project Performance

Finally in Figure 4 we show the performance using low-rank projection. We believe that in reality customers from the
same cluster (e.g., region/job based) would express similar behaviors, thus we created another synthetic environment
where the customers form 4 clusters with equal size and thus have the same demand/transportation cost within each
cluster. We can see that as long as the dimension goes above 80, our approach can automatically learn the low-dimension
structure, and achieve much better performance than the baseline SDDP-mean. Given that the original decision problem
is in 310-dimensional space, we expect having 310/4 dimensions would be enough, where the experimental results
veriﬁed our hypothesis. We also show the low-dimension projection results for the problems with full-rank structure in
Appendix E.1.

6 Conclusion
We present Neural Stochastic Dual Dynamic Programming, which learns a neural-based V -function and dimension
reduction projection from the experiences of solving similar multi-stage stochastic optimization problems. We carefully
designed the parametrization of V -function and the learning objectives, so that it can exploit the property of MSSO
and be seamlessly incorporated into SDDP. Experiments show that ν-SDDP signiﬁcantly improves the solution time
without sacriﬁcing solution quality in high-dimensional and long-horizon problems.

9

101100Time (s)0.00.20.40.60.8Relative ErrorSDDP-2SDDP-4SDDP-8SDDP-16SDDP-32SDDP-mean-SDDP-fast100101Time (s)0.000.250.500.751.001.251.501.75Relative ErrorSDDP-2SDDP-4SDDP-8SDDP-16SDDP-32SDDP-mean-SDDP-fast0.02.55.07.510.012.515.017.520.0Time (s)101102Error ratio/%-SDDP-accurate-SDDP-fastSDDP01020304050# generated cutting planes02004006008001000Error ratio/%020406080100# generated cutting planes0255075100125150175200Error ratio/%50100150200250300Dimension of generated cutting planes0102030405060Error ratio/%-SDDP-fastSDDP-meanNeural Stochastic Dual Dynamic Programming

A PREPRINT

References

John R Birge and Francois Louveaux. Introduction to stochastic programming. Springer Science & Business Media,

2011.

Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczy´nski. Lectures on stochastic programming: modeling and

theory. SIAM, 2014.

George B Dantzig and Gerd Infanger. Multi-stage stochastic linear programs for portfolio optimization. Annals of

Operations Research, 45(1):59–76, 1993.

Mila Nambiar, David Simchi-Levi, and He Wang. Dynamic inventory allocation with demand learning for seasonal

goods. Production and Operations Management, 30(3):750–765, 2021.

Mario V. F. Pereira and Leontina M. V. G. Pinto. Multi-stage stochastic optimization applied to energy planning.

Mathematical Programming, 52(2):359–375, 1991.

Hanxi Bao, Zhiqiang Zhou, Georgios Kotsalis, Guanghui Lan, and Zhaohui Tong. Lignin valorization process control
under feedstock uncertainty through a dynamic stochastic programming approach. Reaction Chemistry & Engineering,
4(10):1740–1747, 2019.

Alexander Shapiro and Arkadi Nemirovski. On complexity of stochastic programming problems. In Continuous

optimization, pages 111–146. Springer, 2005.

Alexander Shapiro. On complexity of multistage stochastic programs. Operations Research Letters, 34(1):1–8, 2006.
R Tyrrell Rockafellar and Roger J-B Wets. Scenarios and policy aggregation in optimization under uncertainty.

Mathematics of operations research, 16(1):119–147, 1991.

Guanghui Lan and Zhiqiang Zhou. Dynamic stochastic approximation for multi-stage stochastic optimization. Mathe-

matical Programming, pages 1–46, 2020.

John R Birge. Decomposition and partitioning methods for multistage stochastic linear programs. Operations research,

33(5):989–1007, 1985.

Christian F¨ullner and Steffen Rebennack. Stochastic dual dynamic programming - a review. 2021.
Guanghui Lan. Complexity of stochastic dual dynamic programming. Mathematical Programming, pages 1–38, 2020.
G. Bal´azs, A. Gy¨orgy, and Cs. Szepesv´ari. Near-optimal max-afﬁne estimators for convex regression. In AISTATS,

pages 56–64, 2015.

Elias B Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms

over graphs. In NIPS, 2017.

Binghong Chen, Bo Dai, Qinjie Lin, Guo Ye, Han Liu, and Le Song. Learning to plan in high dimensions via neural

exploration-exploitation trees. In International Conference on Learning Representations, 2019.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia

Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.

Gabriel Peyr´e, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations

and Trends® in Machine Learning, 11(5-6):355–607, 2019.

Boris Defourny, Damien Ernst, and Louis Wehenkel. Multistage stochastic programming: A scenario tree based
approach to planning under uncertainty. In Decision theory models for applications in artiﬁcial intelligence: concepts
and solutions, pages 97–143. IGI Global, 2012.

Svetlozar T Rachev and Werner R¨omisch. Quantitative stability in stochastic programming: The method of probability

metrics. Mathematics of Operations Research, 27(4):792–818, 2002.

Kjetil Høyland, Michal Kaut, and Stein W Wallace. A heuristic for moment-matching scenario generation. Computa-

tional optimization and applications, 24(2):169–185, 2003.

Ronald Hochreiter and Georg Ch Pﬂug. Financial scenario generation for stochastic multi-stage decision processes as

facility location problems. Annals of Operations Research, 152(1):257–272, 2007.

Gah-Yi Ban and Cynthia Rudin. The big data newsvendor: Practical insights from machine learning. Operations

Research, 67(1):90–108, 2019.

Dimitris Bertsimas and Nathan Kallus. From predictive to prescriptive analytics. Management Science, 66(3):

1025–1044, 2020.

Afshin Oroojlooyjadid, Lawrence V Snyder, and Martin Tak´aˇc. Applying deep learning to the newsvendor problem.

IISE Transactions, 52(4):444–463, 2020.

10

Neural Stochastic Dual Dynamic Programming

A PREPRINT

Afshin Oroojlooyjadid, MohammadReza Nazari, Lawrence V Snyder, and Martin Tak´aˇc. A deep q-network for the beer
game: Deep reinforcement learning for inventory optimization. Manufacturing & Service Operations Management,
2021.

Christian D Hubbs, Hector D Perez, Owais Sarwar, Nikolaos V Sahinidis, Ignacio E Grossmann, and John M Wassick.
Or-gym: A reinforcement learning library for operations research problem. arXiv preprint arXiv:2008.06319, 2020.
Bharathan Balaji, Jordan Bell-Masterson, Enes Bilgin, Andreas Damianou, Pablo Moreno Garcia, Arpit Jain, Runfei
Luo, Alvaro Maggiar, Balakrishnan Narayanaswamy, and Chun Ye. Orl: Reinforcement learning benchmarks for
online stochastic optimization problems. arXiv preprint arXiv:1911.10641, 2019.

Souvik Barat, Harshad Khadilkar, Hardik Meisheri, Vinay Kulkarni, Vinita Baniwal, Prashant Kumar, and Monika
Gajrani. Actor based simulation for closed loop control of supply chain using reinforcement learning. In Proceedings
of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pages 1802–1804, 2019.
Daniel Avila, Anthony Papavasiliou, and Nils L¨ohndorf. Batch learning in stochastic dual dynamic programming.

submitted, 2021.

Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. Learning an embedding

space for transferable robot skills. In International Conference on Learning Representations, 2018.

Kate Rakelly, Aurick Zhou, Chelsea Finn, Sergey Levine, and Deirdre Quillen. Efﬁcient off-policy meta-reinforcement
learning via probabilistic context variables. In International conference on machine learning, pages 5331–5340.
PMLR, 2019.

Lin Lan, Zhenguo Li, Xiaohong Guan, and Pinghui Wang. Meta reinforcement learning with task embedding and

shared policy. arXiv preprint arXiv:1905.06527, 2019.

Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander J Smola. Meta-q-learning. arXiv preprint

arXiv:1910.00125, 2019.

Isac Arnekvist, Danica Kragic, and Johannes A Stork. Vpe: Variational policy embedding for transfer reinforcement

learning. In 2019 International Conference on Robotics and Automation (ICRA), pages 36–42. IEEE, 2019.

Roberta Raileanu, Max Goldstein, Arthur Szlam, and Rob Fergus. Fast adaptation to new environments via policy-
dynamics value functions. In International Conference on Machine Learning, pages 7920–7931. PMLR, 2020.
Stephen C Graves and Sean P Willems. Strategic inventory placement in supply chains: Nonstationary demand.

Manufacturing & service operations management, 10(2):278–287, 2008.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ

International Conference on Intelligent Robots and Systems, pages 5026–5033. IEEE, 2012.

J. Birge. The value of the stochastic solution in stochastic linear programs with ﬁxed recourse. Mathematical

Programming, 24:314–325, 1982.

Pascal Van Hentenryck and Russell Bent. Online stochastic combinatorial optimization. The MIT Press, 2006.
Gilles Bareilles, Yassine Laguel, Dmitry Grishchenko, Franck Iutzeler, and J´erˆome Malick. Randomized progressive
hedging methods for multi-stage stochastic programming. Annals of Operations Research, 295(2):535–560, 2020.
Kai Huang and Shabbir Ahmed. The value of multistage stochastic programming in capacity planning under uncertainty.

Operations Research, 57(4):893–904, 2009.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Dimitri P Bertsekas. Dynamic Programming and Optimal Control, Two Volume Set. Athena Scientiﬁc, 2001.
Richard Bellman. Dynamic Programming. Princeton University Press, Princeton, NJ, USA, 1 edition, 1957.
Jorge Nocedal and Stephen Wright. Numerical optimization. Springer Science & Business Media, 2006.
RE Mahony, U Helmke, and JB Moore. Gradient algorithms for principal component analysis. The ANZIAM Journal,

37(4):430–450, 1996.

Bo Xie, Yingyu Liang, and Le Song. Scale up nonlinear component analysis with doubly stochastic gradients. CoRR,

abs/1504.03655, 2015.

T. D. Sanger. Optimal unsupervised learning in a single-layer linear feedforward network. Neural Networks, 2:459–473,

1989.

K. Kim, M. O. Franz, and B. Sch¨olkopf. Iterative kernel principal component analysis for image modeling. IEEE

Transactions on Pattern Analysis and Machine Intelligence, 27(9):1351–1366, 2005.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin

Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.

11

Neural Stochastic Dual Dynamic Programming

A PREPRINT

Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and

Daan Wierstra. Continuous control with deep reinforcement learning. In ICLR, 2016.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization

algorithms. arXiv preprint arXiv:1707.06347, 2017.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep
reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1861–1870.
PMLR, 2018.

John Schulman. Optimizing expectations: From deep reinforcement learning to stochastic computation graphs. PhD

thesis, UC Berkeley, 2016.

12

A More Related Work

Appendix

To scale up the MSSO solvers, a variety of hand-designed approximation schemes have been investigated. One natural
approach is restricting the size of the scenario tree using either a scenario-wise or state-wise simpliﬁcation. For example,
as a scenario-wise approach, the expected value of perfect information (EVPI, (Birge, 1982; Hentenryck and Bent,
2006)) has been investigated for optimizing decision sequences within a scenario, which are then heuristically combined
to form a full solution. Bareilles et al. (2020) instantiates EVPI by considering randomly selected scenarios in a
progressive hedging algorithm (Rockafellar and Wets, 1991) with consensus combination. For a stage-wise approach, a
two-stage model can be used as a surrogate, leveraging a bound on the approximation gap (Huang and Ahmed, 2009).
All of these approximations rely on a ﬁxed prior design for the reduction mechanism, and cannot adapt to a particular
distribution of problem instances. Consequently, we do not expect such methods to be competitive with learning
approaches that can adapt the approximation strategy to a given problem distribution.

Difference to Learning from Cuts in Avila et al. (2021):
the Batch Learning-SDDP (BL-SDDP) is released recently,
where machine learning technique is also used for accelerating MSSO solver. However, this work is signiﬁcant different
from the proposed ν-SDDP:

• Firstly and most importantly, the setting and target of these works are orthogonal: the BL-SDDP speeds up the SDDP
for a particular given MSSO problem via parallel computation; while ν-SDDP works for the meta-learning setting
that learns from a dataset composed by plenty of MSSO problems sampled from a distribution, and the learning target
is to generalize to new MSSO instances from the same distribution well;

• The technique contribution in BL-SDDP and ν-SDDP are different. Speciﬁcally, BL-SDDP exploits existing off-
policy RL tricks for accelerating the SDDP Q-update; while we proposed two key techniques for quick initialization
i), with predicted convex functions; ii), dimension reduction techniques, to generalize different MSSOs and alleviate
curse-of-dimension issues in SDDP, which has not been explored in BL-SDDP.

Despite being orthogonal, we think that the BL-SDDP can be used in our framework to provide better supervision for
our cut function prediction, and serve as an alternative for ﬁne-tuning after ν-SDDP-fast.

One potential drawback of our ν-SDDP is that, when the test instance distributions deviate a lot from what has been
trained on, the neural initialization may predict cutting planes that are far away from the good ones, which may slow
down the convergence of SDDP learning. Characterizing in-distribution v.s. out-of-distribution generalization and
building the conﬁdence measure is an important future work of current approach.

B Practical Problem Instantiation

In this section, we reformulate the inventory control and portfolio management as multi-stage stochastic decision
problems.

B.1 Inventory Control

Let S, V, C be the number of suppliers, inventories, and customers, respectively. We denote the parameters of the
inventory control optimization as:

• procurement price matrix: pt ∈ RSV ×1;
• sales price matrix: qt ∈ RV C×1;
• unit holding cost vector: ht ∈ RV ×1;
• demand vector: dt ∈ RC×1;
• supplier capacity vector: ut ∈ RS×1;
• inventory capacity vector: vt ∈ RV ×1;
• initial inventory vector w0 ∈ RV ×1.

The decision variables of the inventory control optimization are denoted as:

13

Neural Stochastic Dual Dynamic Programming

A PREPRINT

• sales variable: yt ∈ RV C×1, indicating the amount of sales from inventories to customers at the beginning of

stage t;

• procurement variable: zt ∈ RSV ×1, indicating the amount of procurement at the beginning of stage t after

sales;

• inventory variable: wt ∈ RV ×1, indicating the inventory level at the beginning of stage t after procurement.

We denote the decision variables as

the state as

xt = [yt, zt, wt],

ξt = [pt, qt, ht, dt, ut, vt, w0].

The goal of inventory management is to maximize the net proﬁt for each stage, i.e.,

t xt := p(cid:62)
c(cid:62)

t zt + h(cid:62)

t wt − q(cid:62)

t yt,

and subject to the constraints of 1) supplier capacity; 2) inventory capacity; 3) customer demand, i.e.,

χt (xt−1, ξt) =

yt, zt, wt

(cid:26)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

V
(cid:88)

v=1
V
(cid:88)

yv
t

(cid:54) dt

(demand bound constraints),

zv
t

(cid:54) ut

(supplier capacity constraints)

v=1
wt (cid:54) vt
C
(cid:88)

c=1
S
(cid:88)

s=1

zs
t −

C
(cid:88)

c=1

zt, yt, wt, (cid:62) 0

(inventory capacity constraints)

t − wt−1 (cid:54) 0
yc

(sales bounded by inventory)

yc

t + wt−1 = wt

(inventory transition)

(non-negativity constraints)

.

(cid:27)

To sum up, the optimization problem can be deﬁned recursively as follows:
(cid:20)

(cid:20)

(cid:20)

c(cid:62)
1 x1 + Eξ2

c2 (ξ2)(cid:62) x2 (ξ2) + Eξ3

· · · + EξT

min
x2

min
xT

cT (ξT )(cid:62) xT (ξT )

(cid:21)

(cid:21)(cid:21)

· · ·

,

min
x1
s.t.

xt ∈ χt(xt−1, ξt), ∀t = {1, . . . , T } .

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

In fact, the inventory control problem (18) is simpliﬁed the multi-stage stochastic decision problem (3) by considering
state independent transition.

B.2 Portfolio Management

Let I be the number of assets, e.g., stocks, being managed. We denote the parameters of the portfolio optimization are:

• ask (price to pay for buying) open price vector pt ∈ RI×1;
• bid (price to pay for sales) open price vector qt ∈ RI×1;
• initial amount of investment vector w0 ∈ RI×1;
• initial amount of cash r0 ∈ R+.

The decision variables of the portfolio optimization are:

• sales vector yt ∈ RI×1, indicating the amount of sales of asset i at the beginning of stage t.
• purchase vector zt ∈ RI×1, indicating the amount of procurement at the beginning of stage t;
• holding vector wt ∈ RI×1, indicating the amount of assets at the beginning of stage t after purchase and sales;
• cash scalar rt, indicating the amount of cash at the beginning of stage t.

14

Neural Stochastic Dual Dynamic Programming

A PREPRINT

We denote the decision variables as

and the state as

xt = [yt, zt, wt, rt] ,

ξt = [pt, qt] .

The goal of portfolio optimization is to maximize the net proﬁt i.e.,
t zt − q(cid:62)

t xt := p(cid:62)
c(cid:62)

t yt,

subject to the constraints of initial investment and the market prices, i.e.,

χt (xt−1, ξt) :=

yt, zt, wt, rt

(cid:26)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

yt − wt−1 (cid:54) 0

(individual stock sales constraints)

(stock purchase constraints)

t zt − rt−1 (cid:54) 0
p(cid:62)
yt − zt + wt − rt−1 = 0
(individual stock position transition)
t yt − p(cid:62)
q(cid:62)

t zt − rt + rt−1 = 0

(cid:27)

(20)

(21)
(22)

(23)

(cash position transition)

.

With the ct and χt (xt−1, ξt) deﬁned above, we initiate the multi-stage stochastic decision problem (3) for portfolio
management.

C Details on Stochastic Dual Dynamic Programming

We have introduced the SDDP in Section 2. In this section, we provide the derivation of the updates in forward and
backward pass,

• Forward pass, updating the action according to (5) based on the current estimation of the value function at each

stage via (4). Speciﬁcally, for i-th iteration of t-stage with sample ξj

t , we solve the optimization

xt ∈

argmin
xt∈χt(xt−1,ξj
t )

(cid:17)(cid:62)

(cid:16)

ξj
t

ct

xt + V i

t+1 (xt).

In fact, the V -function is a convex piece-wise function. Speciﬁcally, for i-th iteration of t-stage, we have

Then, we can rewrite the optimization (24) into standard linear programming, i.e.,

V i
t+1 (xt) = max
k(cid:54)i

(cid:110)(cid:0)βk

t+1

(cid:1)(cid:62)

xt + αk

t+1

(cid:111)

,

(cid:17)(cid:62)

xt + θt+1

min
xt,θt+1

ct

s.t.

(cid:16)

ξj
t
(cid:16)

ξj
At
t
− (cid:0)βk
t+1
xt (cid:62) 0,

(cid:17)

xt = bt

(cid:17)

(cid:16)

ξj
t

− Bt−1

(cid:17)

(cid:16)

ξj
t

xt−1,

(cid:1)(cid:62)

xt + θt+1 (cid:62) αk

t+1, ∀k = 1, . . . , i,

(24)

(25)

(26)

(27)
(28)

• Backward pass, updating the estimation of the value function via the dual of (25), i.e., for i-th iteration of t-stage

with sample ξj

t , we calculate

max
ωt,ρt

(cid:16)

bt

(cid:17)

(cid:16)

ξj
t

− Bt−1

(cid:17)(cid:17)(cid:62)

(cid:16)

ξj
t

ωt +

i
(cid:88)

k=1

t αk
ρk

t+1,

s.t.

(cid:17)(cid:62)

(cid:16)
ξj
t

At

ωt −

ρk
t

(cid:0)βk

t+1

(cid:1)(cid:62) (cid:54) ct

(cid:17)

(cid:16)

ξj
t

,

i
(cid:88)

k=1

−1 (cid:54) ρ(cid:62)

t 1 (cid:54) 1.

Then, we have the

V i+1
t

(xt−1) = max (cid:8)V i

t (xt−1) , vi+1

t

(xt−1)(cid:9) ,

15

(29)

(30)

(31)

(32)

Neural Stochastic Dual Dynamic Programming

A PREPRINT

Figure 5: Hypernet style parameterization of neural V -function.

which is still convex piece-wise linear function, with

vi+1
t

(xt−1) := (cid:0)βi+1

t

(cid:1)(cid:62)

xt−1 + αi+1

t

,

(33)

where

(cid:0)βi+1

t

(cid:1)(cid:62)

:=

1
m

m
(cid:88)

j=1

m
(cid:88)

(cid:20)
−Bt−1

(cid:16)

ξj
t

(cid:17)(cid:62)

(cid:16)
ξj
t

ωt

(cid:17)(cid:21)

,

(cid:34)

bt

(cid:17)(cid:62)

(cid:16)

ξj
t

(cid:17)

(cid:16)

ξj
t

+

ωt

i
(cid:88)

αk

t+1

(cid:17)

(cid:16)

ξj
t

ρk
t

(cid:16)

ξj
t

(cid:35)

(cid:17)

,

αi+1
t

:=

1
m

k=1
with (ωt (ξt) , ρt (ξt)) as the optimal dual solution with realization ξt.

j=1

In fact, although we split the forward and backward pass, in our implementation, we exploit the primal-dual method for
LP, which provides both optimal primal and dual variables, saving the computation cost.

Note that SDDP can be interpreted as a form of TD-learning using a non-parametric piecewise linear model for the
V -function. It exploits the property induced by the parametrization of value functions, leading to the update w.r.t.
V -function via adding dual component by exploiting the piecewise linear structure in a closed-form functional update.
That is, TD-learning (Sutton and Barto, 2018; Bertsekas, 2001) essentially conducts stochastic approximate dynamic
programming based on the Bellman recursion (Bellman, 1957).

D Neural Network and Learning System Design

Neural network design:
In Figure 5 we present the design of the neural network that tries to approximate the
V -function. The neural network takes two components as input, namely the feature vector that represents the problem
conﬁguration, and the integer that represents the current stage of the multi-stage solving process. The stage index is
converted into ‘time-encoding‘, which is a 128-dimensional learnable vector. We use the parameters of distributions
as the feature of φ (P (ξ)) for simplicity. The characteristic function or kernel embedding of distribution can be also
used here. The feature vector will also be projected to the same dimension and added together with the time encoding
to form as the input to the MLP. The output of MLP is a matrix of size k × (N + 1), where k is the number of linear
pieces, N is the number of variable (and we also need one additional dimension for intercept). The result is a piecewise
linear function that speciﬁes a convex lowerbound. We show an illustration of 2D case on the right part of the Figure 5.

This neural network architecture is expected to adapt to different problem conﬁgurations and time steps, so as to have
the generalization and transferability ability across different problem conﬁgurations.

Remark (Stochastic gradient computation): Aside from Gψ, unbiased gradient estimates for all other variables
in the loss (9) can be recovered straightforwardly. However, Gψ requires special treatment since we would like it to
satisfy the constraints G(cid:62)
ψ Gψ = Ip. Penalty method is one of the choices, which switches the constraints to a penalty
(cid:13)
(cid:13)
(cid:13)G(cid:62)

in objective, i.e., η
. However, the solution satisﬁes the constraints, only if η → ∞ (Nocedal and
Wright, 2006, Chapter 17). We derive the gradient over the Stiefel manifold (Mahony et al., 1996), which ensures the
orthonormal constraints,

ψ Gψ − Ip

(cid:13)
2
(cid:13)
(cid:13)
2

gradGψ (cid:96) = (cid:0)I − G(cid:62)

ψ Gψ

(cid:1) ΞG(cid:62)
ψ ,

(34)

16

{                  } Linear ProjectionTime Position Encoding MLPxV(x)Problem ContextStep IndexNeural Stochastic Dual Dynamic Programming

A PREPRINT

Figure 6: Illustration of the overall system design.

(cid:0)xi∗

t

i=1

(cid:80)n

(cid:80)m

j xi∗
tj

with Ξ := (cid:80)
as an expectation over samples.
The gradients on Stiefel manifold (cid:8)G|G(cid:62)G = I(cid:9) can be found in Mahony et al. (1996). We derive the gradient (34)
via Lagrangian for self-completeness, following Xie et al. (2015).

. Note that this gradient can be estimated stochastically since Ξ can be recognized

tj

(cid:1)(cid:62)

Consider the Lagrangian as

L (Gψ, Λ) =

(cid:88)

z∈Dn

(cid:96) (W ; z) + tr (cid:0)(cid:0)G(cid:62)

ψ Gψ − I(cid:1) Λ(cid:1) ,

where the Λ is the Lagrangian multiplier. Then, the gradient of the Lagrangian w.r.t. Gψ is

∇Gψ L = 2ΞG(cid:62)

ψ + G(cid:62)
ψ

(cid:0)Λ + Λ(cid:62)(cid:1) .

With the optimality condition

(cid:40)

∇Gψ,ΛL = 0 ⇒

G(cid:62)
2ΞG(cid:62)

ψ Gψ − I = 0
ψ + G(cid:62)
ψ

(cid:0)Λ + Λ(cid:62)(cid:1) = 0

Plug (36) into the gradient (35), we have the optimality condition,
(cid:1) ΞG(cid:62)
ψ
(cid:125)

(cid:0)I − G(cid:62)
(cid:124)

ψ Gψ
(cid:123)(cid:122)
gradGψ

⇒ −2GψΞG(cid:62)

ψ = (cid:0)Λ + Λ(cid:62)(cid:1) .

= 0.

(35)

(36)

(37)

To better numerical isolation of the individual eigenvectors, we can exploit Gram-Schmidt process into the gradient
estimator (34), which leads to the generalized Hebbian rule (Sanger, 1989; Kim et al., 2005; Xie et al., 2015),

(cid:94)gradGψ (cid:96) = (cid:0)I − LT (cid:0)G(cid:62)

(cid:1)(cid:1) ΞG(cid:62)
The LT (·) extracts the lower triangular part of a matrix, setting the upper triangular part and diagonal to zero,
therefore, is mimicking the Gram-Schmidt process to subtracts the contributions from each other eigenvectors to achieve
orthonormality. Sanger (1989) shows that the updates with (38) will converges to the ﬁrst p eigenvectors of Ξ.

ψ − LT (cid:0)G(cid:62)

ψ = ΞG(cid:62)

(cid:1) ΞG(cid:62)
ψ .

ψ Gψ

ψ Gψ

(38)

System design: Next we present the entire system end-to-end in Figure 6.

• Task sampling: the task sampling component draws the tasks from the same meta distribution. Note that each task is
a speciﬁcation of the distribution (e.g., the center of Gaussian distribution), where the speciﬁcation follows the same
meta distribution.

• We split the task instances into train, validation and test splits:

– Train: We solve each task instance using SDDP. During the solving of SDDP we need to perform multiple
rounds of forward pass and backward pass to update the cutting planes (V -functions), as well as sampling
trajectories for monte-carlo approximation. The learned neural V -function will be used as initialization. After
SDDP solving converges, we collect the corresponding task instance speciﬁcation (parameters) and the resulting
cutting planes at each stage to serve as the training supervision for our neural network module.

17

Task SamplingTask instanceTraining TasksTask instanceTask instanceValidation TasksTask instanceTest TasksSDDPForward passBackward passV-functionsTrajectory SamplingSDDPSDDPTraining ExamplesValidation ExamplesTask paramsV-func paramsTask paramsV-func params............Task paramsV-func params...𝞶-SDDP InferenceForward passNeural V-Functions𝞶-SDDP TrainingNeural V-FunctionsNeural Stochastic Dual Dynamic Programming

A PREPRINT

Sml-Sht-mean (µd)

Sml-Sht-joint (µd & σd)

Mid-Lng-mean (µd)

Mid-Lng-joint (µd & σd)

Mid-Lng-joint (µd & σd & µc)

Figure 7: Time-solution trade-off.

Mid-Lng-mean (µd)

Mid-Lng-joint (µd & σd)

Mid-Lng-joint (µd & σd & µc)

Figure 8: Time-solution trade-off when ν-SDDP-accurate improves the solution from ν-SDDP-fast further.

– Validation: We do the same thing for validation tasks, and during training of neural network we will dump the

models that have the best validation loss.

– Test: In the test stage, we also solve the SDDP until convergence as groundtruth, which is only used for
evaluating the quality of different algorithms. For our neural network approach, we can generate the convex
lower-bound using the trained neural nework, conditioning on each pair of (test task instance speciﬁcation, stage
index). With the predicted V -functions, we can run the forward pass only once to retrieve the solution at each
stage. Finally we can evaluate the quality of the obtained solution with respect to the optimal ones obtained by
SDDP.

E More Experiments

E.1 Inventory Optimization

E.1.1 Additional Results on ν-SDDP

We ﬁrst show the full results of time-solution quality trade-off in Figure 7, and how ν-SDDP-accurate improves from
ν-SDDP-fast with better trade-off than SDDP solver iterations in Figure 8. We can see the conclution holds for all the
settings, where our proposed ν-SDDP achieves better trade-off.

Then we also show the ablation results of using different number of predicted cutting planes in Figure 9. We can see in
all settings, generally the more the cutting planes the better the results. This suggests that in higher dimensional case

18

101100Time (s)0.00.20.40.60.8Relative ErrorSDDP-2SDDP-4SDDP-8SDDP-16SDDP-32SDDP-mean-SDDP-fast101100Time (s)0.00.20.40.60.8Relative ErrorSDDP-2SDDP-4SDDP-8SDDP-16SDDP-32SDDP-mean-SDDP-fast100101Time (s)0.00.20.40.60.81.01.21.4Relative ErrorSDDP-2SDDP-4SDDP-8SDDP-16SDDP-32SDDP-mean-SDDP-fast100101Time (s)0.00.20.40.60.81.01.21.4Relative ErrorSDDP-2SDDP-4SDDP-8SDDP-16SDDP-32SDDP-mean-SDDP-fast100101Time (s)0.000.250.500.751.001.251.501.75Relative ErrorSDDP-2SDDP-4SDDP-8SDDP-16SDDP-32SDDP-mean-SDDP-fast02468101214Time (s)101102Error ratio/%-SDDP-accurate-SDDP-fastSDDP0.02.55.07.510.012.515.017.5Time (s)101102Error ratio/%-SDDP-accurate-SDDP-fastSDDP0.02.55.07.510.012.515.017.520.0Time (s)101102Error ratio/%-SDDP-accurate-SDDP-fastSDDPNeural Stochastic Dual Dynamic Programming

A PREPRINT

Sml-Sht-mean (µd)

Sml-Sht-joint (µd & σd)

Mid-Lng-mean (µd)

Mid-Lng-joint (µd & σd)

Mid-Lng-joint (µd & σd & µc)

Figure 9: Ablation: number of generated cutting planes.

Mid-Lng-mean (µd)

Mid-Lng-joint (µd & σd)

Mid-Lng-joint (µd & σd & µc)

Figure 10: Low-dim projection results when the underlying problem does not have a low-rank structure.

it might be harder to obtain high quality cutting planes, and due to the convex-lowerbound nature of the V -function,
having a bad cutting plane could possibly hurt the overall quality. How to prune and prioritize the cutting planes will be
an important direction for future works.

We provide the full ablation results of doing low-dimensional projection for solving SDDP in Figure 10. The trend
generally agrees with our expectation that, there is a trade-off of the low-dimensionality that would balance the quality
of LP solving and the difﬁculty of neural network learning.

Longer horizon: we further experiment with the Mid-Lng-joint (µd&σd&µc) by varying T in {10, 20, 30, 40, 50}.
See Table 4 for more information.

Table 4: Average error ratio of ν-SDDP-fast on Mid-Lng-joint (µd&σd&µc) setting with varying T .

Horizon length
Average error ratio

10

30
3.29% 3.47% 3.53% 2.65% 0.82%

40

20

50

E.1.2 Additional Results on Model-Free RL Algorithms

We implemented the inventory control problem as an environment in the Tensorﬂow TF-Agents library and used
the implementation of DQN (Mnih et al., 2013)2, DDPG (Lillicrap et al., 2016), PPO (Schulman et al., 2017) and

2We provided a simple extension of DQN to support multi-dimensional actions.

19

01020304050# generated cutting planes02004006008001000Error ratio/%01020304050# generated cutting planes02004006008001000Error ratio/%020406080100# generated cutting planes020406080100120140Error ratio/%020406080100# generated cutting planes020406080100120140160Error ratio/%020406080100# generated cutting planes0255075100125150175200Error ratio/%280285290295300305310Dimension of generated cutting planes2.02.22.42.62.83.03.2Error ratio/%280285290295300305310Dimension of generated cutting planes5.45.65.86.06.26.4Error ratio/%280285290295300305310Dimension of generated cutting planes91011121314151617Error ratio/%Neural Stochastic Dual Dynamic Programming

A PREPRINT

Table 6: Average Error Ratio of Objective Value.

Task

Parameter Domain

DQN

DDPG

PPO

SAC

t
h
S
-
l

m
S

g
n
o
L
-
d
i
M

demand mean (µd)
joint (µd & σd)

demand mean (µd)
joint (µd & σd)
joint (µd & σd & µc)

1157.86 ± 452.50%

28.62 ± 8.69 %

2849.931 ± 829.91%

38.42 ± 17.78%

3609.62 ± 912.54%

100.00 ± 0.00 %

3273.71 ± 953.13%

33.08 ± 8.05%

5414.15 ± 1476.21%

100.00 ± 0.00%

5411.16 ± 1474.19%

17.81 ± 10.26%

5739.68 ± 1584.63%

100.00 ± 0.00%

5734.75± 1582.68 %

50.19 ± 5.57%

6382.87 ± 2553.527%

100.00 ± 0.00%

6377.93 ±2550.03 %

135.78 ± 17.12%

Table 7: Objective Value Variance.

Task

Parameter Domain

DQN

DDPG

PPO

SAC

t
h
S
-
l

m
S

g
n
o
L
-
d
i
M

demand mean (µd)
joint (µd & σd)

46.32 ± 85.90

86.097 ± 100.81

demand mean (µd)
joint (µd & σd)
joint (µd & σd & µc)

1334.30 ± 270.00

1983.71 ± 1874.61

1983.74 ± 1874.65

0.34 ± 0.22

0.00 ± 0.00

0.00 ± 0.00

0.00 ± 0.00

0.00 ± 0.00

119.08 ±112.00

3.90± 8.39

169.08 ± 147.24

1.183± 4.251

339.97 ± 620.01

1.98± 2.65

461.27 ± 1323.24

205.51 ± 150.90

462.74 ± 1332.30

563.19 ± 114.03

SAC (Haarnoja et al., 2018) from the TF-Agent to evaluate the performance of these four model-free RL algorithms.
Note that the TF-Agent environment follows a MDP formulation. When adapting to the inventory control problem, the
states of the environment are the levels of the inventories and the actions are the amount of procurement and sales. As
a comparison, the states and actions in the MDP formulation collectively form the decision varialbles in the MSSO
formulation and their relationship – inventory level transition, is captured as a linear constraint. Following Schulman
(2016), we included the timestep into state in these RL algorithms to have non-stationary policies for ﬁnite-horizon
problems. In the experiments, input parameters for each problem domain and instances are normalized for model
training for policy gradient algorithms and the results are scaled back for reporting.

We report the average error ratio of these four RL algorithms in Table. 6 along with the average variance in Table. 7.
Note that the SAC performance is also reported in the main text in Table. 2 and Table. 3. The model used in the
evaluation is selected based on the best mean return over the 50 trajectories from the validation environment, based on
which the hyperparameters are also tuned. We report the selected hyperparameters for each algorithm in Table. 5. We
use MLP with 3 layers as the Q-network for DQN, as the actor network and the critic/value network for SAC, PPO and
DDPG. All networks have the same learning rate and with a dropout parameter as 0.001.

Table 5: Hyperparameter Selections.

Algorithm
SAC
PPO
DQN
DDPG

Hyperparameters
learning rate(0.01), num MLP units (50), target update period (5), target update tau (0.5)
learning rate(0.001), num MLP units (50), target update period (5), target update tau (0.5)
learning rate(0.01), num MLP units (100), target update period (5), target update tau (0.5)
learning rate(0.001), num MLP units (50), ou stddev (0.2), ou damping (0.15)

We see that SAC performs the best among the four algorithms in terms of solution quality. All the algorithms can not
scale to Mid-Long setting. DDPG, for example, produces a trivial policy of no action in most of setups (thus has an
error ratio of 100). The policies learned by DQN and PPO are even worse, producing negative returns3.

To understand the behavior of each RL algorithm, we plotted the convergence of the average mean returns in Figure. 11
for the Sml-Sht task. In each plot, we show four runs of the respective algorithm under the selected hparameter.
We could see that though SAC converges the slowest, it is able to achieve the best return. For all algorithms, their
performance is very sensitive to initialization. DDPG, for example, has three runs with 0 return, while one run with a
return of 1.2. For PPO and DQN, the average mean returns are both negative .

We further check the performance of these algorithms in the validation environment based on the same problem instance
(i.e., the same problem parameters) as in SDDP-mean, where the model is trained and selected. We expect this would
give the performance upper bound for these algorithms. Again similar results are observed. The best return mean over
the validation environment is −38.51 for PPO, 0.95 for DQN, 1.31 for SAC and 1.41 for DDPG, while the SDDP

3Negative returns are caused by over-procurement in the early stages and the leftover inventory at the last stage.

20

Neural Stochastic Dual Dynamic Programming

A PREPRINT

SAC

PPO

DQN

DDPG

Figure 11: Average mean return (values are normalized with optimal mean value as 1.736 ).

(a) 5 Stocks with AR order = 2

(b) 5 Stocks with AR order = 5

(c) 8 Stock Clusters with AR order = 5

Figure 12: Evidence lower bound (ELBO) loss curve.

optimal return value is 1.736. It is also worth noting that DDPG shows the strongest sensitivity to initialization and its
performance drops quickly when the problem domain scales up.

E.2 Portfolio Optimization

We use the daily opening prices from selected Nasdaq stocks in our case study. This implies that the asset allocation
and rebalancing in our portfolio optimization is performed once at each stock market opening day. We ﬁrst learn a
probabilistic forecasting model from the historical prices ranging from 2015-1-1 to 2020-01-01. Then the forecasted
trajectories are sampled from the model for the stochastic optimization. Since the ask price is always slightly higher
than the bid price, at most one of the buying or selling operation will be performed for each stock, but not both, on a
given day.

E.2.1 Stock Price Forecast

The original model for portfolio management in Appendix B.2 is too restrict. We generalize the model with autoregres-
sive process (AR) of order o with independent noise is used to model and predict the stock price:

o
(cid:88)

pt =

(φipt−i + (cid:15)r

i ) + (cid:15)o

where φi is the autoregressive coefﬁcient and (cid:15)r
o) is a white
noise of the observation. Each noise term is (cid:15) assumed to be independent. It is easy to check that the MSSO formulation
for portfolio management is still valid by replacing the expectation for (cid:15), and setting the concatenate state [pt−o
, qt],
where pt−o

i ) is a white noise of order i. (cid:15)o ∼ N (0, σ2

:= [pt−i]o

t

i=0.

t

i=1
i ∼ N (0, σ2

We use variational inference to ﬁt the model. A variational loss function (i.e., the negative evidence lower bound
(ELBO)) is minimized to ﬁt the approximate posterior distributions for the above parameters. Then we use the posterior
samples as inputs for forecasting. In our study, we have studied the forecasting performance with different length of
history, different orders of the AR process and different groups of stocks. Figure. 12 shows the ELBO loss convergence
behavior under different setups. As we can see, AR models with lower orders converge faster and smoother.

We further compare the forecasting performance with different AR orders. Figure. 13 plots a side-by-side comparison
of the forecasted mean trajectory with a conﬁdence interval of two standard deviations (95%) for 5 randomly selected
stocks (with tickers GOOG, COKE, LOW, JPM, BBBY) with AR order of 2 and 5 from 2016-12-27 for 5 trading days.
As we could see, a higher AR order (5) provides more time-variation in forecast and closer match to the ground truth.

21

Neural Stochastic Dual Dynamic Programming

A PREPRINT

AR order = 2

AR order = 5

Figure 13: Probablistic Forecast of 5 Stocks with Different AR Orders.

In addition, we cluster the stocks based on their price time series (i.e., each stock is represented by a T -dimensional
vector in the clustering algorithm, where T is the number of days in the study). We randomly selected 1000 stocks from
Nasdaq which are active from 2019-1-1 to 2020-1-1 and performed k-means clustering to form 8 clusters of stocks.
We take the cluster center time series as the training input. Figure. 12(c) shows the ELBO loss convergence of an AR
process of order 5 based on these 8 cluster center time series. As we see, the stock cluster time series converge smoother
and faster compared with the individual stocks as the aggregated stock series are less ﬂuctuated. The forecasting
trajectories of these 8 stock clusters starting from 2019-03-14 are plotted in Figure. 14.

E.2.2 Solution Quality Comparison

Table 8: Portfolio optimization with synthetic standard deviation of stock price.

5 stocks (STD scaled by 0.01)
5 stocks (STD scaled by 0.001)
8 clusters (STD scaled by 0.1)
8 clusters (STD scaled by 0.01)

SDDP-mean
290.05 ± 221.92 %
271.65 ± 221.13 %
69.18 ± 77.47 %
65.81 ± 77.33 %

ν-SDDP-zeroshot
1.72 ± 4.39 %
1.84 ± 3.67 %
1.43e−6 ± 4.30e−5%
3.25e−6 ± 3.44e−5%

With an AR forecast model of order o, the problem context of a protfolio optimizaton instance is then captured by
the joint distribution of the historical stock prices over a window of o days. We learn this distribution using kernel
density estimation. To sample the scenarios for each stage using the AR model during training for both SDDP and
ν-SDDP, we randomly select the observations from the previous stage to seed the observation sampling in the next
stage. Also we approximate the state representation by dropping the term of pt−o
. With such treatments, we can obtain
SDDP results with manageable computation cost. We compare the performance of SDDP-optimal, SDDP-mean and
ν-SDDP-zeroshot under different forecasting models for a horizon of 5 trading days. First we observe that using the
AR model with order 2 as the forecasting model, as suggested by the work (Dantzig and Infanger, 1993), produce a
very simple forecasting distribution where the mean is monotonic over the forecasting horizon. As a result, all the
algorithms will lead to a simple “buy and hold” policy which make no difference in solution quality. We further increase
the AR order gradually from 2 to 5 and ﬁnd AR order at 5 produces sufﬁcient time variation that better ﬁts the ground
truth. Second we observe that the variance learned from the real stock data based on variational inference is signiﬁcant.
With high variance, both SDDP-optimal and ν-SDDP-zeroshot would achieve the similar result, which is obtained by a
similar “buy and hold” policy. To make the task more challenging, we rescale the standard deviation (STD) of stock

t

22

Neural Stochastic Dual Dynamic Programming

A PREPRINT

price by a factor in Table 8 for both the 5-stock case and 1000-stock case with 8 clusters situations. For the cluster case,
the ν-SDDP-zeroshot can achieve almost the same performance as the SDDP-optimal.

E.3 Computation resource

For the SDDP algorithms we run using multi-core CPUs, where the LP solving can be parallelized at each stage. For
RL based approaches and our ν-SDDP, we train using a single V100 GPU for each hparameter conﬁguration for at
most 1 day or till convergence.

23

Neural Stochastic Dual Dynamic Programming

A PREPRINT

Figure 14: Probablistic Forecast of 8 Stock Clusters.

24

