1
2
0
2

b
e
F
4
2

]
E
S
.
s
c
[

2
v
6
1
1
8
0
.
1
0
1
2
:
v
i
X
r
a

Improving type information inferred by decompilers
with supervised machine learning

Javier Escaladaa, Ted Scullyb, Francisco Ortina,b,∗

aUniversity of Oviedo, Computer Science Department,
c/Federico Garcia Lorca 18, 33007, Oviedo, Spain

bCork Institute of Technology, Computer Science Department,
Rossa Avenue, Bishopstown, Cork, Ireland

Abstract

In software reverse engineering, decompilation is the process of recovering source
code from binary ﬁles. Decompilers are used when it is necessary to understand or ana-
lyze software for which the source code is not available. Although existing decompilers
commonly obtain source code with the same behavior as the binaries, that source code
is usually hard to interpret and certainly diﬀers from the original code written by the
programmer. Massive codebases could be used to build supervised machine learning
models aimed at improving existing decompilers. In this article, we build diﬀerent clas-
siﬁcation models capable of inferring the high-level type returned by functions, with
signiﬁcantly higher accuracy than existing decompilers. We automatically instrument
C source code to allow the association of binary patterns with their corresponding high-
level constructs. A dataset is created with a collection of real open-source applications
plus a huge number of synthetic programs. Our system is able to predict function
return types with a 79.1% F1-measure, whereas the best decompiler obtains a 30% F1-
measure. Moreover, we document the binary patterns used by our classiﬁer to allow
their addition in the implementation of existing decompilers.

Keywords: Big code, machine learning, syntax patterns, decompilation, binary
patterns, big data

1. Introduction

A decompiler is a tool that receives binary code as input and generates high-level
code with the same semantics as the input. Although the decompiled source code can

∗Corresponding author
Email addresses: escaladajavier@uniovi.es (Javier Escalada), ted.scully@cit.ie (Ted

Scully), ortin@uniovi.es (Francisco Ortin)

URL: http://cs.cit.ie/research-staff.ted-scully.biography (Ted Scully),

http://www.reflection.uniovi.es/ortin (Francisco Ortin)

Preprint submitted to arXiv.org

February 25, 2021

 
 
 
 
 
 
be recompiled to produce the original binary code, the high-level source code is not
commonly the one originally written by the programmer. In fact, the source code is
usually much less readable than the original one [1]. This is because obtaining the
original source code from a binary ﬁle is an undecidable problem [1]. The cause is that
the compiler discards high-level information in the translation process, such as type
information, that cannot be recovered in the inverse process.

In the implementation of current decompilers, experts analyze source code snip-
pets and the associated binaries generated by the compiler to identify decompilation
patterns. Such patterns associate sequences of assembly instructions with high-level
code constructs. These patterns are later included in the implementation of decom-
pilers [2, 3]. The identiﬁcation of these code generation patterns is not an easy task,
because of many factors such as the optimizations implemented by compilers, the high
expressiveness degree of high-level languages, the compiler used, the target CPU, and
the compilation parameters.

The use of large volumes of source code has been used to create tools aimed at
improving software development [4]. This approach has been termed “big code” since
it applies big data techniques to source code. In the big code area, existing source-
code corpora have already been used to create diﬀerent systems such as JavaScript
deobfuscators [5], automatic translators of C# code into Java [6], and tools for detecting
program vulnerabilities [7]. Probabilistic models are built with machine learning and
natural language processing techniques to exploit the abundance of patterns in source
code [8].

Our idea is to use large portions of high-level source code and their related binaries
to train machine learning models. Then, these models will help us ﬁnd code generation
patterns not used by current decompilers. The patterns found can be used to improve
existing decompilers. Machine learning has already been used for decompilation. Diﬀer-
ent recurrent neural networks have been used to recover the number and some built-in
types of function parameters [9]. Extremely randomized trees and conditional random
ﬁelds have provided good results inferring basic type information [10]. Decompilation
has also been tackled with encoder-decoder translation neural networks [11] and with
a genetic programming approach [12] (these works are detailed in Section 3).

The main contribution of this paper is the usage of supervised machine learning
to improve type information inferred by decompilers. Particularly, we improve the
performance of existing decompilers in predicting the types returned by functions in
high-level programs. For that purpose, we instrument C source code to label binary
patterns with high-level type information. That labeled information is then used to
build predictive models. Moreover, the dataset created is used to document the binary
patterns found by the classiﬁers and facilitate its inclusion in the implementation of
any decompiler. Our current work is just focused on the Microsoft C compiler for 32-
bit Windows binaries, with the default compiler parameters. However, the proposed
method could be applied to other languages and compiler settings.

The rest of the paper is structured as follows. Section 2 describes a motivating
example, and related work is discussed in Section 3. Section 4 describes the architecture

2

Figure 1: Example C source code.

of the dataset-generation system. In Section 5, we detail the methodology used, and
the evaluation results are presented in Section 6. Section 7 discusses some interesting
patterns discovered with our dataset, and Section 8 presents the conclusions and future
work.

2. A motivating example

Figure 1 shows an example C program that we compile with Microsoft cl 32-bit
compiler. The generated binary ﬁle is then decompiled with four diﬀerent decompilers,
obtaining the following function signatures:

• Signature of the stats update function:

– IDA Decompiler: int

cdecl sub 401000(int a1, int a2, char a3)

– RetDec: int32 t function 401000(int32 t* a1, int32 t a2, int32 t a3)

– Snowman: void** fun 401000(void** ecx, void** a2, void** a3,

void** a4, void** a5, void** a6)

– Hopper: int sub 401000(int arg0, int arg1, int arg2)

• Signature of the mean function:

– IDA Decompiler: int

cdecl sub 4010A0(int a1, unsigned int a2)

– RetDec: int32 t function 4010a0(int32 t a1, uint32 t a2)

3

#include <stdlib.h> #include <stdio.h> #include <stdbool.h>  struct stats { int count; int sum; int sum_squares; };  void stats_update(struct stats * s, int x, bool reset) {     if (s == NULL) return;     if (reset) * s = (struct stats) { 0, 0, 0 };     s->count += 1;     s->sum += x;     s->sum_squares += x * x; }  double mean(int data[], size_t len) {     struct stats s;     for (int i = 0; i < len; ++i)         stats_update(&s, data[i], i == 0);     return ((double)s.sum) / ((double)s.count); }  void main() {     int data[] = { 1, 2, 3, 4, 5, 6 };     printf("MEAN = %lf\n", mean(data, sizeof(data) / sizeof(data[0]))); } – Snowman: void** fun 4010a0(void** ecx, void** a2, void** a3)

– Hopper: int sub 4010a0(int arg0, int arg1)

For both functions, no decompiler infers the correct return type. Although IDA
Decompiler, RetDec and Hopper detect the correct number of arguments, most of the
parameter types are not correctly recovered by any decompiler1.

As mentioned, our work is focused on inferring the high-level type returned by
functions by analyzing its binary code, which is not an easy task. The reason is that the
value is returned to the caller in a registry (bool, char, short, int, long, pointer, and
struct2 values are returned in the accumulator; long long in edx:eax; and float,
double and long double in the FPU register stack), but the value stored in that
registry could be the result of a temporary computation in a function returning void.
Therefore, we search for binary patterns before returning from a function and after
their invocations, to see if we can recover the high-level return type written by the
programmer.

The problem to be solved is a multi-label classiﬁcation problem, where the target
variable is an enumeration of all the high-level built-in types of the C programming
language (including void), plus the type constructors that can be returned3 (pointer
and struct). In fact, the models we build (Section 6.2.1) are able to infer the high-level
C types returned by the stats update and mean functions in Figure 1.

3. Related Work

3.1. Type inference

There are some research works aimed at inferring high-level type information from
binary code. Chua et al. [9] use recurrent neural networks (RNN) [13] to detect the
number and type of function parameters. First, they transform each instruction into
word embeddings (256 double values per instruction) [14]. Then, a sequence of in-
structions (vectors) is used to build 4 diﬀerent RNNs for counting caller arguments
and function parameters, recovering types of caller arguments, and recovering types of
function parameters. In this work, they infer seven diﬀerent types: int, float, char,
pointer, enum, union and struct. They only consider int for integer values and float
for real ones. They achieved 84% accuracy for parameter counting and 81% for type
recovery.

He et al. [10] build a prediction system that takes as an input a stripped binary and
outputs a new binary with debug information that includes type information. They

1size t and uint32 t are aliases of unsigned int in a 32-bit architecture.
2A struct is commonly returned as a pointer to struct (i.e., its memory address is returned

instead of its value).

3In C, a function returning an array actually returns a pointer. For Microsoft cl, the union type

is actually represented as int, long or struct, depending on its size (explained in Section 6.1).

4

combine extremely randomized trees (ERT) with conditional random ﬁelds (CRFs) [15].
The ERT model aims to extract identiﬁers. Although identiﬁers are always mapped to
registers and memory oﬀsets, not every register and memory oﬀset stores identiﬁers.
Then, the CRF model predicts the name and type of the identiﬁers discovered by ERT.
They use a maximum a posteriori (MAP) estimation algorithm to ﬁnd a globally opti-
mal assignment. This tool handles 17 diﬀerent types, but it lacks ﬂoating-point types
support. This is because the library used to handle the assembly code, Binary Anal-
ysis Platform (BAP) [16], does not support ﬂoating-point instructions. Their system
achieves 68.8% precision and 68.3% recall.

Some works are not focused on type inference exclusively, but they undertake de-
compilation as a whole, including type inference. The works in [11, 17, 18] propose
diﬀerent systems based on neural machine translation [19, 20]. They use RNNs with
an encoder-decoder scheme to learn high-level code fragments from binary code, for a
given compiler.

Katz (Deborah) et al. [11] use RNN models for snippet decompilation with addi-
tional post-processing techniques. They tokenize the binary input with a byte-by-byte
approach and the output with a C lexer. C tokens are ranked by their frequency and
replaced by the ranking position. Less frequent tokens (below a frequency threshold)
are replaced by a common number to minimize the vocabulary size. This transforma-
tion reduces the number of tokens, speeding up the training of the RNN. For the binary
input, a language model is created, and byte embeddings are found for the binary in-
formation. Once the encoder-decoder scheme is trained, translation from binary code
into C tokens is performed. The ﬁnal step is to apply several post-processing trans-
formations, such as deleting extra semicolons, adding missing commas, and balancing
brackets, parenthesis, and curly braces.

The previous work was later modiﬁed by Katz (Omer) et al. to reduce the compiler
errors in the output C code [17]. In this previous work, most of the output C code could
not be compiled because errors were found Therefore, they modiﬁed the decoder so that
it produces preﬁxed templates to be ﬁlled. This idea is inspired by delexicalization [21].
Delexicalized features are n-gram features where references to a particular slot or value
are replaced with a generic symbol. In this way, locations in the output C source code
are substituted with placeholders. After the translation takes place, those placeholders
are replaced with values and constants taken from the binary input, improving the code
recovery process up to 88%.

Coda [18] is an end-to-end neural-based framework for code decompilation. First,
Coda employs an instruction type-aware encoder and a tree decoder for generating an
abstract syntax tree (AST), using attention feeding during the code sketch generation
stage. Afterwards, it updates the code sketch using an iterative error-correction ma-
chine, guided by an ensembled neural error predictor. An approximate candidate is
found ﬁrst. Then, the candidate is ﬁxed to produce a compilable program. Evalua-
tion results show that Coda achieves 82% program recovery accuracy on unseen binary
samples.

Schulte et al. [12] propose genetic programming to generate readable C source code

5

from compiled binaries. Taking binary code as the input, an evolutionary search seeks
a combination of source code excerpts from a big code database. That source code is
compiled into an executable, which should be byte-equivalent to the original binary.
The decompiled source code reproduces the behavior, both intended and unintended,
of the original binary. As they use evolutionary search, decompilation time can vary
dramatically between executions.

Mycroft [22] proposes a type reconstruction algorithm that uses uniﬁcation to re-
cover types from binary code. Mycroft starts by transforming the binary code into a
register transfer language (RTL) intermediate representation. The RTL representation
is then transformed into a single static assignment (SSA) form to undo some opti-
mizations performed by the compiler. Then, each code instruction is used to generate
constraints about the type of its operands, regarding their use. Those constraints are
used to recover types by applying a modiﬁed version of Milner’s W algorithm [23].
In this variation, any constraint violation causes type reconstruction (recursive structs
and unions) instead of premature termination. This work does not discuss stack-based
variables, only register-based ones.

All these works represent complement methods, rather than alternatives, to the
established approaches used in decompiler implementations, such as value-based, ﬂow-
based and memory-based analyses [24].

3.2. Other uses of machine learning for code reversing

Apart from recovering high-level type information from binaries, other works use
machine learning for diﬀerent code reversing purposes [25]. Rosenblum et al. [26] use
CRFs to detect function entry points (FEPs). They use n-grams of the generalized
instructions surrounding FEPs, together with a call graph representing the interaction
between FEPs. The FEP detection problem consists in ﬁnding the boundaries of each
function in the binary code. CRFs allow using both sources of information together.
Since standard inference methods for CRFs are expensive, they speed up training with
approximate inference and feature selection. Nonetheless, feature selection took 150
days of computation on 1171 binaries. This approach does not seem to be tractable for
big code scenarios.

Bao et al. [27] utilize weighted preﬁx trees (or weighted tries) [28] to detect FEPs,
considering generalized instructions as tree nodes. Once trained, each node represents
the likelihood that the sequence from the root node to the current node will be an FEP.
They trained the model with 2064 binaries in 587 computing hours, obtaining better
results than [26]. The approach of Shin et al. [29] uses RNNs for the same problem.
The internal feedback loops of RNNs makes them suitable to handle sequences of bytes.
This approach reduces training time to 80 computing hours using the same dataset as
Bao et al. [27], while performing slightly better.

Rosenblum et al. [30] use CRFs to detect the compiler used to generate binary
ﬁles. Binaries frequently exhibit gaps between functions. These gaps may contain data
such as jump tables, string constants, regular padding instructions and arbitrary bytes.
While the content of those gaps sometimes depends on the functionality of the program,

6

they sometimes denote compiler characteristics. A CRF model is built to exploit these
diﬀerences among binaries and infer the compiler used to generate the binaries. They
later extend this idea to detect diﬀerent compiler versions and optimization levels [31].
Malware detection is another ﬁeld related to binary analysis where machine learning
has been used [32]. Alazab et al. [33] separate malware from benign software by analyz-
ing the sequence of Windows API calls performed by the program. First, they process
the binaries to extract all the Windows API invocations. Then, those sequences are
vectorized and used to build eight diﬀerent supervised machine learning models. Those
models are ﬁnally evaluated, ﬁnding that support vector machine (SVM) with a nor-
malized poly-kernel classiﬁer is the method with the best results. SVM achieves 98.5%
accuracy.

Rathore et al. [34] detect malware by analyzing opcodes frequency. They use various
machine learning algorithms and deep learning models. In their experiments, random
forest outperforms deep neural networks. Static analysis of the assembly code is used to
generate multi-dimensional datasets representing opcode frequencies. Diﬀerent feature
selection strategies are applied to reduce dimensionality. They collect binaries from
diﬀerent sources, selecting 11,688 ﬁles with malware and 2,819 benign executables.
The dataset is balanced with adaptive synthetic (ADASYN) sampling. Random forest
obtained 99.78% accuracy.

4. System architecture

Figure 2 shows the architecture of our system. It receives C source code as an input
and generates diﬀerent machine learning models. Each model is aimed at predicting
the high-level type returned by the functions in a program, by just receiving its binary
representation.

The system starts instrumenting C source code. This step embeds annotations in
the input program to allow associating high-level constructs to their binary representa-
tion (Section 4.1) [35]. After that, the instrumented source code is compiled to obtain
the binaries. A pattern extraction process analyzes the binaries looking for the anno-
tations, collecting the set of binary patterns related to each function invocation and
return expression. Finally, the resulting dataset is created, where binary patterns are
associated with the return type of each function.

Table 1 shows the simpliﬁed structure of datasets generated by our system. Each
row (individual, instance or sample) represents a function from the input C source
code. Each column (feature or independent variable) but the last one represents a
binary pattern found by the pattern extraction process. For example, the ﬁrst pattern
in Table 1 is the assembly code for a return expression of some functions returning an
int literal. That value is moved to the eax 32-bit register, followed by the code that all
functions use to return to the caller (callee epilogue). The second feature is the binary
code of a function invocation (caller epilogue) followed by a cwde instruction. Since
cwde converts the signed integer representation from ax (16 bits) to eax (32 bits), the

7

Figure 2: System architecture.

target class in the dataset is set to the short high-level type (Section 4.3 explains how
the dataset is built).

After creating the dataset, the system trains diﬀerent classiﬁers following the method-
ology described in Section 5. The forthcoming subsections detail each of the modules
in the architecture.

4.1. Instrumentation

As mentioned, much high-level information is discarded in the compilation process.
One example is the association between a high-level return statement and its related
assembler instructions. There is not a direct way to identify the binary code gener-
ated for a return statement. For this reason, our instrumentation process includes
no-operational code around some syntactic constructs in the input C program. The
instrumented code does not change the semantics of the program, but help us ﬁnd the
binary code generated for diﬀerent high-level code snippets.

The left-hand side of Figure 3 shows an example of the original C function, and its
right-hand side presents the instrumented version. The function dummy performs no

8

Generation ofsupervised machinelearning models(RET)
mov eax, literal
callee epilogue
1
0
...
0
0

func1
func2
...
funcn-1
funcn

(POST CALL)
caller epilogue
cwde

0
1
...
1
0

... Return type

...
...
...
...
...

int
short
...
short
double

Table 1: Example dataset generated by our system.

Figure 3: Original C source code (left) and its instrumented version (right).

action. Its invocation is added to provide information about the name and return type
of the high-level function in the binary code.

The example in Figure 3 also shows the instrumentation included to delimit the
binary code of the expressions to be returned by a function. To this end, diﬀerent
labels are added before every return statement. The end of the returned

RETURNn

expression is delimited by the retn assembly instruction.

4.2. Compilation

The instrumented C source code is compiled to obtain the binaries. In this work, we
only use the Microsoft cl compiler to build native applications for Intel x86 32-bit mi-
croprocessors. The compilation parameters are the default ones. We plan to apply the
method proposed in this article with other compilers, parameters and microprocessors.

4.3. Pattern extraction

The pattern extraction module performs three processes: extraction of binary chunks,

pattern generalization and dataset generation.

4.3.1. Extraction of binary chunks

RETURNn

The ﬁrst task is to extract the binary chunks associated with every return and
function invocation statements. For return statements, we get the binary code be-
label and the next retn instruction. Since we do not know how
tween a
many instructions are suﬃcient to predict the return type (one high-level expression
may produce many low-level instructions), we generate diﬀerent binary patterns with a
growing number of binary instructions before retn. Figure 4 shows an example binary
chunk for a given return statement. The smallest pattern goes from instr1 to retn.

9

char toupper(char c) {     if (c >= a && c <= z)         return c + A - a;     return c; } char toupper(char c) {     __dummy__("__O4D__:toupper:char");    if (c >= a && c <= z)         __RETURN1__: return c + A - a;    __RETURN2__: return c; } binary chunk

RETURNn :
instrn
instrn-1
...
instr2
instr1
retn

Figure 4: Diﬀerent RET patterns taken from a binary chunk.

The following contains instr2, instr1 and retn. The biggest one contains the whole
chunk, from instrn to retn. We call these types of binary sequences RET patterns.

The other kind of binary chunks we retrieve are the sequences of instructions after
each function invocation. We consider the call instruction, the optional add esp, lit-
eral instruction used to pop the invocation arguments from the stack, and the following
assembly instructions. We call these binary sequences POST CALL patterns.

As for RET patterns, we ﬁrst created diﬀerent POST CALL patterns with an in-
creasing number of instructions after function call and stack restoration. However,
after evaluating the classiﬁcation models built from the datasets, we realized that only
the ﬁrst assembly instruction was used by the models to predict the returned type.
Therefore, we only consider the ﬁrst instruction after stack restoration in POST CALL
patterns.

4.3.2. Pattern generalization

If we use the exact representation of binary instructions as features in the dataset,
the machine learning algorithms will consider instructions such as mov eax, 32 and
mov eax, 33 to be diﬀerent. If we do not generalize such instructions to represent the
same feature, the predictive models will not be accurate enough and the information
extracted from them (Section 7) will not be understandable. For instance, the previous
assembly instructions are generalized by our system to the two following patterns:
mov eax, literal and mov reg, literal (one instruction may be represented by diﬀerent
generalizations).

Table 2 shows diﬀerent generalization examples implemented by our system. Operand
is the most basic generalization, which groups some types of operands, such as literals,
addresses and indirections. Mnemonic generalizations group instructions with similar
functionalities, such as mov, movzx y movsx. The last type of generalizations, Sequence,
clusters sequences of instructions that appear multiple times in the binary code. For
instance, callee epilogue and caller epilogue appear, respectively, before returning one
expression and after invoking a function.

10

Instruction sequences

Generalized pattern

d
n
a
r
e
p
O

sub al, 1
mov ecx, [ebp+var 1AC8]
mov ecx, [ebp+var 1AC8]
push offset $SG25215
movsd xmm0, ds: real@43e2eb565391bf9e
jmp loc 22F
mov cx, [ebp+eax*2+var 10]
mov cx, [ebp+eax*2+var 10]
mov cx, [ebp+eax*2+var 10]

sub al, literal
mov ecx, [ebp+literal ]
mov ecx, [reg]
push address
movsd xmm0, *address
jmp oﬀset
mov cx, [ebp+eax*literal1+literal2]
mov cx, [ebp+reg*literal1+literal2]
mov ecx, [reg]

. movzx ecx, [ebp+var A]
m
e
n
M

movsx ecx, global var 1234
mov [eax], edx

mov ecx, [ebp+var A]
mov ecx, global var 1234
mov [eax], edx

pop esi
pop edi
mov esp, ebp
pop ebp
retn

mov esp, ebp
pop ebp
retn

pop ebp
retn

call func56
add esp, 4

call proc2

e
c
n
e
u
q
e
S

mov eax, 0
mov ebx, eax
mov [ebp+var 8], ebx

ja loc D9B6B
mov [ebp+var 10], 1
jmp loc D9B72
mov [ebp+var 10], 0

callee epilogue

callee epilogue

callee epilogue

caller epilogue

caller epilogue

mov chain([ebp+var 8], ebx, eax, 0)

bool cast([ebp+var 10])

Table 2: Generalization examples made by our system. reg variables represent registers, literal inte-
ger literals, address absolute addresses, *address absolute addresses dereferencing and oﬀset relative
addresses.

4.3.3. Dataset creation

After pattern generalization, the datasets are created before training the models
(Table 1). Each cell in the dataset indicates the occurrence of each pattern (column)
in every single function (row) in the program. RET patterns are associated with the
function bodies, but POST CALL patterns are related to the invoked function. For
example, if a function f is invoked in the body of a function g, the POST CALL pattern
will be associated with the row representing the function f, not g.

Finally, the return type (target) of each function should be added to the dataset.

Our system gets that information from the string parameter passed to the
function added in the instrumentation process.

dummy

11

Project

Functions

LoC Description

arcadia
bgrep
c ray tracer

jansson

libsodium

lua 5.2.3
masscan
slre

Total

121
5
52

176

642

820
496
17

3,590

Implementation of Arc, a Lisp dialecta.

1,063

7,020

252 Grep for binary codeb.
Simple ray tracerc.
Library for encoding, decoding and manipulating JSON
data d.
Library for encryption, decryption, signatures and password
hashinge.

35,645

14,588 The Lua programming languagef.
26,316

IP port scannerg.

564 Regular expression libraryh.

2,329

89,038

a https://github.com/kimtg/arcadia
b https://github.com/elektrischermoench/bgrep
c https://web.archive.org/web/20150110171135/http://patrickomatic.com/c-ray-tracer
d https://github.com/akheron/jansson
e https://github.com/jedisct1/libsodium
f https://lua.org/download.html
g https://github.com/robertdavidgraham/masscan
h https://github.com/cesanta/slre

Table 3: Open-source C projects used.

5. Methodology

5.1. Dataset

Table 3 shows the diﬀerent open source C projects we used to create the dataset.
Although they sum 2329 functions and 89,038 lines of code, they do not represent suﬃ-
cient data to infer the types returned by functions. Unfortunately, there are not many
open-source C (not C++) programs compilable with Microsoft cl compiler. Moreover,
we want to generate a dataset with a balanced number of instances for each return
type, so the number of functions in Table 3 would even be lower. To increase the size of
our dataset, we developed an automatic C source code generator called Cnerator [36].
Cnerator generates valid C programs to be compiled with any standard ANSI C com-
piler. One of its modes allows the user to specify the number of functions to be created.
It also allows specifying diﬀerent probabilities of the synthetic code to be generated,
such as the average number of statements in a function, expression types, number and
types of local variables, and the kind of syntactic constructs to be generated, among
others. The generated programs fulﬁll the type rules of the C programming language,
so they are compiled without errors. Using those probabilities, we make Cnerator gen-
erate synthetic programs with unusual language constructs, which programmers rarely
use. This facilitates the creation of datasets covering a wide range of C programs.

The ﬁnal dataset comprises the source code of the “real” projects in Table 3 plus
the synthetic code generated by Cnerator. On one hand, the synthetic code provides a
huge number of functions, a balanced dataset, and all the language constructs we want
to include. On the other hand, real projects increase the probability of those patterns
that real programmers often use (e.g., most C programmers use int expressions instead

12

of bool for Boolean operations). This combination of real and synthetic source code
improves the predictive capability of our dataset.

5.2. Dataset size

Since Cnerator [36] allows us to generate any number of functions (individuals), we
should ﬁnd out the necessary number of synthetic functions to include in our dataset
in order to build models with the highest performance. To determine this number, we
conduct the following experiment. We start with a balanced dataset with 100 functions
for each return type. Then, we build diﬀerent classiﬁers (see Section 5.3) and evaluate
their accuracy. Next, we add 1000 more synthesized functions to the dataset, re-build
the classiﬁers and re-evaluate them. This process is repeated until the accuracy of
classiﬁers converge. To detect this convergence, we compute the coeﬃcient of variation
(CoV) of the last accuracies, stopping when that coeﬃcient is lower than 2%.

5.3. Classiﬁcation algorithms

We use the following 14 classiﬁers from scikit-learn [37]: logistic regression (Logis-
ticRegression), perceptron (Perceptron), multilayer perceptron (MLPClassifier),
Bernoulli na¨ıve Bayes (BernoulliNB), Gaussian na¨ıve Bayes (GaussianNB), multino-
mial na¨ıve Bayes (MultinomialNB), decision tree (DecisionTreeClassifier), ran-
dom forest (RandomForestClassifier), extremely randomized trees (ExtraTrees-
Classifier), support vector machine (SVC), linear support vector machine (Linear-
SVC), AdaBoost (AdaBoostClassifier), gradient boosting (GradientBoostingCla-
ssifier), k-nearest neighbors (KNeighborsClassifier).

In the process described in Section 5.2 to ﬁnd the optimal size of the dataset, we use
a stratiﬁed and randomized division of the dataset (StratifiedShuffleSplit class in
scikit-learn). 80% of the instances in the dataset are used for training and the remaining
20% for testing. Since each classiﬁer has a diﬀerent optimal size, we choose the greatest
optimal size among all the classiﬁers (results are shown in Sections 6.1.1 and 6.2).

5.4. Feature selection

As mentioned, our system generates a lot of features because, for each pattern, dif-
ferent generalizations are produced. Therefore, a feature selection mechanism would be
beneﬁcial to avoid the curse of dimensionality and enhance the generalization property
of the classiﬁers. Consequently, after creating the datasets with the optimal size, we
select the appropriate features to build each model.

We follow a wrapper approach [38] for each classiﬁcation algorithm, evaluating dif-
ferent feature selection techniques and selecting the one that obtains the best classiﬁca-
tion performance. If performance diﬀerences between two feature selection techniques
are not signiﬁcantly diﬀerent, we choose the one that selects a lower number of fea-
tures (see the results in Sections 6.1.2 and 6.2). The performance of feature selection
is evaluated with the training dataset (80% of the original one) using 3-fold stratiﬁed
cross-validation (StratifiedShuffleSplit).

13

We use both recursive feature elimination (RFECV) and selection from a model
(SelectFromModel) feature selection techniques. Given an external estimator that as-
signs weights to features, RFECV selects the features by recursively considering smaller
feature sets. SelectFromModel discards the features that have been rejected by some
other classiﬁers like tree-based ones. Four SelectFromModel conﬁgurations were used:
random forest and extremely randomized trees as classiﬁers to select the features; and
the mean and median thresholds to ﬁlter features by their importance score.

5.5. Hyperparameter tuning

After feature selection, we tune the hyperparameters of each model. To that end,
we use GridSearchCV, which performs an exhaustive search over the speciﬁed hyper-
parameter values. Similar to the feature selection process, the 80% training set is used
to validate the hyperparameters with 3-fold stratiﬁed cross-validation (Stratified-
ShuffleSplit).

The ﬁnal hyperparameters selected for each classiﬁer are available at [39]. For the
multilayer perceptron neural network, we use a single hidden layer with 100 units, the
sigmoid activation function, Adam optimizer, and softmax as the output function.

5.6. Evaluation of model performance

After feature selection and hyperparameter tuning, we create and evaluate diﬀerent
models (one for each algorithm in Section 5.3) to predict the type returned by functions.
As mentioned, the dataset has real functions coded by programmers, and synthetic ones
generated by Cnerator. We consider these two types of code to deﬁne three diﬀerent
methods to evaluate the performance of the classiﬁers:

1. Mixing real and synthetic functions. This is the simplest evaluation method,
where real and synthetic functions are merged in the dataset. 80% of them are
used for training and the remaining 20% for testing, so sets have the same percent-
age of real and synthesized functions. The training and test sets are created with
stratiﬁed randomized selection, making all the classes to be equally represented
in both sets.

2. Estimate the necessary number of real functions for training. Since we have lots
of synthetic functions, we want to estimate to what extent synthetic programs
can be used to classify code written by real programmers. We ﬁrst create a
model only with all the synthetic functions generated to determine the dataset
size (Section 5.2) and test it with the real functions. Then, we include 1% of
real functions in the training dataset, rebuild and retest the models, and see the
accuracy gain. This process stops when the CoV is lower than 1% for the last 10
accuracies. The obtained percentage of real functions in the training set indicates
how many real functions are necessary to build accurate predictive models (32%
for the experiment in Section 6.1 and 48% for that in Section 6.2). Decision tree
was the classiﬁer used to estimate this value.

14

3. Prediction of complete real programs. This evaluation method measures predic-
tion for source code written by programmers whose code has not been included
in the test dataset. One real program is used to build the test dataset, and no
functions of that program are used for training. In this case, we evaluate whether
our system is able to predict return types for unknown programming styles.

5.7. Selected decompilers

We compare our models with the following existing decompilers:

– IDA Decompiler [40]. This is a plugin of the commercial Hex-Rays IDA dis-
assembler [41]. This product is the result of the research works done by Ilfak
Guilfanov [42, 43]. This tool is the current de facto standard in software reverse
engineering.

– RetDec [44]. An open-source decompiler developed initially by Kˇroustek [45],
currently maintained by the AVAST company. It can be used as a standalone
application or as a Hex-Rays IDA plugin. To avoid the inﬂuence of the Hex-Rays
IDA decompiler, we use the standalone version.

– Snowman [46]. Open-source decompiler based on the TyDec [47] and Smart-
Dec [48] proposals. Similar to RetDec, it can also be used as a standalone appli-
cation or as a Hex-Rays IDA plugin. We use the standalone version.

– Hopper [49]. A commercial decompiler developed by Cryptic Apps. Although it
is mainly focused on decompiling Objective-C, it also provides C decompilation
of any Intel x86 binary.

We also considered other alternatives that we ﬁnally did not include in our evalua-
tion. DCC [2] and DISC [50] decompilers do not work with modern executables. The
former is aimed at decompiling MS-DOS binaries, while the latter only decompiles bi-
naries generated with TurboC. Boomerang [3] and REC [51] are no longer maintained.
Phoenix [52] is built on the top of the Binary Assembly Platform [16], which lacks
support for ﬂoating-point instructions. Lastly, we could not ﬁnd the implementations
of the DREAM [53] and DREAM++ [54] decompilers.

5.8. Data analysis

For each classiﬁer, we compute its performance following the three diﬀerent evalu-
ation methods described in Section 5.6. We repeat the training plus testing process 30
times, computing the mean, standard deviation and 95% conﬁdence intervals of model
accuracies. This allows us to compare accuracies of diﬀerent models, checking whether
two evaluations are signiﬁcantly diﬀerent when their two 95% conﬁdence intervals do
not overlap [55]. Figures showing model accuracies (Figures 8 and 11) display the 95%
conﬁdence intervals as whiskers.

15

In a balanced multi-class classiﬁcation, overall precision and recall are usually com-
puted as the average values for each class. These aggregate metrics are called macro-
precision and macro-recall [56]. Likewise, macro-F1-score can be computed as the
average of per-class F1-score [56], or as the harmonic mean of macro-precision and
macro-recall [57]. We use the ﬁrst alternative because it is less sensitive to error type
distribution [58]. For the sake of brevity, we use precision, recall and F1-score to refer
to the actual macro-precision, macro-recall and macro-F1-score measurements.

We run all the code in a Dell PowerEdge R530 server with two Intel Xeon E5-2620
v4 2.1 GHz microprocessors (32 cores) with 128GB DDR4 2400 MHz RAM, running
an updated version of Windows 10 for 64 bits.

6. Evaluation

In the assembly language, the concept of type is related to the size of values more
than to the operations that can be done with those values. For example, the integer
add instruction works with 8 (ah), 16 (ax) and 32 bits (eax), but it is not checked
whether the accumulator register is actually holding an integer. For this reason, in this
paper we evaluate two diﬀerent kinds of models: those considering types by their size
and representation (Section 6.1), and those considering types by the operations they
support—i.e., high-level C types (Section 6.2).

The ﬁrst kind of models predicts return types when they have diﬀerent size or
representation. In this way, these models separate short (2 bytes) from int (4 bytes).
They also tell the diﬀerence between int and float, because, even though their size is
4 bytes, their representations are diﬀerent (integer and real). On the contrary, char and
bool are not distinguished in the ﬁrst kind of models since they both are 1-byte sized
and hold integer values (C does not provide diﬀerent operations for char and bool).

After building and evaluating these type-by-size-and-representation models (Sec-
tion 6.1), we deﬁne additional mechanisms to distinguish among types with similar sizes
to improve our models. Thus, Section 6.2 shows additional generalization patterns to
improve the classiﬁcation of high-level return types. With those enhancements, our
models improve the diﬀerentiation among types with the same size such as char and
bool, and int and pointer4.

6.1. Grouping types by size and representation

In binary code, the value returned by a function is passed to the caller via registers.
CPUs have diﬀerent kinds of registers depending on their sizes and representations
(integer or a ﬂoating-point number). In the particular case of Intel x86, registers can
hold integer values of 8-, 16- or 32-bit, and 32- or 64-bit ﬂoating-point numbers.

Table 4 shows the target variable deﬁned for this ﬁrst kind of models and their
corresponding C type. For INT 2, INT 8, REAL 4 and VOID, the class used corresponds

4Note that, in assembly, there is no diﬀerence in the representation of integers, characters, Booleans

and pointers, because, for the microprocessor, all of them hold integer values.

16

Target C high-level type

INT 1
INT 2
INT 4
INT 8
REAL 4
REAL 8
VOID

bool and char
short
int, long, pointer, enum and struct
long long
float
double and long double
void

Table 4: Relationship between the target variable (types grouped by size and representation) and the
C high-level types.

Figure 5: The left-side code is transformed by cl into the right-side code to allow returning structs in
eax, which are actually passed to the caller as pointers.

with a single high-level type. REAL 8 groups double and long double, while INT 8
considers all the C types returned in the 32-bit eax register (structs are actually returned
as pointers).

Pointers are represented as INT 4 because the size of memory addresses in Intel
x86 are 32 bits (4 bytes). The struct type is also clustered as INT 4, because the cl
compiler transforms returned structs into pointers to structs, as depicted in Figure 5.
The returned pointer to struct is actually the result pointer passed as an argument.
In this way, the actual struct is a local variable created in the scope of the caller (s
variable in Figure 5), making easy the management of the memory allocated for the
struct. This is the reason why the actual value returned is not a struct but a pointer
(4 bytes).

The union type constructor is not listed in Table 4 because it has variable size and
representation. When the size of the biggest ﬁeld is not bigger than 32 bits, 4 bytes are
used. When it is higher than 4 bytes and lower or equal to 8, 64 bits are used. In case it
is greater than 8 bytes, the compiler generates the same code as for structs (Figure 5).

17

typedef struct stats stats_t;  stats_t init_stats() {     stats_t s;     /* … function code … */     return s; }  void main() {     stats_t s = init_stats();     /* … some code … */     double mean = ((double)s.sum) /     ↪    ((double)s.count); } typedef struct stats stats_t;  stats_t* init_stats(stats_t *result) {     stats_t s;     /* … function code … */     *result = s;     return result; }  void main() {     stats_t s;     stats_t *result = init_stats(&s);     /* … some code … */     double mean = ((double)result->sum) /     ↪    ((double)result->count); } 6.1.1. Data size

Figure 6: Classiﬁers accuracy for increasing number of functions (classiﬁers of types with diﬀerent size
and representation).

As mentioned in the methodology section, we use Cnerator to produce a dataset
with such a number of functions that make models accuracies to converge. Figure 6
shows how classiﬁers accuracies grow as the dataset size increases. Figure 7 presents
the CoV of the last 10 values. We can see how, with 26,000 functions, the CoVs of
the accuracies for all the classiﬁers are below 2%. Since CoV is computed for the last
10 iterations, and each iteration increases 1,000 functions, we build the dataset with
16,000 functions (and their invocations). In addition to those 16,000 instances, we add
the 2,329 functions retrieved from real programs (Table 3).

6.1.2. Feature selection

We apply the ﬁve feature-selection methods described in Section 5.4 (RFECV and
SelectFromModel with random forest and extremely randomized trees, with mean and
median thresholds) and select, for each classiﬁer, the one with best accuracy. Table 5
shows the best feature selection method for each classiﬁer, together with the number of
features selected. The 1,019 features of the original dataset are reduced, on average, to
366. The selected features produce statistically signiﬁcant higher accuracy for Gaussian
na¨ıve Bayes (3.22% better) and multilayer perceptron (3.52%). Moreover, the lower
number of features reduces training times and overﬁtting.

18

0%10%20%30%40%50%60%70%80%90%Numberof functionsSupport vector machineLinear support vector machineMultinomial naïve BayesGaussian naïve BayesBernoulli naïve BayesMultilayer perceptronPerceptronLogistic regressionRandom forestExtremely randomized treesDecision treeK-nearest neighborsAdaboostGradient boostingAccuracyFigure 7: Coeﬃcient of variation of the last 10 accuracy values in Figure 6 (classiﬁers of types with
diﬀerent size and representation).

6.1.3. Hyperparameter tuning

We tune hyperparameters of the models as described in Section 5.5. For the hy-
perparameters found, AdaBoost increased its accuracy by 7.85%. However, the rest of
the classiﬁers obtained accuracy gains below 2%, compared to the scikit-learn default
parameters. The hyperparameters used for each model are detailed in [39].

6.1.4. Results

Figure 8 shows the accuracies of the 14 trained models (left-hand side) and the
selected decompilers (right-hand side). All the systems are evaluated with the three
methods described in Section 5.6. It can be seen how all the classiﬁers created with our
dataset perform better than the existing decompilers, for all the evaluation methods.

In Figure 8, we can also see that there are signiﬁcant diﬀerences between the ﬁrst
evaluation method and the two last ones, for all the machine learning models. This
shows how the common evaluation method that takes 80% for training and 20% for
testing is too optimistic for this project. We need to feed the models with suﬃcient
code written by real programmers, so that we are able to predict return types with
diﬀerent programming styles. Existing decompilers show no inﬂuence on the evaluation
method, because they use deterministic algorithms to infer return types.

One discussion related to the second evaluation method is ﬁnding out the number
of real functions to be included in the training set, so that return types could be
inferred for unknown real code. As described in Section 5.6, we start with a test set of

19

0%1%2%3%4%5%6%7%8%Coefficientof variationof accuracies(10 lastvalues)Support vector machineLinear support vector machineMultinomial naïve BayesGaussian naïve BayesBernoulli naïve BayesMultilayer perceptronPerceptronLogistic regressionRandom forestExtremely randomized treesDecision treeK-nearest neighborsAdaboostGradient boostingNumberof functionsClassiﬁer

Applied method (wrapped algorithm, accuracy threshold)

Selected features

AdaBoost
Bernoulli na¨ıve Bayes
Decision tree
Extremely randomized trees
Gaussian na¨ıve Bayes
Gradient boosting
K-nearest neighbors
Linear support vector machine
Logistic regression
Multilayer perceptron
Multinomial na¨ıve Bayes
Perceptron
Random forest
Support vector machine

SelectFromModel(Random forest, Mean)
SelectFromModel(Random forest, Median)
SelectFromModel(Extremely randomized trees, Median)
SelectFromModel(Extremely randomized trees, Median)
SelectFromModel(Extremely randomized trees, Mean)
SelectFromModel(Extremely randomized trees, Median)
SelectFromModel(Random forest, Median)
SelectFromModel(Random forest, Median)
SelectFromModel(Random forest, Median)
SelectFromModel(Extremely randomized trees, Median)
SelectFromModel(Random forest, Median)
SelectFromModel(Random forest, Median)
SelectFromModel(Random forest, Mean)
SelectFromModel(Extremely randomized trees, Median)

134
439
419
419
130
419
439
439
439
419
439
439
134
419

Table 5: Best feature selection method used for each classiﬁer (classiﬁers of types with diﬀerent size
and representation).

Figure 8: Accuracies of our classiﬁers and the existing decompilers, using the three diﬀerent evaluation
methods described in Section 5.6 (classiﬁers of types with diﬀerent size and representation).

20

0%10%20%30%40%50%60%70%80%90%ClassifiersDecompilersEvaluation method 1Evaluation method 2Evaluation method 3AccuracyFigure 9: Accuracy of a decision tree for diﬀerent percentage of real functions included in the training
dataset (classiﬁers of types with diﬀerent size and representation). The red dot indicates the value
where the CoV of the last 10 accuracies is lower than 1%.

Accuracy

Precision

Recall

F1-score

s
r
e
ﬁ
i
s
s
a
l
C

AdaBoost
Bernoulli na¨ıve Bayes
Decision tree
Extremely randomized trees
Gaussian na¨ıve Bayes
Gradient boosting
K-nearest neighbors
Linear support vector machine
Logistic regression
Multilayer perceptron
Multinomial na¨ıve Bayes
Perceptron
Random forest
Support vector machine

0.816 ± 1.40%
0.801 ± 0.51%
0.812 ± 1.45%
0.833 ± 1.39%
0.616 ± 0.59%
0.839 ± 1.44%
0.785 ± 1.51%
0.801 ± 0.95%
0.821 ± 1.52%
0.814 ± 1.37%
0.815 ± 1.51%
0.821 ± 1.53%
0.833 ± 1.37%
0.774 ± 0.64%

0.805 ± 1.01%
0.815 ± 0.54%
0.821 ± 1.10%
0.798 ± 0.42%
0.830 ± 0.35%
0.801 ± 0.43%
0.803 ± 1.06%
0.817 ± 1.21%
0.814 ± 0.55%
0.823 ± 1.04%
0.839 ± 1.22% 0.831 ± 0.50%
0.687 ± 0.61%
0.650 ± 0.60%
0.736 ± 0.35%
0.863 ± 1.15% 0.832 ± 0.51% 0.838 ± 1.05%
0.783 ± 1.07%
0.805 ± 0.54%
0.798 ± 1.22%
0.806 ± 0.90%
0.827 ± 1.32%
0.823 ± 0.43%
0.812 ± 1.15%
0.817 ± 1.37% 0.834 ± 0.50%
0.809 ± 0.94%
0.821 ± 1.24% 0.831 ± 0.46%
0.807 ± 1.14%
0.829 ± 0.52%
0.819 ± 1.32%
0.818 ± 1.28%
0.815 ± 0.82%
0.856 ± 1.74%
0.823 ± 1.00%
0.840 ± 1.15% 0.830 ± 0.49%
0.782 ± 0.70%
0.809 ± 0.37%
0.807 ± 1.35%

. IDA decompiler
p
m
o
c
e
D

RetDec
Snowman
Hopper

0.583
0.290
0.544
0.333

0.495
0.111
0.365
0.132

0.413
0.133
0.328
0.132

0.415
0.110
0.322
0.079

Table 6: Performance of the classiﬁers and existing decompilers using the third evaluation method
(classiﬁers of types with diﬀerent size and representation). 95% conﬁdence intervals are expressed as
percentages. Bold font represents the best values. If one column has multiple cells in bold, it means
that values are not signiﬁcantly diﬀerent.

21

0%10%20%30%40%50%60%70%80%90%0%2%4%6%8%10%12%14%16%18%20%22%24%26%28%30%32%34%36%38%40%42%44%46%48%50%Real functions(training set) / real functions(dataset)Accuracy100% synthesized functions, and incrementally add real functions until model accuracy
converges. Figure 9 shows this inﬂuence of real functions on the classiﬁer accuracy. The
red dot shows that with 32% of real functions, CoV of model accuracy falls below 1%.
We ﬁxed that value for the second evaluation method.

Figure 8 also shows that there is no statistically signiﬁcant diﬀerence between the
second method and the third one (i.e., 95% conﬁdence intervals overlap [55]). This
means that we need to include in the training dataset at least 32% of real functions, so
that the trained models are able to predict return types of code written by programmers
not included in the training dataset.

Table 6 shows the accuracy, precision, recall and F1-score of our models and the
existing compilers, using the third evaluation method (Method 3 in Section 5.6). Gra-
dient boosting is the classiﬁer with the best results: 0.839 accuracy and 0.838 F1-score.
Sometimes, there are no signiﬁcant diﬀerences between random forest and extremely
randomized trees. Gradient boosting accuracy and F1-score are, respectively, 43.9%
and 101.9% higher than the best decompiler (IDA).

6.2. Classifying with high-level types

In the previous subsection, we measured the performance of our predictive mod-
els, considering types by their size and representation. However, the objective of a
decompiler is to infer the high-level C types, even if they share the exact size and rep-
resentation. Following the same methodology, we now conduct a new experiment to
reconstruct C types from binary code.

In this case, we consider the C built-in types bool, char, short, int, long long,
float, double and void. For the particular case of Microsoft cl and 32-bit architecture,
the long type is exactly the same as int, since the semantic analyzer allows the very
same operations and its target size and representation are the same; the same occurs for
double and long double. For this reason, long and long double types are considered
the same as, respectively, int and double. The signed and unsigned type speciﬁers
are not considered, as there is no diﬀerence between their binary representations.

We also consider pointer and struct type constructors. Arrays are not classiﬁed
because C functions cannot return arrays (they actually return pointers) [59]. As dis-
cussed in Section 6.1, the union type constructor is actually represented as one single
variable with the size of the biggest ﬁeld, so cl generates no diﬀerent code when the
type of the biggest ﬁeld is used instead of union. The same happens with enum and
int.

Our classiﬁers detect the struct and pointer type constructors, but not the struct
ﬁelds or the pointed type. Once we know the return type is pointer or struct, the
subtypes used to build the composite types could be obtained with existing deterministic
approaches [24, 22].

We ﬁrst analyze how well the exiting pattern generalizations (Table 2) classify high-
level return types. To that end, we conduct the following experiment. First, we deﬁne
the target variable as the high-level C types described above. Then, we build a decision
tree classiﬁer and evaluate it with a balanced test dataset comprising 6,000 functions.

22

bool

char

short

int

pointer

struct

long long

float

double

void

Predicted class

s
s
a
l
c

l
a
u
t
c
A

bool
char
short
int
pointer
struct
long long
float
double
void

483
285
7
4
4
0
0
0
0
3

61
179
85
38
16
0
4
1
0
2

0
60
395
67
3
0
0
1
0
0

1
16
38
184
48
10
5
0
0
2

12
13
24
111
365
6
28
16
12
3

0
0
0
132
106
584
3
0
0
0

41
45
44
59
55
0
554
40
52
41

0
0
0
0
0
0
0
358
170
0

0
0
1
0
0
0
0
184
365
0

2
2
6
5
3
0
6
0
1
549

Table 7: Confusion matrix for the decision tree classiﬁcation of high-level types, with a balanced
dataset comprising 6,000 functions.

The confusion matrix obtained is shown in Table 7. If we analyze the two types with
1-byte size (bool and char), we can see that 28.8% of the instances are misclassiﬁed
between bool and char. A similar misclassiﬁcation issue occurs for the 4-byte-size
types int, struct and pointer, mistaking 22.9% of the instances among these three
types.

Assembly pattern

cond jmp oﬀset1
mov [reg1], literal1
jmp oﬀset2
mov [reg1], literal2

Feature (Generalization)

where: cond jmp ∈ {jo, jno, js, jns, je, jz, jne, jnz, jb, jnae, jc,
(cid:44)→ jnb, jae, jnc, jbe, jna, ja, jnbe, jl, jnge, jge, jnl, jle,
(cid:44)→ jng, jg, jnle, jp, jpe, jnp, jpo, jcxz, jecxz}
literal1, literal2 ∈ {0, 1}
literal1 (cid:54)= literal2
oﬀset1 (cid:54)= oﬀset2

bool cast(reg1)

mov arg2, arg1
mov arg3, arg2
...
mov argn, argn-1

where: mov ∈ {mov, movzx, movsx}

arg1 ∈ {reg, [reg], *address, literal}
arg2, ..., argn ∈ {reg, [reg], *address}

mov arg2, arg1
mov arg3, arg2
...
mov argn, argn-1

where: arg1 ∈ {reg, [reg], *address, literal}

arg2, ..., argn ∈ {reg, [reg], *address}

bool cast(reg1)
mov chain(regax, regn, ..., reg1)
callee epilogue

where: regax ∈ {eax, ax, al, ah}

gen mov chain(argn, ..., arg1)

mov chain(argn, ..., arg1)

return bool cast(regax, regn, ..., reg1)

(continues)

23

Assembly pattern

Feature (Generalization)

mov chain(regax, regn, ..., reg1, literal)
callee epilogue

where: regax ∈ {eax, ax, al, ah}

literal ∈ {0, 1}

gen mov chain(reg2, reg1)
add reg2, 1
mov chain(reg1, reg2)

gen mov chain(reg2, reg1)
sub reg2, 1
mov chain(reg1, reg2)

gen mov chain(reg2, reg1)
add reg2, literal
mov chain(reg1, reg2)

where: literal > 1

gen mov chain(reg2, reg1)
sub reg2, literal
mov chain(reg1, reg2)

where: literal > 1

shl reg2, literal
mov chain(reg1, *address)
sub reg1, reg2

allmul

call
mov chain(reg, *address)
sub reg, eax

allmul

call
add eax, *address

sub int to ptr (reg1, *address, [reg2])
mov chain(regax, reg1)
callee epilogue

where: regax ∈ {eax, ax, al, ah}

sub int to ptr (reg1, *address, [reg2])
gen mov chain(regax, arg1, reg1)
callee epilogue

where: regax ∈ {eax, ax, al, ah}

arg1 ∈ {reg, [reg], *address}

add int to ptr (reg1, *address)
gen mov chain(regax, arg1, reg1)
callee epilogue

where: regax ∈ {eax, ax, al, ah}

arg1 ∈ {reg, [reg], *address}

lea reg1, *address
gen mov chain(regax, arg1, reg1)
callee epilogue

where: regax ∈ {eax, ax, al, ah}

arg1 ∈ {reg, [reg], *address}

return bool literal(regax, regn, ..., reg1)

inc int(reg1)

dec int(reg1)

inc ptr (reg1)

dec ptr (reg1)

sub int to ptr (reg1, *address, reg2)

sub int to ptr (reg, *address)

add int to ptr (*address)

return sub int to ptr (regax, reg1, *address,
(cid:44)→ [reg2])

return sub assign int to ptr (regax, arg1,
(cid:44)→ reg1, *address, [reg2])

return add assign int to ptr (regax, arg1,
(cid:44)→ reg1, *address)

return add assign int to ptr (regax, arg1,
(cid:44)→ reg1, *address)

(continues)

24

Assembly pattern

math op arg1
callee epilogue

where: math op ∈ {div, idiv}

arg1 ∈ {reg, [reg], *address}

math op arg1
mov chain(regax, regrem)
callee epilogue

where: math op ∈ {div, idiv}

arg1 ∈ {reg, [reg], *address}
regax ∈ {eax, ax, al}
regrem ∈ {edx, dx, ah}

math op arg1 arg2
mov chain(argax, reg1)
callee epilogue

Feature (Generalization)

return int math op(arg1)

return int math op(regax, arg1)

where: math op ∈ {imul, sub, add, sar, sal, shr, shl, xor, or, and}

return int math op(regax, arg1, arg2)

arg1 ∈ {reg, [reg], *address}
arg2 ∈ {reg, [reg], *address, literal}
regax ∈ {eax, ax, al, ah}

math op arg2
mov chain(regax, arg1)
callee epilogue

where: math op ∈ {div, idiv}

arg1 ∈ {reg, [reg], *address}
arg2 ∈ {reg, [reg], *address}
regax ∈ {eax, ax, al, ah}

math op arg2
mov chain(regax, arg1, regrem)
callee epilogue

where: math op ∈ {div, idiv}

arg1 ∈ {reg, [reg], *address}
arg2 ∈ {reg, [reg], *address}
regrem ∈ {edx, dx, ah}

math op arg2 arg3
mov chain(regax, arg1, arg2)
callee epilogue

return int math op assign(regax, arg1, arg2)

return int math op assign(regax, arg1, arg2)

where: math op ∈ {imul, sub, add, sar, sal, shr, shl, xor, or, and}

arg1 ∈ {reg, [reg], *address}
arg2 ∈ {reg, [reg], *address}
arg3 ∈ {reg, [reg], *address, literal}
regax ∈ {eax, ax, al, ah}

return int math op assign(regax, arg1, arg2,
(cid:44)→ arg3)

Table 8: New generalizations added to improve the classiﬁcation of high-level return types. reg vari-
ables represent registers, literal integer literals, address absolute addresses, *address absolute addresses
dereferences and oﬀset relative addresses. Variables between normal-font brackets ([]) represent op-
tional arguments, while typewriter-font brackets ([]) are the assembly brackets denoting register-based
dereferences.

This experiment shows us how there is still room for improving the classiﬁcation of
high-level types with similar size and representation. Aware of that, we include new
generalization patterns aimed at diﬀerentiating among high-level types with the same

25

Figure 10: Classiﬁers accuracy for increasing number of functions (classiﬁers of high-level types).

size and representation. To this end, we search for the misclassiﬁed functions in Table 7
and analyze the sequences of assembly code related to the same type. Once detected,
we generalize and include them in our pattern extractor (Figure 2) to improve the
classiﬁers. Therefore, we use machine learning to detect some of the limitations of the
existing classiﬁers, analyze potential binary patterns, deﬁne new features of the dataset,
and create better models to classify the return type of decompiled functions. The new
generalizations deﬁned are detailed in Table 8.

A new dataset is created with all the new generalization features in Table 8 to
classify the ten diﬀerent high-level types mentioned. We compute the optimal size of
the dataset with the algorithm described in Section 5.2. The resulting dataset has
18,000 synthetic functions (Figure 10) plus the 2,339 functions implemented by real
programmers. Following the methodology described in Section 5, we run diﬀerent fea-
ture selection algorithms, obtaining the features in Table 9. The existing 1,036 features
were reduced, on average, to 433. We then tune the hyperparameters of the mod-
els, achieving similar values to the previous experiment (their values can be consulted
in [39]).

6.2.1. Results

Figure 11 compares the accuracies of the new models and the selected decompilers
(detailed data is depicted in Table 10). All the models outperform the existing decom-
pilers. As in the previous case, the ﬁrst evaluation method has signiﬁcant diﬀerences
with the two last ones (which obtain similar results). For the second method, we found

26

0%10%20%30%40%50%60%70%80%Support vector machineLinear support vector machineMultinomial naïve BayesGaussian naïve BayesBernoulli naïve BayesMultilayer perceptronPerceptronLogistic regressionRandom forestExtremely randomized treesDecision treeK-nearest neighborsAdaboostGradient boostingNumberof functionsAccuracyClassiﬁer

Applied method (wrapped algorithm, accuracy threshold)

Selected features

AdaBoost
Bernoulli na¨ıve Bayes
Decision tree
Extremely randomized trees
Gaussian na¨ıve Bayes
Gradient boosting
K-nearest neighbors
Linear support vector machine
Logistic regression
Multilayer perceptron
Multinomial na¨ıve Bayes
Perceptron
Random forest
Support vector machine

SelectFromModel(Random forest, Mean)
SelectFromModel(Extremely randomized trees, Median)
SelectFromModel(Random forest, Median)
SelectFromModel(Random forest, Median)
SelectFromModel(Extremely randomized trees, Median)
SelectFromModel(Extremely randomized trees, Median)
SelectFromModel(Random forest, Median)
SelectFromModel(Random forest, Median)
SelectFromModel(Random forest, Median)
SelectFromModel(Extremely randomized trees, Median)
SelectFromModel(Extremely randomized trees, Median)
SelectFromModel(Random forest, Median)
SelectFromModel(Random forest, Median)
SelectFromModel(Random forest, Median)

147
456
455
455
456
456
455
455
455
456
456
455
455
455

Table 9: Best feature selection method used for each classiﬁer (classiﬁers of high-level types).

Figure 11: Accuracies of our classiﬁers and the existing decompilers, using the three diﬀerent evaluation
methods described in Section 5.6 (classiﬁers of high-level types).

27

0%10%20%30%40%50%60%70%80%Evaluation method 1Evaluation method 2Evaluation method 3AccuracyClassifiersDecompilersAccuracy

Precision

Recall

F1-score

s
r
e
ﬁ
i
s
s
a
l
C

AdaBoost
Bernoulli na¨ıve Bayes
Decision tree
Extremely randomized trees
Gaussian na¨ıve Bayes
Gradient boosting
K-nearest neighbors
Linear support vector machine
Logistic regression
Multilayer perceptron
Multinomial na¨ıve Bayes
Perceptron
Random forest
Support vector machine

0.764 ± 0.92%

0.741 ± 0.42%

0.740 ± 0.49%
0.741 ± 0.65%
0.783 ± 0.29% 0.822 ± 0.33% 0.785 ± 0.31% 0.792 ± 0.31%
0.772 ± 0.85%
0.751 ± 0.59%
0.750 ± 0.47%
0.753 ± 0.47%
0.793 ± 0.82%
0.772 ± 0.56%
0.771 ± 0.42%
0.774 ± 0.45%
0.692 ± 0.35%
0.595 ± 0.67%
0.619 ± 0.51%
0.671 ± 0.88%
0.783 ± 0.31% 0.791 ± 0.30%
0.786 ± 0.31% 0.820 ± 0.29%
0.743 ± 0.57%
0.749 ± 0.41%
0.759 ± 0.78%
0.745 ± 0.40%
0.780 ± 0.59%
0.803 ± 0.81%
0.779 ± 0.42%
0.782 ± 0.34%
0.785 ± 0.37%
0.802 ± 0.58% 0.785 ± 0.36%
0.780 ± 0.36%
0.788 ± 0.43%
0.815 ± 0.65% 0.784 ± 0.37%
0.783 ± 0.36%
0.789 ± 0.30%
0.818 ± 0.29% 0.785 ± 0.31%
0.781 ± 0.28%
0.749 ± 1.23%
0.747 ± 1.18%
0.789 ± 1.20%
0.745 ± 1.35%
0.773 ± 0.52%
0.773 ± 0.46%
0.796 ± 0.75%
0.771 ± 0.43%
0.784 ± 0.32%
0.779 ± 0.33%
0.816 ± 0.34%
0.774 ± 0.35%

. IDA decompiler
p
m
o
c
e
D

RetDec
Snowman
Hopper

0.40
0.15
0.29
0.14

0.33
0.06
0.22
0.08

0.34
0.10
0.26
0.09

0.30
0.06
0.21
0.03

Table 10: Performance of the classiﬁers and existing decompilers using the third evaluation method
(classiﬁers of types with diﬀerent size and representation). 95% conﬁdence intervals are expressed as
percentages. Bold font represents the best values. If one column has multiple cells in bold, it means
that values are not signiﬁcantly diﬀerent.

bool

char

short

int

pointer

struct

long long

float

double

void

Predicted class

s
s
a
l
c

l
a
u
t
c
A

bool
char
short
int
pointer
struct
long long
float
double
void

491
231
6
6
12
0
5
0
0
4

49
212
76
44
8
0
8
0
0
1

0
64
379
72
0
0
0
0
0
1

8
20
45
279
35
1
6
0
0
1

10
23
36
111
419
12
28
11
12
4

0
0
0
21
72
587
4
0
0
0

39
45
49
59
47
0
543
48
44
48

0
0
0
0
0
0
0
355
177
0

0
0
0
0
0
0
1
184
365
0

3
5
9
8
7
0
5
2
2
541

Table 11: Confusion matrix of the model in Table 7, including the generalizations of Table 8.

that at least 48% of the real functions must be included in the training dataset to obtain
accuracy convergence (Figure 12). With this percentage of real functions, our models
are able to predict functions of programmers whose code is not included in the training
set (i.e., there are no signiﬁcant diﬀerences between the second and third evaluation
method).

Table 10 shows how gradient boosting obtains the best performance: 0.786 accuracy
and 0.791 F1-score for the third evaluation method. Comparing these values with the
existing decompilers, the performance of gradient boosting is from 96.5% (accuracy)
to 163.6% (F1-score) higher than the decompiler with the highest performance (IDA).
Therefore, the gradient boosting vs IDA beneﬁt is increased by 90.1% when predicting
high-level C types.

Table 11 shows the new values of the confusion matrix presented in Table 7, running
the same experiment with the new generalizations. The performance gains obtained

28

Figure 12: Accuracy of a decision tree for diﬀerent percentage of real functions included in the training
dataset (classiﬁers of high-level types). The red dot indicates the value where the CoV of the last 10
accuracies is lower than 1%.

Accuracy gain Precision gain Recall gain F1-score gain

1 bool
char

T
N
I

4 int
T
N
I

pointer
struct

0.84%
1.01%

1.81%
0.58%
2.56%

5.85%
14.86%

16.70%
1.69%
21.23%

1.66%
18.44%

51.63%
14.79%
0.51%

3.98%
17.01%

37.76%
7.90%
11.55%

Table 12: Performance gains obtained for high-level type classiﬁcation, when the generalizations in
Table 8 are included in the dataset.

when classifying types with the same size and representation are summarized in Ta-
ble 12. We obtain a 10.5% average F1-score gain for 1-byte-size types and 19.1% for
types of 4 bytes, due to the additional generalizations detailed in Table 8.

7. Extracted patterns

In addition to creating models for classifying return types, our dataset can be used
to discover and document binary patterns to be included in existing decompilers. That
is, we can mine the dataset to document binary patterns associated with high-level
return types. That documentation can be helpful to improve the implementation of
current decompilers. This is the objective of this section.

We discover binary patterns with association rules that correlate RET and POST
CALL patterns with return types of function. Since the dataset has a high number
of features, we ﬁrst select the most important features with the ﬁve feature-selection
algorithms described in Section 5.4. We choose the intersection of the feature sets
selected by the classiﬁers. Then, we run the Apriori algorithm for association rule
mining [60], saving the rules whose consequent is a return type.

In this paper, we only analyze the rules with 100% conﬁdence (i.e., the consequent
always holds when the antecedent is true). In this way, the association rules retrieved
represent a mechanism to document those RET and POST CALL binary patterns that
are unambiguously associated with a high-level return type.

29

0%10%20%30%40%50%60%70%80%90%0%4%8%12%16%20%24%28%32%36%40%44%48%52%56%60%64%68%AccuracyReal functions(training set) / real functions(dataset)Table 13 shows some of the rules obtained with at most two antecedents. The rest of
them are detailed in [61]. The support of each rule is the relative frequency of instances
covered by a rule. Rules with very low support are not included in Table 13.

The cdecl calling convention [62] returns 32-bit values through eax; ax is used for
16-bit values, and al for 8-bits. 64-bit integers are returned via edx and eax registers.
The 32- and 64-bit real values are returned through st0. The main problem is to
determine whether such registers are actually returning their values to the caller, or
they just hold temporary values of previous computations. Another important problem,
as mentioned, is to classify the high-level types with equal size and representation.

Antecedents

(RET)

mov al, literal
callee epilogue

1

(POST CALL)

caller epilogue
movzx edx, al

2

3

4

where: literal ∈ {0, 1}

(RET)

mov al, literal
callee epilogue

where: literal ∈ {0, 1}

(RET)

binary op eax arg1
callee epilogue

(POST CALL)

caller epilogue
mov arg2, al

where: binary op ∈ {imul, sub, add, sar, sal, shr, shl, xor, or, and}

arg1 ∈ {reg, [reg], *address, literal}
mov ∈ {mov, movzx}
arg2 ∈ {reg, [reg], *address}

(RET)

idiv arg1
callee epilogue

(POST CALL)

caller epilogue
mov arg2, al

where: arg1 ∈ {reg, [reg], *address}
mov ∈ {mov, movzx}
arg2 ∈ {reg, [reg], *address}

(continues)

Consequent

Support

bool

0.00059

char

0.00536

char

0.00207

char

0.00023

30

Consequent

Support

char

0.00049

short

0.01190

short

0.02455

short

0.00871

int

0.01594

int

0.00103

int

0.00221

5

6

7

8

9

Antecedents

(RET)

unary op eax
callee epilogue

(POST CALL)

caller epilogue
mov arg, al

where: unary op ∈ {not, neg}

mov ∈ {mov, movzx}
arg ∈ {reg, [reg], *address}

(POST CALL)

caller epilogue
cwde

(POST CALL)

caller epilogue
mov arg, ax

where: mov ∈ {mov, movzx, movsx}

arg ∈ {eax, ecx, edx, cx, si, [reg], *address}

(RET)

mov ax, arg
callee epilogue

where: mov ∈ {mov, movzx, movsx}

arg ∈ {dx, al, cx, cl, [reg], *address}

(RET)

cond jmp oﬀset1
mov [reg1], literal1
jmp oﬀset2
mov [reg1], literal2
mov eax, [reg1]
callee epilogue

where: cond jmp ∈ {jo, jno, js, jns, je, jz, jne, jnz, jb, jnae, jc,
(cid:44)→ jnb, jae, jnc, jbe, jna, ja, jnbe, jl, jnge, jge, jnl, jle,
(cid:44)→ jng, jg, jnle, jp, jpe, jnp, jpo, jcxz, jecxz}
literal1, literal2 ∈ {0, 1}
literal1 (cid:54)= literal2
oﬀset1 (cid:54)= oﬀset2

(RET)

div ecx
mov eax, edx
callee epilogue

10

where: div ∈ {div, idiv}

(RET)

mov eax, literal
callee epilogue

11

(POST CALL)

caller epilogue
mov arg, eax

where: arg ∈ {[reg], *address}

(continues)

31

Antecedents

(RET)

binary op eax, address
callee epilogue

where: binary op ∈ {mov, movzx, movsx, add, sub}

(RET)

lea eax, [reg]
callee epilogue

(RET)

mov eax, [ebp+8]
callee epilogue

(RET)
cdq
callee epilogue

(RET)

mov edx, arg
callee epilogue

where: arg ∈ {reg, [reg], *address, literal}

(RET)

fstp [reg]
fld [reg]
callee epilogue

where: opcode(fstp)[0] = 0xD9

opcode(fld)[0] = 0xD9

(POST CALL)

caller epilogue
fstp arg

where: opcode(fstp)[0] = 0xD9
arg ∈ {[reg], *address}

(RET)

fld arg
callee epilogue

where: opcode(fld)[0] = 0xDD
arg ∈ {[reg], *address}

(RET)

mov arg1 arg2
callee epilogue

where: arg1 ∈ {[reg], *address}

arg2 ∈ {reg, [reg], *address, literal}

12

13

14

15

16

17

18

19

20

Consequent

Support

pointer

0.00949

pointer

0.01304

struct

0.06321

long long

0.04442

long long

0.01063

float

0.03817

float

0.01545

double

0.06783

void

0.00817

Table 13: Example association rules obtained from the dataset. reg variables represent registers, literal
integer literals, address absolute addresses, *address absolute addresses dereferences and oﬀset relative
addresses.

Rules 1-5 return the value with al, so they classify bool and char types. The ax
register in rules 6-8 is used to return short. Rules 9-14 analyze eax for 4-byte-size
types. Rules 15 and 16 check edx to infer long long, and rules 17-19 use st0 to return
float and double. The last rule checks that the value copied before returning from

32

Textual
representation

Opcodes

Float

Double

fstp regfp
fld regfp

D9 ?? ??
D9 ?? ??

DD ?? ??
DD ?? ??

Table 14: Binary encodings of fld and fstp instructions.

the function call is not moved to a register (but to a memory address), classifying the
function type as void.

Some functions return literal values, such as true or 32. The value of those literals
is used by some patterns to infer the return type. For example, rule 2 classiﬁes as
char the 1-byte type returned when the returned literal is neither 0 nor 1 (low-level
representation of false and true). The opposite is not true; when 0 or 1 is returned,
it could be a character (‘\0’ character is widely used in C). For this reason, rule 1
adds a POST CALL check after the invocation. If 0 or 1 is returned and it is moved to
edx with zero extension using movzx (i.e., high bits are set to zero, without sign), the
type is bool; for chars, movsx is used instead (copy with sign). Likewise, rule 12 uses
address literals to classify pointers.

Our system also detects operations that can only be applied to certain types. For
example, division (div and idiv) can be applied to neither pointer nor struct. There-
fore, rule 10 infers a 4-byte type to int, when division operations are applied to it. Rule
13 classiﬁes as pointer any 4-byte type where a lea instruction is used, since lea loads
a memory address into the target register (eax).

Another classiﬁcation mechanism used by our models is based on the binary repre-
sentation of assembly instructions. For example, the fstp and fld assembly instruc-
tions for real numbers share the start of the binary opcode5 (Table 14). When they
operate with 32-bit ﬂoating-point numbers, the opcode starts with 0xD9. However,
their opcode starts with 0xDD when applied to 64-bit operands. This diﬀerence is used
by rules 17-19 to tell the diﬀerence between float and double.

The classiﬁers generated with our dataset also detect binary patterns of the code
generation templates implemented by compilers [63]. For example, rule 9 detects the
code generation template used by cl to return the result of a comparison as an int. Of
course, these kinds of templates are compiler dependent, so the compiler used should
be discovered before using them [30].

Rule 14 is another rule for a particular code generation template. As described
in Section 6.1, cl performs a code transformation to return struct types (Figure 5).
The struct is passed as an argument, and its memory address is actually returned
as a pointer. This code transformation generates a particular sequence of assembly
instructions that our models use to identify structs among types of 4-bytes size.

5Opcode stands for operation code. It is the portion of the numeric representation of an assembly

instruction that speciﬁes the operation to be performed.

33

Although the assembly instructions used to return a value (RET patterns) are very
important to infer return types, the binary code used after the invocation (POST CALL
patterns) is also valuable. For example, the usage of ax just after an invocation is used
by rules 6 and 7 to identify short types. Another example is rule 18, which stores the
returned ﬂoating-point value from the mathematical coprocessor stack.

Finally, our system is also able to combine RET and CALL POST patterns to infer
return types. Since these kinds of rules are more speciﬁc, they commonly have low
support and high conﬁdence. For example, the best rule found to classify bool with
one RET pattern provides 76% conﬁdence; whereas rule 1 provides 100% conﬁdence by
adding a CALL POST pattern. These types of rules commonly classify types among
others with similar size and representation, such as rules 1, 3 and 4 (1 byte), and rule 11
(4 bytes).

8. Conclusions

We show how machine learning can be used with a large amount of binary code to
improve existing decompilers, particularly for the problem of inferring the high-level
type returned by functions. Our system obtains 79.1% F1-score when predicting return
types of code written by programmers whose code has not been used in the training
set; whereas the best existing decompiler achieves 30% F1-score. Gradient boosting is
the best classiﬁcation algorithm for the given dataset.

Additionally, we discuss and document the binary patterns found to classify return
types. The classiﬁcation rules combine binary patterns for returning expressions and
the opcodes after function invocation. They focus on how data are passed between the
function and the caller. The binary patterns found not only distinguish among diﬀerent
sizes and representations of data, but also among types with the same binary size and
representation. The publication and documentation of these patterns [61] can be used
for diﬀerent purposes, including the improvement of current decompilers.

We plan to study the appropriateness of sequence classiﬁers such as recurrent neural
networks (RNN) for inferring the return type of functions. Its ability to exploit the order
of binary instructions, for both the return and invocation sequences, seems to be ade-
quate for the return type problem [9]. Another possible direction to improve our work
is the augmentation of pattern generalizations with information gathered from symbolic
execution and type-based constraints [22]. Apart from improving the performance of
the classiﬁers, those techniques could also be used to give more information about the
C composite types. We also plan to use other compilers, compilation parameters and
architectures to check if the proposed method is applicable to those scenarios. Finally,
we would like to apply this methodology to reconstruct the type of other language
constructs such as global and local variables, and function parameters.

The binaries and source code of our system, the C code corpus used to create
the dataset, the Python code to build and evaluate the models, the hyper-parameters
selected for each model, the datasets, and the evaluation data used in this article can

34

be freely downloaded from
http://www.reflection.uniovi.es/bigcode/download/2021/expertsa

Acknowledgments

This work has been partially funded by the Spanish Department of Science, Innova-
tion and Universities: project RTI2018-099235-B-I00. The authors have also received
funds from the University of Oviedo through its support to oﬃcial research groups
(GR-2011-0040).

References

[1] R. N. Horspool, N. Marovac, An Approach to the Problem of Detranslation of

Computer Programs, The Computer Journal 23 (3) (1980) 223–229.

[2] C. Cifuentes, Reverse compilation techniques, Ph.D. thesis, School of Computing

Science, Queensland University of Technology, AU (1994).

[3] M. J. Van Emmerik, Static single assignment for decompilation, Ph.D. the-
sis, School of Information Technology and Electrical Engineering, University of
Queensland, AU (2007).

[4] F. Ortin, J. Escalada, O. Rodriguez-Prieto, Big Code: New Opportunities for
Improving Software Construction, Journal of Software 11 (11) (2016) 1083–1088.

[5] V. Raychev, M. Vechev, A. Krause, Predicting Program Properties from ”Big
Code”, in: Proceedings of the 42nd Annual ACM SIGPLAN-SIGACT Symposium
on Principles of Programming Languages - POPL ’15, Vol. 50, ACM Press, New
York, New York, USA, 2015, pp. 111–124.

[6] S. Karaivanov, V. Raychev, M. Vechev, Phrase-Based Statistical Translation of
Programming Languages, in: Proceedings of the 2014 ACM International Sympo-
sium on New Ideas, New Paradigms, and Reﬂections on Programming & Software
- Onward! ’14, ACM Press, New York, New York, USA, 2014, pp. 173–184.

[7] F. Yamaguchi, N. Golde, D. Arp, K. Rieck, Modeling and Discovering Vulnera-
bilities with Code Property Graphs, in: 2014 IEEE Symposium on Security and
Privacy, IEEE, 2014, pp. 590–604.

[8] F. Ortin, O. Rodriguez-Prieto, N. Pascual, M. Garcia, Heterogeneous tree structure
classiﬁcation to label java programmers according to their expertise level, Future
Generation Computer Systems 105 (2020) 380 – 394.

[9] Z. L. Chua, S. Shen, P. Saxena, Z. Liang, Neural Nets Can Learn Function Type
Signatures From Binaries, in: 26th USENIX Security Symposium (USENIX Secu-
rity 17), USENIX Association, Vancouver, BC, 2017, pp. 99–116.

35

[10] J. He, P. Ivanov, P. Tsankov, V. Raychev, M. Vechev, Debin: Predicting Debug
Information in Stripped Binaries, in: Proceedings of the 2018 ACM SIGSAC Con-
ference on Computer and Communications Security, ACM, New York, NY, USA,
2018, pp. 1667–1680.

[11] D. S. Katz, J. Ruchti, E. Schulte, Using recurrent neural networks for decompila-
tion, in: 25th IEEE International Conference on Software Analysis, Evolution and
Reengineering, SANER 2018 - Proceedings, Vol. 2018-March, Institute of Electrical
and Electronics Engineers Inc., 2018, pp. 346–356.

[12] E. Schulte, J. Ruchti, M. Noonan, D. Ciarletta, A. Loginov, Evolving Exact De-
compilation, in: Proceedings 2018 Workshop on Binary Analysis Research, Internet
Society, Reston, VA, 2018, pp. 1–11.

[13] D. E. Rumelhart, G. E. Hinton, R. J. Williams, Learning representations by back-

propagating errors, Nature 323 (6088) (1986) 533–536.

[14] Y. Bengio, R. Ducharme, P. Vincent, C. Janvin, A neural probabilistic language

model, J. Mach. Learn. Res. 3 (null) (2003) 1137–1155.

[15] J. D. Laﬀerty, A. McCallum, F. C. N. Pereira, Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Sequence Data, in: In Proceedings
of the 18th International Conference on Machine Learning, ICML ’01, Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 2001, pp. 282–289.

[16] D. Brumley, I. Jager, T. Avgerinos, E. J. Schwartz, BAP: A binary analysis plat-
form, in: G. Gopalakrishnan, S. Qadeer (Eds.), Computer Aided Veriﬁcation,
Springer Berlin Heidelberg, Berlin, Heidelberg, 2011, pp. 463–469.

[17] O. Katz, Y. Olshaker, Y. Goldberg, E. Yahav, Towards Neural Decompilation,

ArXiv (2019). arXiv:1905.08325.

[18] C. Fu, H. Chen, H. Liu, X. Chen, Y. Tian, F. Koushanfar, Zhao, Coda: An End-to-
End Neural Program Decompiler, in: H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, R. Garnett (Eds.), Advances in Neural Information Pro-
cessing Systems 32, Curran Associates, Inc., 2019, pp. 3708–3719.

[19] N. Kalchbrenner, P. Blunsom, Recurrent continuous translation models, in: Pro-
ceedings of the 2013 Conference on Empirical Methods in Natural Language Pro-
cessing, Association for Computational Linguistics, Seattle, Washington, USA,
2013, pp. 1700–1709.

[20] I. Sutskever, O. Vinyals, Q. V. Le, Sequence to Sequence Learning with Neural
Networks, Advances in Neural Information Processing Systems 4 (January) (2014)
3104–3112. arXiv:1409.3215.

36

[21] M. Henderson, B. Thomson, S. J. Young, Robust dialog state tracking using delexi-
calised recurrent neural networks and unsupervised adaptation, 2014 IEEE Spoken
Language Technology Workshop (SLT) (2014) 360–365.

[22] A. Mycroft, Type-based decompilation (or program reconstruction via type re-
construction), in: Proceedings of the 8th European Symposium on Programming
Languages and Systems, ESOP ’99, Springer-Verlag, Berlin, Heidelberg, 1999, p.
208–223.

[23] R. Milner, A theory of type polymorphism in programming, Journal of Computer

and System Sciences 17 (1978) 348–375.

[24] J. Caballero, Z. Lin, Type Inference on Executables, ACM Computing Surveys

48 (4) (2016) 1–35.

[25] H. Xue, S. Sun, G. Venkataramani, T. Lan, Machine Learning-Based Analysis of
Program Binaries: A Comprehensive Study, IEEE Access 7 (2019) 65889–65912.

[26] N. N. Rosenblum, X. Zhu, B. B. Miller, K. Hunt, Learning to analyze binary
computer code, in: Proceedings of the 23rd Conference on Artiﬁcial Intelligence,
Chicago, 2008, pp. 798–804.

[27] T. Bao, J. Burket, M. Woo, R. Turner, D. Brumley, BYTEWEIGHT: Learning to
recognize functions in binary code, in: Proceedings of the 23rd USENIX Security
Symposium, USENIX Association, San Diego, CA, 2014, pp. 845–860.

[28] R. De La Briandais, File searching using variable length keys, in: Western Joint
Computer Conference, IRE-AIEE-ACM ’59 (Western), Association for Computing
Machinery, New York, NY, USA, 1959, p. 295–298.

[29] E. C. R. Shin, D. Song, R. Moazzezi, Recognizing Functions in Binaries with
Neural Networks, in: 24th USENIX Security Symposium, USENIX Association,
Washington, D.C., 2015, pp. 611–626.

[30] N. E. Rosenblum, B. P. Miller, X. Zhu, Extracting compiler provenance from
program binaries, in: Proceedings of the 9th ACM SIGPLAN-SIGSOFT workshop
on Program analysis for software tools and engineering, PASTE ’10, ACM Press,
Toronto, Ontario, Canada, 2010, pp. 21–28.

[31] N. Rosenblum, B. P. Miller, X. Zhu, Recovering the toolchain provenance of binary
code, in: Proceedings of the 2011 International Symposium on Software Testing
and Analysis, ISSTA ’11, ACM Press, Toronto, Ontario, Canada, 2011, pp. 100–
110.

[32] D. Ucci, L. Aniello, R. Baldoni, Survey of Machine Learning Techniques for Mal-
ware Analysis, Computers & Security 81 (2017) 123–147. arXiv:1710.08189.

37

[33] M. Alazab, S. Venkatraman, P. Watters, M. Alazab, Zero-day malware detection
based on supervised learning algorithms of api call signatures, in: Proceedings
of the Ninth Australasian Data Mining Conference - Volume 121, AusDM ’11,
Australian Computer Society, Inc., AUS, 2011, p. 171–182.

[34] H. Rathore, S. Agarwal, S. K. Sahay, M. Sewak, Malware detection using machine

learning and deep learning, Lecture Notes in Computer Science (2018) 402–411.

[35] J. Escalada, F. Ortin, T. Scully, An Eﬃcient Platform for the Automatic Extrac-
tion of Patterns in Native Code, Scientiﬁc Programming 2017 (2017) 1–16.

[36] F. Ortin,

A C source
https://github.com/computationalreﬂection/cnerator (2021).

J. Escalada,

Cnerator:

code

generator,

[37] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,
M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay, Scikit-learn: Machine learn-
ing in Python, Journal of Machine Learning Research 12 (2011) 2825–2830.

[38] R. Kohavi, G. H. John, Wrappers for feature subset selection, Artiﬁcial Intelligence

97 (1-2) (1997) 273–324.

[39] J. Escalada, F. Ortin,

pilers with
http://www.reﬂection.uniovi.es/bigcode/download/2021/expertsa (2021).

supervised machine

Improving type information inferred by decom-
(support material webpage),

learning

[40] Hex-Rays, Hex Rays Decompiler, https://www.hex-rays.com/products/decompiler

(2021).

[41] Hex-Rays, Hex Rays IDA, https://www.hex-rays.com/products/ida (2021).

[42] I. Guilfanov, Simple type system for program reengineering, in: Proceedings Eighth
Working Conference on Reverse Engineering, IEEE Computer Society, 2001, pp.
357–361.

[43] I. Guilfanov, Decompilers and beyond, in: Black Hat USA, 2008, pp. 1–12.

[44] P. Matula, RetDec, https://github.com/avast/retdec (2021).

[45] J. Kˇroustek, Retargetable analysis of machine code, Ph.D. thesis, Faculty of Infor-

mation Technology, Brno University of Technology, CZ (2015).

[46] Y. Danilov, Snowman, https://github.com/yegord/snowman (2021).

[47] K. Troshina, A. Chernov, Y. Derevenets, C Decompilation: Is It Possible?, in:
M. A. Bulyonkov, R. Gl¨uck (Eds.), Proceedings of International Workshop on
Program Understanding, Ershov Institute of Informatics Systems, Siberian Branch
of the Russian Academy of Sciences, Altai Mountains, Russia, 2009, pp. 18–27.

38

[48] A. Fokin, E. Derevenetc, A. Chernov, K. Troshina, SmartDec: Approaching C++
Decompilation, in: 2011 18th Working Conference on Reverse Engineering, IEEE,
2011, pp. 347–356.

[49] Cryptic Apps EURL, Hopper,

the macOS

and Linux

disassembler,

https://www.hopperapp.com (2021).

[50] S.

Decompiler
DisC
https://www.debugmode.com/dcompile/disc.htm (2021).

Kumar,

-

for

TurboC,

[51] G. Caprino, REC Decompiler, http://www.backerstreet.com/rec/rec.htm (2021).

[52] E. J. Schwartz, M. Woo, D. Brumley, J. Lee, Native x86 Decompilation Using
Semantics-Preserving Structural Analysis and Iterative Control-Flow Structuring,
in: USENIX Security Symposium, USENIX, Washington, D.C., 2013, pp. 353–368.

[53] K. Yakdan, S. Eschweiler, E. Gerhards-Padilla, M. Smith, No More Gotos: De-
compilation Using Pattern-Independent Control-Flow Structuring and Semantics-
Preserving Transformations, in: Proceedings 2015 Network and Distributed Sys-
tem Security Symposium, Internet Society, Reston, VA, 2015, pp. 1–15.

[54] K. Yakdan, S. Dechand, E. Gerhards-Padilla, M. Smith, Helping Johnny to An-
alyze Malware: A Usability-Optimized Decompiler and Malware Analysis User
Study, in: Proceedings - 2016 IEEE Symposium on Security and Privacy, Institute
of Electrical and Electronics Engineers Inc., 2016, pp. 158–177.

[55] A. Georges, D. Buytaert, L. Eeckhout, Statistically rigorous Java performance

evaluation, SIGPLAN Not. 42 (10) (2007) 57–76.

[56] Y. Yang, X. Liu, A re-examination of text categorization methods, in: Proceed-
ings of the 22nd Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’99, Association for Computing Ma-
chinery, New York, NY, USA, 1999, p. 42–49.

[57] M. Sokolova, G. Lapalme, A systematic analysis of performance measures for clas-

siﬁcation tasks, Information Processing & Management 45 (4) (2009) 427 – 437.

[58] J. Opitz, S. Burst, Macro F1 and macro F1, ArXiv (2019). arXiv:1911.03347.

[59] B. W. Kernighan, D. M. Ritchie, The C Programming Language, 2nd Edition,

Prentice Hall Professional Technical Reference, 1988.

[60] R. Agrawal, R. Srikant, Fast algorithms for mining association rules in large
databases, in: Proceedings of the 20th International Conference on Very Large
Data Bases, VLDB ’94, Morgan Kaufmann Publishers Inc., San Francisco, CA,
USA, 1994, p. 487–499.

39

[61] J. Escalada, F. Ortin, Association rules obtained from the dataset described in Im-
proving type information inferred by decompilers with supervised machine learning,
http://www.reﬂection.uniovi.es/bigcode/download/2021/expertsa/tr.pdf (2021).

[62] Microsoft, Calling Conventions — Microsoft Docs, https://docs.microsoft.com/en-

us/cpp/cpp/calling-conventions (2021).

[63] S. S. Muchnick, Advanced Compiler Design and Implementation, Morgan Kauf-

mann Publishers Inc., San Francisco, CA, USA, 1998.

40

