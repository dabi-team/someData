SQRP: Sensing Quality-aware Robot Programming System for
Non-expert Programmers

Yi-Hsuan Hsieh1, Pei-Chi Huang2, and Aloysius K Mok1

1
2
0
2

n
u
J

0
3

]

O
R
.
s
c
[

1
v
7
2
1
0
0
.
7
0
1
2
:
v
i
X
r
a

Abstract— Robot programming typically makes use of a set of
mechanical skills that is acquired by machine learning. Because
there is in general no guarantee that machine learning produces
robot programs that are free of surprising behavior, the safe
execution of a robot program must utilize monitoring modules
that take sensor data as inputs in real time to ensure the
correctness of the skill execution. Owing to the fact that sensors
and monitoring algorithms are usually subject to physical
restrictions and that effective robot programming is sensitive to
the selection of skill parameters, these considerations may lead
to different sensor input qualities such as the view coverage
of a vision system that determines whether a skill can be
successfully deployed in performing a task. Choosing improper
skill parameters may cause the monitoring modules to delay
or miss the detection of important events such as a mechanical
failure. These failures may reduce the throughput in robotic
manufacturing and could even cause a destructive system crash.
To address above issues, we propose a sensing quality-aware
robot programming system that automatically computes the
sensing qualities as a function of the robot’s environment and
uses the information to guide non-expert users to select proper
skill parameters in the programming phase. We demonstrate
our system framework on a 6DOF robot arm for an object
pick-up task.

I. INTRODUCTION

Skill-based robot programming that composes a set of
low-level skills into a high-level capability has been widely
used in robotic manufacturing systems because of the need
for reusability [1]. Successful execution of a skill requires
real-time sensor inputs for monitoring the correctness of
the skill execution. One popular sensing method is to use
cameras to provide different views to cover certain critical
aspects in a skill execution. There are several
technical
challenges to this approach. First, sensors have their own
coverage limitations, such as a camera’s limited ﬁeld of
view, object occlusions in the work environment and also the
physical requirements imposed by the detection algorithm.
Second, a robot skill usually requires the proper setting of the
skill parameters to achieve the task goal. Without sufﬁcient
camera coverage, a robot system may miss a crucial deadline
in the detection of an execution failure that results in reduced
system performance or even a catastrophic system crash.
Some extant work assumes that there are enough sensors to
achieve the monitoring requirements [2] while other works
focus on reconﬁguring the cameras to meet the goals of the

*This work is partially supported by the Ofﬁce of Naval Research under

ONR Award N00014-17-2216

1Yi-Hsuan Hsieh and Aloysius K Mok are with Department
of Computer Science, University of Texas at Austin, yihsuan,
mok@cs.utexas.edu

2Department of Computer Science, University of Nebraska Omaha,

phuang@unomaha.edu

Fig. 1: Real world environment.

robotic tasks [3]. However, it requires time and expertise for
performing reconﬁguration, and we do not want to reconﬁg-
ure the cameras if it can be avoided. These are important
issues if robot programming is to be made accessible to
non-expert programmers who need to know if the current
camera settings can or cannot support a robotic skill. This
paper is a ﬁrst step towards treating these issues by providing
meaningful feedback to the programmer that quantiﬁes the
task-effectiveness of the chosen system parameters such as
the adequacy in camera coverage. To address the above is-
sues, we propose a sensing quality-aware robot programming
system we name SQRP that incorporates explicit sensing
requirements in the skill deﬁnition. We include temporal
sensing requirements in Metric Temporal Logic (MTL) [4]
formulas that prescribe what to monitor and when to monitor.
We also include spatial sensing requirements that prescribe
where to monitor. In the programming phase, our system
examines the sensing requirements to determine if the current
system conﬁgurations and camera settings can support a
robotic skill and to guide the programmer to choose the
proper skill parameters based on the sensing quality.

The contributions of this paper are twofold. Firstly, we
introduce the sensing requirements in the robot skill that
includes both temporal and spatial sensing requirements.
Secondly, based on these requirements, we compute the
sensing qualities in the programming phase to assist users
to choose the proper skill parameters. In our experimental
evaluation, we show the beneﬁt of exposing sensing quality
in the programming phase as it assists users to choose a
proper set of skill parameters to reduce the execution time

© 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,
including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution
to servers or lists, or reuse of any copyrighted component of this work in other works.

 
 
 
 
 
 
1) Temporal Sensing Requirements: Temporal sensing re-
quirements deﬁne the desired targets, when and how long
that the targets have to be covered by the sensors. In this
paper, we deﬁne our skill by using Metric Temporal Logic
(MTL) [4], a logic speciﬁcation formalism that is used to
specify a temporal property in the context of time intervals.
The syntax of a MTL formula ϕ is deﬁned as follows:

ϕ ::= a | ¬ϕ | ϕ ∧ ϕ | (cid:3)I ϕ | ♦I ϕ

where a ∈ A and A is a set of atomic propositions. The set of
atomic propositions of a skill is obtained from the grounded
predicates of the skill’s preconditions and postconditions in
the programming phase. The temporal operator (cid:3) and ♦ are
the “always” and “eventually” modal operators respectively.
The I ∈ R≥0 ×R≥0 deﬁnes the time interval of the temporal
operator. In our system, the time interval I is delimited by
the start time and the end time of the primitive actions.
Formula (cid:3)I ϕ is true iff ϕ is always true in the time interval
I. The (cid:3) operator deﬁnes the persistent states of grounded
predicates; we use it to specify the correctness criterion
of the skill execution. For instance,
the following MTL
formula (cid:3)[a1.ts,a2.te](open) speciﬁes that the robot gripper
remains open during the time interval [a1.ts, a2.te]. On the
other hand, formula ♦I ϕ is true iff ϕ is true sometime in
the time interval I. This speciﬁcation is useful to describe
a runtime event, such as a runtime fault that the system
needs to focus on. For example, ♦[a4.ts,a4.te] (obj on table∧
¬ open ∧ ¬hold) speciﬁes the state after the runtime fault
event “object mistakenly slip” occurs. It speciﬁes that at
some time between a4.ts and a4.te a slip fault occurs; the
object is (still) on the table, the robot gripper is not open
and the gripper does not hold anything.

To determine the satisfaction of the MTL formulas at
runtime, we need to provide adequate sensing capability in
hardware and software. The grounded predicates that are
speciﬁed in the MTL formulas concern the target objects
that have to be within the sensing coverage of the sensors.
The time interval speciﬁed in the MTL formulas prescribes
when and for how long the targets have to be monitored by
the sensors.

2) Spatial Sensing Requirements: Spatial sensing require-
ments prescribe where each of the target,
the grounded
predicate mentioned in the MTL formulas, has to be mon-
itored by the sensors. We denote a target by Pi, where
1 ≤ i ≤ N and the N is the number of the total targets.
For each target, we deﬁne a set of 3D bounding boxes
Boxf
that together enclose the physical objects that make
i
up the target Pi. One can reduce one of the dimensions
of a 3D bounding box to form a 2D bounding box based
on the application’s need. As an example, for the literal
open, our system deﬁnes a set of two bounding boxes that
enclose two ArUco markers at the tip of the robot gripper,
as shown in Figure 8 (a). Each bounding box boxf
i,j ∈ Boxf
i
is represented as eight vertices in the Cartesian coordinates
relative to the f coordinate frame, where 1 ≤ j ≤ b and the
b is the number of bounding boxes in the set Boxf
i . We note
that some coordinate frames may change their 3D locations
relative to the world coordinate as time progresses.

Fig. 2: The overview of the SQRP.

of a robotic task, especially when a fault occurs during
execution which may require a sub-task to be redone. We
use a 6DOF robot arm to demonstrate the application of one
of its skill sets - “Pickup” skill, both in the simulation and
the real world environment, as shown in Figure 1.

II. SYSTEM OVERVIEW

Building on our previous work [5], we develop our robot
programming system in three phases, as shown in Figure 2.
First, in the Robot Knowledge Construction Phase, a robot
knowledge base is constructed by system developers which
will be automatically adapted for the the actual operating
environment. Then, in the Programming Phase, the user
programmer speciﬁes the tasks to be performed with the
selected robot skills and a set of parameters. Our system
checks if the speciﬁcations are supported by the robot system
and then computes the corresponding sensing qualities for
the chosen parameter set which is the feedback to guide the
application programmer. In the subsequent Runtime Phase,
the robot executes the task by deploying the selected skills
while our system monitors the correctness of the robot’s
execution. If faults are detected, the system fault handler
will determine the response.

III. SKILL DEFINITION AND REQUIREMENT
SPECIFICATION
The section describes how to deﬁnes a robot skill with the

corresponding sensing requirements.
A. Skill Deﬁnition

A skill

is speciﬁed in the planning domain deﬁnition
language (PDDL) [6] which describes each skill’s pre-
conditions and postconditions. A skill Si consists of ni
number of sequential primitive actions as follows: Si =<
a1, a2, . . . , an >. Each primitive action is further annotated
with a symbolic execution time interval [ts, te], where ts and
te are the start time and end time respectively. These two
parameters will be grounded in the programming phase based
on the target robot system and the target work environment.

B. Sensing Requirements
sensing

The

a

of

requirements

deﬁne
what/when/where the targets have to be covered by
the robotic system sensors in order
to determine the
applicability of the skill. There are two types of sensing
requirements:
temporal sensing requirements and spatial
sensing requirements.

skill

We require that boxf

i,j meets the following two spatial
sensing requirements: (1) It is covered inside the sensing
range of the sensors, in our case the camera viewing frus-
tum [3]; (2) It is covered inside the detection range of the
object detection algorithm. For accurate object detection,
we need the target to be within a range of some nominal
distances from the camera and the pose of the target to be
within a certain tolerance of some nominal poses.

IV. SENSING QUALITY-AWARE ROBOT PROGRAMMING
(SQRP) SYSTEM

A. Robot Knowledge Construction Phase

To equip our robot system with the knowledge of its
surroundings, the system developers deﬁne four types of
information as follows: 1) Sensing knowledge: each camera
that monitors the work space is calibrated; 2) Object appear-
ances and locations: each object that can be manipulated in
the work space is represented by its name, type (e.g., colors),
size (e.g., volume of a 3D bounding box) and its 3D location
if available; 3) Robotic arm capability: the arm’s spatial
reachability subspace, motion proﬁle and its forward/inverse
kinematics; and 4) Skill requirements: the skills mentioned in
Section III that can be used by the application programmer.

B. Programming Phase

1) World State Construction: The world state at time t is
expressed as a conjunction of n literals, W = ∧i=n
i=1 Li. For
the initial state, the truth values of all literals are determined
by the sensor inputs. Here, we assume that all literals that are
not deﬁned by sensor input in the initial state are assumed
to be false.

2) Dependency Check: First, our system takes the user
task speciﬁcations as inputs to check if there exist other skill
parameters that satisfy the user inputs. For instance, given
Pickup red screw as the user’s input, our system includes
all the red screws that are in the work environment as the
possible values of the skill parameters. Our system then
performs a dependency check to see if the world state meets
the preconditions of the chosen skill. If not, we will apply
the Fast-forward solver [7] to generate execution plans as
suggestions to the user. Our systems also checks whether our
robot can reach the target or not and only outputs the feasible
parameters. If the speciﬁed skill and the corresponding skill
parameters do not pass the check, our system will not allow
the user to proceed.

3) Skill Grounding: For each skill parameter, we ground
the skill deﬁnition and the information from the robot’s
knowledge base to the target robot system. We ﬁrst obtain
the overall execution time of the skill with the speciﬁed
skill parameters by checking the S-curve motion proﬁle for
our real-world robot and by using the time parameterization
algorithm from Moveit! [8] for our simulation robot. From
the MTL formulas, we obtain T p, the total execution time
that spans the robot’s motion trajectory. We then sample the
robot’s state as it moves along the trajectory. With respect
to a skill parameter p and sample time tj, we deﬁne a skill
state, denoted by Sp
i=1 Lp
,
i,tj
where nj is the number of conjunction literals of each time

tj to be the conjunctive form ∧i=nj

sample tj. We obtain nj from the MTL formulas in the skill
deﬁnition. Depending on the sampling method, the temporal
distance between two samples in the trajectory may not be
constant. For a given trajectory, we have a sequence of skill
states Sp
tm, where m is the number of samples
of the trajectory.

t2 , . . . Sp

t1, Sp

4) Sensing Quality Computation: We provide two metrics
to deﬁne sensing quality. The ﬁrst metric is called “overall
average sensing coverage”, which deﬁnes the percentage of
camera coverage of the entire skill. A higher value implies
that more time intervals of the skill execution are monitored
by cameras. Accordingly, there is a higher chance to capture
events of concerns in time, such as runtime faults that may
not be explicitly speciﬁed by application developer. However,
some runtime faults tend to happen in a speciﬁc time interval.
For instance, for the “Pickup” skill that is used to lift a
target object up and move it to another location, the target
usually slips during the “lift up” process instead of while
moving to another location. Thus a high overall coverage
does not necessary guarantee that the critical time period of
the “lift up” process will be adequately monitored. Without
monitoring this time interval, the pertinent runtime fault may
not be detected fast enough.

To address the above issue, we introduce the second met-
ric, the “event of interest average sensing coverage”, which
computes the average coverage for all the time intervals
that are spanned by the interval arguments of all the ♦
modal operators that appear in the MTL formulas in the skill
deﬁnition.

i,tj

To compute the two metrics, we ﬁrst deﬁne the concept
of sensing coverage. We say that camera cx covers a literal
Lp
if the camera meets the sensing requirements that are
sufﬁcient to determine the truth value of the literal. We deﬁne
the coverage of a literal Lp
in a skill state by camera cx
to be

i,tj

(cid:40)

C(Lp

i,tj

, cx) =

if cx covers Lp

1,
0, otherwise

i,tj

For a given set of cameras, the coverage of the literal Lp
is deﬁned as

i,tj

(cid:40)

C(Lp

i,tj

) =

if (cid:80)ω
1,
0, otherwise

x=1 C(Lp

i,tj

, cx) ≥ k

where k is the minimum number of cameras that are required
to determine the truth of the literal, and ω is the number of
the camera in the set. For a skill state, denoted by Sp
tj , we
deﬁne the coverage of the skill state as

C(Sp
tj

) =

(cid:40)

T rue,
F alse,

if C(Lp
i,tj
otherwise

) = 1, ∀Lp

i,tj

in Sp
tj

We say that a skill state is covered if all its literals are covered
a set of cameras.

avg. Suppose SecT p

We now deﬁne the ﬁrst metric of the skill parameter
p to be Qp
γ is a sequence of time
points < tj, tj+1, . . . , tj+nγ > such that all the skill states
represented by these time points are all covered. In other
words, the conjunction C(Sp
) is true,

tj ) ∧ · · · ∧ C(Sp

tj+nγ

where nγ + 1 is the number of the trajectory sample points
and 1 ≤ γ ≤ Γ, where Γ is the number of such time
segments in the trajectory. Note that we have either C(Sp
tj−1 )
is f alse or tj = t1, the start of the trajectory. Also, we have
either C(Sp
tj+nγ +1) is f alse or tj+nγ = tm, the end of the
trajectory. We deﬁne ∆SecT p
γ = (tj+nγ −tj) to be the length
γ . Finally, Qp
of the time interval of SecT p

avg is deﬁned as

Qp

avg =

(cid:80)γ=Γ

γ=1 ∆SecT p
T p

γ

We deﬁne the second metric of the skill parameter p to
eoi. The length of the time intervals that are spanned
eoi. Similar to
eoi,γ to denote a sequence of
eoi where the skill states are covered. We use
eoi,γ. With

be Qp
by all the ♦ modal operators is denoted by T p
avg, we use SecT p
computing Qp
time points in T p
∆SecT p
above notations we deﬁne

eoi,γ to denote the time duration of SecT p

Qp

eoi =

(cid:80)γ=Γ

γ=1 ∆SecT p
T p
eoi

eoi,γ

where Γ is the total number of such time segments within
T p
eoi.
5) Preference Speciﬁcations and Program Updates: A
programmer may determine what skill parameters s/he wants
or based on the following criteria: p∗ = arg max
avg or
eoi. After determining the skill parameters,
our programming system saves the speciﬁed skill and updates
the world state based on the skill’s postcondition to allow the
programmer to specify the next skill to be deployed.
C. Runtime Phase

p∗ = arg max

Qp

Qp

p

p

After all the skill speciﬁcations are completed, our system
is ready to execute the skills. Concurrently, the skill mon-
itoring module which consists of several parallel processes
takes the camera images and coverage information as input
to determine the result of the skill execution. When an
execution fault is detected, a fault handler in our SQRP
system will determine the proper responses, such as stopping
the robot and performing a retry.

V. PERFORMANCE EVALUATION
This section describes the experimental evaluation and
discusses the performance efﬁciency of our SQRP system.
We use the “Pickup” skill with robot suction in the simulation
environment and we use the robot two-ﬁnger gripper in our
real-world experiment to pick up an object as the scenario
of our experiment.

A. Simulation-based Experiments

1) Experimental Setup and Deployment: The simulation
environment, as shown in Figure 3 (a) is implemented in the
Gazebo simulator [9] with a 6DOF robot arm equipped with
a suction cup at its end-effecter. Our system is built on top of
the Robot Operating System (ROS) [10] that communicates
with the simulation camera sensors and the robot simulation
system. Moveit! [8] is used for robot planning and collision
detection.

Six cameras are placed in locations to highlight
the
differences and beneﬁts of using different sensing quality

Fig. 3: Simulation scene: (a) Environment (b) Spatial sensing
requirements as illustrated in orange and red boxes.

Fig. 4: Primitive actions. (a) Move above the target. (b) Lift
the object up. (c) Move back to initial pose.

metrics. There are six green objects that can be selected as
the skill parameter for the “Pickup” skill. g1 and g5 are
deployed at the locations that take similar execution time,
while the remaining green objects are randomly generated
with the constraint that they are able to be seen by one of
the cameras.

The “Pickup” skill consists of ﬁve primitive actions <
a1, . . . , a5 > with the following motion preferences. a1:
From the initial pose, Figure 3 (a), the robot arm moves
to two centimeters above the target, Figure 4 (a). a2: The
arm moves down for two centimeters. a3: suction. a4: The
arm lifts itself up, Figure 4 (b). a5: it moves back to the
initial position, Figure 4 (c).

The temporal sensing requirements are obtained from
the following MTL formulas: (cid:3)[a1.ts,a2.te] (¬ hold ∧
obj on table) ∧ (cid:3)[a4.ts,a5.te] (hold ∧ ¬ obj on table).
A runtime fault, object-mistakenly-slips, is deﬁned as fol-
lows: ♦[a4.ts,a4.te] (¬hold ∧ obj on table). The hold and
obj on table prescribe that the object is stuck to the suction
cup and the target green object is on the table respectively.
The spatial sensing requirements are shown in Figure 3
(b). The skill states are sampled based on the waypoints
generated from the Moveit! [8]’s planner. Our system then
computes two sensing quality metrics for each of the green
object.

2) Performance of Computing Sensing Quality: Since our
sensing quality metrics are computed based on skill states
that are sampled in discrete time, we ﬁrst evaluate the
performance and the “overall average sensing quality” when
using different numbers of skill states to pick up g1. To get a
different number of skill states, we set the minimum number
of waypoints, min n, in Moveit! [8]’s planner from 5 to
40 for each primitive action except, a3. In Figure 5 (a), x
axis shows min n and y axis shows the execution time to
compute Qg1
avg. The computation time increases from 0.388
sec to 1.31 sec as the min n increase from 5 to 40, as the
number of the states that need to be evaluated increase. In
Figure 5 (b), x axis shows min n and y axis shows the value

Fig. 5: Results for using different min n. (a) Execution time
v.s. min n. (b) Qg1

avg v.s. min n.

Fig. 6: Different C(Sg1
F represent Truth and False respectively.

t ) when using different min n. T and

of Qg1
avg. We can see a convergence to within 67.6% to 67.9%
for min n values 10, 20, 30, and 40. The intuition here is
that with a bigger number of skill states, the discrete time
simulation approaches continuous time motion and therefore
outputs similar results. However, with insufﬁcient number
of skill states, min n = 5, we only get 57.5%, since our
system misses signiﬁcant coverage information, as shown in
Figure 6, where the x axis represents time in seconds, and
y axis represents the skill coverage, C(Sg1
t ). The areas that
are pointed to by two black arrows show that when using
min n = 5, there is no skill state that can be evaluated
at around 8.1 and 15.7 seconds, whereas there are such
states for min n = 10. Since we compute ∆SecT g1
r more
conservatively and only compute it when the covered skill
states are sequentially covered, the lack of skill states results
in less coverage in the computation. Different values of
min n that are required depend on the robot’s motion and
the relative location between the targets and the cameras.
Since our experiment scenarios involve motion so as to
picking g1, we choose 10 points as our min n for the rest
of experiments.

avg, it still has 100% of Qg5

3) Sensing Quality Results: The results of sensing quality
and the skill execution time for each of the green object are
shown in Table I. Objects g1 and g5 take almost the same
time for execution. However, there is a tradeoff between two
sensing quality metrics. Even though choosing g5 results in
lower Qg5
eoi owing to the coverage
from cam2, as shown in Figure 7 (a). If the pre-speciﬁed
runtime fault occurs, e.g., the object mistakenly slips, our
system can provide fast detection time. However, if a fault
occurs outside the pre-speciﬁed runtime fault interval, our
system may not be able to capture it in time. On the other
hand, for selecting g1, even though Qg1
avg is up to 67.80%,
Qg1
eoi is 0% owing to the occlusion from the robot arm, as
shown in Figure 7 (b). If the pre-speciﬁed runtime fault

Fig. 7: Simulation experimental scene: (a) g5 in cam2’s
perspective. (b) g1 is occluded from the perspective of cam1.

occurs, our system may delay to ﬁnd out.

For object g6, even though it can be seen by the camera
cam2, it is too far for the robot arm to reach. Therefore,
it fails at the dependency check module. For objects g3
and g4, both have good overall sensing coverage but the
“event of interest average sensing quality” are lower than
g2. According to Table I, g2 has the shortest execution time,
large “overall average sensing quality” 80.25% and 100%
of the “event of interest average sensing quality”. If the
user selects g2 as the target object, our system will obtain
better execution time and sensing coverage. Therefore, the
best option may be to pick up g2.

TABLE I: Sensing Quality for Green Objects

Metric
Qgi
avg(%)
Qgi
eoi(%)
T ime(sec)

g1
67.80
0.0
23.81

g2
80.25
100.0
13.83

g3
83.87
66.12
19.62

g4
79.88
55.41
18.78

g5
36.67
100.0
23.89

g6
N/A
N/A
N/A

B. Real-World Experiments

1) Experimental Setup and deployment: In the real-world
experiments, we demonstrate the beneﬁt of selecting a proper
skill parameter when a pre-speciﬁed runtime fault, object-
mistakenly-slips occurs and requires the system to perform
a retry that results in a shorter skill execution time. We
select the “Pickup” skill for the case where with the object
to be lifted up may slip out of the gripper as a fault. The
environment setup is shown in Figure 1. Four cameras,
cam1, . . . , cam4, are used and are calibrated with the robot’s
coordinates. There are two identical red screws, red1 and
red2 that can be selected to be picked up. The robot motion
is similar to the simulation setup. The temporal sensing
requirements are similar to the simulation with an additional
speciﬁcation: (cid:3)[a1.ts,a2.te] (open) ∧ (cid:3)[a4.ts,a5.te] (¬open).
The spatial sensing requirements are shown in Figure 8 (a).
The spatial sensing requirements of the literal open and
the literal hold both are deﬁned by two markers, illustrated
the literal hold also includes the
in orange boxes, but
blue bounding box. The sensing requirements of the literal
obj on table deﬁnes as a box that encloses the red screw,
shown in the red box. To generate the skill states, we use a
S-curve motion proﬁle and sample each primitive action with
equal-length time intervals 1 second long in our experiment
except the end of each primitive action. In this experiment
setup, we only consider the occlusion from the robot arm.
To mainly focus on sensing quality measurements, we use
relatively simple detection methods, such as ArUco marker
detection [11] and color detection.

and exposing it in the programming phase so as to reduce
execution time and increase system throughput.

VI. RELATED WORK

Skill-based programming has been widely studied in
robotic systems because it facilitates reusability [12], [13].
Programmers are often given ﬂexibility to choose different
skills and parameters based on their preferences [14], [15].
However, most of the extant works do not pay attention to the
impact on the effectiveness of the skill monitoring modules
when different skill parameters are admissible.

Robot skills often include monitor modules to ascertain
correct skill execution [12]. The monitor modules usually
get inputs from sensors, e.g., cameras, and perform critical
event detection based on machine learning algorithms [2].
However, these works usually assume that sensors are located
at the right locations that cover robot motion adequately. To
know if the camera setup is actually sufﬁcient for the current
robot tasks, we incorporate camera coverage as a sensing
quality and expose it in the programming phase.

Linear Temporal Logic (LTL) [16] is a formal speciﬁcation
language that can be used to specify temporal properties
for robot motions or tasks [17] [18]. Instead of focusing on
verifying correctness of robot tasks, we focus on the sensing
requirements that are extracted from temporal logic formulas
that specify the robot task.

Describing 3D locations of target objects has been widely
studied in the area of active sensing in robot applica-
tions [19]. Enclosing target objects in 3D bounding boxes is
an intuitive way to describe target locations [5]. Therefore,
in our work, 3D bounding boxes that enclose targets are used
to deﬁne spatial sensing requirements.

VII. CONCLUSION

This paper presents the SQRP system which computes
two sensing quality metrics, as deﬁned by two types of
average camera coverage that are used in the robot task
programming phase to assist non-expert programmers to
select a proper skill parameter setting for the robotic task.
We use a Robot Knowledge module to encode the robot’s
knowledge of the operational environment and the sensing
requirements of the skill deﬁnitions. Temporal sensing re-
quirements are expressed in Metric Interval Logic formulas
to prescribe what the skill monitor system monitors and when
to monitor. Spatial sensing requirements are prescribed by
using 3D bounding boxes, relative poses and the distance
between the target objects and the cameras. By evaluating
the camera conﬁgurations in the operational environment,
the SQRP system can compute the sensing qualities and
provide the programmer with feedback in the programming
phase. We deploy our system in both simulation and a
real-world environment to obtain experimental results. We
present
the performance results and show that exposing
the sensing quality in the programming phase can have
signiﬁcant beneﬁts, both in optimizing execution time to
meet run-time deadlines and in detecting run-time faults to
determine if the robotic system needs to redo a sub-task.

Fig. 8: Real-world experimental scene: (a) Spatial sensing
requirements. (b) The view from cam4. The red dashed circle
encloses the occluded red2. (c) The views from cam1.

Fig. 9: tf is the time point when the fault, slip, occurs. tr is
the time point when we detect that the object is mistakenly
on the table. tredo is the time point when we collect enough
information to decide to retry the task. te is the time point
when the skill execution is completed after the retry.

avg and Qred1

2) Experimental Results: In the Programming Phase, we
compute the sensing quality and the execution time for red1
and red2. Picking either red1 or red2 requires similar time,
28.47 and 28.34 seconds respectively. However, selecting
red1 as the skill parameter results in good sensing quality,
where Qred1
eoi are 92.27% and 100.0% respectively,
because it is almost fully covered by cam1, cam2, and
cam3. On the other hand, choosing to pick up red2 has very
low sensing quality, where Qred2
avg and Qred2
eoi are 11.19% and
0% respectively. The zero coverage of Qred2
is caused by
eoi
the occlusion from the robot arm, as shown in Figure 8 (b).
We also evaluate the precomputed sensing quality with
the actual detection for each skill state. Since we know the
sampling time for each skill state, we examine the detection
accuracy from the corresponding timestamps of the recorded
video in our Runtime Phase as our ground truth. The only
time that the system fails to detect the targets while it is
supposed to be covered is when the robot arm moves close
to the top of red1, as shown in Figure 8 (c) from the
cam1’s perspective. One of the ArUco markers fails to be
detected probably because of the lighting condition in the
experiment. For picking up red2, the system is able to detect
the correctness of all skill states.

To show the beneﬁt of selecting a good skill parameter,
we create a scenario where the robot slips the target red
screw while it is lifting it up and then our robot performs
a retry. We program the robot to perform a retry only when
our robot can detect that the object is mistakenly on the table
and the robot gripper is functioning correctly. Figure 9 shows
our runtime results. The results show the beneﬁts of having
better coverage that allows earlier fault detection as the robot
ﬁnishes the retry 13 seconds earlier. The data shows the
advantage of incorporating coverage as the sensing quality

REFERENCES
[1] P.-C. Huang, Y.-H. Hsieh, and A. K. Mok, “A skill-based programming
system for robotic furniture assembly,” in 2018 IEEE 16th Interna-
tional Conference on Industrial Informatics (INDIN).
IEEE, 2018,
pp. 355–361.

[2] A. Inceoglu, G. Ince, Y. Yaslan, and S. Sariel, “Failure detection using
proprioceptive, auditory and visual modalities,” in 2018 IEEE/RSJ
International Conference on Intelligent Robots and Systems (IROS).
IEEE, 2018, pp. 2491–2496.

[3] S. Hanoun, A. Bhatti, D. Creighton, S. Nahavandi, P. Crothers, and
C. G. Esparza, “Target coverage in camera networks for manufacturing
workplaces,” Journal of intelligent manufacturing, vol. 27, no. 6, pp.
1221–1235, 2016.

[4] R. Koymans, “Specifying real-time properties with metric temporal

logic,” Real-time systems, vol. 2, no. 4, pp. 255–299, 1990.

[5] Y.-H. Hsieh, P.-C. Huang, Q. Huang, and A. K. Mok, “Lasso: Location
assistant for seeking and searching objects,” in 2019 IEEE Inter-
national Conference on Industrial Cyber Physical Systems (ICPS).
IEEE, 2019, pp. 94–100.

[6] D. McDermott, M. Ghallab, A. Howe, C. Knoblock, A. Ram,
M. Veloso, D. Weld, and D. Wilkins, “Pddl-the planning domain
deﬁnition language,” 1998.

[7] J. Hoffmann and B. Nebel, “The ff planning system: Fast plan
generation through heuristic search,” Journal of Artiﬁcial Intelligence
Research, vol. 14, pp. 253–302, 2001.

[8] S. Chitta, I. Sucan, and S. Cousins, “Moveit![ros topics],” IEEE
Robotics & Automation Magazine, vol. 19, no. 1, pp. 18–19, 2012.
[9] N. Koenig and A. Howard, “Design and use paradigms for gazebo, an
open-source multi-robot simulator,” in 2004 IEEE/RSJ International
Conference on Intelligent Robots and Systems (IROS)(IEEE Cat. No.
04CH37566), vol. 3.

IEEE, 2004, pp. 2149–2154.

[10] M. Quigley, K. Conley, B. Gerkey, J. Faust, T. Foote, J. Leibs,
R. Wheeler, and A. Y. Ng, “Ros: an open-source robot operating
system,” in ICRA, vol. 3, no. 3.2. Kobe, Japan, 2009, p. 5.

[11] “OpenCV ArUco Detection,” https://docs.opencv.org/3.3.0/d5/dae/

tutorial aruco detection.html.

[12] M. R. Pedersen, L. Nalpantidis, R. S. Andersen, C. Schou, S. Bøgh,
V. Kr¨uger, and O. Madsen, “Robot skills for manufacturing: From
concept to industrial deployment,” Robotics and Computer-Integrated
Manufacturing, vol. 37, pp. 282–291, 2016.

[13] R. Jeong, Y. Aytar, D. Khosid, Y. Zhou, J. Kay, T. Lampe, K. Bous-
malis, and F. Nori, “Self-supervised sim-to-real adaptation for visual
robotic manipulation,” in 2020 IEEE International Conference on
Robotics and Automation (ICRA).

IEEE, 2020, pp. 2718–2724.

[14] F. Wildgrube, A. Perzylo, M. Rickert, and A. Knoll, “Semantic mates:
Intuitive geometric constraints for efﬁcient assembly speciﬁcations,” in
Proceedings of the IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS), 2019.

[15] D. Kuhner, J. Aldinger, F. Burget, M. G¨obelbecker, W. Burgard, and
B. Nebel, “Closed-loop robot task planning based on referring ex-
pressions,” in 2018 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS).

IEEE, 2018, pp. 876–881.

[16] A. Pnueli, “The temporal logic of programs,” in 18th Annual Sympo-
IEEE, 1977,

sium on Foundations of Computer Science (sfcs 1977).
pp. 46–57.

[17] K. He, A. M. Wells, L. E. Kavraki, and M. Y. Vardi, “Efﬁcient sym-
bolic reactive synthesis for ﬁnite-horizon tasks,” in 2019 International
Conference on Robotics and Automation (ICRA).
IEEE, 2019, pp.
8993–8999.

[18] A. Pacheck, S. Moarref, and H. Kress-Gazit, “Finding missing skills
for high-level behaviors,” in 2020 IEEE International Conference on
Robotics and Automation (ICRA).

IEEE, 2020, pp. 10 335–10 341.

[19] D. K. Misra, J. Sung, K. Lee, and A. Saxena, “Tell me dave: Context-
sensitive grounding of natural language to manipulation instructions,”
The International Journal of Robotics Research, vol. 35, no. 1-3, pp.
281–300, 2016.

