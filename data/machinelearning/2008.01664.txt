Inducing game rules from varying quality
game play

Alastair Flynn
Hilary term, 2020

0
2
0
2

g
u
A
4

]

G
L
.
s
c
[

1
v
4
6
6
1
0
.
8
0
0
2
:
v
i
X
r
a

 
 
 
 
 
 
Contents

1 Introduction

2 Related work

2.2

2.1 General Game Playing . . . . . . . . . . . . . . . . . . . . . .
2.1.1 Game Description Language . . . . . . . . . . . . . . .
Inductive General Game Playing . . . . . . . . . . . . . . . .
2.2.1
Inductive Logic Programming . . . . . . . . . . . . . .
2.2.2 Back to IGGP . . . . . . . . . . . . . . . . . . . . . .
ILP systems used . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.1 Metagol
. . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.2 Aleph . . . . . . . . . . . . . . . . . . . . . . . . . . .
ILASP . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3.3

2.3

3 IGGP problem

4 Generating Traces

4.1 Sancho . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4.1.1 Monte Carlo Tree Search . . . . . . . . . . . . . . . .
4.1.2 Other aspects of Sancho . . . . . . . . . . . . . . . . .
Sancho and IGGP . . . . . . . . . . . . . . . . . . . .
4.1.3
. . . . . . . . . . . .
. . . . . .

4.2.1 Transforming game traces into IGGP tasks

4.2 Generating and transforming the traces

5 Experimental methodology

5.1 Materials
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2 Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2.1 Training . . . . . . . . . . . . . . . . . . . . . . . . . .
5.2.2 Testing . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Results

6.1 Results summary . . . . . . . . . . . . . . . . . . . . . . . . .
6.1.1 E1: Varying the quality of game traces . . . . . . . . .
. . . . . . . .
6.1.2 E2: Varying the amount of game traces

1

1

7
7
7
9
9
10
10
10
12
13

14

16
16
17
18
19
19
21

23
23
24
24
25

26
26
26
29

7 Conclusions

32

2

Abstract

General Game Playing (GGP) is a framework in which an artiﬁcial in-
telligence program is required to play a variety of games successfully. It acts
as a test bed for AI and motivator of research. The AI is given a random
game description at runtime (such as checkers or tic tac toe) which it then
plays. The framework includes repositories of game rules written in a logic
programming language.

The Inductive General Game Playing (IGGP) problem challenges ma-
chine learning systems to learn these GGP game rules by watching the game
being played. In other words, IGGP is the problem of inducing general game
rules from speciﬁc game observations. Inductive Logic Programming (ILP),
a subﬁeld of ML, has shown to be a promising approach to this problem
though it has been demonstrated that it is still a hard problem for ILP
systems.

Existing work on IGGP has always assumed that the game player being
observed makes random moves. This is not representative of how a human
learns to play a game, to learn to play chess we watch someone who is
playing to win. With random gameplay situations that would normally be
encountered when humans play are not present. Some games rules may not
come into eﬀect unless the game gets to a certain state such as castling in
checkers.

To address this limitation, we analyse the eﬀect of using intelligent versus
random gameplay traces as well as the eﬀect of varying the number of traces
in the training set.

We use Sancho, the 2014 GGP competition winner, to generate intelli-
gent game traces for a large number of games. We then use the ILP systems,
Metagol, Aleph and ILASP to induce game rules from the traces. We train
and test the systems on combinations of intelligent and random data includ-
ing a mixture of both. We also vary the volume of training data.

Our results show that whilst some games were learned more eﬀectively
in some of the experiments than others no overall trend was statistically
signiﬁcant.

The implications of this work are that varying the quality of training
data as described in this paper has strong eﬀects on the accuracy of the
learned game rules; however one solution does not work for all games.

Chapter 1

Introduction

General Game Playing (GGP) is a framework in which artiﬁcial intelligence
programs are required to play a large number of games successfully [22].
Traditionally game playing AI have focused on a single game [39, 24, 38, 42].
Famous AI include programs such as IBMs Deep Blue which is able to beat
grand masters at chess but is completely unable to play checkers [24]. These
traditional AI also only do part of the work. A lot of the analysis of the
game is often done outside of the system [38]. A more interesting challenge
is building AI that can play games without any prior knowledge. In GGP
the AI are given the description of the rules of a game at runtime. Games in
the framework range greatly in both number of players and complexity; from
the single player eight puzzle to the six player chinese checkers, and from
the relatively simple rock paper scissors to chess [21]. The progress in the
ﬁeld is consolidated annually at the GGP competition where participants
compete to ﬁnd the best GGP AI [22].

The GGP framework includes a large database of games.

In a GGP
match games from these databases are selected at random and sent to the
competitors [22]. The games are speciﬁed in the Game Description Language
(GDL), a logic programming language built for describing games as state
machines [30]. An example of GDL rules is given in listing 1.

These GDL game descriptions form the basis for the Inductive General
Game Playing (IGGP) problem [11]. The task is an inversion of the GGP
problem. Rather than taking game rules and using them to play the game in
IGGP the aim is to learn the rules from observations of gameplay, similar to
how a human might work out the rules of a game by watching someone play
it. Cropper et al. [11] deﬁne the IGGP problem in their 2019 paper; given
a set of gameplay observations the goal is to induce the rules of the game.
The games used in IGGP are selected from the GGP competition problem
set meaning they are varied in complexity. An example IGGP problem for
rock paper scissors is given in table 1.1 and the rules for the GGP game
description in table 1.2.

1

BK
beats(scissors,paper).
beats(paper,stone).
beats(stone.scissors).

succ(0,1).
succ(1,2).
succ(2,3).

player(p1).
player(p2).

true step(0).
true score(p1,0).
true score(p2,0).

E+

E−

next score(p1,0).
next score(p2,1).

next step(1).

next score(p1,1).
next score(p1,2).
next score(p1,3).
next score(p2,0).
next score(p2 2).
next score(p2,3).

next step(0).
next step(2).
next step(3).

Table 1.1: An example of an IGGP task for Rock Paper Scissors. The task
consists of the background knowledge (BK), the positive exam-
ples (E+) and the negative examples (E−). Here the predicate
to be learned is next. We treat the next score and next step
as two separate tasks

next step(N):-

true step(M),
succ(M,N).

next score(P,N):-

true score(P,N),
draws(P).

next score(P,N):-

true score(P,N),
loses(P).

next score(P,N2):-

true score(P,N1),
succ(N2,N1),
wins(P).

draws(P):-

does(P,A),
does(Q,A),
distinct(P,Q).

loses(P):-

does(P,A1),
does(Q,A2),
distinct(P,Q),
beats(A2,A1).

wins(P):-

does(P,A1),
does(Q,A2),
distinct(P,Q),
beats(A1,A2).

Table 1.2: The GGP rules for the next step and next score predicates.

Translated from GDL to Prolog for readability.

2

Figure 1.1: A selection of games from the GGP competition. From the
top left: checkers, chinese checkers, chess, tic tac toe, rubik’s
cube and eight puzzle

An eﬀective way of solving the IGGP problem is a form of machine
learning: Inductive Logic Programming (ILP) [11, 32]. In ILP, the learner is
tasked with learning logic programs given some background knowledge and a
set of values for which the programs are true or false. In the IGGP paper, the
authors showed through empirical evaluations that ILP systems achieve the
best score in this task compared to other machine learning techniques [11].
The ILP system derives a hypothesis, a logic program that when combined
with the background knowledge entails all of the positive and none of the
negative examples [32]. In the IGGP paper it is also shown that the problem
is hard for current ILP systems, with on average only 40% of the rules being
learned by the best performing systems [11].

However, the existing work has limitations. All work so far has assumed
that the gameplay being observed is randomly generated [11]. Rather than
agents playing to win they simply make moves at random. Often this has the
result of the game terminating before it reaches a goal state due to a cap on
trace length or sections of the rule set remaining completely unused. There
has also not been any research done to ascertain the eﬀects that increasing
the number of game observations has on the quality of the induced rules.
It is unknown whether there is a threshold at which the adding new game
traces does not further increase the accuracy. In this paper we evaluate the
ability of ILP agents to correctly induce the rules of a game given diﬀerent
sets of gameplay observations - intelligent gameplay, random gameplay and
combinations of the two. To generate the intelligent gameplay we use the
system Sancho, winner of the 2014 GGP competition1 [41].

Ideally in this paper we would compared optimal play versus random
play or possibly human play against random play. However, it is clear gath-
ering gameplay data at a consistent level human of play across a large set of

1http://ggp.stanford.edu/iggpc/winners.php accessed 28/04/2020

3

1)
2)
3)

(succ 0
(succ 1
(succ 2
(beats scissors paper)
(beats paper stone)
(beats stone scissors)
(<= (legal ?p scissors) (player ?p))
(<= (legal ?p paper) (player ?p))
(<= (legal ?p stone) (player ?p))
(<= (draws ?p) (does ?p ?a) (does ?q ?a) (distinct ?p ?q))
(<= (wins ?p) (does ?p ?a1) (does ?q ?a2) (distinct ?p ?q) (beats ?a1 ?a2))
(<= (loses ?p) (does ?p ?a1) (does ?q ?a2) (distinct ?p ?q) (beats ?a2 ?a1))

Listing 1: A sample of rules from the GDL description of Rock Paper Scis-
sors. The ? indicates a variable and <= indicates an implication
with the ﬁrst expression after it being the head and the conju-
gation of the rest making up the body

game is not feasible. Gathering optimal gameplay data is also not realistic
despite the fact that all the games have an optimal solution. The games
used in this paper are ﬁnite, discrete, deterministic games of complete in-
formation, that is, games to which the minimax procedure can be applied.
Using the minimax procedure to generate game traces would guarantee a
form of optimality [8]. Namely each player would maximize their minimum
gain. Despite this being an attractive prospect it is computationally in-
feasible to run this for the majority of games in the dataset. Checkers for
example has a game tree far too large to reasonably compute [38]. We could
instead apply A* search, which is guaranteed to ﬁnd an optimal solution
when given an admissible heuristic. Instead Sancho was used since in the
cases where the minimax procedure was easily solvable or known admissible
heuristic were available for a game Sancho was able to provide optimal or
near optimal play. It also brought the advantage of being able to play all
the games indiscriminately.

Every year the GGP competition includes a “Carbon versus Silicone”
event where humans pit themselves against the winner of the main com-
petition. The machines almost always win [22], so we can assume Sancho
has above amateur human level play hence we use the word ‘intelligent’ to
describe its level of play. In this paper we also vary the number of gameplay
traces from which the ILP systems learn to determine the optimal number
of traces.

It is not obvious which of random or intelligent gameplay will achieve the
best results. When learning the rules of chess would a human rather watch
moves being made randomly, or a match between two grandmasters? It is
not an easy question to answer. Both situations will result in a restricted

4

view of the game, with certain situations never occurring in each one. This
is not only a dilemma in the context of learning game rules. For example,
teaching a self driving car to navigate roads requires training it on examples
of driving. We would clearly not train it on highly intelligent Formula One
quality driving and neither would we train it on random movement of the
car. The question to be asked is what is the ideal quality level of training
data to use to best teach a system the rules you want it to learn. In this
paper, we try to help give some insight into this fundamental question.
Speciﬁcally, we ask the following research questions:

Q1 Does varying the quality of game traces inﬂuence the ability for learners
to solve the IGGP problems? Speciﬁcally, does the quality of game
play aﬀect predictive accuracy?

Q2 Does varying the amount of game traces inﬂuence the ability for learners
to solve the IGGP problems? Speciﬁcally, does the quality of game
play aﬀect predictive accuracy?

Q3 Can we improve the performance of a learner by mixing the quality of

traces?

We will train a range of ILP systems that each use a diﬀerent approach
to the problem on diﬀerent sets of training data. The results for Q1 are the
most interesting as it is not clear what the expected outcome is. Q2 has
more of a natural answer. It is generally accepted that for machine learning
problems the more training data you have the better the predictive accuracy
of the ML system [31]. Often in ILP only a small amount of training data
is needed, adding more data may not signiﬁcantly aﬀect accuracy [32].

The third question is interesting.

Intuitively greater diversity in the
training data should give a result closer to the rule that generated the data.
However if a learner is trained on random data and only tested on random
data we would expect this to perform better than a learner trained on a mix
of intelligent and random data and then tested on random data since in the
ﬁrst case the training and test distributions match [31]. This question thus
highlights an issue we face: how do we test the learned game rules?

Ideally the generated rules would be compared directly against the rules
in the GDL game descriptions. We would take the generated rule and see for
what percentage of all possible game states the reference rule and the learned
rule gave the same output. Unfortunately we do not have the computational
resources to do this with a lot of the games having too many possible states
such as checkers which has a state-space complexity of roughly 5.0 · 1020 [43]
and sudoku which exceeds 6.6 · 1021 [19]. Instead we will test the learned
programs on both intelligently generated and randomly generated data of
the same quality used in training.

5

We would expect models trained on the same distribution as they are
tested on to perform best since it is generally accepted that the accuracy of
a model increases the closer the test data distribution is to the training data
distribution [31]. However, Gonzales and Abu-Mostafa [23] suggest that a
system trained on a diﬀerent domain to the one it is tested can outperform
a system trained and tested on the same domain. Given a test distribution
there exists a dual distribution that, when used to train, gives better results.
The dual distribution gives a lower out-of-sample2 error than using the test
distribution. This dual distribution can be thought of as the point in the
input space where the least out-of-sample error occurs [23].

As well as optimising the single training distribution we can take data
from multiple distinct distributions. Ben-David et. al. [3] show that training
data taken from multiple diﬀerent domains can in fact give lower error on
testing data that training data taken from any single domain, including the
testing domain. It is not clear in our case what selection of training data
will result in the most eﬀective learning.

To help answer questions 1-3, we make the following contributions:

Contributions

• We implement a system to play GGP games at random and highly

intelligent levels (Chapter 4).

• We transform the GGP traces to IGGP problems (Chapter 4).

• We train the ILP systems Metagol [13], Aleph [33] and ILASP [29,
28, 27] on diﬀerent combinations of intelligent and random data as
well as testing them on each individually (Chapter 5) to show that the
diﬀerence in accuracy of the learned programs is small (Chapter 6).

• We train the ILP systems on diﬀering quantities of game traces (Chap-
ter 5) to show through testing that an increase in the number of traces
does not necessarily cause an increase in the accuracy of the learned
programs (Chapter 6).

2out-of-sample data is data that is not in the training set

6

Chapter 2

Related work

2.1 General Game Playing

General Game Playing (GGP) is a framework for evaluating an agent’s gen-
eral intelligence across a wide range of tasks [11, 22]. The agents accept
declarative descriptions of arbitrary games at run time and use the descrip-
tions to play those games eﬀectively. All the games are ﬁnite, discrete,
deterministic multi-player games of complete information [30]. The games
in the GGP competition game set vary in number of players, dimensions
and complexity. For example games such as rock paper scissors have 0 di-
mensions and only 10 rules in the given GGP rule set, more complex games
such as checkers has 52 rules and are 2 dimensional. There are also single
player games such as eight puzzle or ﬁzz buzz.

In 2005 an annual International General Game Playing Competition (IG-
GPC) was set up [25]. Each year hopeful participants pit GGP agents
against one another to determine the most eﬀective system. The competi-
tors take part in a series of rounds of increasing complexity. The agent that
wins the most games in these rounds is declared victorious. The 2014 winner
Sancho is used in this paper to generate traces of intelligent game play for
the IGGP task.

The game are speciﬁed in the Game Description Language (GDL). These
descriptions are used as example rule sets in the Inductive General Game
Playing (IGGP) problem as the output that the learners would ideally gener-
ate. The descriptions of the games used in GGP are not necessarily minimal
so it is possible that an ILP system could generate a more concise rule set
than the GGP descriptions.

2.1.1 Game Description Language

Game Description Language (GDL) is the formal language used in the GGP
competition to specify the rules of the games [30]. The language is based on
a logical programming language Datalog, a subset of Prolog. To understand

7

the semantics of GDL it helps to ﬁrst cover logic programming as a ﬁeld of
study.

Logic Programming

Logic programming is a programming paradigm based on formal logic. Pro-
grams are made up of facts and rules. Rules are made up of two parts:
the head and the body. They can be read as logical implications where the
conjunction of all the elements in the body imply the head. The syntax is
diﬀerent for diﬀerent logical programming languages but the head is usually
written before the body in reverse implication. For example a :- b,c. A
fact is simply a rule without a body, that is, a statement that is taken as
true. The language interpreter takes logical statements as queries and re-
turns whether they are true or false. If there are free variables in the query
the interpreter assigns them values for which the query is true. Logical
programming is good for symbolic non-numeric computation [5]. It is well
suited to solving problems that involve well deﬁned objects and relations
between them, such as a GGP game.

Usefulness of GDL

The game description language was designed speciﬁcally to represent ﬁnite,
discrete, deterministic multi-player games of complete information. The
language is based on Datalog and the property of any question of logical
entailment being decidable is retained.

The games are deﬁned in terms of a set of true facts and logical rules

that capture the information needed to give the following predicates:

• The initial game state

• The goal states

• The terminal states

• The legal moves for a given player from a given state

• The next state for a given player, state and move

The GDL language is an extension of Datalog¬, that is Datalog with
stratiﬁed negation [30]. Datalog allows only universally quantiﬁed rules con-
sisting of a conjunction of positive atoms that imply a single atom. Datalog¬
allows for negative as well as positive atoms [1]. GDL also allows for some
functional symbols, that is predicates containing other predicates however
it restricts to keep the ﬁnite model property that is inherent to Datalog [30].
In GDL variables are written with the symbol ? before them and rules
are written starting with => followed by the head followed by the body

8

true(?f)
does(?r,?m)
next(?f)
legal(?r,?m)
goal(?r,?n)
terminal
init(?f)
role(?n)
distinct(?x,?y)

Atom ?f is true in the current game state
Player ?r performs action ?m in the current state
Atom ?f will be true in the next game state
Action ?m is a legal move for player ?r in the current game state
Player ?r performs action ?m in the current game state
The current state is terminal
Atom ?f is true in the initial game state
The Constant ?n denotes a player
?x and ?y are syntactically diﬀerent

Table 2.1: The reserved predicates in GDL

predicates which are interpreted as a conjunction. All rules are universally
quantiﬁed. For example

(<= (next (cell ?x ?y ?player)) (does ?player (mark ?x ?y)))

in ﬁrst order logic this would be written

∀x.∀y.∀player.does(player, (mark(x, y))) → cell(x, y, player)

GDL also reserves certain words as listed in table 2.1

In the IGGP problem (section 2.2) the given task is to generate the rules

to predict the goal, next, legal and terminal predicates.

2.2

Inductive General Game Playing

The Inductive General Game Playing (IGGP) problem is an inversion of
the GGP problem. Rather than using game rules to generate gameplay the
learner must learn the rules of the game from observations of game play.
The learner is given a set of game traces and is tasked with using them to
induce (learn) the rules of the game that produced the traces [11]. IGGP
was designed as a way of benchmarking machine learning systems.

The deﬁnition of task itself is based on the Inductive Logic Programming

problem.

2.2.1

Inductive Logic Programming

Inductive Logic Programming (ILP) is a form of machine learning that
uses logic programming to represent examples, background knowledge, and
learned programs [14]. To learn the ML system is supplied with positive
examples, negative examples and the background knowledge. In the general
inductive setting we are provided with three languages.

9

• LO: the language of observations (positive and negative examples)

• LB: the language of background knowledge

• LH : the language of hypotheses

The general inductive problem is as follows: given a set of positive examples
E+ ⊆ LO, negative examples E− ⊆ LO and background knowledge B ⊆ LB
ﬁnd an hypothesis H ∈ LH such that

B ∪ H |= E+ and B ∪ H (cid:54)|= E−

[32] That is that the generated hypothesis and the background knowledge
imply the positive examples and do not imply the negative examples.

2.2.2 Back to IGGP

In IGGP we also have the idea of background knowledge and positive or
negative observations. The task itself is closely based on the ILP problem
and is described in chapter 3. The games used for the IGGP problem are
taken from the IGGP dataset. It is a collection of 50 games, speciﬁed in
GDL. The purpose of this database is to standardise the set of games used
in the IGGP problem to allow for results to be easily compared. It is the set
of games used in this paper. A mechanism is also provided by the authors
to turn these GDL game descriptions in the set into new IGGP tasks. This
mechanism simulates random play of the games to generate the observations.
In this paper we modify it to generate optimal game traces.

2.3

ILP systems used

We use three ILP systems to compare the eﬀects of diﬀerent learning data,
Metagol, Aleph and ILASP. There are many approaches to the ILP problem
[4, 10]. The three systems here all represent diﬀerent approaches to the
problem but certainly do not give a full representation of the techniques
available.

2.3.1 Metagol

The Metagol ILP system is a meta-interpreter for Prolog, that is, it is written
in the same language is evaluates [9, 12, 13]. Metagol takes positive and
negative examples, background knowledge and metarules. Metarules are
speciﬁc to Metagol. They determine the shape of the induced rules and are
used to guide the search for a hypothesis. An example of a metarule is the
chain rule

P (A, B) ← Q(A, C), R(C, B)

10

The letters P , Q and R represent existentially quantiﬁed second order vari-
ables, A, B and C are regular ﬁrst order variables. When trying to in-
duce rules the second order variables are substituted for predicates from the
background knowledge or the hypothesis itself. To illustrate this consider a
metarule being applied when learning the predicate last(A,B) where a is a
list and b is the last element in it. Given the positive example

last([a,l,g,o,r,i,t,h,m],m).

As well as the background predicates reverse/2 and head/2 the chain

rule might be used to derive the rule

last(A,B) :- reverse(A,C), head(C,B)

1. Select a positive example (an atom) to generalise. If none exists, stop,

otherwise proceed to the next step.

2. Try to prove an atom using background knowledge by delegating the
proof to Prolog. If successful, go to step 1, otherwise proceed to the
next step.

3. Try to unify the atom with the head of a metarule and either choose
predicates from the background knowledge that imply the head to ﬁll
the body. Try proving the body predicate, if it cannot be proved
try diﬀerent predicate in the background knowledge1. If no predicate
can be found that proves the positive example then try adding a new
invented predicate and attempt to prove this2.

4. Once you ﬁnd a metarule substitution that works add it to the program

and move to the next atom

The end hypothesis is all the metarule substitutions. It is then checked
that the negative atoms are not implied by the hypothesis, if they are a new
one is generated. When the hypothesis is combined with the background
knowledge the positive examples, but not the negative examples, are implied.
The choice of metarules determines the structure of the hypothesis. Dif-
ferent choices of metarules will allow diﬀerent hypotheses to be generated.
Deciding which metarules to use for a given task is an unsolved problem [9].
For this task a set of 9 derivationally irreducible metarules are used which
remain consistent across all tasks.

1To prove the body predicate the whole procedure is called again with the body
predicates as the positive examples. For example if we had last([a,b],b) as our
positive example and have tried to use the chain metarule to to prove it with reverse
and head we would then call the whole procedure again with the positive examples as
[reverse([a,b],C),head(C,b)] if this was successful then we continue, otherwise we try
diﬀerent predicates

2For example we might replace head with an invented predicate in the previous footnote

example

11

2.3.2 Aleph

Aleph is a Prolog variant of the ILP system Progol [33]. As input Aleph
takes positive and negative examples represented as a set of facts along with
It also requires mode declarations and deter-
the background knowledge.
minations which are speciﬁc to Aleph. Mode declarations specify the type
of the inputs and outputs of each predicate used. Determinations specify
which predicates can go in the body of a hypothesis. These determinations
take the form of pairs of predicates, the ﬁrst being the head of the clause
and the second a predicate that can appear in its body.

For each predicate we would like to learn in the IGGP problem we give
Aleph the determinations consisting of every target predicate (next, goal
and legal) paired with every background predicate (which are speciﬁc to
each game). Luckily there has been some work to induce mode declarations
from the determinations [17] so we do not need to come up with our own
mode declarations.

A basic outline of the Aleph algorithm is taken from the aleph website

3:

1. Select an example to be generalised.

If none exist, stop, otherwise

proceed to the next step.

2. Construct the most speciﬁc clause (also known as the bottom clause
[33]) that entails the example selected and is within language restric-
tions provided.

3. Search for a clause more general than the bottom clause. This step is
done by searching for some subset of the literals in the bottom clause
that has the ’best’ score.

4. The clause with the best score is added to the current theory and all

the examples made redundant are removed. Return to step 1.

Mode declarations and determinations are used in step 2 of this pro-
cedure to bound the hypothesis space. Only predicates that are in the
determinations of the hypothesis and are of the correct type are tried. The
bottom clause constructed is the most speciﬁc clause that entails the exam-
ple. Therefore a clause with the same head and any subset of the predicates
of the body will be more general than the bottom clause. Aleph only con-
siders these generalisations of this bottom clause.

By default Aleph preforms a bounded breath ﬁrst search on all the pos-
sible rules, enumerating shorter clauses before longer clauses. The search is
bounded by several parameters such as maximum clause size and maximum
proof depth.

3http://www.cs.ox.ac.uk/activities/programinduction/Aleph/aleph.html

accessed

26/03/2020

12

2.3.3

ILASP

ILASP is an ILP system based on Answer Set Programming (ASP). An
introduction to ASP can be found here [7]. ILASP uses a subset of ASP
that is deﬁned in these papers [26, 28, 27] The ILASP process eﬀectively
generates all possible rules of a certain length, turns the problem into an
ASP problem that adds a predicate to each rule allowing it to be active
or inactive. It then uses the ASP solver Clingo [20] to check which rules
should be active if the program is to be consistent with the positive and the
negation of the negative examples [28, 27]. In this paper we use a version of
ILASP based on ILASP2i [29] which was developed with the IGGP problem
in mind [11]. As one input ILASP takes a hypothesis search space, i.e. the
set of all hypotheses to be considered. This is constructed using the type
signatures given for each problem that are provided in the IGGP dataset.
An ILASP task is deﬁned as a tuple T = (cid:104)B, SM , E+E−(cid:105) where B is the
background knowledge, SM is the hypothesis space, and E+ and E− are the
positive and negative examples. The ILASP procedure is given in algorithm

Algorithm 1: ILASP outline

n = 0;
solutions = [];
while solutions.empty do

1.

SN = all possible hypotheses of length N from SM ;
ns = all subsets of SN that imply E− (Using an ASP solver);
vs = the set of rules that for each set of rules in ns imply false if

exactly those rules are active;
solutions = all subsets of SN that imply E+ and satisfy vs (using
asp solver);

n = n + 1;

end

The approaches used by ILASP have proved to be well suited to the

IGGP task [11].

13

Chapter 3

IGGP problem

The IGGP problem is deﬁned in the 2019 paper Inductive General Game
Playing [11]. Much like the problem of ILP described in section 2.2.1 the
problem setting consists of examples about the truth or falsity of a predicate
F and a hypothesis H which covers F if H entails F . We assume the
languages of:

• E the language of examples (observations)

• B the language of background knowledge

• H the language of hypotheses

Each of these languages can be seen as a subset of those deﬁned for the
ILP task. In our experiments we transform data generated from the GDL
descriptions of games in the IGGP dataset and transform it into the lan-
guages of B and E . GDL allows for functional symbols in rules albeit in
a restricted form. For example any atom appearing inside a true predi-
cate such as true(count(9)). We ﬂatten all of these to single, non nested
predicates, i.e. true_count(9). This is needed as not all ILP systems sup-
port function symbols. We can therefore assume that both E and B are
made up of function-free ground atoms. The language of hypotheses H can
be assumed to consist of datalog programs with stratiﬁed negation as de-
scribed here [37]. Stratiﬁed negation is not necessary but in practice allows
signiﬁcantly more concise programs, and thus often makes the learning task
computationally easier. We ﬁrst deﬁne an IGGP input then use it to deﬁne
the IGGP problem. The input is based on the general input for the Logical
induction problem.

The IGGP Input: An input ∆ is a set of triples {(Bi, E+

i , E−

i )}m
i=0

where

• Bi ⊂ B represents background knowledge
• E+

i ⊆ E and E−

i ⊆ E represent positive and negative examples re-

spectively

14

The IGGP input composes the IGGP problem as follows:

The IGGP Problem: Given an IGGP input ∆, the IGGP problem is
i ) ∈ ∆ it holds

to return a hypothesis H ∈ H such that for all (Bi, E+
that H ∪ Bi |= E+
i and H ∪ Bi (cid:54)|= E−
i .

i , E−

In this paper we use the IGGP problem to analyse the ability of ILP
agents to learn from a range of training data of diﬀering quality. To analyse
the ability of the ILP systems to learn we use, for each IGGP task ∆ and
hypothesis H, a testing set T = T + ∪ T − where

• T + ⊆ E and T − ⊆ E are the positive and negative testing observa-

tions.

• T + ∩ (cid:83)m

i=0 E+

i = ∅. The positive testing examples are distinct from

the positive training examples

• T − ∩ (cid:83)m

i=0 E−

i = ∅. The negative testing examples are distinct from

the negative training examples

To test the systems we check for each t+ ∈ T + whether

H ∪

and for each t− ∈ T − whether

H ∪

m
(cid:91)

i=0

m
(cid:91)

i=0

Bi |= t+

Bi (cid:54)|= t−

The success of the system is deﬁned as the percentage of correctly classiﬁed
t ∈ T . We use this deﬁnition of success to answer the research questions
in chapter 1. The experiments performed to answer these questions are
described in chapter 5.

15

Chapter 4

Generating Traces

In this section we describe the process of generating game traces then trans-
forming them into to IGGP tasks. The ﬁrst step is to generate the intelligent
and random gameplay. To do this we used the Sancho system [41].

4.1 Sancho

Generating intelligent gameplay is a nebulous task. Deﬁning intelligence is
no small ask. Using optimal gameplay traces or human generated gameplay
would be ideal, however this is infeasible in the scope of this project. Prov-
ably optimal traces were experimented with. An optimal player for eight
puzzle using A* with an admissible heuristic was created with the intention
of writing optimal players for more games such as tic tac toe and knights
tour. However when testing the Sancho system it was found that game
traces produced were often of near optimal quality. In fact for eight puzzle
it generates provably optimal traces as described later in the chapter.
It
was decided that it would be easier and we would generate a possibly more
a consistent level of play across all games using the GGP player Sancho.

The winner of the 2014 GGP competition “Sancho”1 was selected since it
is the most recent winner with published code [41]. Some small modiﬁcations
to the information logged by the game server are made but otherwise the
code is unchanged. Since the aim of the GGP competition is to ﬁnd the
program that performs best at the set of games used in the IGGP problem
we conclude that this system was the best general game player to use. We
can assume Sancho can play above amateur human level across the majority
of games in the the dataset. At the annual GGP competition there is also a
match held at the end where the human creators of the AI themselves play
against the winner. The humans almost never win [22].

Sancho uses a range of techniques to generate gameplay. The core of

algorithm used by Sancho is the Monte Carlo tree search (MCTS).

1http://ggp.stanford.edu/iggpc/winners.php accessed on 12/03/2020

16

4.1.1 Monte Carlo Tree Search

Given a game state the basic MCTS will return the most promising next
move. The algorithm achieves this by simulating random playouts of the
game many times. The technique was developed for Computer Go but has
since been applied to play a wide range of games eﬀectively including board
games and video games [40, 6]. The use of random simulation to evaluate
game states is a powerful tool. No extra information about the game such
as heuristics for evaluating states are needed at all, the rules of the game
suﬃce. In the case of the GGP problem this is ideal.

In our case all games being played are sequential, ﬁnite and discrete so

we only need to consider MCTS for this case.

Figure 4.1: A section of a game tree of tic tac toe showing some of the

possible moves. Stannered/CC-BY-SA-3.0

The MCTS is a tree search algorithm, the tree being searched is the
game tree. Each node on the game tree represents a state (ﬁgure 4.1). The
search is a sequence of traversals of the game tree. A traversal is a path
that starts at the root node and continues down until it reaches a node that
has at least one unvisited child (not necessarily a terminal state). One of
these unvisited children is then chosen to be the start state for a simulation
of the rest of the game. The simulation selects moves randomly, playing the
game out to a terminal state. The result of the simulation is propagated
back from the node it started at all the way to the root node, updating
statistics attached to each node. These statistics are used to choose future
paths to traverse so that more promising moves are investigated with higher
probability.

17

Use of MCTS in Sancho

Sancho makes a few modiﬁcations to MCTS2. The main one being the adding
of heuristics. It also makes optimisations to increase eﬃciency.

In GGP matches a period of time before the match is given in which to
do pre match calculations. Sancho uses this period to derive basic heuristics.
A heuristic should take a game state or move and return a value based on
how promising that state or move is in relation to the goal state.

The identiﬁcation of possible heuristics takes place in two stages. The
ﬁrst of these is static analysis of the game rules. This static analysis iden-
tiﬁes possible heuristics that can be applied to the current game. These
if certain rules indicate capture of the
include things like piece capture:
players piece these can be selected against. Another is numeric quantity
detection: a number in the state acts as a heuristic (like number of coins
a player has). The second stage is simulation of the game. Many (possi-
bly tens of thousands) of full simulations of the game are run. After the
simulations are complete a correlation coeﬃcient is calculated between each
candidate heuristic’s observed values and the eventual score achieved in the
game. Those heuristics that show correlation above a ﬁxed threshold are
then enabled, and will be used during play to guide the tree exploration in
MCTS.

4.1.2 Other aspects of Sancho

Monte Carlo simulations are used for most of the games available however
Sancho identiﬁes some games that can be solved more eﬃciently in other
ways. Any single player game in the GGP dataset can be analysed using
puzzle solving techniques.

Puzzle Solving

A single player game in the GGP dataset is necessarily a deterministic game
of complete information due to the constraints of the Game Description
Language. This gives the useful constraint that any solution found in a
playout of the game will always remain valid since the game is completely
deterministic. Sancho identiﬁes these single player games and attempts to
derive heuristics as described above. Where an obvious goal state exists
a distance metric such as the hamming distance between two states can
be applied. A* search is then used to ﬁnd optimal solutions. For some
games, such as eight puzzle the hamming distance is an admissible heuristic
so Sancho ﬁnds a provably optimal solution.

2https://sanchoggp.blogspot.com/2014/05/what-is-sancho.html

accessed

on

15/03/2020

18

Static analysis

Sancho also does static analysis of the game rules.
It determines game
predicates that if true imply that either the goal will never be reached or
the goal will always be reached. An example of this is a game such as
untwisty corridor (the game is eﬀectively a maze where you immediately
lose if you step oﬀ a “safe” path). If the safe path is stepped oﬀ then the
goal can never be reached so any state not on the safe path is avoided.

4.1.3 Sancho and IGGP

Sancho generally provides good quality intelligent play when run on the
games in the IGGP dataset however in some cases it struggles.

The dataset contains several games based on game theory. Games such as
the prisoner dilemma where player can choose to either cooperate or defect.
If both players cooperate then they both get 3 points. If one defects and the
other tried to cooperate the cooperator gets 0 points and the defector gets 5.
If they both defect both get 0. There are several rounds of this in one game
playout. A human playing this might choose to try and cooperate, hoping
that that the other player will too however Sancho has been observed to
always defect. This result is predictable since this is the only strong Nash
equilibrium for this game however it is hard to justify this as “human quality
play”.

4.2 Generating and transforming the traces

To generate the intelligent and random traces for the experiments an auto-
matic system was constructed. The system runs Sancho or a random game
player on the GDL game descriptions and converts the traces into IGGP
tasks.

The general game playing community have developed a codebase that
provides the basic functionality for hosting a match between GGP agents3.
This codebase along with the GGP agent Sancho is used to generate the
datasets. The codebase includes a very basic random GGP player that will,
given a GDL game description, play a random legal move every turn. To
generate the random traces for a game of x players x Sancho or random
GGP players were pitted against one another.

For the IGGP problem information about games is represented as sets
of ground atoms. These atoms can be divided into two sets, the atoms that
can change during the game and the atoms that can not. The set of atoms
that can change can again be divided in two, the action atoms A and the
state atoms S.

3https://github.com/ggp-org/ggp-base accessed on 13/03/2020

19

The set S represents all atoms that can change from one state to the
other. Examples of elements of this set S might be the state of the board
in tic tac toe along with which of the players is to take their turn next:

( control noughts )
( cell 1 1 b ) ( cell 1 2 x ) ( cell 1 3 o )
( cell 2 1 x ) ( cell 2 2 o ) ( cell 2 3 o )
( cell 3 1 b ) ( cell 3 2 x ) ( cell 3 3 b )

The set A is the set of ground atoms representing the actions that can
be taken. The moves that each player makes are taken from this set. For a
game such as eight puzzle these would consist of the set of atoms:

{( move i j )|0 < i ≤ 9, 0 < j ≤ 9}

Each atom represent sliding the tile at (i, j) into the space on the board. To
represent a player not making a move we also always have noop ∈ A.

Game traces

We modiﬁed the GGP matchmaker to log the following information:

• Game state trace - The sequence of game states: states = (s1, ..., sn)
where each si ⊆ S is the set of ground atoms true in the ith state.

• Game roles - The list of roles of each player in the game: roles =

(r1, ..., rk) e.g. noughts and crosses in tic tac toe

• Move trace - The sequence of moves made by each player after each
state: moves = ((m1,r1, ..., m1,rk ), ..., (mn,r1, ..., mn,rk )) where mi,rj ∈
A is the ith move of player rj. In games where not all players move
every turn then mi,rj = noop shows that rj made no move

• Legal move trace - The sequence of the legal moves for each player
li,rj ⊆ A is

in each state: legal = ((l1,r1, ..., l1,rk ), ..., (ln,r1, ..., ln,rk )).
the set of possible moves for rj in state si

• Goal value trace - The sequence of goal value for each player on ev-
ery state: goals = ((g1,r1, ..., g1,rk ), ..., (gn,r1, ..., gn,rk )). gi,rj ∈ [0, 100]
represents how well player rj has achieved the goal in state si.

This represents all the data needed from the match to generate IGGP tasks.
In each state all the positive atoms in S (that is the atoms true in the
current state) are arguments of the true predicate. For example if at the
start of the game the ﬁrst cell is blank on the board then we would have
( true ( cell 1 1 b ) ) ∈ s1. All the positive atoms in A are arguments
of the does predicate in a similar fashion.

20

4.2.1 Transforming game traces into IGGP tasks

We deﬁne a function that transforms the sequences into four separate in-
duction tasks. An induction task is the set of triples {(Bi, E+
i=0. We
ﬁrst deﬁne a function trace that translates the sequences given in the log
of the match into a set of pairs {(Bi, E+
i=0 . We then deﬁne a function
triple which gives the the set of triples {(Bi, E+
i=0 from the set of
pairs {(Bi, E+

i , E−

i , E−

i )}n

i )}n

i )}n

i )}n

i=0.

We ﬂatten the sequences of (mi,r1, ..., mi,rk ) in moves, (l1,r1, ..., l1,rk ) in
legals and (g1,r1, ..., g1,rk ) in goals to a set that includes the role name as an
extra argument of the predicate (the arity is increased by 1). For example
if

(mi,r1, mi,r2) = [( move 2 2 ), ( move 2 3 )]

and r1 = black and r2 = white then it would be replaced by mi

mi = {( move black 2 2 ), ( move white 2 3 )}

moves is now a sequence (m1, ..., mn). With legals and goals a similar intu-
itive ﬂattening procedure is applied. All four predicates are now represented
by a single ﬂat list.

Building the traces

The trace function is deﬁned for legal, goal, next and terminal. We treat the
output of the zip function, which takes a pair of lists and turns them into
a list of pairs, as a set rather than a list. We deﬁne S[p/q] on a set S for
predicates p and q to be the substitution of all instances of q in S for p.

Λlegal
Λgoal
Λnext
Λterminal = traceterminal(states)

= tracelegal(states, legal)
= tracegoal(states, goals)
= tracenext(states, moves)

= zip states legal[legal/true]
= zip states goal

= zip states (tail states[next/true])
= zip states t

where t = take n (repeat ∅) ++ [{( terminal )}]
and n = (length states) − 1

The substitutions [legal/true] and [next/true] ensure that all positive
examples in E+ are in the relevant predicate for training. Before, the list
legal consisted of the only atoms in the predicate true. For the ILP systems
to identify this as an example of legal we need to surround these with the
legal predicate. That is, we need all the move atoms in legal to be inside
the legal predicate, e.g.:

( legal ( move 1 1 ) )

21

Since the predicates are already in the true predicate we only need to sub-
stitute one out for the other.

Some of the ILP systems being tested cannot work with function symbols
of arity greater than 0. To allow them to operate we merge the function
and their arguments into one predicate. For example ( legal ( move 1
1 ) ) becomes ( legal_move 1 1 ) where legal_move is a newly formed
predicate.

To generate the triples we use the two functions triples1 and triples2.
Let pred X p be the subset of atoms in the set X that use the predicate p.

triple1(Λp) = map f Λ

where f (B, E+) = (B, E+, (pred S p) − E+)

triple2(Λp) = map f Λ

where f (B, E+) = (B, E+, (pred A p) − E+)

We generate the IGGP task with triples1 and triples2 as below:

∆ = triples1(Λgoal) ∪ triples1(Λterminal) ∪ triples2(Λnext) ∪ triples2(Λlegal)

The IGGP task ∆ is set to the ILP systems as described in chapter 5.

22

Chapter 5

Experimental methodology

In this section we describe the exact methods used to run the experiments
and the experiments themselves. Two experiments were run. Experiment
E1 answers questions Q1 and Q3. The ILP systems are trained on random,
intelligent and mixed quality traces. In experiment E2 they were trained on
a varying number of mixed quality traces.

5.1 Materials

Generating training data

The training data was generated according to the methods described in
chapter 4. To generate the game traces the Sancho version 1.61 was used
[41]. The only modiﬁcations made to this were the changes to the logging
system as described in section 4.2.

To generate traces matches between multiple random players or multiple
instances of Sancho were conducted. Each match was run with 30 seconds
pre game warm up time and a maximum of 15 seconds per move. The games
used are listed in table 5.1. Due to issues of compatibility only 36 out of
the 50 games from the IGGP dataset were used. Some games caused issues
with Sanchos simulations and thus is was not possible to run experiments
with them.

All games used from the IGGP dataset have a limit on the number of

moves however it varied from game to game.

Due to restraints on computational power 30 random traces and 30 San-
cho generated traces were generated. The traces were generated on a ma-
chine with 20GB of RAM and an 8 core processor.

The ILP systems

The three ILP systems were trained on the random, intelligent and mixed
game traces using the same settings as were used in the IGGP paper [11].

23

dont touch
alquerque
duikoshi
asylum
eight puzzle
battle of numbers
farming
breakthrough
buttons and lights ﬁzz buzz
centipede
checkers
coins
connect4team

forager
gt centipede
gt chicken
gt prisoner

gt ultimatum
hex for three
horseshoe
hunter
knights tour
kono
light board
multiple buttons and lights
nine board tic tac toe

pentago
platform jumpers
rainbow
sheep and wolf
sudoku
sukoshi
tic tac toe
ttcc4
untwisty corridor

Table 5.1: Games used in the experiments. gt stands for “game theory”

These settings are:

• Metagol - Metagol 2.2.3 with YAP 6.2.2. The metarules used can be

found in the IGGP code repository1.

• Aleph - Aleph 5 with YAP 6.2.2. The default Aleph parameters were

used.

• ILASP - A specialised version ILASP* developed for this task was
used and can be found in the IGGP code repository. It is based on
ILASP2i.

The systems were run concurrently in both training and testing.

5.2 Methods

5.2.1 Training

Each system was given 15 minutes to generate a hypothesis for each predi-
cate. If the predicate was not learned the default hypothesis was true.

In E1 The ILP systems were trained on 8 full game traces. They were
each trained on intelligent random and mixed traces. The mixed traces were
made up of a 50/50 mix of random and intelligent traces.

In E2 we trained each system on 8, 16 and 24 mixed traces where each set
was made up of a 50/50 mix of random and intelligent traces. It was found
that the results for 16 and 24 traces were very similar across all systems so
no larger training sets were tested.

In the IGGP paper [11] the IGGP task was deﬁned across four predicates
for each game: goal, next, terminal and legal. After testing the systems on
all four predicates it was found that the score for the terminal predicates do
not accurately represent the ability of each system. Generally the randomly

1https://github.com/andrewcropper/mlj19-iggp accessed 23/05/20

24

generated game traces did not complete the game before the move limit was
reached. Since the terminal predicate is true on the last game state in each
game and the game state includes a move counter this allowed the systems
to simply learn the correlation between the maximum move and the terminal
predicate. They would learn the program: terminal :- true_step(MAX)
where MAX is the maximum number of moves for the game. Since when
a game is played randomly the max move count is almost always hit the
predicate is often perfectly solved. However if when Sancho plays the game
it solves it before the maximum move count is hit it is less likely to induce
a correct rule. For this reason the terminal predicate was not included in
training data.

5.2.2 Testing

Each system was tested on 4 randomly generated and 4 intelligently gener-
ated traces. To evaluate the performance of the ILP systems on the training
dataset we use two metrics: balanced accuracy and perfectly solved.

Balanced accuracy In the datasets used for testing the ILP systems the
vast majority of examples are negative. Balanced accuracy takes this into
account when evaluating approaches. The generated logical hypothesis H
along with the background knowledge B for the relevant game is tested. The
test data is the set of combined positive and negative testing examples E+ ∪
E−. We deﬁne the number of positive examples p = |E+|, the number of
negative examples n = |E−|, the number of correctly predicted positives as
tp = |{e ∈ E+|B ∪ H |= e}| and the number of correctly predicted negatives
as tn = |{e ∈ E−|B ∪ H (cid:54)|= e}|. The balanced accuracy is subsequently
deﬁned as ba = 100 · (tp/p + tn/n)/2.

Perfectly solved This metric considers the number of predicates for
which the ILP system correctly classiﬁed all examples. It is equivalent to
the number of games with a balanced accuracy score of 100. This metric is
important since for all predicates there exists an exact solution (the rules
that were used to generate the examples). A system that has correctly pred-
icated 99% of the results is not nearly as useful as one that predicts 100%.

In the results section (chapter 6) we present only the aggregate scores
in the results since the full results are too large for this paper. To evaluate
the systems we compare the average balanced accuracy and the number of
perfectly solved games when tested on optimal and random traces.

25

Chapter 6

Results

We now describe the results of testing the ILP systems. We conducted the
experiments in accordance with the methods described in chapter 5.

To recap the research questions are as follows:

• Q1 - Does varying the quality of game traces inﬂuence the ability for
learners to solve the IGGP problems? Speciﬁcally, does the quality of
game play aﬀect predictive accuracy?

• Q2 - Does varying the amount of game traces inﬂuence the ability for
learners to solve the IGGP problems? Speciﬁcally, does the quality of
game play aﬀect predictive accuracy?

• Q3 - Can we improve the performance of a learner by mixing the

quality of traces?

Experiment E1 answers questions Q1 and Q3 and experiment E2 an-

swers question Q2.

6.1 Results summary

6.1.1 E1: Varying the quality of game traces

In experiment E1 we train and test each system on random and intelligent
traces as well as a 50/50 mixture of both. Table 6.1 shows the average
balanced accuracy of each test of the systems. Table 6.2 shows the number
of games for which the rule was perfectly learned for each test. The the
diﬀerences for each system in the average balanced accuracy for the diﬀerent
training/testing combinations were unpronounced. To show this we use the
χ2 (chi squared) test to calculate the statistical signiﬁcance of the results.
The biggest diﬀerence between any two balanced accuracy testing results
between two distributions for a single system was for Aleph where it was
trained on Sancho traces and then tested on Random and Sancho traces.

26

Balanced Accuracy
Sancho
Random Sancho Random Sancho Random Sancho

Random

Mixed

51
66
72

51
65
72

51
65
71

51
72
72

51
67
73

51
66
73

Systems

Metagol
Aleph
ILASP

Table 6.1: The average balanced accuracies across all games and predicates
for each system in E1. Each row gives the balanced accuracies of
one system. Each was trained on random, Sancho generated and
mixed traces then tested against random and Sancho generated.
The top of the two header rows gives the training distribution
and the lower gives the test distribution

Perfectly Solved
Sancho
Random Sancho Random Sancho Random Sancho

Random

Mixed

0
1
7

0
0
8

0
0
5

0
4
10

0
1
5

0
0
8

Systems

Metagol
Aleph
ILASP

Table 6.2: The number of perfectly solved scores for each system trained

and tested as described in E1

The χ2 test was performed on these two scores. The result was a p-value
of 0.6723. This value is well above any reasonable threshold for statistical
signiﬁcance meaning the results can be viewed as not signiﬁcant. Despite
this there are still diﬀerences worth discussing between the diﬀerent results.

Aleph

Throughout all experiments conducted the programs learned by Aleph con-
sisted almost entirely of a disjunction of cases seen in testing. It appears that
the most general clause (see section 2.3.2) that could be found for a lot of the
examples was also the most speciﬁc clause. This simplistic method resulted
in programs that performed better when the training and testing data was
closely matched. This can be seen in the results for Aleph when trained and
tested on Sancho generated traces. For some games all Sancho generated
traces were the same, in ﬁzz buzz Sancho always says the correct thing and
in the prisoner dilemma both Sancho players always defect. This allowed
Aleph to achieve a perfect score on the next predicate. In cases where the
training data is diverse Aleph learns a program that over-approximates the
solution doing well on the tests with positive examples and badly on the
tests with negative examples.

27

Generally Aleph was best at learning the goal predicate. Unlike next
which takes a game state and player moves as input the goal predicate often
only takes the player role. The limited number of output values, 0 to 100,
also meant the output space was smaller than the other predicates. The
program Aleph produced would often simply associate each role to all the
diﬀerent goal values it had at any point in the training data. This is a
subsection of the goal values it learned for checkers:

goal(red,0).
goal(black,0).
goal(black,8).
goal(red,8).

This method does not capture the original game rule at all but achieves
reasonable scores in testing.

Aleph did not do particularly better when trained on the mixed training
It achieved similar scores to the other tests apart from when it was

set.
trained and tested on intelligent traces.

Metagol

A limitation with the Metagol system is that if it cannot learn a rule that
covers 100% of the positive training examples and 0% of the testing examples
it will not output any rule at all. With the time limitations placed on the
experiments Metagol rarely managed this. If some rule contains a special
case such as castling in checkers Metagol may have learned the general next
predicate but would not give any output since this case was not covered. The
search space scales exponentially with the size of the rules to be learned.

Metagol never learned any legal or goal predicates. The goal predicate
is often relatively complex. It can rely on game concepts which would be
obvious to humans but hard to learn for an ML system. For example the
idea of three of the same symbol in a row, column or diagonal in tic tac toe.
There might not be enough training examples to guide Metagol to concepts
such as these. The legal predicate is usually even more complex than the
goal, however in some cases the legal moves were constant across all states.
It is possible that the metarules supplied to Metagol for this experiment
were not ideal for this task.

Where Metagol learned the next predicate it often only managed the
simple parts of it. The predicate is split up into diﬀerent parts such as
next state and next step to reﬂect the diﬀerent predicates that change each
If one of the predicates was next step(N) where N increased by 1
move.
each turn Metagol managed to learn that however it failed to produce any
program that was more than two or three lines long.

28

ILASP

ILASP performed by far the best out of the three systems. The diﬀerences
in results between the systems trained on Sancho and random data was not
large. Out of them it scored the best when trained and tested on Sancho
traces presumably for the same reasons that Aleph did (the testing data
was very similar to training data). The margin by which this scored better
however was almost negligible. There was generally less than 2% diﬀerence
between the balanced accuracies for each predicate of each game.

ILASP scored several percentage points better across the board when
trained on the mixed data. ILASP beneﬁted the most from the combination
of training data out of all the systems. It scored better on the goal and next
predicates but not on the legal predicate. This makes sense because the legal
predicate changes the least between the intelligent and random traces, the
same complexity of rule still generally has to be learned in each. The next
predicate for tic tac toe was learned a lot better by ILASP on the mixed
data. It is unclear exactly why this is the case but it does appearer ILASP
learns better with greater diversity of examples.

6.1.2 E2: Varying the amount of game traces

In experiment E2 each system was trained three times on mixed traces. The
ﬁrst time with a total of 8 traces, the second with 16 traces and the third
with 24 traces. When designing this experiment it was assumed that the
more traces they were given to train on the better the systems would per-
form. However, as can been see in the scatter graph of the results this was
not the case.

29

75

70

65

60

55

50

y
c
a
r
u
c
c
A
d
e
c
n
a
l
a
B

Metagol tested on Random
Metagol tested on Sancho
Aleph tested on Random
Aleph tested on Sancho
ILASP tested on Random
ILASP tested on Sancho

8

16
Number of traces

24

When ILASP and Metagol have not been given enough time to fully learn
a predicate they output nothing. Both systems scale exponentially in time
taken according to the size of the input. Whilst for Metagol it didn’t have
time even for the 8 traces ILASP ran out of time only on the 16 and 24
traces training sets. This had the eﬀect for ILASP of a large drop in ef-
fectiveness. For 16 and 24 the default program consisting of just true was
used. This gave the balanced accuracy of 50 for each predicate. Metagol
still managed to learn the simple predicates that it had managed on the 8
trace training set however did not increase at all in eﬀectiveness over the
rest of the experiment.

Unlike the other systems Aleph was consistently able to process the
entire input since its method of learning is less computationally intense.
When tested on the random traces Aleph did better with the increased
number of traces. This was most likely due to the fact that more cases were
encountered in the training data. Since Aleph usually ends up putting the
cases it encounters straight into the program this simply meant there was a
greater number of situations in the testing set that Aleph had seen before
and was thus correctly able to classify. When tested on the intelligent traces
Alephs score was lower. This is possibly due to the maximum size of Aleph
programs taking eﬀect. The clauses relevant to the intelligent traces may
be pushed out of the program by the vast number of clauses taken from the
random gameplay examples.

If this experiment were to be conducted again much more enlightening
results than these could be obtained by increasing the time given to each
system to learn each predicate or even using a system with greater compu-

30

tational power to run the experiments.

31

Chapter 7

Conclusions

In this paper we have compared the ability of ILP systems to learn the
rules of a game from gameplay observations in the IGGP framework. Three
ILP systems have been trained on a range of diﬀerent datasets. These
datasets consisted of random gameplay, intelligent gameplay as generated
by the Sancho system, and a mix of both. The generated hypotheses of
the learners have been tested on random and intelligent gameplay from the
same distributions as the training data. Diﬀerences in the eﬀectiveness of
the learned programs in correctly classifying legal and illegal gameplay were
observed however there was not enough statistical signiﬁcance in the results
to disprove the null hypothesis.

To test the signiﬁcance of the size of the training dataset on the ability
of the ILP systems to learn the systems were trained on varying numbers of
game traces. The results of the experiment were inconclusive as not all the
systems had enough time to learn a hypothesis when trained on the large
datasets.

Limitations

There are several aspects that could be improved on in this paper.

Computational resources The computational resources and thus the
time given for each system to learn the rules were severely limited for the
experiments in this paper. If they were to be conducted again with greater
computational resource with more time given to each system more signiﬁcant
results may be achieved.

ILP systems In this paper we only test on three ILP systems, it is clear
a more representative result could be obtained by testing on more systems.
Techniques such as probabilistic [2, 36] and interactive [35] learning are
not used by any of the systems tested here.
ILASP is the only system
designed to handle noisy data [29], it would be interesting to try others
with this approach [34, 18]. The systems used also have many customisable

32

settings for example Metagols metarules which have major implications for
the programs learnt [15]. Aleph has many diﬀerent search methods on the
hypothesis space many of which may yield better results than the default
algorithm used in this paper.

Generating traces Whilst Sancho generates good examples of intelli-
gent play it would be interesting to see traces generated by other general
game playing methods [44, 25]. For some games there exists a provably
optimal set of moves in some cases such as for eight puzzle Sancho gener-
ates these moves (see section 4.1) however it does not for all such games
[38]. Future research could compare the eﬀects on the learned hypotheses
when trained on optimal data. It may also be more insightful to look at the
diﬀerence between human generated data and random or optimal.

Other machine learning systems There exists a huge variety of ma-
chine learning systems other than ILP. It is possible some of these may
exhibit more of a bias toward one of the training sets. Testing a neural
network or genetic algorithm based approach may provide insightful results.
Intelligent versus random in other contexts There are plenty of
other domains other than game playing in which datasets can be considered
intelligent or random. An example of this might be training a car to drive
where you could train it on diﬀerent quality levels of driving or training a
facial recognition system on poor or high quality photos. These would prove
an interesting area to study.

Levels of intelligence So far we have only considered a single level of
intelligent play. It is clear that in humans for any given game we do not
simply have good players and random players. There is a scale of skill. The
ELO system captures this concept well and is used as a generic measure
of skill for zero sum games but has been extended to more [16]. It would
be informative to pick a game and use game traces generated by player
of a range diﬀerent ELO score.
It would then be clear if the systems in
fact learned best from a particular level of play rather than just optimal
or random. We expect that the best level of play for training would diﬀer
among games as has been shown in this paper. However it would still be
useful to discover what level was optimal for any game.

How do humans compare We have spent a lot of this paper inves-
tigating whether machines learn better on intelligent gameplay or random
gameplay with the assumption that a human would learn better from watch-
ing another human play than random legal moves. Whilst in reality we
might generally choose to watch a human playing when trying to learn a
game rather than observing random legal moves it may not be the most
eﬀective way. Any real world example of this is almost always accompanied
by additional information. For example humans would often have access to
some description of the rules or someone explaining the edge cases that may
not occur. It is hard to compare how humans learn to how machines learn
since the background knowledge of a human is so much greater. It would

33

be interesting to see how humans would fare in similar experiments to those
in this paper. Looking into this question would give new insights into the
results of this paper. As with the similar question posed in this paper the
experiment does not have clear predicted outcome.

34

Bibliography

[1] Serge Abiteboul, Richard Hull, and Victor Vianu. Foundations of

Databases. Addison-Wesley, 1995.

[2] Elena Bellodi and Fabrizio Riguzzi. Structure learning of probabilis-
tic logic programs by searching the clause space. Theory Pract. Log.
Program., 15(2):169–212, 2015.

[3] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando
Pereira, and Jennifer Wortman Vaughan. A theory of learning from
diﬀerent domains. Machine Learning, 79(1-2):151–175, 2010.

[4] Svetla Boytcheva. Overview of inductive logic programming (ilp) sys-

tems. 01 2002.

[5] Ivan Bratko. Prolog Programming for Artiﬁcial Intelligence, 4th Edi-

tion. Addison-Wesley, 2012.

[6] Guillaume Chaslot, Sander Bakkes, Istvan Szita, and Pieter Spronck.
Monte-carlo tree search: A new framework for game AI. In Proceed-
ings of the Fourth Artiﬁcial Intelligence and Interactive Digital Enter-
tainment Conference, October 22-24, 2008, Stanford, California, USA,
2008.

[7] Domenico Corapi, Alessandra Russo, and Emil Lupu. Inductive logic
programming in answer set programming. In Inductive Logic Program-
ming - 21st International Conference, ILP 2011, Windsor Great Park,
UK, July 31 - August 3, 2011, Revised Selected Papers, pages 91–97,
2011.

[8] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clif-
ford Stein. Introduction to Algorithms, Third Edition. The MIT Press,
3rd edition, 2009.

[9] Andrew Cropper. Eﬃciently learning eﬃcient programs. PhD thesis,

Imperial College London, UK, 2017.

35

[10] Andrew Cropper, Sebastijan Dumancic, and Stephen H. Muggleton.
Turning 30: New ideas in inductive logic programming. CoRR,
abs/2002.11002, 2020.

[11] Andrew Cropper, Richard Evans, and Mark Law.
game playing. Machine Learning, Nov 2019.

Inductive general

[12] Andrew Cropper, Rolf Morel, and Stephen Muggleton. Learning higher-

order logic programs. Machine Learning, 2019.

[13] Andrew Cropper and Stephen H. Muggleton. Metagol system.

https://github.com/metagol/metagol, 2016.

[14] Andrew Cropper and Stephen H. Muggleton. Learning eﬃcient logic

programs. Machine Learning, 108(7):1063–1083, 2019.

[15] Andrew Cropper and Sophie Tourret. Logical reduction of metarules.

Machine Learning, Nov 2019.

[16] Arpad E. Elo. The Rating of Chessplayers, Past and Present. Arco

Pub., New York, 1978.

[17] Arun Sharma Eric McCreath. Extraction of meta-knowledge to restrict
In Proc. of the 8th Australian

the hypothesis space for ilp systems.
Joint Conf. on AI, 1995.

[18] Richard Evans and Edward Grefenstette. Learning explanatory rules

from noisy data. J. Artif. Intell. Res., 61:1–64, 2018.

[19] Bertram Felgenhauer and Frazer Jarvis. Mathematics of sudoku i.

Mathematical Spectrum, 39, 2006.

[20] Martin Gebser, Roland Kaminski, Benjamin Kaufmann, Patrick L¨uhne,
Philipp Obermeier, Max Ostrowski, Javier Romero, Torsten Schaub,
Sebastian Schellhorn, and Philipp Wanko. The potsdam answer set
solving collection 5.0. KI, 32(2-3):181–182, 2018.

[21] Michael Genesereth. Gdl website, 2010.

[22] Michael R. Genesereth, Nathaniel Love, and Barney Pell. General game
playing: Overview of the AAAI competition. AI Magazine, 26(2):62–72,
2005.

[23] Carlos R. Gonz´alez and Yaser S. Abu-Mostafa. Mismatched training
and test distributions can outperform matched ones. Neural Computa-
tion, 27(2):365–387, 2015.

[24] Feng-Hsiung Hsu.

Ibm’s deep blue chess grandmaster chips.

IEEE

Micro, 19(2):70–81, 1999.

36

[25] Jakub Kowalski and Marek Szykula. Experimental studies in general
game playing: An experience report. CoRR, abs/2003.03410, 2020.

[26] Mark Law. Ilasp manuel. Technical report, 2016.

[27] Mark Law.

Inductive learning of answer set programs. PhD thesis,

Imperial College London, UK, 2018.

[28] Mark Law, Alessandra Russo, and Krysia Broda. Inductive learning of
answer set programs. In Logics in Artiﬁcial Intelligence - 14th European
Conference, 2014, Funchal, Madeira, Portugal, September 24-26, 2014.
Proceedings, pages 311–325, 2014.

[29] Mark Law, Alessandra Russo, and Krysia Broda. Iterative learning of
answer set programs from context dependent examples. Theory Pract.
Log. Program., 16(5-6):834–848, 2016.

[30] Nathaniel Love, Timothy Hinrichs, David Haley, Eric Schkufza, and
Michael Genesereth. General game playing: Game description language
speciﬁcation, 2006.

[31] Tom M. Mitchell. Machine learning, International Edition. McGraw-

Hill Series in Computer Science. McGraw-Hill, 1997.

[32] Stephen Muggleton.

Inductive logic programming. New Generation

Comput., 8(4):295–318, 1991.

[33] Stephen Muggleton. Inverse entailment and progol. New Generation

Comput., 13(3&4):245–286, 1995.

[34] Andrej Oblak and Ivan Bratko. Learning from noisy data using a non-
covering ILP algorithm. In Paolo Frasconi and Francesca A. Lisi, ed-
itors, Inductive Logic Programming - 20th International Conference,
ILP 2010, Florence, Italy, June 27-30, 2010. Revised Papers, volume
6489 of Lecture Notes in Computer Science, pages 190–197. Springer,
2010.

[35] Luc De Raedt and Maurice Bruynooghe. Interactive concept-learning
and constructive induction by analogy. Mach. Learn., 8:107–150, 1992.

[36] Luc De Raedt, Anton Dries, Ingo Thon, Guy Van den Broeck, and
Inducing probabilistic relational rules from proba-
Mathias Verbeke.
bilistic examples. In Qiang Yang and Michael J. Wooldridge, editors,
Proceedings of the Twenty-Fourth International Joint Conference on
Artiﬁcial Intelligence, IJCAI 2015, Buenos Aires, Argentina, July 25-
31, 2015, pages 1835–1843. AAAI Press, 2015.

[37] Kenneth A. Ross. Modular stratiﬁcation and magic sets for datalog

programs with negation. J. ACM, 41(6):1216–1266, 1994.

37

[38] Jonathan Schaeﬀer, Neil Burch, Yngvi Bj¨ornsson, Akihiro Kishimoto,
Martin M¨uller, Robert Lake, Paul Lu, and Steve Sutphen. Checkers is
solved. Science, 317(5844):1518–1522, 2007.

[39] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Lau-
rent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis
Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Diele-
man, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever,
Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore
Graepel, and Demis Hassabis. Mastering the game of go with deep
neural networks and tree search. Nature, 529(7587):484–489, 2016.

[40] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Lau-
rent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis
Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander Diele-
man, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever,
Timothy P. Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore
Graepel, and Demis Hassabis. Mastering the game of go with deep
neural networks and tree search. Nature, 529(7587):484–489, 2016.

[41] Andrew

Rose

Steve

Draper.

Sancho

ggp

player.

https://github.com/SanchoGGP/ggp-base, 2015.

[42] Gerald Tesauro. Temporal diﬀerence learning and td-gammon. Com-

mun. ACM, 38(3):58–68, 1995.

[43] Jan-Jaap van Horssen. Complexity of checkers and draughts on diﬀerent

board sizes. ICGA Journal, 40(4):341–352, 2018.

[44] Maciej wiechowski, HyunSoo Park, Jacek Madziuk, and Kyung-Joong
Kim. Recent advances in general game playing. TheScientiﬁcWorld-
Journal, 2015:986262, 09 2015.

38

