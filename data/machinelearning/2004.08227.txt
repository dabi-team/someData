r
p
A
6
1

]

G
L
.
s
c
[

1
v
7
2
2
8
0
.
4
0
0
2
:
v
i
X
r
a

MPLP++: Fast, Parallel Dual Block-Coordinate Ascent
for Dense Graphical Models

0
2
0
2

Siddharth Tourani1, Alexander Shekhovtsov2, Carsten Rother1, Bogdan Savchynskyy1

1 Visual Learning Lab, Uni. Heidelberg,
2 Centre for Machine Perception, Czech Technical University in Prague

Abstract. Dense, discrete Graphical Models with pairwise potentials are a pow-
erful class of models which are employed in state-of-the-art computer vision
and bio-imaging applications. This work introduces a new MAP-solver, based
on the popular Dual Block-Coordinate Ascent principle. Surprisingly, by making
a small change to the low-performing solver, the Max Product Linear Program-
ming (MPLP) algorithm [1], we derive the new solver MPLP++ that signiﬁcantly
outperforms all existing solvers by a large margin, including the state-of-the-art
solver Tree-Reweighted Sequential (TRW-S) message-passing algorithm [2]. Ad-
ditionally, our solver is highly parallel, in contrast to TRW-S, which gives a fur-
ther boost in performance with the proposed GPU and multi-thread CPU imple-
mentations. We verify the superiority of our algorithm on dense problems from
publicly available benchmarks, as well, as a new benchmark for 6D Object Pose
estimation. We also provide an ablation study with respect to graph density.

Keywords: Graphical models, MAP-Inference, Block-Coordinate-Ascent, Dense
Graphs, Message Passing Algorithms

1 Introduction

Undirected discrete graphical models with dense neighbourhood structure are known
to be much more expressive than their sparse counterparts. A striking example is the
fully-connected Conditional Random Field (CRF) model with Gaussian pairwise po-
tentials [3], signiﬁcantly improving the image segmentation ﬁeld, once an efﬁcient
solver for the model was proposed. More recently, various applications in computer
vision and bio-imaging have successfully used fully-connected or densely-connected,
pairwise models with non-Gaussian potentials. Non-Gaussian potentials naturally arise
from application-speciﬁc modelling or the necessity of robustness potentials. A promi-
nent application of the non-Gaussian fully-connected CRF case achieved state-of-the-
art performance in 6D object pose estimation problem [4], with an efﬁcient application-
speciﬁc solver. Other examples of densely connected models were proposed in the area
of stereo-reconstruction [5], body pose estimation [6, 7, 8], bio-informatics [9] etc.

An efﬁcient solver is a key condition to make such expressive models efﬁcient in
practice. This work introduces such a solver, which outperforms all existing methods
for a class of dense and semi-dense problems with non-Gaussian potentials. This in-
cludes Tree-Reweighted Sequential (TRW-S) message passing, which is typically used
for general pairwise models. We would like to emphasize that efﬁcient solvers for this

 
 
 
 
 
 
2

Siddharth Tourani1, Alexander Shekhovtsov2, Carsten Rother1, Bogdan Savchynskyy1

class are highly desirable, even in the age of deep learning. The main reason is that
such expressive graphical models can encode information which is often hard to learn
from data, since very large training datasets are needed to learn the application speciﬁc
prior knowledge. In the the above mentioned 6D object pose estimation task, the pair-
wise potentials encode length-consistency between the observed data and the known 3D
model and the unary potentials are learned from data. Other forms of combining graph-
ical models with CNNs such as Deep-Structured-Models [10] can also beneﬁt from the
proposed solver.

Linear Programming (LP) relaxation is a powerful technique that can solve exactly
all known tractable maximum a posteriori (MAP) inference problems for undirected
graphical models (those known to be polynomially solvable) [11]. Although there are
multiple algorithms addressing the MAP inference, which we discuss in § 2, the lin-
ear programs obtained by relaxing the MAP-inference problem are not any simpler
than general linear programs [12]. This implies that algorithms solving it exactly are
bounded by the computational complexity of the general LP and do not scale well to
problems of large size. Since LP relaxations need not be tight, solving it optimally is
often impractical. On the other hand, block coordinate ascent (BCA) algorithms for the
LP dual problem form an approach delivering fast and practically useful approximate
solutions. TRW-S [2] is probably the most well-known and efﬁcient solver of this class,
as shown in [13]. Since due to the graph density the model size grows quadratically
with the number of variables, a scalable solver must inevitably be highly parallelizable
to be of practical use. Our work improves another well-known BCA algorithm of this
type, MPLP [1] (Max Product Linear Programming algorithm) and proposes a parallel
implementation as explained next.
Contribution We present a new state-of-the-art parallel BCA algorithm of the same
structure as MPLP, i.e. an elementary step of our algorithm updates all dual variables
related to a single graph edge. We explore the space of such edge-wise updates and
propose that a different update rule inspired by [14] can be employed, which signiﬁ-
cantly improves the practical performance of MPLP. Our method with the new update
is termed MPLP++. The difference in the updates stems from the fact that the optimiza-
tion in the selected block of variables is non-unique and the way the update utilizes the
degrees of freedom to which the block objective is not sensitive signiﬁcantly affects
subsequent updates and thus the whole performance.

We propose the following theoretical analysis. We show that MPLP++ converges to-
wards arc-consistency, similarly to the convergence result of [15] for min-sum diffusion.
We further show, that given any starting point, an iteration of the MPLP++ algorithm,
which processes all edges of the graph in a speciﬁed order always results in a better
objective than the same iteration of MPLP. For multiple iterations this is not theoreti-
cally guaranteed, but empirically observed in all our test cases. All proofs relating to
the main paper are given in the supplement.

Another important aspect that we address is parallelization. TRW-S is known as
a “sequential” algorithm. However it admits parallelization, especially for bipartite
graphs [2], which is exploited in specialized implementations [16, 17] for 4-connected
grid graphs. The parallelization there gives a speed-up factor of O(n), where n is the
number of nodes in the graph. A parallel implementation for dense graphs has not been
proposed. We observe that in MPLP a group of non-incident edges can be updated

MPLP++: Fast, Parallel Dual Block-Coordinate Ascent for Dense Graphical Models

3

in parallel. We pre-compute a schedule maximizing the number of edges that can be
processed in parallel using an exact or a greedy maximum matching. The obtainable
theoretical speed-up is at least n/2 for any graph, including dense ones. We consider
two parallel implementations, suitable for CPU and GPU architectures respectively. A
further speed-up is possible by utilizing parallel algorithms for lower envelopes in mes-
sage passing (see § 4 and [18]).

The new MPLP++ method consistently outperforms all its competitors, including
TRW-S [2], in the case of densely (not necessarily fully) connected graphs, even in the
sequential setting: In our experiments it is 2 to 10 times faster than TRW-S and 5 to 150
times faster than MPLP depending on the dataset and the required solution precision.
The empirical comparison is conducted on several datasets. As there are only few pub-
licly available ones, we have created a new dataset related to the 6D pose estimation
problem [4]. Our code and the new benchmark dataset will be made publicly available
together with the paper.

2 Related work

The general MAP inference problem for discrete graphical models (formally deﬁned
in § 3) is NP-hard and is also hard to approximate [19]. A natural linear programming
relaxation is obtained by formulating it as a 0-1 integer linear program (ILP) and re-
laxing the integrality constraints [20] (see also the recent review [21]). A hierarchy of
relaxations is known [22], from which the so-called Base LP relaxation, also considered
in our work, is the simplest one. It was shown [11, 23] that this relaxation is tight for all
tractable subclasses of the problem. For many other classes of problems it provides ap-
proximation guarantees, reviewed [19]. A large number of specialized algorithms were
developed for this relaxation.

Apart from general LP solvers, a number of specialized algorithms exist that take
advantage of the problem structure and guarantee convergence to an optimal solution
of the LP relaxation. This includes proximal [24, 25, 26, 27], dual sub-gradient [28,
29, 30], bundle [31], mirror-descent [32] smoothing-based [33, 34, 35] and (quasi-)
Newton [36] methods.

However, as it was shown in [37], the linear programs arising from the relaxation
of the MAP inference have the same computational complexity as general LPs. At the
same time, if the problem is not known to belong to a tractable class, solving the re-
laxation to optimality may be of low practical utility. A comparative study [13] notes
that TRW-S [2] is the most efﬁcient solver for the relaxation in practice. It belongs to
the class of BCA methods for the LP dual that includes also MPLP [1], min-sum diffu-
sion [38, 15] and DualMM [14] algorithms. BCA methods are not guaranteed to solve
the LP dual to optimality. They may get stuck in a suboptimal point and be unable to
compute primal LP solutions unless integer solutions are found (which is not always the
case). However, they scale extremely well, take advantage of fast dynamic programming
techniques, solve exactly all submodular problems [39] and provide good approximate
solutions in general vision benchmarks. Suboptimal solutions of the relaxation can also
be employed in exact solvers [40, 41] using cutting plane or branch-and-cut techniques
and in methods identifying a part of optimal solution [42, 43].

4

Siddharth Tourani1, Alexander Shekhovtsov2, Carsten Rother1, Bogdan Savchynskyy1

3 Preliminaries

Notation G = (V, E) denotes an undirected graph, with vertex set V (we assume V =
{1, . . . |V|}) and edge set E. The notation uv ∈ E will mean that {u, v} ∈ E and u < v
with respect to the order of V. Each node u ∈ V is associated with a label from a ﬁnite
set of labels Y (for brevity w.l.o.g. we will assume that it is the same set for all nodes).
The label space for a pair of nodes uv ∈ E is Y 2 and for all nodes it is Y V .

For each node and edge the unary θu : Y → R, u ∈ V and pairwise cost functions
θuv : Y 2 → R, uv ∈ E assign a cost to a label or label pair respectively. Let I =
(V×Y)∪(E ×Y 2) be the index set enumerating all labels and label pairs in neighbouring
graph nodes. Let the cost vector θ ∈ RI contain all values of the functions θu and θuv
as its coordinates.

The MAP-inference problem for the graphical model deﬁned by a triple (G, Y V , θ)

consists in ﬁnding the labelling with the smallest total cost, i.e.:

(cid:34)

y∗ = arg min

E(y|θ) :=

y∈Y V

(cid:88)

v∈V

θv(yv) +

(cid:88)

uv∈E

(cid:35)

θuv(yuv)

.

(1)

This problem is also known as energy minimization for graphical models and is
closely related to weighted and valued constraint satisfaction problems. The total cost
E is also often called energy.

The problem (1) is in general NP-hard and is also hard to approximate [19]. A
number of approaches to tackle it in different practical scenarios is reviewed in [13, 44].
One of the widely applicable techniques is based on (approximately) solving its linear
programming (LP) relaxation as discussed in § 2.
Dual Problem Most existing solvers for the LP relaxation tackle its dual form, which
we introduce now. This is because the LP dual has much fewer variables than the pri-
mal and can also be written in the form of unconstrained concave (piecewise-linear)
maximization.

It is based on the fact that the representation of the energy function E(y|θ) using
unary θu and pairwise θuv costs is not unique. There exist other costs ˆθ ∈ RI such that
E(y|ˆθ) = E(y|θ) for all labelings y ∈ Y V .

It is known (see e.g. [21]) and straightforward to check that such equivalent costs
can be obtained with an arbitrary vector φ := (φv→u(s) ∈ R | u ∈ V, v ∈ Nb(u), s ∈
Y), where Nb(u) is the set of neighbours of u in G, as follows:

ˆθu(s) ≡ θφ

u(s) := θu(s) +

(cid:88)

φv→u(s)

v∈Nb(u)

(2)

ˆθuv(s, t) ≡ θφ

uv(s, t) := θuv(s, t) − φv→u(s) − φu→v(t) .

The cost vector θφ is called reparametrized and the vector φ is known as reparame-
trization. Costs related by (2) are also called equivalent. Other established terms for
reparametrization are equivalence preserving [45] or equivalent transformations [20].
By swapping min and (cid:80) operations in (1) one obtains a lower bound on the energy

D(θφ) ≤ E(y|θ) for all y, which reads

D(θφ) :=

(cid:88)

u∈V

min
s∈Y

θφ
u(s) +

(cid:88)

uv∈E

min
(s,t)∈Y 2

θφ
uv(s, t) .

(3)

MPLP++: Fast, Parallel Dual Block-Coordinate Ascent for Dense Graphical Models

5

Although the energy E(y|θ) remains the same for all equivalent cost vectors (e.g.
E(y|θ) = E(y|θφ)), the lower bound D(θφ) depends on reparametrization, which
is D(θ) (cid:54)= D(θφ). Therefore, a natural maximization problem arises as maximiza-
tion of the lower bound over all equivalent costs: maxφ D(θφ). It is known (e.g. [21])
that this maximization problem can be seen as a dual formulation of the LP relaxation
of (1). We will write D(φ) to denote D(θφ), since the cost vector θ is clear from the
context.The function D(φ) is concave, piece-wise linear and therefore non-smooth. In
many applications the dimensionality of φ often exceeds 105 to 106 and the respective
dual problem maxφ D(φ) is large scale.

4 Dual Block-Coordinate Ascent

As we discussed in § 2, BCA methods, although not guaranteed to solve the dual to
the optimality, provide good solutions for many practical instances and scale very well
to large problems. The fastest such methods are represented by methods working with
chain subproblems [2], [14] or their generalizations [46].

The TRW-S algorithm can be seen as updating a block of dual variables (φv→u, v ∈
Nb(u)) “attached” to a node u, during each elementary step. The same block of vari-
ables is also used in the min-sum diffusion algorithm [15], as well as in the convex
message passing [47], and [46] gives a generalization of such methods.

However, the update coefﬁcients in TRW-S are related to the density of the graph
and its advantage diminishes when the graph becomes dense. We show that for dense
graphs updating a block of dual variables φu↔v = (φv→u, φu→v) associated to an
edge uv ∈ E can be more efﬁcient. Such updates were previously used in the MPLP
algorithm [1]. We show that our MPLP++ updates differ in detail but bring a signiﬁcant
improvement in performance.

Block Optimality Condition For further analysis of BCA algorithms we will require
a sufﬁcient condition of optimality w.r.t. the selected block of dual variables. The re-
striction of the dual to the block of variables φu↔v is given by the function:

Duv(φu↔v) := min
st∈Y 2

θφ
uv(s, t) + min
s∈Y

θφ
u(s) + min
t∈Y

θφ
v (t) ,

(4)

Maximizing Duv is equivalent to performing a BCA w.r.t. φu↔v for D(φ). The neces-
sary and sufﬁcient condition of maximum of Duv are given by the following.

Proposition 1. Reparametrization φu↔v maximizes Duv(·) iff there exist (s, t) ∈ Y 2
such that s minimizes θφ

v (·) and (s, t) minimizes θφ

u(·), t minimizes θφ

uv(·, ·).

This condition is trivial to check, it is a special case of arc consistency [21] or weak
tree-agreement [2] when considering a simple graph with one edge. It is also clear that
φu↔v satisfying this condition is not unique. For BCA algorithms it means that there
are degrees of freedom in the block, which do not directly affect the objective. By
moving on a plateau, they can nevertheless affect subsequent BCA updates. Therefore
the performance of the algorithm will be very much dependent on the particular BCA
update rule satisfying Proposition 1.

6

Siddharth Tourani1, Alexander Shekhovtsov2, Carsten Rother1, Bogdan Savchynskyy1

Fig. 1. Illustration of the considered BCA-updates. The gray boxes in the ﬁgure represent graph
nodes. The black dots in them the labels. The edges connecting the black dots make up the
pairwise costs. The numbers adjacent to the edges and labels are the pairwise and unary costs
respectively.

Block Coordinate Ascent Updates Given the form of the restricted dual (4) on the
edge uv, all BCA-updates can be described as follows. Assume θ is the current repara-
metrized cost vector, i.e. θ = ¯θ ¯φ for some initial ¯θ and the current reparametrization ¯φ.

Deﬁnition 1. A BCA update takes on the input an edge uv ∈ E and costs θuv(s, t),
θu(s) and θv(t) and outputs a reparametrization φu↔v satisfying Proposition 1. W.l.o.g.,
we assume that it will also satisfy mins,t θφ

uv(s, t) = 0.1

According to (2) a BCA-update results in the following reparametrized potentials:

u = θu + φv→u, θφ
θφ

v = θv + φu→v,

, θφ

uv(s, t) = θuv − φv→u − φu→v.

(5)

Note that since all reparametrizations constitute a vector space, after a BCA-update φ
we can update the current total reparametrization as ¯φ := ¯φ + φ.

We will consider BCA-updates of the following form: ﬁrst construct the aggregated
cost guv(s, t) = θuv(s, t) + θu(s) + θv(t). This corresponds to applying a reparametri-
zation ˚φu↔v = (−θu, −θv), which gives θ˚φ
v = 0. After that, a BCA
update forms a new reparametrization φu↔v such that θ˚φ+φ satisﬁes Proposition 1.

uv = guv, θ˚φ

u = θ˚φ

Such BCA-updates can be represented in the following form, which will be simpler

for deﬁning and analysing the algorithms.

Deﬁnition 2. Consider a BCA-update using a composite reparametrization ˚φu↔v +
φu↔v. It can be then fully described by the reparametrization mapping γ : guv →
(θγ

v ), where guv ∈ RYuv , θγ

u = φv→u and θγ

v = φu→v.

u, θγ

By construction, θγ

u matches the reparametrized unary term θ˚φ+φ

v is alike and the
reparametrized pairwise term is given by θγ
u − θγ
v .
In what follows, BCA-update will mean speciﬁcally the reparametrization mapping γ.
We deﬁne now several BCA-updates that will be studied further.

uv = guv − φv→u − φu→v = guv − θγ

, θγ

u

• The uniform BCA-update is given by the following reparametrization map-

ping U:

1 This ﬁxes the ambiguity w.r.t. a constant that can be otherwise added to the edge potential and
subtracted from one of the unary potentials. This constant does not affect the performance of
algorithms.

5479000010352222InitialUniform0.501.5323.52.52MPLP00012532MPLP++MPLP++: Fast, Parallel Dual Block-Coordinate Ascent for Dense Graphical Models

7

u (s) = θU
θU

v (t) := 1

2 mins(cid:48),t(cid:48)∈Y guv(s(cid:48), t(cid:48)), ∀s, t ∈ Y.

(U)

This is indeed just an example, to illustrate the problem of non-uniqueness of the mini-
mizer. It is easy to see that this update satisﬁes Proposition 1 since both θU
v (·)
are constant and therefore any pairwise minimizer of θU

uv is consistent with them.

u (·) and θU

• The MPLP BCA-update is given by the following reparametrization mapping M:

θM
u (s) := 1
θM
v (t) := 1

2 mint∈Y guv(s, t), ∀s ∈ Y,
2 mins∈Y guv(s, t), ∀t ∈ Y.

(M)

The MPLP algorithm [1] can now be described as performing iterations by applying
BCA-update M to all edges of the graph in a sequence.

• The new MPLP++ BCA-update, that we propose, is based on the handshake op-
eration [14]. It is given by the following procedure deﬁning the reparametrization map-
ping H:

θH
u (s) := θM
θH
v (t) := θH
θH
u (s) := θH

v (s) := θM
θH
u (s),
v (t) + mins∈Y [guv(s, t) − θH
u (s) + mint∈Y [guv(s, t) − θH

v (s), ∀s ∈ Y ,

v (t) − θH
v (t) − θH

u (s)], ∀t ∈ Y ,
u (s)], ∀s ∈ Y .

(H)

In other words, the MPLP++ update ﬁrst performs the MPLP update and then pushes

as much cost from the pairwise factor to the unary ones, as needed to fulﬁll mint θM
mins θM
uv (s, t) = 0 for all labels s and t in the nodes u and v respectively. It is also easy
v (s) := θM
to see that the assignment θH
v (s) together with the second line in H can be
v (t) := mins∈Y [guv(s, t) − θH
equivalently substituted by θH
u (s)], ∀t ∈ Y. This allows
to perform the MPLP++ update with 3 minimizations over Y 2 instead of 4. Fig. 1 shows
the result of applying the three BCA-updates on a simple two-node graph.

uv (s, t) =

It is straightforward to show that M and H also satisfy Deﬁnition 1 (see supple-
ment) and therefore are liable BCA-updates. In spite of that, the behavior of all three
updates is notably different, as it is shown by example in Fig. 2. Therefore, proving
only that some algorithm is a BCA, does not imply its efﬁciency.

Message Passing Importantly for performance, updates U, M and H can be computed
using a subroutine computing mint[θuv(s, t) + a(t)] for all s, where θ is a ﬁxed initial
pairwise potential and a is an arbitrary input unary function. This operation occurring in
dynamic programming and all of the discussed BCA algorithms, is known as message
passing. In many cases of practical interest it can be implemented in time O(|Y|) (e.g.
for Potts, absolute difference and quadratic costs) rather than O(|Y|2) in the general
case, using efﬁcient sequential algorithms, e.g., [48]. Using |Y| processors, the compu-
tation time can be further reduced to O(log |Y|) with appropriate parallel algorithms,
e.g., [18].

Primal Rounding BCA-algorithms iterating BCA-updates give only a lower bound to
the MAP-inference problem (1). To obtain a primal solution, we use a sequential round-
ing procedure similar to the one proposed in [2]. Assuming we have already computed

8

Siddharth Tourani1, Alexander Shekhovtsov2, Carsten Rother1, Bogdan Savchynskyy1

(a) Uniform

(b) MPLP

Fig. 2. Choosing the right BCA-update is important. Notation has the same meaning as in Fig. 1,
black circles denote locally optimal labels, white circles - the non-optimal ones. Solid lines corre-
spond to the locally optimal pairwise costs connected to the locally optimal labels. Omitted lines
in pairwise interactions denote inﬁnite pairwise costs. ei denotes edge indexes, edge processing
order is according to the subscript i. (a) Uniform update gets stuck and is unable to optimize the
dual further. (b) MPLP and MPLP++ attain the dual optimum in one iteration.

a primal integer solution x∗
following equation for the assignment

v for all v < u, we want to compute x∗

u. To do so, we use the

x∗
u ∈ arg minxu∈Y

(cid:104)

θu(xu) + (cid:80)

(cid:105)
v<u|uv∈E θuv(xu, x∗
v)

,

(6)

where θ is the reparametrized potential produced by the algorithm.

5 Theoretical Analysis

As we prove below, MPLP++ in the limit guarantees to fulﬁll a necessary optimality
condition, related to the arc-consistency [21] and the weak tree-agreement [2].
be the Iverson bracket, i.e.
= 1 if A holds. Other-
Arc-Consistency Let
θu(s) = mins(cid:48) θu(s(cid:48))
θuv(s, t) =
wise
(cid:75)
(cid:75)
mins(cid:48),t(cid:48) θuv(s(cid:48), t(cid:48))
be binary vectors with values 1 assigned to the locally minimal la-
(cid:75)
bels and label pairs. Let also logical and and or operations be denoted as ∧ and ∨. To
the binary vectors they apply coordinate-wise.

A
and ¯θuv(s, t) :=
(cid:74)

= 0. Let ¯θu(s) :=

A
(cid:74)

·
(cid:74)

(cid:74)

(cid:75)

(cid:74)

(cid:75)

Deﬁnition 3. A vector ¯θ ∈ {0, 1}I is called arc-consistent, if (cid:87)
for all {u, v} ∈ E and s ∈ Y.

t∈Y

¯θuv(s, t) = ¯θu(s)

However, arc-consistency itself is not necessary for dual optimality. The necessary
condition is existence of the node-edge agreement, which is a special case of the weak
tree agreement [2] when individual nodes and edges are considered as the trees in a
problem decomposition. This condition is also known as a non-empty kernel [21] / arc-
consistent closure [45] of the cost vector θ.

Deﬁnition 4. We will say that the costs θ ∈ RI fulﬁll the node-edge agreement, if there
is an arc-consistent vector ξ ∈ {0, 1}I such that ξ ∧ ¯θ = ξ.

MPLP++: Fast, Parallel Dual Block-Coordinate Ascent for Dense Graphical Models

9

Convergence of MPLP++ It is clear that all BCA algorithms are monotonous and con-
verge in the dual objective value as soon as the dual is bounded (i.e., primal is feasible).
However, such convergence is weaker than convergence in the reparametrization than
is desired. To this end we were able to show something in between the two: the con-
vergence of MPLP++ in a measure quantifying violation of the node-edge agreement, a
result analogous to [15].

Theorem 1. The MPLP++ algorithm converges to node-edge agreement.

Another important question is the comparison of different BCA methods. When
comparing different algorithms, an ultimate goal is to prove faster convergence of one
compared to the other one. We cannot show that the new MPLP++ algorithm has a
better theoretical convergence rate. First, such rates are generally unknown for BCA
algorithms for non-smooth functions. Second, considered algorithms are all of the same
family and it is likely that their asymptotic rates are the same. Instead, we study the
dominance, the condition that allows to rule out BCA updates which are always inferior
to others. Towards this end we show that given the same starting dual point, one iteration
of MPLP++ always results in a better objective value than that of MPLP and uniform
BCA. While this argument does not extend theoretically to multiple iterations of each
method, we show that it is still true in practice for all used datasets and a signiﬁcant
speed-up (up to two orders) is observed. The experimental comparison in § 7 gives
results in wall-clock time as well as in a machine-independent count of the message
passing updates performed.

5.1 Analysis of BCA-updates

Deﬁnition 5. A BCA-iteration α is deﬁned by the BCA-update γ applied to all edges
E in some chosen order. Let also D(α) represent the dual objective value with the
reparametrization deﬁned by the iteration α on the input costs θ.

We will analyze BCA-iterations of different BCA-updates w.r.t. the same sequence of
edges. The goal is to show that an iteration of the new update dominates the baselines
in the dual objective. This property is formally captured by the following deﬁnition.

Deﬁnition 6. We will say that a BCA-iteration α dominates a BCA-iteration β, if for
any input costs it holds D(α) ≥ D(β).

In order to show it, we introduce now and prove later the dominance relations of
individual BCA-updates. They are deﬁned not on the dual objective but on all unary
components.

Deﬁnition 7. Let γ and δ be two BCA-updates. We will say that update γ dominates
δ (denoted as γ ≥ δ) if for any guv ∈ RY 2
it holds that γ[guv] ≥ δ[guv], where
the inequality is understood as component-wise inequalities θγ
u[guv] and
θγ
v [guv] ≥ θδ

u[guv] ≥ θδ

v[guv].

We can show the following dominance results. Recall that U, H and M are the

uniform, MPLP and MPLP++ BCA-updates, respectively.

10

Siddharth Tourani1, Alexander Shekhovtsov2, Carsten Rother1, Bogdan Savchynskyy1

Proposition 2. The following BCA-dominances hold: H ≥ M ≥ U.

It is easy to see that the dominance Deﬁnition 7 is transitive and so also H ≥ U.
We will prove that such coordinate-wise dominance of BCA-updates implies also
the dominance in the dual objective whenever the following monotonicity property
holds:

Deﬁnition 8. A BCA-update γ is called monotonous if (θu ≥ θ(cid:48)
γ[θu + θuv + θv] ≥ γ[θ(cid:48)

v] for all θ, θ(cid:48).

u + θuv + θ(cid:48)

u, θv ≥ θ(cid:48)

v) implies

Proposition 3. Updates U and M are monotonous. The update H is not monotonous.

With these results we can formulate our main claim about domination in the objec-

tive value for the whole iteration.

Theorem 2. Let BCA-update γ dominate BCA-update µ and let µ be monotonous. Then
a BCA-iteration with γ dominates a BCA-iteration with µ.

From Proposition 2, Proposition 3 and Theorem 2 it follows now that BCA-iteration

of MPLP++ dominates that of MPLP, which in its turn dominates uniform.

6 Parallelization

To optimize D(θφ), we have to perform local operations on a graph, that per repara-
metrization inﬂuence only one edge uv ∈ E and it’s incident vertices u and v. The
remaining graph G(cid:48) = (V − {u, v}, E − {Iu ∪ Iv}) remains unchanged. This gives rise
to opportunities for parallelization. However, special care has to be taken to prevent
race conditions which occurs when two or more threads access shared data and they try
to change it at the same time.

Consider the case of Fig. 3, choosing edges 1 and 2 or 1 and 6 to process in parallel.
These edges have vertex A in common, which would lead to race conditions. Processing
edges 1 and 3 in parallel would lead to more parallelization as there are no conﬂicting
nodes. Thus, for maximal parallelization we have to come up with an ordering of edges
where threads working in parallel do not have common vertices.

Finding such edges without intersecting vertices is a well-studied problem in com-
binatorial optimization [49]. A matching M ⊂ E in graph G is a set of edges such that
no two edges in M share a common vertex. A matching is maximum if it includes the
largest number of edges, |M|. Every edge in a matching can be processed in parallel
without race conditions ensuing. There exist efﬁcient greedy algorithms to ﬁnd a max-
imum matching which we use. This gives rise to Algorithm 1 for covering all edges
of the graph while ensuring good parallelization. To cover the entire graph, we call a
matching algorithm repeatedly, until all edges are exhausted.

Initially, in line 1 the edge queue QE is empty. In line 3, a maximum match-
ing EM is found. This is added to QE in line 4. This continues until all edges have
been exhausted, i.e. the edges remaining ER is empty. The queue thus has a structure
M being the ith matching computed.
QE = (E 1

M), ordered left to right. E i

M, ..., E n

M, E 2

MPLP++: Fast, Parallel Dual Block-Coordinate Ascent for Dense Graphical Models

11

(a) Parallelization Illustration

(b) GPU vs CPU Performance

Fig. 3. Figure shows parallelization details and performance comparison for CPU and GPU. (a)
shows the details of how the edge schedule is computed for maximizing throughput. The ﬁrst
row shows how edge selection is carried out by ﬁnding matchings and adding the edges in these
matchings to the queue. The second row shows how the threads are launched for the CPU and
GPU. For the CPU, threads are launched dynamically at different time instances, and no syn-
chronization is carried out across all threads. This is due to a local memory locking mechanism
(mutex) for the CPU. For the GPU, many threads are launched simultaneously and synchronized
simultaneously via a memory barrier. This barrier is shown as the vertical line with Synch. (b)
shows the running time comparison between single-threaded, multi-threaded and GPU versions
of the MPLP++ algorithm. The GPU takes some time to load memory from the host (CPU) to
device (GPU). This is why it takes longer to get started than the CPU versions.

Algorithm 1 Compute Edge Schedule
Require: G = (V, E)
1: Initial: QE := ∅

(Empty edge queue),

ER := E

(Initial pool of edges)

2: while ER! = ∅ do
3:
4: QE .push(EM)
5:
6: end while

EM := Maximum Matching(V, ER)

ER := ER − EM (Remove matched edges from ER)

(Push maximum-matching EM to the queue)

The threads running in parallel can keep popping edges from QE and processing them
without much need for mutex locking.

We have different implementation algorithms for GPUs and multi-core CPUs.
CPU Implementation: Modern CPUs consist of multiple cores with each core
having one hardware threads. Hyper-threading allows for an additional thread per core,
but with lesser performance in the second thread compared to the ﬁrst.

Processing an edge is a short-lived task and launching a separate thread for each
edge would have excessive overhead. To process lightweight tasks we use the thread-
pool design pattern. A thread-pool keeps multiple threads waiting for tasks to be exe-
cuted concurrently by a supervising program. The threads are launched only once and
continuously process tasks from a task-queue. As they are launched only once, the la-
tency in execution due to overhead in thread creation and destruction is avoided.

In the case of our algorithm the task queue is QE . The thread picks up the index
of the edge to process and performs the MPLP++ operation (H). During the MPLP++

Queue Head13=+1324+132456ABCD123456ABCD123456ABCD123456ABCD123456ThreadTimeQueue Head PositionTimeSynch.Synch.Synch.Initially1st Matching2nd Matching3rd MatchingEM1={1,3}EQ=EQ+EM1ThreadsThreadsMulti-Core CPUGPUFinal Circular QueueEQ=ϕEM2={2,4}EQ=EQ+EM2EM2={5,6}EQ=EQ+EM313245613245613245613245613245613245613245613245613245613245613245613245613245612

Siddharth Tourani1, Alexander Shekhovtsov2, Carsten Rother1, Bogdan Savchynskyy1

operation the node and edge structures are locked by mutexes. One iteration of the
algorithm is complete when all edges have been processed. Since multiple iterations
maybe required to reach convergence, our task queue is circular, letting the threads
restart the reparameterization process from the ﬁrst element of QE . The ordering of QE
prevents heavy lock contention of mutexes.

GPU Implementation: Unlike CPUs, GPUs do not use mutexes for synchroniza-
tion. The threads in each GPU processor are synchronized via a hardware barrier syn-
chronization. A barrier for a group of threads stops the threads at this point and prevents
them from proceeding until all other threads/processes reach this barrier.

This is where the ordering of QE comes handy. Recall the structure of QE =
M). Barrier synchronization can be used between the matchings E i
M, ..., E n
M, E 2
(E 1
M
and E i+1
M , allowing for the completion of the MPLP++ BCA update operations for E i
M,
before beginning the processing of E i+1
M . This minimizes the time threads spend waiting
while the other threads complete, as they have no overlapping areas to write to in the
memory.

7 Experimental Evaluation

In our experiments, we use a 4-core Intel i7-4790K CPU @ 4.00GHz, with hyperthread-
ing, giving 8 logical cores. For GPU experiments, we used the NVIDIA Tesla K80 GPU
with 4992 cores.
Compared Algorithms We compare different algorithms in two regimes: the wall-
clock time and the machine-independent regime, where we count the number of oper-
ations performed. For the latter one we use the notion of the oracle call, which is an
operation like mint∈Y guv(s, t), ∀s ∈ Y involving single evaluation of all costs of a
pairwise factor. When speaking about oracle complexity we mean the number of oracle
calls per single iteration of the algorithm. As different algorithms have different ora-
cle complexities we deﬁne a normalized iteration as exactly |E| messages for an even
comparison across algorithms. We compare the following BCA schemes:

– The Tree-Reweighted Sequential (TRWS) message processing [2] algorithm has
consistently been the best performing method on several benchmarks like [13]. Its
oracle complexity is 2|E|. We use the multicore implementation introduced in [42]
for comparison, which is denoted as TRWS(MT) when run on multiple cores.

– The Max-Product Linear Programming MPLP [1] algorithm with BCA-updates (M).
It’s oracle complexity is thus 2|E|. For MPLP we have our own multi-threaded im-
plementation that is faster than the original one by a factor of 4

– The Min-Sum Diffusion Algorithm MSD [15], is one of the earliest (in the 70s) pro-
posed BCA algorithms for graphical models. The oracle complexity of the method
is 4|E|.

– The MPLP++ algorithm with BCA-updates (H) has the oracle complexity of 3|E|.
The algorithm is parallelized as described in § 6 for both CPU and GPU. The cor-
responding legends are MPLP++(MT) and MPLP++(GPU).

Dense datasets To show the strength of our method we consider the following datasets
with underlying densely connected graphs:

MPLP++: Fast, Parallel Dual Block-Coordinate Ascent for Dense Graphical Models

13

– worms dataset [50] consists of 30 problem instances coming from the ﬁeld of
bioimaging. The problems’ graphs are dense but not fully connected with about
0.1 · V 2 of edges, up to 600 nodes and up to 1500 labels.

– protein-folding dataset [51] taken from the OpenGM benchmark [13]. This
dataset has 21 problem instances with 33 − 1972 variables. The models are fully
connected and have 81 − 503 labels per node.

– pose is the dataset inspired by the recent work [4] showing state-of-the-art perfor-
mance on the 6D object pose estimation problem. In [4] this problem is formulated
as MAP-inference in a fully connected graphical model. The set of nodes of the
underlying graph coincides with the set of pixels of the input image (up to some
downscale factor), which requires specialized heuristics to obtain practically useful
solutions in reasonable time due to the large problem size. Contrary to the origi-
nal work, we assume that the position of the object is given by its mask (we used
ground-truth data from the validation set, but assume the mask could be provided by
some segmentation method) and treat only the pixels inside the mask as the nodes
of the corresponding graphical model. Otherwise, the unary and pairwise costs are
constructed in the same way as in [4]. We have got the learned unary costs from
the authors of [4] and merely tuned the hyper-parameters on the validation set. This
dataset has 32 problem instances with 600 − 4800 variables each and 13 labels per
node. The models are all fully connected.

Sparse datasets Although our method is not the best performing one on sparse graph-
ical models, we include the comparison on the following four-connected grid-graph
based benchmark datasets for fairness:

– color-seg from [13] has Potts pairwise costs. The nodes contain up to 12 labels.
– stereo from the Middlebury MRF benchmark [52]. This dataset consists of 3
models with truncated linear pairwise costs and 16, 20 and 60 labels respectively.

Algorithm Convergence Fig. 4 shows convergence of the considered algorithms in
a sequential setting for the protein and stereo datasets as representatives of the
dense and sparse problems. For other datasets the behavior is similar, therefore we
moved the corresponding plots to the supplement § A. The proposed MPLP++ method
outperforms all its competitors on all dense problem instances and is inferior to TRWS
only on sparse ones. This holds for both comparisons: the implementation-independent
normalized iteration and the implementation-dependent running time ones. Fig. 5
shows relative time speed-ups of the considered methods as a function of the attained
dual precision. The speed of MPLP, as typically the slowest one, is taken to be 1, i.e.,
for other algorithms the speed-up compared to MPLP is plotted. This includes also the
CPU-parallel versions of MPLP++, TRWS and the GPU-parallel MPLP++. Fig. 5 also
shows that for dense models MPLP++ is 2 − 10 faster than TRWS in the sequential set-
ting and 7 − 40 times in the parallel setting. The speed-up w.r.t. MPLP is 5 − 150 times
depending on the dataset and the required precision.
Performance Degradation with Graph Sparsiﬁcation In this test we gradually and
randomly removed edges from the graphs of the pose dataset and measures perfor-
mance of all algorithms. Fig. 6 shows that up to at least 10% of all possible edges
MPLP++ leads the board. Only when the number of edges drops to 5% and less, TRWS

14

Siddharth Tourani1, Alexander Shekhovtsov2, Carsten Rother1, Bogdan Savchynskyy1

starts to outperform MPLP++. Note, the density of edges in grid graphs considered
above does not exceed the 0.007% level.

(a) protein, iter

(b) protein, sec

(c) stereo, iter

(d) stereo, sec

Fig. 4.
Improvement in dual as a function of time and iterations for the protein-folding and
stereo dataset. The algorithms we compare have different message-passing schemes and end up
doing different amounts of work per iteration. Hence, for a fair comparison across algorithms we
deﬁne a normalized iteration as exactly |E| messages. This is also equal to number of messages
passed in a normal iteration divided by its oracle complexity. (a) and (b) are for the dense protein-
folding dataset, where both per unit time and per iteration, MPLP++ outperforms TRWS and all
other algorithms. In the sparse stereo dataset (c,d) TRWS beats all other algorithms. Results are
averaged over the entire dataset. The dual is normalized to 1 for equal weighing of every instance
in the dataset.

(a) pose

(b) protein

(c) worms

(d) stereo

Fig. 5. Relative speed w.r.t the MPLP algorithm in converging to within ε of the best attained
dual optima D∗(θφ), i.e. D∗(θφ) − ε. The plot shows the speedups of all the algorithms relative
to MPLP for ε’s 0.001, 0.01, 0.1, 1 and 10 of D∗(θφ). Figure (a) shows MPLP++ is 50× faster
than MPLP in converging to within 10% of D∗(θφ). Figure (b) and (c) likewise show an order of
magnitude speedup. Figure (d) shows that for the stereo dataset consisting of sparse graphs TRWS
dominates MPLP++. Convergence only till 0.1% of D∗(θφ) is shown for stereo as only TRWS
converges to the required precision.

8 Conclusions and Outlook

Block-coordinate ascent methods remain a perspective research direction for creating
efﬁcient parallelizable dual solvers for MAP-inference in graphical models. We have
presented one such solver beating the state-of-the-art on dense graphical models with

050100150200250Normalized Iteration0.850.90.9511.051.11.151.2Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS0510152025Time (s)0.850.90.9511.051.1Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS050100Normalized Iteration0.70.80.911.11.21.3Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS0200400600800Time (s)0.80.850.90.9511.051.11.15Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS10-310-210-11001010100200300400500SpeedUpMPLP++ (GPU)MPLP++ (MT)MPLP++ (ST)MSDMPLPTRWS10-310-210-110010101020304050SpeedUpMPLP++ (MT)MPLP++ (ST)MSDMPLPTRWS10-310-210-110010101020304050SpeedUpMPLP++ (MT)MPLP++ (ST)MSDMPLPTRWS10-1100101102030405060SpeedUpMPLP++ (GPU)MPLP++ (MT)MPLP++ (ST)MSDMPLPTRWS (ST)TRWS (MT)MPLP++: Fast, Parallel Dual Block-Coordinate Ascent for Dense Graphical Models

15

(a) 100%

(b) 40%

(c) 10%

(d) 5%

Fig. 6. Degradation with Sparsity (Dual vs Time): (a)-(d) show graphs with decreasing average
connectivity given as percentage of possible edges in ﬁgure subcaption. In (a)-(c) MPLP++ out-
performs TRWS. MPLP++ is resilient to graph sparsiﬁcation even when 90% of the edges have
been removed. Only when more than 95% of the edges have been removed as in (d) TRWS out-
performs MPLP++.

arbitrary potentials. The method is directly generalizable to higher order models, which
we plan to investigate in the future.

020406080Time (s)0.60.70.80.911.11.2Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS02468Time (s)0.70.80.911.11.2Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS00.511.5Time (s)0.80.911.1Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS00.050.10.150.2Time (s)0.70.80.911.1Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWSReferences

[1] Globerson, A., Jaakkola, T.S.: Fixing Max-Product: Convergent Message Passing Algo-
rithms for MAP LP-Relaxations. In: Advances in Neural Information Processing Systems
20. (2008)

[2] Kolmogorov, V.: Convergent tree-reweighted message passing for energy minimization.

IEEE transactions on pattern analysis and machine intelligence 28(10) (2006) 1568–1583

[3] Kr¨ahenb¨uhl, P., Koltun, V.: Efﬁcient inference in fully connected CRFs with gaussian edge

potentials. In: Advances in neural information processing systems. (2011) 109–117

[4] Michel, F., Kirillov, A., Brachmann, E., Krull, A., Gumhold, S., Savchynskyy, B., Rother,
C.: Global hypothesis generation for 6D object pose estimation. arXiv preprint (2017)
[5] Kolmogorov, V., Rother, C.: Comparison of energy minimization algorithms for highly
connected graphs. In: European Conference on Computer Vision, Springer (2006) 1–15
[6] Bergtholdt, M., Kappes, J., Schmidt, S., Schn¨orr, C.: A study of parts-based object class
detection using complete graphs. International journal of computer vision 87(1-2) (2010)
93

[7] Nowozin, S., Rother, C., Bagon, S., Sharp, T., Yao, B., Kohli, P.: Decision Tree Fields. In:
Computer Vision (ICCV), 2011 IEEE International Conference on, IEEE (2011) 1668–1675
[8] Kirillov, A., Schlesinger, D., Zheng, S., Savchynskyy, B., Torr, P.H., Rother, C.: Joint train-
ing of generic CNN-CRF models with stochastic optimization. In: Asian Conference on
Computer Vision, Springer (2016) 221–236

[9] Kainmueller, D., Jug, F., Rother, C., Myers, G.: Active graph matching for automatic joint
segmentation and annotation of C. elegans. In: International Conference on Medical Image
Computing and Computer-Assisted Intervention, Springer (2014) 81–88

[10] Chen, L.C., Schwing, A., Yuille, A., Urtasun, R.: Learning deep structured models. In:

International Conference on Machine Learning. (2015) 1785–1794

[11] Kolmogorov, V., Thapper, J., Zivny, S.: The power of linear programming for general-

valued CSPs. SIAM Journal on Computing 44(1) (2015) 1–36

[12] Pr˚uˇsa, D., Werner, T.: LP relaxation of the potts labeling problem is as hard as any linear

program. IEEE Trans. Pattern Anal. Mach. Intell. 39(7) (2017) 1469–1475

[13] Kappes, J.H., Andres, B., Hamprecht, F.A., Schn¨orr, C., Nowozin, S., Batra, D., Kim, S.,
Kausler, B.X., Kr¨oger, T., Lellmann, J., Komodakis, N., Savchynskyy, B., Rother, C.: A
comparative study of modern inference techniques for structured discrete energy minimiza-
tion problems. International Journal of Computer Vision (2015) 1–30

[14] Shekhovtsov, A., Reinbacher, C., Graber, G., Pock, T.: Solving dense image matching in

real-time using discrete-continuous optimization. In: CVWW. (2016) 13

[15] Schlesinger, M., Antoniuk, K.: Diffusion algorithms and structural recognition optimization

problems. Cybernetics and Systems Analysis 47(2) (2011) 175–192

[16] Choi, J., Rutenbar, R.A.: Hardware implementation of mrf map inference on an fpga plat-
form. In: Field Programmable Logic and Applications (FPL), 2012 22nd International Con-
ference on, IEEE (2012) 209–216

[17] Hurkat, S., Choi, J., Nurvitadhi, E., Mart´ınez, J.F., Rutenbar, R.A.: Fast hierarchical imple-
mentation of sequential tree-reweighted belief propagation for probabilistic inference. In:
Field Programmable Logic and Applications (FPL), 2015 25th International Conference on,
IEEE (2015) 1–8

[18] Chen, W., Wada, K.: On computing the upper envelope of segments in parallel. In: Pro-
ceedings. 1998 International Conference on Parallel Processing (Cat. No.98EX205). (Aug
1998) 253–260

[19] Li, M., Shekhovtsov, A., Huber, D.: Complexity of discrete energy minimization problems.

In: European Conference on Computer Vision. (2016) 834–852

MPLP++: Fast, Parallel Dual Block-Coordinate Ascent for Dense Graphical Models

17

[20] Schlesinger, M.I.: Syntactic analysis of two-dimensional visual signals in noisy conditions.

Kibernetika 4(113-130) (1976) 1

[21] Werner, T.: A linear programming approach to max-sum problem: A review. IEEE transac-

[22]

tions on pattern analysis and machine intelligence 29(7) (2007)
ˇZivn´y, S., Werner, T., Pr˚uˇsa, D.a. In: The Power of LP Relaxation for MAP Inference. The
MIT Press, Cambridge, USA (December 2014) 19–42

[23] Thapper, J., ˇZivn´y, S.: The power of linear programming for valued CSPs. In: Symposium

on Foundations of Computer Science (FOCS). (2012) 669–678

[24] Ravikumar, P., Agarwal, A., Wainwright, M.: Message-passing for Graph-structured Linear
Programs: Proximal Methods and Rounding Schemes. JMLR 11 (2010) 1043–1080
[25] Martins, A.F.T., Figueiredo, M.A.T., Aguiar, P.M.Q., Smith, N.A., Xing, E.P.: An Aug-

mented Lagrangian Approach to Constrained MAP Inference. In: ICML. (2011)

[26] Meshi, O., Globerson, A.: An Alternating Direction method for Dual MAP LP Relaxation.

In: ECML/PKDD (2). (2011) 470–483

[27] Schmidt, S., Savchynskyy, B., Kappes, J., Schn¨orr, C.: Evaluation of a ﬁrst-order primal-

dual algorithm for mrf energy minimization. In: EMMCVPR 2011. (2011)

[28] Storvik, G., Dahl, G.: Lagrangian-based methods for ﬁnding MAP solutions for MRF mod-

els. IEEE Transactions on Image Processing 9(3) (2000) 469–479

[29] Schlesinger, M., Giginyak, V.: Solution to structural recognition (max,+)-problems by their
equivalent transformations. in 2 Parts. Control Systems and Computers (1-2) (2007)
[30] Komodakis, N., Paragios, N., Tziritas, G.: MRF optimization via dual decomposition:
In: Computer Vision, 2007. ICCV 2007. IEEE 11th Interna-

Message-passing revisited.
tional Conference on, IEEE (2007) 1–8

[31] Kappes, J.H., Savchynskyy, B., Schn¨orr, C.: A bundle approach to efﬁcient MAP-inference
by lagrangian relaxation. In: Computer Vision and Pattern Recognition (CVPR), 2012 IEEE
Conference on, IEEE (2012) 1688–1695

[32] Luong, D.V., Parpas, P., Rueckert, D., Rustem, B.: Solving MRF minimization by mirror
descent. In: International Symposium on Visual Computing, Springer (2012) 587–598
[33] Savchynskyy, B., Kappes, J., Schmidt, S., Schn¨orr, C.: A study of Nesterov’s scheme for
Lagrangian decomposition and MAP labeling. In: Computer Vision and Pattern Recognition
(CVPR), 2011 IEEE Conference on, IEEE (2011) 1817–1823

[34] Savchynskyy, B., Schmidt, S., Kappes, J., Schn¨orr, C.: Efﬁcient MRF energy minimization

via adaptive diminishing smoothing. arXiv preprint arXiv:1210.4906 (2012)

[35] Meshi, O., Globerson, A., Jaakkola, T.S.: Convergence rate analysis of MAP coordinate
minimization algorithms. In: Advances in Neural Information Processing Systems. (2012)
3014–3022

[36] Kannan, H., Komodakis, N., Paragios, N.: Newton-type methods for inference in higher-
order markov random ﬁelds. In: IEEE International Conference on Computer Vision and
Pattern Recognition. (2017)

[37] Prusa, D., Werner, T.: Universality of the local marginal polytope. PAMI 37(4) (April 2015)
[38] Kovalevsky, V., Koval, V.: A diffusion algorithm for decreasing energy of max-sum labeling

problem. Glushkov Institute of Cybernetics, Kiev, USSR (1975) Unpublished.

[39] Schlesinger, M.I., Flach, B.: Some solvable subclasses of structural recognition problems.

In: Czech Pattern Recognition Workshop. Volume 2000. (2000) 55–62

[40] Cooper, M.C., De Givry, S., S´anchez, M., Schiex, T., Zytnicki, M., Werner, T.: Soft arc

consistency revisited. Artiﬁcial Intelligence 174(7-8) (2010) 449–478

[41] Savchynskyy, B., Kappes, J.H., Swoboda, P., Schn¨orr, C.: Global MAP-optimality by
In: Advances in Neural

shrinking the combinatorial search area with convex relaxation.
Information Processing Systems. (2013) 1950–1958

[42] Shekhovtsov, A., Swoboda, P., Savchynskyy, B.: Maximum persistency via iterative relaxed

inference with graphical models. PAMI (2017)

18

Siddharth Tourani1, Alexander Shekhovtsov2, Carsten Rother1, Bogdan Savchynskyy1

[43] Swoboda, P., Shekhovtsov, A., Kappes, J.H., Schnorr, C., Savchynskyy, B.: Partial opti-
mality by pruning for MAP-inference with general graphical models. PAMI 38(7) (2016)
1370–1382

[44] Hurley, B., OSullivan, B., Allouche, D., Katsirelos, G., Schiex, T., Zytnicki, M., De Givry,
S.: Multi-language evaluation of exact solvers in graphical model discrete optimization.
Constraints 21(3) (2016) 413–434

[45] Cooper, M., Schiex, T.: Arc consistency for soft constraints. Artiﬁcial Intelligence 154(1-2)

(2004) 199–227

[46] Kolmogorov, V.: A new look at reweighted message passing. IEEE transactions on pattern

analysis and machine intelligence 37(5) (2015) 919–930

[47] Hazan, T., Shashua, A.: Norm-Product Belief Propagation: Primal-Dual Message-Passing

for approximate inference (2008)

[48] Aggarwal, A., Klawe, M.M., Moran, S., Shor, P., Wilber, R.: Geometric applications of a

matrix-searching algorithm. Algorithmica 2(1) (Nov 1987) 195–208

[49] Schrijver, A.: Combinatorial Optimization: Polyhedra and Efﬁciency. Volume 24. Springer

Science & Business Media (2003)

[50] Kainmueller, D., Jug, F., Rother, C., Meyers, G.: Graph matching problems for annotating
c. elegans. http://dx.doi.org/10.15479/AT:ISTA:57 (2017) Accessed: 2017-
09-10.

[51] Yanover, C., Schueler-Furman, O., Weiss, Y.: Minimizing and learning energy functions for

side-chain prediction. Journal of Computational Biology 15(7) (2008) 899–911

[52] Szeliski, R., Zabih, R., Scharstein, D., Veksler, O., Kolmogorov, V., Agarwala, A., Tappen,
M., Rother, C.: A comparative study of energy minimization methods for markov random
ﬁelds with smoothness-based priors. IEEE transactions on pattern analysis and machine
intelligence 30(6) (2008) 1068–1080

[53] Boyd, S., Vandenberghe, L.: Convex optimization. Cambridge university press (2004)
[54] Bartle, R., Sherbert, D.: Introduction to real analysis. John Wiley & Sons Canada, Limited

(2000)

MPLP++: Fast, Parallel Dual Block-Coordinate Ascent
for Dense Graphical Models
(ECCV’18 Appendix)

A Additional Experimental Results

(a) Worms

(b) Pose

(c) Color-Seg

Fig. A.1. Dual vs Normalized Iterations on three other datasets. The experimental setup and no-
tation is the same as in Fig. 4. Problems in cases (a) and (b) have dense graphs where MPLP++
outperforms all other algorithms by a substantial margin. In the case (c) graphs are sparse and
TRWS is dominant.

(a) Worms

(b) Pose

(c) Color-Seg

Fig. A.2. Dual vs Time (Single Threaded): Fig. shows dual as a function of time for the single
threaded versions of all the algorithms. Following the pattern in Fig. A.1 for dense graphs (a)
and (b) MPLP++ dominates all other algorithms by a considerable margin. For sparse graphs,
(c) TRWS is the fastest. Both the dual D(φ) and time have been averaged over the entire dataset
and normalized to 1.The curves have been normalized such that each instance of the dataset is
weighed equally.

050100150Normalized Iteration0.90.9511.051.1Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS050100150Normalized Iteration0.850.90.9511.051.1Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS01020304050Normalized Iteration0.80.850.90.9511.051.11.15Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS051015Time (s)0.90.9511.05Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS05001000Time (s)0.90.9511.05Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS00.511.522.5Time (s)0.80.911.1Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS(a) 100%

(b) 80%

(c) 60%

(d) 40%

(e) 20%

(f) 10%

(g) 5%

(h) 1%

Fig. A.3. Degradation With Sparsity (Dual vs Iterations): (a)-(h) show graphs with decreasing av-
erage connectivity given as percentage of possible edges in ﬁgure subcaption. In (a)-(f) MPLP++
outperforms TRWS. Handshake is resilient to graph sparsiﬁcation even when 90% of the edges
have been removed. For (g) and(h) TRWS outperforms handshake.

B Formal Proofs

B.1 Additional Notation

To reduce clutter in the proofs we deﬁne relevant short-forms here. The set [n] is the
set of the ﬁrst n natural numbers, i.e. [n] = {1, . . . , n}. [m][n] denotes the Carte-
sian product [m] × [n]. We denote the same reparametrized unaries of two different
u . θ ∈ RI is the vector stacked up of unary θu and
algorithms A and B as θA
u(θ) = {s | θu(s) ≤ mins(cid:48) θu(s(cid:48)) + ε} be the set of
pairwise θuv potentials. Let Oε
labellings within ε > 0 of the optimal θu. Oε
uv(θ) is similarly deﬁned for θuv. Let also
Oε(θ) = {Oε

u(θ) | ∀u ∈ V} ∪ {Oε

uv(θ) | ∀uv ∈ E}.

u and θB

Deﬁnition 1. Tolerance factor ε is the minimum value for which Oε contains a consis-
tent labelling, i.e. one with node-edge agreement.

O0
bellings. O0
{O0

ε is a function of θ and will be written sometimes as ε(θ). Oε(θ) can also be thought
of as the subset of unary and pairwise labels that are within ε of the optimal labelling.
u(θ) = {s | θu(s) = mins(cid:48)∈Y θu(s(cid:48))}, then represents the set of optimal la-
uv(θ) can be similarly deﬁned for pairwise potentials θuv. Then, O0(θ) =

uv(θ) | uv ∈ E}.

u(θ) | u ∈ V} ∪ {O0
The MPLP++ operator H can act both on guv like pairwise costs and on the entire set
of costs θ ∈ RI. In the former case, it is exactly as deﬁned in Eqn. (H). The latter case
corresponds to an iteration of H. The i-times composition operation of H on θ denotes
= Hi(θ). Which is the case would be clear
i iterations of H on θ, i.e. H(H(...(H(θ))))
(cid:125)

(cid:124)

(cid:123)(cid:122)
i times

20

050100Normalized Iteration0.60.811.2Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS050100Normalized Iteration0.60.811.21.4Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS020406080Normalized Iteration0.60.70.80.911.1Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS010203040Normalized Iteration0.70.80.911.11.2Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS020406080Normalized Iteration0.60.811.21.4Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS0204060Normalized Iteration0.70.80.911.11.2Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS01020304050Normalized Iteration00.20.40.60.811.2Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWS0246810Normalized Iteration00.20.40.60.81Normalized DualMPLP++ PrimalMPLP++MSDMPLPTRWSfrom the argument the H operator takes. Also, to denote the resulting cost vector after
i iterations of H on θ, we use notation θi = Hi(θ) = H(θi−1).

unary reparameterizations θφ and θκ, deﬁned by Θun(φ, κ) = maxu∈V,s∈Y |θφ
v (s)|. Likewise for pairwise costs, Θpw(φ, κ) = maxuv∈E,st∈Y 2 |θφ
θκ

Let Θun(φ, κ) be a function that measures the max absolute difference between
u(s) −
uv(s, t)|.
We also assume that both unary and pairwise costs have been normalized, i.e.
mins∈Y θu(s) = 0 and minst∈Y 2 θuv(s, t) = 0. A consequence of normalization is
θu ≥ 0 and θuv ≥ 0. The subtracted cost does not affect the labelling and is added to a
constant term.

uv(s, t)−θκ

Proposition 2. The following BCA-dominances hold: H ≥ M ≥ U.

Proof. H ≥ M: Let guv = θu + θv + θuv. Consider the ﬁrst line of (H).

θH
u (s) := θM

u (s),

v (s) := θM
θH

v (s), ∀s ∈ Y

(1)

The ﬁrst line assigns to the reparameterized MPLP++ unaries, (θH

u , θH

v ) the unaries

resulting from applying the MPLP update to guv. At this point we have H = M.
Now, consider the subsequent equations of H

θH
v (t) := θH
θH
u (s) := θH

v (t) + mins∈Y [guv(s, t) − θH
u (s) + mint∈Y [guv(s, t) − θH

v (t) − θH
v (t) − θH
v (t), a non-negative quantity mins∈Y [guv(s, t) − θH

u (s)], ∀t ∈ Y
u (s)], ∀s ∈ Y .
v (t) − θH

To θH

Similarly, for θH

u . Thus, H ≥ M.
u (s) = mint(cid:48)guv(s, t(cid:48)) ≥ mins(cid:48),t(cid:48)guv(s(cid:48), t(cid:48)) = θU
v (t). Thus, M ≥ U.

v (t) = mins(cid:48)guv(s(cid:48), t) ≥ mins(cid:48),t(cid:48)guv(s(cid:48), t(cid:48)) = θU
θM

M ≥ U :θM

u (s)] is added.

u (s). Likewise,
(cid:3)

Proposition 3. Updates U and M are monotonous. The update H is not monotonous.

Proof. Let θu ≥ θ(cid:48)
θuv. Thus we have guv ≥ g(cid:48)
U is monotonous:

u and θv ≥ θ(cid:48)

uv.

v. We deﬁne guv = θu + θv + θuv and g(cid:48)

uv = θ(cid:48)

u + θ(cid:48)

v +

Performing update γU [guv] → (θU

u , θU

v ) (Eqn. (U)) yields

u (s) = θU
θU

v (t) := min
s(cid:48),t(cid:48)

Likewise γU [g(cid:48)

uv] → (θ(cid:48)U

u , θ(cid:48)U

v ) yields

u (s) = θ(cid:48)U
θ(cid:48)U

v (t) := min
s(cid:48),t(cid:48)

1
2

1
2

guv(s(cid:48), t(cid:48))

uv(s(cid:48), t(cid:48))
g(cid:48)

(2)

(3)

As guv ≥ g(cid:48)
u , θU

u ≥ θ(cid:48)U
θU

v ≥ θ(cid:48)U

v . Hence proved.

uv, we have mins(cid:48)t(cid:48) guv(s(cid:48), t(cid:48)) ≥ mins(cid:48)t(cid:48) g(cid:48)

uv(s(cid:48), t(cid:48)). This implies,

M is monotonous:

Performing update γM[guv] → (θM

u , θM

v ) (Eqn. M) yields

21

θM
u (s) :=

1
2

min
t(cid:48)

guv(s, t(cid:48)), θM

v (t) :=

1
2

min
s(cid:48)

guv(s(cid:48), t)

For γM[g(cid:48)

uv] → (θ(cid:48)M

u , θ(cid:48)M

v

) yields

θ(cid:48)M
u (s) :=

1
2

min
t(cid:48)

uv(s, t(cid:48)), θ(cid:48)M
g(cid:48)

v

(t) :=

1
2

min
s(cid:48)

uv(s(cid:48), t)
g(cid:48)

As guv ≥ g(cid:48)
uv(s(cid:48), t). This implies θM
g(cid:48)

uv, we have mint(cid:48) guv(s, t(cid:48)) ≥ mint(cid:48) g(cid:48)
u ≥ θ(cid:48)M

v ≥ θ(cid:48)M

u , θM

v

. Hence proved.

uv(s, t(cid:48)) & min
s(cid:48)

guv(s(cid:48), t) ≥

min
s(cid:48)

H is not monotonous:

To prove that H is not monotonous we show a counter-example to the monotonous
condition stated in theorem 3.

Consider the following unary and pairwise costs,

θu =

(cid:19)

(cid:18)4
0

θv =

(cid:19)

(cid:18)2
0

θ(cid:48)
u =

(cid:19)

(cid:18)0
0

θ(cid:48)
v =

(cid:19)

(cid:18)0
0

θuv =

(cid:19)

(cid:18)0 1
7 5

These costs satisfy θu ≥ θ(cid:48)
Now, applying the H operation to guv and g(cid:48)

u and θv ≥ θ(cid:48)
v.

uv, we get the following potentials

θH
u =

(cid:19)

(cid:18)2.5
2.5

θH
v =

(cid:19)

(cid:18)3.5
2.5

θ(cid:48)H
u =

(cid:19)

(cid:18)0
4

θ(cid:48)H
v =

(cid:19)

(cid:18)0
1

We thus have θH
u

(cid:3) θ(cid:48)H

u , providing the necessary counter-example.

(cid:3)

Theorem 2. Let BCA-update γ dominate BCA-update µ and let µ be monotonous. Then
a BCA-iteration with γ dominates a BCA-iteration with µ.

Proof. From the deﬁnition of dominance we have, if γ dominates µ (γ ≥ µ) then
γ[guv] ≥ µ[guv], ∀guv.

µ[g2

By the deﬁnition of monotonous, if µ is monotonous, g1
uv].
Now during the 1st iteration we have three cases,

uv ≥ g2

uv =⇒ µ[g1

uv] ≥

– Case A: Nodes u and v of edge uv have not been reparametrized before.
– Case B: Nodes u and v of edge uv have been reparametrized before.
– Case C: Only node u or v of edge uv have been reparametrized before.

If case A we have by dominance γ[guv] ≥ µ[guv].
If case B, the unaries u and v have been reparametrized. Let the reparametrized
u, θµ
u and

v ). From γ ≥ µ we know θγ

v ) and for µ be (θµ

u ≥ θµ

unaries for γ be (θγ

u, θγ

22

v . Then, gγ

θγ
v ≥ θµ
Consider the chain of inequalities

uv = θγ

u + θγ

v + θuv and gµ

uv = θµ

u + θµ

v + θuv. It follows gγ

uv ≥ gµ

uv.

γ[gγ

uv] ≥ µ[gγ

uv] ≥ µ[gµ

uv]

(4)

The ﬁrst is true by γ ≥ µ dominance. The second is true by µ-monotonicity. Thus
for case B also, γ results in reparametrized unaries that are co-ordinate wise greater
than µ.

Case C, can be proven in much the same way as case B.

Theorem 1. The MPLP++ algorithm converges to node-edge agreement.

To prove node-edge agreement we have to show that

ε(Hi(θ)) = 0

lim
i→∞

(cid:3)

(5)

where ε is the tolerance factor deﬁned in § B.1.
By saying that MPLP++ converges to node-edge agreement, we mean that as the
algorithm progresses ε tends to 0 and the set of labels belonging to Oε is sequentially
pruned only to leave an optimal labelling satisfying the node-edge agreement condition,
converting Oε to O0.

The proof is dependent on several lemmas which are sequentially proved.

Lemma B.1. H is a continuous function.

Proof. Stated differently, we show the proposed reparameterizations in this paper are
continuous. To generalize the result across all proposed reparameterizations we recall
the idea of an oracle call from the main paper, i.e. operations of the type mint∈Y guv(s, t),
∀s ∈ Y. We show the continuity of the ﬁrst equation of the MPLP++ operation only (1st
equation of Eqn. (H) in the main paper). The other equations of MPLP++ and other al-
gorithms can be proved similarly.

We prove continuity for only one label of one unary cost (label s and unary u ). Let
δ > 0. Similar proofs hold for all other unaries and pairwise costs. Consider two edges
denoted by the triplet (θu, θv, θuv) and (θ(cid:48)

u, θv, θuv), such that

Then, guv := θu + θv + θuv and g(cid:48)

uv := θ(cid:48)

u + θv + θuv. By (6), we have

|θ(cid:48)

u(s) − θu(s)| < δ

∀t |g(cid:48)

uv(s, t) − guv(s, t)| < δ

Applying the ﬁrst operation of MPLP++ to guv and g(cid:48)

uv, we get

θH
u (s) :=

θ(cid:48)H
u (s) :=

1
2
1
2

min
t∈Y

min
t∈Y

guv(s, t)

g(cid:48)
uv(s, t)

23

(6)

(7)

(8)

(9)

Let | θ(cid:48)H

u (s) − θH

u (s) |< ν, then by choosing δ := ν/2, we have

∀θ ∈ RI, ∀ν > 0, ∃δ > 0, ∀θ(cid:48) ∈ RI, | θ(cid:48)

u(s)−θu(s) |< δ =⇒ | θ(cid:48)H

u (s)−θH

u (s) |< ν
(10)
(cid:3)

Lemma B.2. The dual function D is a continuous function in it’s input θ.

Proof. Consider two different reparameterizations θφ, θκ ∈ RI. To prove D(.) is a
continuous function, it sufﬁces to show ∀ν > 0, ∃δ > 0, such that |θφ − θκ| <
δ =⇒ |D(θφ) − D(θκ)| < ν. To do so, we need to recall from § B.1, Θpw(φ, κ) =
maxuv∈E,st∈Y 2 |θφ
u(s)|.
We then have

uv(s, t)| and Θun(φ, κ) = maxu∈V,s∈Y |θφ

uv(s, t) − θκ

u(s) − θκ

θφ
uv(s, t)) − (

(cid:88)

u∈V

θκ
u(s) +

min
s

(cid:88)

uv∈E

θκ
uv(s, t)) |

min
s,t

| D(θφ) − D(θκ) |=| (

(cid:88)

θφ
u(s) +

min
s

(cid:88)

=|

u∈V
θφ
u(s) − min

s

(min
s

θκ
u(s)) +

(min
s,t

θφ
uv(s, t)) − min
s,t

θκ
uv(s, t)) |

u∈V
(cid:88)

u∈V

(min
s

≤|

(cid:88)

u∈V

≤|

θφ
u(s) − min

θκ
u(s)) | + |

s

(min
s,t

θφ
uv(s, t)) − min
s,t

θκ
uv(s, t)) |

Θun(φ, κ) | + |

(cid:88)

uv∈E

Θpw(φ, κ) | ≤| V || Θun(φ, κ) | + | E || Θpw(φ, κ) |

(cid:88)

uv∈E

min
(s,t)

(cid:88)

uv∈E
(cid:88)

uv∈E

Thus for |D(θφ)−D(θκ)| < ν, we need to choose a δ as a function of |Θpw| < 1
|Θun| < 1
maxima are also continuous [53].

|E| and
|V| . This can be done as H is continuous (by proposition B.1) and pointwise-
(cid:3)

Lemma B.3. Tolerance factor ε is continuous on θ.

Proof. Let θφ and θκ be two reparameterizations. We prove ε is continuous for unaries
only. The proof for pairwise terms is similar. Let Θun(φ, κ) = δ. From the deﬁnition of
Θun we have maxu∈V,s∈Y |θφ

u(s)| = δ. Thus,

u(s) − θκ

|θφ

u(s) − θκ

u(s)| ≤ δ

(11)

(11) can be rewritten as

Now consider the set Oε

Oε

u(θ)

u(s) − δ ≤ θφ
θκ
u(θ) = {s|θu(s) ≤ mins(cid:48)(θu(s(cid:48)) + ε(θ))}. For a unary in

u(s) ≤ θκ

u(s) + δ

(12)

θφ
u(s) ≤ min
s(cid:48)

(θφ

u(s(cid:48)) + ε(θ))

(13)

Substituting (12) in (13) we get,

24

θκ
u(s) − δ ≤ min
s(cid:48)

(θκ

u(s(cid:48)) + δ + ε(θ)) =⇒ θκ

u(s) ≤ min
s(cid:48)

(θκ

u(s(cid:48)) + ε(θ) + 2δ)

Thus if s satisﬁes (13), it also satisﬁes (14), which has ε(cid:48) ≥ ε + 2δ.

(14)

(cid:3)

Lemma B.4. D(Hi+1(θ)) ≥ D(Hi(θ)), ∀i, i.e. the MPLP++ reparametrization never
decreases the dual D.

Proof. Let’s consider D to be ﬁxed for all variables except the block Duv. Also, we
assume that the costs have been normalized, i.e. mins∈Y θu(s) = 0, mint∈Y θv(t) = 0
and minst∈Y 2 θuv(s, t) = 0. So, we have Duv(θ) = 0. We thus have to show that
Duv(H(θ)) ≥ Duv(θ) = 0.

The aggregated potential guv = θu + θv + θuv can be written in the form of a Y × Y

matrix






r1 + ∆1,1
...

. . .
. . .
r|Y| + ∆|Y|,1 . . . r|Y| + ∆|Y|,|Y|

r1 + ∆1,|Y|
...






where rs is the row minimum and ∆s,t ≥ 0. So, ∆s,t is 0 for all elements of the
row that are equal to rs. As each element of row s of guv contains θu(s), we know that
rs ≥ θu(s), ∀s ∈ Y.

Now, consider the 1st equation of the MPLP + + operation (H)

θH
u (s) := 1
θH
v (t) := 1

2 mint∈Y [guv(s, t)] = 1
2 mins∈Y [guv(s, t)] = 1

2 mint∈Y rs + ∆s,t = rs
2 mins∈Y rs + ∆s,t, ∀t ∈ Y

2 , ∀s ∈ Y ,

It is rs ≥ θu(s) ≥ 0 and therefore, θH
mins∈Y [rs + ∆s,t], where rs ≥ 0 and ∆s,t ≥ 0 ∀t, thus θH

u (s) = rs/2 ≥ 0. Likewise, θH

v (t) =
v (t) ≥ 0. Hence,
(cid:3)

1
2
D(H(θ)) ≥ 0.

Lemma B.5. H converges to a ﬁxed point.

Proof. Initially, the dual bound D(θφ) is computed using (3) which takes the min over
all unaries θu(s) and pairwise terms θuv(s, t) individually. Thus, D(θφ) is bounded
unless for any θu(s) or θuv(s, t) all the elements are ∞. If the dual is unbounded, then
by strong duality [53] the primal is also unbounded, and the only energy attainable is ∞.
On the other hand, if D(θφ) is bounded, by the LP duality theorem, the primal energy
E(y|θ) serves as an upper bound for the D(θφ).

We have proven that the BCA-update H brings about a monotonic improvement
of D(θφ). As each algorithm involves performing these updates over a sequence of
edges ei covering the entire graph, we end up at the end of every iteration with a
non-decreasing dual D(θφ). Thus, each algorithm generates an increasing sequence
of D(θφ) which is bounded from above and by the Monotone Convergence Theorem
(cid:3)
for real numbers R [54] (Section 3.3) the algorithm converges.

25

Lemma B.6. H is bounded w.r.t. D: For any θ there exists an M ∈ R such that
D(Hi(θ)) < M for any i.

Proof. As D is a dual LP, we know from LP-Duality Theorem [53] that D(Hi(θ)) ≤
E(y|θ), where E is as in (1) and y is the labelling. Also, an alternative upper bound can
be constructed as follows. Let Mu = maxs θH
uv(s, t). We then
have |θH
u (s) ≤
Mu and minst∈Y 2 θH
Thus we have

uv(s, t)| ≤ Muv ∀s, t. This also implies mins∈Y θH

u (s), Muv = maxst θH

u (s)| ≤ Mu, ∀s and |θH

uv(s, t) ≤ Muv.

D(H(θ)) =

(cid:88)

u∈V

min
s

θH
u (s) +

(cid:88)

uv∈E

min
s,t

θH
uv(s, t) ≤

(cid:88)

u∈V

Mu +

(cid:88)

uv∈E

Muv

Therefore D(H(θ)) is always bounded by MH(θ) = (cid:80)

uv∈E Muv. We
know from Lemma B.5 that H converges to a ﬁxed point, thus after a certain number of
iterations no changes occur in Hi(θ). To compute the required bound, one must simply
(cid:3)
take maxi MHi(θ) over all reparameterizations that have occurred.

u∈V Mu + (cid:80)

Lemma B.7. For any θ there exists C > 0 such that ||Hi(θ)|| ≤ C||θ|| for any i.

Proof. We start off by showing that the lemma is true for one label of a unary. Let
θφ(s) be the reparameterization of θu(s). Consider the edge triplet (θu, θv, θuv). Let
mu := mins∈Y θu(s), Mu := maxs∈Y θu(s), Mv := maxt∈Y θv(t) and Muv :=
maxst∈Y 2 θuv(s, t).

θH
u (s) :=

θu(s) +

{θv(t) + θuv(s, t)}

θH
u (s) :=

min
t

guv(s, t)

1
2
1
2
1
2
(cid:18)

min
t

1
2
1
2
Mv + Muv
θu(s)

θu(s) +

1 +

θH
u (s) ≤

θH
u (s) ≤

(Mv + Muv)
(cid:19)

(cid:18)

θu(s) ≤

1 +

(cid:19)

Mv + Muv
mu

θu(s)

u = (1+ Mu+Muv

) and get a bound ||θH

Thus, we can choose C i

u||θu||. As we
have to ﬁnd an upper bound ∀θu and ∀θuv, we repeat the process and then take the max
over all C i
uv obtaining C i. This gives us a C i that satisﬁes ||θi+1 = H(θi)|| ≤
C i||θi||. By Lemma B.5 after a certain number of iterations n, θn converges, so to get
the required result we simply simply take C = (cid:81)n
i=1 C i, giving ||Hi(θ)|| ≤ C||θ|| for
(cid:3)
any i.

u || ≤ C i

u and C i

mu

Lemma B.8. D(H(θ)) = D(θ) implies O0(H(θ)) ⊆ O0(θ), i.e. at a ﬁxed point no
new optimal labellings are achieved. If additionally ε(θH) > 0, then O0(H(θ)) ⊂
O0(θ), i.e. the inequality is strict.

Proof. Let θH = H(θ), i.e. θH is the reparameterized cost vector obtained after repa-
rameterizing the original cost vector θ. We prove the ﬁrst assertion for unary cost u

26

only, the remaining costs can be proved similarly. To do so, we assume that only after
reparameterization only the optimal label of unary cost u has changed.

Let this label be s∗ = arg mins θH

u (s). This implies s∗ ∈ O(θH). Now, to prove

the ﬁrst assertion, we have to show the inclusion s∗ ∈ O0

u(θH) =⇒ s∗ ∈ O0

u(θ).

Let us additionally deﬁne a function η that takes as input a node index p and outputs
its optimal labelling, i.e. k = η(v) implies k = arg mins∈Y θp(s). We deﬁne this
function over all unary costs except u. As θH and θ differ in only the label u, we can
use η for choosing the optimal labelling for both of them.

Since, D(θH) = D(θ), we have

(cid:88)

v∈V

min
s∈Y

θH
v (s) +

(cid:88)

vw∈E

min
st∈Y 2

θH
vw(s, t) =

(cid:88)

v∈V

min
s∈Y

θv(s) +

(cid:88)

vw∈E

min
st∈Y 2

θvw(s, t) (15)

Since all the labels but the ones for u are the same, the terms on both sides of (15)

cancel out leaving

min
s∈Y

θH
u (s)+

(cid:88)

uv|v∈N b(u)

min
st∈Y 2

θH
uv(s, t) = min
s∈Y

θu(s)+

(cid:88)

uv|v∈N b(u)

min
st∈Y 2

θuv(s, t) (16)

Substituting s∗ = arg mins θH

u (s) and η into (16) we get

u (s∗) +
θH

(cid:88)

uv|v∈N b(u)

uv(s∗, η(v)) = min
θH
s∈Y

θu(s) +

(cid:88)

uv|v∈N b(u)

θuv(s, η(v))

(17)

Now, by the deﬁnition of reparameterization we have, ∀s ∈ Y

θH
u (s) +

(cid:88)

θH
uv(s, η(v)) = θu(s) +

(cid:88)

θuv(s, η(v))

(18)

uv|v∈N b(u)

uv|v∈N b(u)

This also holds true for label s∗, thus

u (s∗) +
θH

(cid:88)

uv(s∗, η(v)) = θu(s∗) +
θH

(cid:88)

θuv(s∗, η(v))

(19)

uv|v∈N b(u)

uv|v∈N b(u)

Now, equating the RHS of (19) and the RHS of (17), we get

θu(s∗) +

(cid:88)

uv|v∈N b(u)

θuv(s∗, η(v)) = min
s∈Y

θu(s) +

(cid:88)

uv|v∈N b(u)

θuv(s, η(v))

(20)

Thus s∗ = arg mins∈Y θu(s) =⇒ s∗ ∈ O0
To prove the second assertion, we have to show that if (cid:15)(θH) > 0 and D(H(θ)) =

u(θ), proving the ﬁrst assertion.

D(θ), there exists s(cid:48) ∈ O0

u(θ) such that s(cid:48) /∈ O0

u(θH).

Since, s(cid:48) ∈ O0

u(θ), we have
(cid:88)

θu(s(cid:48)) +

θuv(s(cid:48), η(v)) = min
s∈Y

v∈N b(u)

θu(s) +

(cid:88)

v∈N b(u)

min
s∈Y

θuv(s, η(v))

(21)

27

From the reparameterization relation we have

θu(s(cid:48)) +

(cid:88)

θuv(s(cid:48), η(v)) = θH

u (s(cid:48)) +

v∈N b(u)

(cid:88)

v∈N b(u)

uv(s(cid:48), η(v))
θH

(22)

Since ε(θH) > 0, it has to be the case that

u (s(cid:48)) +
θH

(cid:88)

v∈N b(u)

uv(s(cid:48), η(v)) > min
θH
s∈Y

θH
u (s) +

(cid:88)

v∈N b(u)

min
s∈Y

θH
uv(s, η(v))

(23)

otherwise ε(θH) = 0. Thus, s(cid:48) /∈ O0

u(θH).

Lemma B.9. There exists an n, such that D(Hn+m(θ)) = D(Hn(θ)) implies
O0(Hn+m(θ)) = O0(Hn(θ)), ∀m ≥ 0.

Proof. Let θ0, θ1, ... be the sequence of vectors generated from H via θi+1 = H(θi).
After a certain number of iterations, we have to show that O0(θn+m) = O0(θn), for
all m ≥ 0. From Lemma B.8 we have after a ﬁxed point has been reached at iteration
i, O0(θi) ⊃ O0(θi+1) ⊃ O0(θi+2) ⊃ .... Since O0(θ) is ﬁnite, it cannot shrink in-
deﬁnitely and after a certain number (n) of iterations O0(θn) will become consistent.
(cid:3)
Yielding, O0(θm+n) = O0(θn), ∀m ≥ 0.

Now, we are ready to prove Theorem 1
Combining the above lemmas, we introduce the notion of consistency-enforcing
algorithms. An algorithm is consistency-enforcing if it satisﬁes Lemmas B.1, B.2, B.3,
B.4, B.5, B.6, B.7, B.8, B.9, .

The MPLP++ operator H is consistency enforcing then limi→∞ ε(Hi(θ)) = 0.

ε(Hi(θ)) = 0

lim
i→∞

(24)

Theorem 1. The MPLP++ algorithm converges to node-edge agreement.

Proof. By virtue of Lemma B.7 and Lemma B.4, the sequence θi = Hi(θ) is bounded.
Therefore, by the Bolzano-Weierstrass Theorem [54], there exists a converging subse-
quence θi(j), j = 1, 2, . . . , where j > j(cid:48) implies i(j) > i(j(cid:48)), i.e. the limit θ∗ :=
limj→∞ θi(j) exists. Let us show that it holds

ε(θ∗) = 0

(25)

for any converging subsequence of θi.
Since due to convergence of H (shown in Lemma B.4) and Lemma B.6 the sequence
D(θi) is non-decreasing and bounded from above, and therefore converges to a limit
point D∗ := lim
i→∞

D(θi). Therefore, it also holds

D∗ = lim
j→∞

D(θi(j)) = lim
j→∞

D(θi(j)+n), ∀n ≥ 0

(26)

This implies

28

0 = lim
j→∞

D(θi(j)) − lim
j→∞

D(θi(j)+n) = lim
j→∞

D(θi(j)) − D(Hn(θi(j))).

(27)

Since D is continuous it holds

0 = lim
j→∞

(D(θi(j)) − D(Hn(θi(j)))).

and therefore, (25) holds by virtue of Lemma B.9.
Since, ε is a continuous function, (25) implies

ε(θi(j)) = 0

lim
j→∞

(28)

(29)

for any converging sub-sequence θi(j).
Now, considering the sequence ε(θi), we know by virtue of Lemma B.1 si
:=
supj≥i ε(θj). Sequence si is a monotonically non-increasing sequence of non-negative
numbers and therefore it has a limit s∗ = lim
i→∞
According to the “Theorem of Superior and Inferior Limits” [54] there exists a

si.

subsequence ε(θi(cid:48)(j)) such that

ε(θi(cid:48)(j(k))) = s∗

lim
j→∞

(30)

The sequence θi(cid:48)(j) is bounded virtue of Lemma B.7 and therefore contains a con-

verging subsequence θi(cid:48)(j(k)). For this subsequence it also holds,

ε(θi(cid:48)(j(k))) = s∗

lim
k→∞

At the same time, as proved in (29), for any converging subsequence it holds

0 = lim
k→∞

ε(θi(cid:48)(j(k))) = s∗ = lim
i→∞

ε(θk)

sup
k≥i

Finally, 0 ≤ ε(θi) ≤ sup
k≥i

ε(θk) implies lim
i→∞

ε(θi) = 0.

(31)

(32)

(cid:3)

Proposition 1. Reparametrization φu↔v maximizes Duv(·) iff there exist (s, t) ∈ Y 2
such that s minimizes θφ

v (·) and (s, t) minimizes θφ

u(·), t minimizes θφ

uv(·, ·).

Proof. The if clause has been proven in ([20], Theorem 2). The only if clause can be
proven as follows:

As mentioned in the main paper and shown in [15, 21] the Dual LP D(φ) is a

concave, piecewise linear function.

For proving optimality of block Duv(φu↔v) we need to show 0 ∈ ∂Duv(φu↔v),
where ∂Duv(φu↔v) is the super-differential of Duv(φu↔v). Reconsidering the dual
Duv(φu↔v)

Duv(φu↔v) := min
st∈Y 2

θφ
uv(s, t) + min
s∈Y

θφ
u(s) + min
t∈Y

θφ
v (t) ,

(33)

29

Let x(cid:48)
u, x(cid:48)(cid:48)

u = arg mins∈Y θφ
v ) = arg minst∈Y 2 θφ

v = arg mint∈Y θφ

u(s), x(cid:48)
v (t) and
uv(s, t). Taking the super-gradient of Duv(φu↔v) w.r.t.

(x(cid:48)(cid:48)
φu→v, we have

∂Duv(φu↔v)
∂φu→v(s)

: =





0,
0,
1,
−1,

s (cid:54)= x(cid:48)
s = x(cid:48)
s = x(cid:48)
s (cid:54)= x(cid:48)

u, s (cid:54)= x(cid:48)(cid:48)
u
u, s = x(cid:48)(cid:48)
u
u, s (cid:54)= x(cid:48)(cid:48)
u
u, s = x(cid:48)(cid:48)
u

Likewise, taking the super-gradient of Duv(φu↔v) w.r.t. φv→u, we have

∂Duv(φu↔v)
∂φv→u(t)

: =


0,

0,
1,

−1,

t (cid:54)= x(cid:48)
t = x(cid:48)
t = x(cid:48)
t (cid:54)= x(cid:48)

v, t (cid:54)= x(cid:48)(cid:48)
v
v, t = x(cid:48)(cid:48)
v
v, t (cid:54)= x(cid:48)(cid:48)
v
v, t = x(cid:48)(cid:48)
v

(34)

(35)

Thus, if s = x(cid:48)

u = x(cid:48)(cid:48)

u and t = x(cid:48)

v = x(cid:48)(cid:48)

v , we have 0 ∈ ∂Duv(φv↔u)

∂φv↔u(s,t) , proving the
(cid:3)

only if clause.

30

