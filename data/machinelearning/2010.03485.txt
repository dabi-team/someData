1
2
0
2

n
u
J

1
1

]
L
P
.
s
c
[

3
v
5
8
4
3
0
.
0
1
0
2
:
v
i
X
r
a

SPPL: Probabilistic Programming with Fast Exact
Symbolic Inference

Feras A. Saad
Massachusetts Institute of Technology
Cambridge, MA, USA

Martin C. Rinard
Massachusetts Institute of Technology
Cambridge, MA, USA

Vikash K. Mansinghka
Massachusetts Institute of Technology
Cambridge, MA, USA

Abstract

1 Introduction

We present the Sum-Product Probabilistic Language (Sppl),
a new probabilistic programming language that automati-
cally delivers exact solutions to a broad range of probabilis-
tic inference queries. Sppl translates probabilistic programs
into sum-product expressions, a new symbolic representation
and associated semantic domain that extends standard sum-
product networks to support mixed-type distributions, nu-
meric transformations, logical formulas, and pointwise and
set-valued constraints. We formalize Sppl via a novel trans-
lation strategy from probabilistic programs to sum-product
expressions and give sound exact algorithms for condition-
ing on and computing probabilities of events. Sppl imposes a
collection of restrictions on probabilistic programs to ensure
they can be translated into sum-product expressions, which
allow the system to leverage new techniques for improving
the scalability of translation and inference by automatically
exploiting probabilistic structure. We implement a proto-
type of Sppl with a modular architecture and evaluate it on
benchmarks the system targets, showing that it obtains up
to 3500x speedups over state-of-the-art symbolic systems on
tasks such as verifying the fairness of decision tree classifiers,
smoothing hidden Markov models, conditioning transformed
random variables, and computing rare event probabilities.

CCS Concepts: • Mathematics of computing → Proba-
bilistic representations; Probabilistic inference problems; • Soft-
ware and its engineering → Formal language definitions.

Keywords: probabilistic programming, symbolic execution
ACM Reference Format:
Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka. 2021.
SPPL: Probabilistic Programming with Fast Exact Symbolic Infer-
ence. In Proceedings of the 42nd ACM SIGPLAN International Confer-
ence on Programming Language Design and Implementation (PLDI
’21), June 20–25, 2021, Virtual, Canada. ACM, New York, NY, USA,
16 pages. https://doi.org/10.1145/3453483.3454078

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for third-
party components of this work must be honored. For all other uses, contact
the owner/author(s).
PLDI ’21, June 20–25, 2021, Virtual, Canada
© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8391-2/21/06.
https://doi.org/10.1145/3453483.3454078

Reasoning under uncertainty is a well-established theme
across diverse fields including robotics, cognitive science,
natural language processing, algorithmic fairness, amongst
many others [14, 21, 31, 60]. A common approach for mod-
eling uncertainty is to use probabilistic programming lan-
guages (PPLs [29]) to both represent complex probability
distributions and perform probabilistic inference within the
language. There is growing recognition of the utility of PPLs
for solving challenging tasks that involve probabilistic rea-
soning in various application domains [8, 27, 34, 35].

Probabilistic inference is central to reasoning about uncer-
tainty and is a central concern for both PPL implementors
and users. Several PPLs leverage approximate inference tech-
niques [28, 59, 67], which have been used effectively in a va-
riety of settings [11, 17, 55]. Drawbacks of approximate infer-
ence, however, include a lack of accuracy and/or soundness
guarantees [18, 38]; difficulties with programs that combine
continuous, discrete, or mixed-type distributions [11, 68];
challenges assessing the quality of iterative solvers [9]; and
the substantial expertise needed to write custom inference
programs that deliver acceptable performance [17, 39]. To
address the shortcomings of approximate inference, several
PPLs instead use exact symbolic techniques [6, 10, 23, 43, 69].
These languages can typically express a large class of models,
using general computer algebra to solve queries. However,
the generality of the symbolic computations causes them to
sometimes fail, even on problems with tractable solutions.

Our Work We introduce the Sum-Product Probabilistic Lan-
guage (Sppl), a system that occupies a new point in the
expressiveness vs. performance trade-off space for exact sym-
bolic inference. A key idea in Sppl is to incorporate certain
modeling restrictions that avoid the need for general com-
puter algebra, instead using a new, specialized class of “sum-
product” symbolic expressions to exactly represent proba-
bility distributions specified by Sppl programs. These new
symbolic expressions extend and generalize sum-product
networks [47], which are computational graphs that have
received widespread attention for their clear probabilistic se-
mantics and tractable properties for exact inference—see [64]
for a comprehensive and curated literature review. These
sum-product expressions are used to automatically obtain
exact solutions to probabilistic inference queries about Sppl
programs, which are fast and scalable in tractable regimes.

 
 
 
 
 
 
PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

Probabilistic
Program

Probability
Distribution

Sum-Product
Expression

Sppl
Translator

Inference Queries

simulate(Vars)
prob(Event)
condition(Event)

Sppl
Inference Engine

simulate

prob

condition

Simulated
Program
Variables

Event
Probability
Values

Conditioned
Probability
Distribution

Sum-Product
Expression

Figure 1. Sppl system overview. Probabilistic programs are translated into symbolic sum-product expressions that represent
the joint distribution over all program variables and are used to deliver exact solutions to probabilistic inference queries.

System Overview Fig. 1 shows an overview of our approach.
Given a probabilistic program written in Sppl (Lst. 2) a trans-
lator (Lst. 3) produces a sum-product expression that repre-
sents the prior distribution over all program variables. Given
this expression and a query specified by the user, the Sppl
inference engine returns an exact answer, where:

1. simulate(Vars) returns random samples of program
variables from their joint probability distribution;
2. prob(Event) returns the probability of an event, which

is a predicate on program variables;

3. condition(Event) returns a new sum-product expres-
sion for the posterior distribution over program vari-
ables, given that the specified event is true.

A key aspect of the system design in Fig. 1 is modularity: mod-
eling, conditioning, and querying are factored into distinct
stages that reflect the essential components of a Bayesian
workflow. Moreover, the dashed back-edge in Fig. 1 indicates
that the new sum-product expression returned by condition
can be reused to interactively invoke additional queries on
the posterior distribution. This closure property enables sub-
stantial runtime gains across multiple datasets and queries.

Trade-offs Sppl imposes restrictions on probabilistic pro-
grams that specifically rule out the following constructs:
(i) unbounded loops; (ii) multivariate numeric transforma-
tions; and (iii) arbitrary prior distributions on continuous
parameters. As a result, Sppl is not designed to express model
classes such as regression with a prior on real coefficients;
neural networks; support-vector machines; spatial Poisson
processes; urn processes; and hidden Markov models with
unknown transition matrices. The aforesaid model classes
cannot be represented as sum-product expressions, and most
of them do not have tractable algorithms for exact inference.
We impose these restrictions to ensure that valid Sppl
programs can always be translated into finite sum-product
expressions, as opposed to general symbolic algebra expres-
sions. The resulting sum-product expressions delivered by
Sppl have a number of characteristics that make them a par-
ticularly useful translation target for probabilistic programs:

• Completeness and Decomposibility: By satisfying im-
portant completeness and decomposability conditions from
the literature [47, Defs. 4,5], sum-product expressions are
guaranteed to represent normalized probability distributions.
• Efficient Factorization: By specifying multivariate prob-
ability distributions compositionally in terms of sums and
products of simpler distributions, sum-product expressions
can be simplified by algebraic “factorization” (Fig. 3d, Fig. 6a).
• Efficient Deduplication: When an Sppl program speci-
fies a generative model with conditional independence struc-
ture, the translated sum-product expression typically con-
tains identical subexpressions that can be “deduplicated” into
a single logical node in memory (Fig. 3d, Fig. 6b).
• Efficient Caching: Inference algorithms for sum-product
expressions proceed from root to leaves to root, allowing
intermediate results to be cached and reused at deduplicated
internal subexpressions in a depth-first graph traversal.
• Closure Under Conditioning: Sum-product expressions
are closed under probabilistic conditioning (Thm. 4.1), which
allows them to be reused across multiple datasets and infer-
ence queries about the same probabilistic program.
• Linear-Time Exact Inferences: For a well-defined class
of common queries, inference scales linearly in the expres-
sion size (Thm. 4.3); when Sppl delivers a “small” expression
after factorization and deduplication, inference is also fast.

It is well-known that a very large class of tractable models
can be cast as sum-product networks [47, Thm. 2]. Sppl auto-
matically constructs these representations from generative
probabilistic programs that use standard constructs such as
arrays, if/else branches, for-loops, and numeric and logical
operators. To enable this translation, Sppl introduces new
sum-product expressions and inference algorithms that ex-
tend standard sum-product networks by supporting (many-
to-one) univariate transformations, mixed-type base mea-
sures, and pointwise and set-valued constraints. These con-
structs make Sppl expressive enough to solve prominent
inference tasks in the PPL literature [2, 36, 46, 68] for which
standard sum-product networks have not been previously

SPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

used. Example model classes include most finite discrete mod-
els, latent variable models with discrete hidden states and
arbitrary observed states, and decision trees over discrete
and continuous variables. Taken together, these character-
istics make Sppl particularly effective for fast and scalable
inference on tractable problems, with low variance runtime
and complete, usable answers to users. Our experimental
evaluation (Sec. 6) indicates that Sppl delivers these benefits
on the problems it is designed to solve, whereas more gen-
eral and expressive techniques in previous solvers [2, 4, 23]
typically exhibit orders of magnitude worse performance on
these problems, runtime has higher variance, and/or results
may be unusable, i.e., with unsimplified symbolic integrals.

Key contributions We identify the following contributions:
• New semantic domain for sum-product expressions
(Sec. 3) that extends sum-product networks [47] by including
mixed-type distributions, numeric transforms, logical formu-
las, and events with pointwise and set-valued constraints.
• Provably sound exact symbolic inference algorithms
(Sec. 4) based on a proof that sum-product expressions are
closed under conditioning on any event that can be specified
in the domain. We use these algorithms to build an efficient
and multi-stage inference architecture that separates model
translation, conditioning, and querying into distinct stages,
enabling interactive workflows and computation reuse.
• The Sum-Product Probabilistic Language (Sec. 5), a
PPL built on a novel translation semantics from generative
code to sum-product expressions, which are used to deliver
exact inferences to queries. We present optimization tech-
niques to improve scalability of translation and inference by
exploiting conditional independences and repeated structure.
• Empirical measurements of efficacy (Sec. 6) on infer-
ence tasks from the literature that Sppl targets, which show
that it delivers substantial improvements over existing base-
lines, including up to 3500x speedup over state-of-the-art
fairness verifiers [2, 4] and symbolic integration [23], as well
as many orders of magnitude speedup over sampling-based
inference [40] for computing rare event probabilities.

2 Overview

We next describe two examples that illustrate the program-

ming style in Sppl and queries supported by the system.

2.1 Indian GPA Problem

The Indian GPA problem is a canonical example that has
been widely considered in the probabilistic programming
literature [44, 45, 48, 57, 68] for its use of a “mixed-type” ran-
dom variable that takes both continuous and discrete values,
depending on the random branch taken by the program.

Specifying the Prior Fig. 2a shows the generative process
for three variables (Nationality, Perfect and GPA) of a
student. The student’s nationality is either India or USA with

equal probability (line 1). Students from India (line 2) have
a 10% probability of a perfect 10 GPA (lines 3-4), otherwise
the GPA is uniform over [0, 10] (line 5). Students from USA
(line 6) have a 15% probability of a perfect 4 GPA (lines 6-7),
otherwise the GPA is uniform over [0, 4] (line 8).

Prior Sum-Product Expression The graph in Fig. 2d shows
the translated sum-product expression for this program,
which represents a sampler for the distribution over pro-
gram variables as follows: (i) if a node is a sum (+), visit a
random child with probability equal to the weight of the edge
pointing to the child; (ii) if a node is a product (×), visit each
child exactly once, in no specific order; (iii) if a node is a leaf,
sample a value from the distribution at the leaf and assign it
to the variable at the leaf. Similarly, the graph encodes the
joint distribution of the variables by treating (i) each sum
node as a probabilistic mixture; (ii) each product node as a
tuple of independent variables; and (iii) each leaf node as a
primitive random variable. Thus, the prior distribution is:
Pr[Nationality = 𝑛, Perfect = 𝑝, GPA ≤ 𝑔]
= 0.5(cid:2)𝛿India (𝑛) · (0.1[(𝛿True (𝑝) · 1 [10 ≤ 𝑔])]
+ 0.9[(𝛿False(𝑝) · (𝑔/10 · 1 [0 ≤ 𝑔 < 10] + 1 [10 ≤ 𝑔]))])(cid:3)
+ 0.5(cid:2)𝛿USA (𝑛) · (0.15[(𝛿True (𝑝) · 1 [4 ≤ 𝑔])]
+ 0.85[(𝛿False (𝑝) · (𝑔/4 · 1 [0 ≤ 𝑔 < 4] + 1 [4 ≤ 𝑔]))])(cid:3) .
Fig. 2b shows Sppl queries for the prior marginal distribu-
tions of the three variables, plotted in Fig. 2e. The two jumps
in the cumulative distribution function (CDF1) of GPA at 4
and 10 correspond to the atoms that occur when Perfect
is true. The piecewise linear behavior on [0, 4] and [4, 10]
follows from the conditional uniform distributions of GPA.

(1)

Conditioning the Program Fig. 2f shows an example of the
condition query, which specifies an event 𝑒 on which to
constrain executions of the program. An event is a predicate
on (possibly transformed) program variables that can be
used for both condition (Fig. 2f) and prob (Fig. 2c). Sppl is
the first system with inference algorithms for sum-product
expressions that handle predicates of this form. Given 𝑒, the
object of inference is the full posterior distribution:
Pr[Nationality = 𝑛, Perfect = 𝑝, GPA ≤ 𝑔 | 𝑒]
(cid:66) Pr[Nationality = 𝑛, Perfect = 𝑝, GPA ≤ 𝑔, 𝑒]/Pr[𝑒].

(2)

Posterior Sum-Product Expression Given the prior expres-
sion (Fig. 2d) and conditioning event 𝑒 (Fig. 2f), Sppl produces
a new expression (Fig. 2g) that specifies a distribution which
is precisely equal to Eq. (2), From Thm. 4.1, conditioning an
Sppl program on any event that can be specified in the lan-
guage results in a posterior distribution that also admits an
exact sum-product expression. Conditioning on 𝑒 performs
several transformations on the prior expression, which are:

1For a real-valued random variable 𝑋 , the cumulative distribution function
𝐹 : Real → [0, 1] is given by 𝐹 (𝑟 ) (cid:66) Pr[𝑋 ≤ 𝑟 ].

PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

Perfect ~ bernoulli(p=0.10)
if Perfect:
else:

1 Nationality ~ choice({'India': 0.5, 'USA': 0.5})
2 if (Nationality == 'India'):
3
4
5
6 else: # Nationality is 'USA'
7
8
9

Perfect ~ bernoulli(p=0.15)
if Perfect:
else:

GPA ~ atom(10)
GPA ~ uniform(0, 10)

GPA ~ atom(4)
GPA ~ uniform(0, 4)

prob (Nationality == 'USA');
prob (Perfect == 1);
prob (GPA <= x/10) # for x = 0, ..., 120

(b) Example Queries on Marginal Probabilities

prob ((Perfect == 1)

or (Nationality == 'India') and (GPA > 3))

(a) Probabilistic Program

(c) Example Query on Joint Probabilities

+

.5

×

.1

×

+

.9

×

𝛿India
Nationality

.5

+

×

.85

×

.15

×

𝛿USA
Nationality

𝛿True
Perfect

𝛿10
GPA

𝛿False
Perfect

𝑈 (0, 10)
GPA

𝛿True
Perfect

𝛿4
GPA

𝛿False
Perfect

𝑈 (0, 4)
GPA

(d) Prior Sum-Product Expression

(e) Prior Marginal Distributions

condition ((Nationality == 'USA') and (GPA > 3)) or (8 < GPA < 10)

(f) Conditioning the Program

.33

+

.67

×

×

𝛿India
Nationality

𝛿False
Perfect

𝑈 (8, 10)
GPA

.41

×

+

.59

×

𝛿USA
Nationality

𝛿True
Perfect

𝛿4
GPA

𝛿False
Perfect

𝑈 (3, 4)
GPA

(g) Posterior Sum-Product Expression

(h) Posterior Marginal Distributions

Figure 2. Analyzing the Indian GPA problem in Sppl.

1. Eliminating the subtree rooted at the parent of leaf 𝛿10,
which is inconsistent with the conditioning event.
2. Rescaling the distribution 𝑈 (0, 10) at the leaf node in

the India subtree to 𝑈 (8, 10).

3. Rescaling the distribution 𝑈 (0, 4) at the leaf node in

the USA subtree to 𝑈 (3, 4).

4. Reweighting the branch probabilities of the sum node
in the USA subtree from [.15, .85] to [.41, .59], where
.41 = .15/(.15 + .2125) is the posterior probability of
(Perfect = 1, GPA = 4) given the condition 𝑒.

5. Reweighting the branch probabilities at the root from
[.5, .5] to [.33, .67] (same rules as in the previous item).
Fig. 2g shows the posterior expression obtained by apply-
ing these transformations. Using this expression, the right-
hand side of Eq. (2), which is the object of inference, is
Pr[Nationality = 𝑛, Perfect = 𝑝, GPA ≤ 𝑔 | 𝑒]
= .33(cid:2)𝛿India(𝑛) · 𝛿False(𝑝) · (

(3)
· 1 [8 ≤ 𝑔 < 10] + 1 [10 ≤ 𝑔])(cid:3)

𝑔−8
2

+ .67(cid:2)𝛿USA(𝑛) · (.41[(𝛿True(𝑝) · 1 [4 ≤ 𝑔])]
+ .59[(𝛿False(𝑝) · (

4 · 1 [0 ≤ 𝑔 < 4] + 1 [4 ≤ 𝑔]))])(cid:3) .

𝑔

USAIndiaNationality0.500.50FalseTruePerfect0.880.130510GPA0.000.250.500.751.00CumulativeProbabilityUSAIndiaNationality0.670.33FalseTruePerfect0.720.280510GPA0.000.250.500.751.00CumulativeProbabilitySPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

(Floats are shown to two decimal places.) We can now run
the prob queries in Fig. 2b on the conditioned program to
plot the posterior marginal distributions, which are shown
in Fig. 2h. The example in Fig. 2 illustrates a typical modular
workflow in Sppl (Fig. 1), where modeling (Fig. 2a), condi-
tioning (Fig. 2f) and querying (Figs. 2b–2c) are separated into
distinct and reusable stages that together express the main
components of Bayesian modeling and inference.

2.2 Scalable Inference in a Hierarchical HMM

The next example shows how to perform efficient smooth-
ing in a hierarchical hidden Markov model (HMM [42]) and
illustrates the optimization techniques used by the Sppl trans-
lator (Sec. 5.1), which exploit conditional independence to
ensure that the size of the sum-product expression grows
linearly in the number of timesteps. The code box in Fig. 3a
shows a hierarchical hidden Markov model with Bernoulli
hidden states 𝑍𝑡 and Normal–Poisson observations (𝑋𝑡, 𝑌𝑡 ).
The separated variable indicates whether the mean val-
ues of 𝑋𝑡 and 𝑌𝑡 at 𝑍𝑡 = 0 and 𝑍𝑡 = 1 are well-separated.
The p_transition vector specifies that the current state 𝑍𝑡
switches from the previous state 𝑍𝑡 −1 with 20% probability.
This example leverages the Sppl array, for, and switch-
cases statements, where the latter is a macro that expands
to if-else statements (as in, e.g., the C language):

switch 𝑥 cases (𝑥 ′in values) {𝐶}
desugar⇝ if (𝑥 in values[0]) {𝐶 [𝑥 ′/values[0]]}

(4)

elif . . .
elif (𝑥 in values[𝑛−1]) {𝐶 [𝑥 ′/values[𝑛 − 1]]},

where 𝑛 is the length of values and 𝐶 [𝑥/𝐸] indicates syntactic
replacement of 𝑥 with expression 𝐸 in command 𝐶.

The top and middle plots in Fig. 3b show a realization of
𝑋 and 𝑌 that result from simulating the process for 100 time
steps. The blue and orange regions along the x-axes indicate
whether the true hidden state 𝑍 is 0 or 1, respectively (these
“ground-truth” values of 𝑍 are not observed but need to be
inferred from 𝑋 and 𝑌 ). The bottom plot in Fig. 3b shows the
exact posterior marginal probabilities Pr[𝑍𝑡 = 1 | 𝑥0:99, 𝑦0:99]
for each 𝑡 = 0, . . . , 99 as inferred by Sppl (an inference known
as “smoothing”). These probabilities track the true hidden
state, i.e., the posterior probabilities that 𝑍𝑡 = 1 are low in
the blue and high in the orange regions.

Fig. 3c shows a “naive” sum-product expression for the
distribution of all program variables up to the first two time
steps. This expression is a sum-of-products, where the prod-
ucts in the second level are an enumeration of all possible re-
alizations of program variables, so that the number of terms
scales exponentially in the number of time steps. Fig. 3d
shows the expression constructed by Sppl, which is (concep-
tually) based on factoring and sharing common terms in the
two level sum-of-products in Fig. 3c. These factorizations

and deduplications exploit conditional independences and
repeated structure in the program (Sec. 5.1), which here deliv-
ers a expression whose size scales linearly in the number of
time points. Sppl can also efficiently solve variants of smooth-
| 𝑥0:𝑡, 𝑦0:𝑡 ]
ing, e.g., computing posterior marginals Pr[𝑍𝑡
(filtering) or the posterior joint Pr[𝑍0:𝑡 | 𝑥0:𝑡, 𝑦0:𝑡 ] for any 𝑡.

3 A Core Calculus for Sum-Product

Expressions

This section presents a semantic domain of sum-product ex-
pressions that generalizes sum-product networks [47] and en-
ables precise reasoning about them. This domain will be used
to (i) establish the closure of sum-product expressions under
conditioning on events expressible in the calculus (Thm. 4.1);
(ii) describe sound algorithms for exact Bayesian inference in
our system (Appx. D); and (iii) describe a procedure for trans-
lating a probabilistic program into a sum-product expression
in the core language (Sec. 5). Lst. 1 shows denotations of
the key syntactic elements (Lst. 9 in Appx. A) in the cal-
culus, which includes real and nominal outcomes (Lst. 1a);
real transforms (Lst. 1b); predicates with pointwise and set-
valued constraints (Lst. 1c); primitive distributions (Lst. 1e);
and multivariate distributions specified compositionally as
sums and products of primitive distributions (Lst. 1f).

Basic Outcomes Random variables in the calculus take val-
ues in the Outcome (cid:66) Real + String domain. The symbol +
here indicates a sum (disjoint-union) data type, whose ele-
𝑟
ments are formed by the injection operation, e.g., ↓ Real
for 𝑟 ∈ Real. This domain is used to model mixed-type ran-
dom variables, such as 𝑋 in the following Sppl program:
Z ~ normal(0, 1)
if
elif (0 < Z < 4): X ~ 2*exp(Z)
X ~ atomic(4)
elif (4 <= Z):

# continuous real
# discrete real

X ~ "negative" # string

(Z <= 0):

Outcome

The Outcomes domain denotes a subset of Outcome as de-
fined by the valuation function V (Lst. 1a). For example,
((𝑏1 𝑟1) (𝑟2 𝑏2)) specifies a (open, closed, or clopen) real
interval and {𝑠1 . . . 𝑠𝑚}𝑏 is a set of strings, where 𝑏 = #t
indicates the complement (meta-variables such as 𝑚 indicate
an arbitrary but finite number of repetitions of a domain vari-
able or subexpression). The operations union, intersection,
and complement operate on Outcomes in the usual way
(while preserving certain semantic invariants, see Appx. B)

A Sigma Algebra of Outcomes To speak precisely about
random variables and measures on Outcome, we define a
sigma-algebra B (Outcome) ⊂ P (Outcome) as follows:
1. Let 𝜏Real be the usual topology on Real.
2. Let 𝜏String be the discrete topology on String.
3. Let 𝜏Outcome (cid:66) 𝜏Real⊎𝜏String be the disjoint-union topology
on Outcome, where 𝑈 is open iff {𝑟 | (↓
𝑟 ) ∈ 𝑈 } is
open in Real and {𝑠 | (↓ String
Outcome
4. Let B (Outcome) be the Borel sigma-algebra of 𝜏Outcome.

Real
Outcome
𝑠) ∈ 𝑈 } is open in String.

PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

1 p_transition = [.2, .8]
2 mu_x = [[5, 7], [5, 15]]
3 mu_y = [[5, 8], [3, 8]]
4
5 n_step = 100
6 Z = array(n_step)
7 X = array(n_step)
8 Y = array(n_step)
9
10 separated ~ bernoulli(p=.4)
11 switch separated cases (s in [0,1]):
12
13
14
15
16
17
18
19
20
21

Z[0] ~ bernoulli(p=.5)
switch Z[0] cases (z in [0, 1]):
X[0] ~ normal(mu_x[s][z], 1)
Y[0] ~ poisson(mu_y[s][z])

switch Z[t-1] cases (z in [0, 1]):
Z[t] ~ bernoulli(p_transition[z])
switch Z[t] cases (z in [0, 1]):
X[t] ~ normal(mu_x[s][z], 1)
Y[t] ~ poisson(mu_y[s][z])

for t in range(1, n_step):

(a) Probabilistic Program

(b) Observed Data 𝑋, 𝑌 and Inferred Hidden States 𝑍

+

.24

×

.06
×

.06
×

.24
×

.16
×

.04
×

.04
×

.16

×

𝛿0
sep

𝛿0
𝑍 [0]

𝑁 (5)
𝑋 [0]

𝑃 (5)
𝑌 [0]

𝛿0
𝑍 [1]

𝑁 (5)
𝑋 [1]

𝑃 (5)
𝑌 [1]

𝛿0
sep

𝛿0
𝑍 [0]

𝑁 (5)
𝑋 [0]

𝑃 (5)
𝑌 [0]

𝛿1
𝑍 [1]

𝑁 (7)
𝑋 [1]

𝑃 (8)
𝑌 [1]

𝛿0
sep

𝛿1
𝑍 [0]

𝑁 (7)
𝑋 [0]

𝑃 (8)
𝑌 [0]

𝛿0
𝑍 [1]

𝑁 (5)
𝑋 [1]

𝑃 (5)
𝑌 [1]

𝛿0
sep

𝛿1
𝑍 [0]

𝑁 (7)
𝑋 [0]

𝑃 (8)
𝑌 [0]

𝛿1
𝑍 [1]

𝑁 (7)
𝑋 [1]

𝑃 (8)
𝑌 [1]

𝛿1
sep

𝛿0
𝑍 [1]

𝑁 (5)
𝑋 [1]

𝑃 (5)
𝑌 [1]

𝛿0
𝑍 [0]

𝑁 (5)
𝑋 [0]

𝑃 (5)
𝑌 [0]

𝛿1
sep

𝛿0
𝑍 [0]

𝑁 (5)
𝑋 [0]

𝑃 (3)
𝑌 [0]

𝛿0
𝑍 [1]

𝑁 (5)
𝑋 [1]

𝑃 (3)
𝑌 [1]

𝛿1
sep

𝛿0
𝑍 [0]

𝑁 (5)
𝑋 [0]

𝑃 (3)
𝑌 [0]

𝛿1
𝑍 [1]

𝑁 (15)
𝑋 [1]

𝑃 (8)
𝑌 [1]

𝛿1
sep

𝛿1
𝑍 [0]

𝑁 (15)
𝑋 [0]

𝑃 (8)
𝑌 [0]

𝛿1
𝑍 [1]

𝑁 (15)
𝑋 [1]

𝑃 (8)
𝑌 [1]

(c) Naive Sum-Product Expression (Scales Exponentially)

×

+

.5

.5

×

×

.6

𝛿0
sep

+

×

.4

+

.5

.5

×

×

𝛿1
sep

𝛿0
𝑍 [0]

𝑁 (5)
𝑋 [0]

𝑃 (5)
𝑌 [0]

+

.8

.2

.2

+

.8

𝛿1
𝑍 [0]

𝑁 (7)
𝑋 [0]

𝑃 (8)
𝑌 [0]

𝛿0
𝑍 [0]

𝑁 (5)
𝑋 [0]

𝑃 (3)
𝑌 [0]

+

.8

.2

.2

+

.8

𝛿1
𝑍 [0]

𝑁 (15)
𝑋 [0]

𝑃 (8)
𝑌 [0]

×

×

×

×

𝛿0
𝑍 [1]

𝑁 (5)
𝑋 [1]

𝛿1
𝑍 [1]
(d) Optimized Sum-Product Expression (Scales Linearly)
Figure 3. Fast smoothing in a hierarchical hidden Markov model using Sppl by constructing an efficient sum-product expression
that exploits conditional independences in the generative process. Optimization techniques are discussed in Sec. 5.1.

𝑁 (15)
𝑋 [1]

𝑁 (7)
𝑋 [1]

𝑁 (5)
𝑋 [1]

𝑃 (5)
𝑌 [1]

𝑃 (8)
𝑌 [1]

𝑃 (3)
𝑌 [1]

𝑃 (8)
𝑌 [1]

𝛿0
𝑍 [1]

𝛿1
𝑍 [1]

Remark 3.1. As measures on Real are defined by their val-
ues on open intervals and measures on String on singletons,
we can speak of probability measures on B (Outcome) as
mappings from Outcomes to [0, 1].

Real Transformations Lst. 1b shows real transformations
that can be applied to variables in the calculus. The Identity
Transform, written Id(𝑥), is a terminal subexpression of
any Transform 𝑡 and contains a single variable name that
specifies the “dimension” over which 𝑡 is defined. The list of
all transforms is in Appx. C.1. The key operation involving
transforms is computing the preimage of Outcomes 𝑣 under

𝑡 using preimg : Transform → Outcomes → Outcomes
which satisfies the following properties:

(↓

Real
Outcome
(↓ String
Outcome

𝑟 ) ∈ V

𝑠) ∈ V

preimg 𝑡 𝑣
(cid:74)
preimg 𝑡 𝑣
(cid:74)

(cid:75)

(cid:75)

⇐⇒ T

(𝑟 ) ∈ V

𝑡
(cid:74)

(cid:75)

𝑣
(cid:74)

(cid:75)

⇐⇒ (𝑡 ∈ Identity) ∧ (𝑠 ∈ V

Appx. C.2 presents a symbolic solver that implements preimg
for each Transform, which is leveraged to enable exact prob-
abilistic inferences on transformed variables in Sppl. Fig. 4
and Appx. C.3 show example inferences with transforms.

𝑣
(cid:74)

).
(cid:75)

Events Lst. 1c shows the Event domain, which specifies pred-
icates on variables. The valuation E
: Var → Outcomes

𝑒
(cid:74)

(cid:75)

02040608010005101520ObservedValueXTrueHiddenStateZ=0Z=10204060801000510152025ObservedValueYTrueHiddenStateZ=0Z=1020406080100Time0.000.250.500.751.00InferredHiddenStateZPosteriorProbabilityofZ=1SPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

(cid:75)

(cid:66) ∅

∅
(cid:74)

Outcome (cid:66) Real + String
V : Outcomes → P (Outcome)
V
V (cid:114){𝑠1 . . . 𝑠𝑚}𝑏 (cid:122) (cid:66) if 𝑏 then ∪𝑚
else { (↓ String
Outcome
Real
𝑖=1 { (↓
Outcome
(cid:66) { (↓
Real
Outcome

(cid:66) ∪𝑚

V
V

{𝑟1 . . . 𝑟𝑚}
𝑟𝑖 ) }
(cid:74)
(cid:75)
((𝑏1 𝑟1) (𝑟2 𝑏2))
𝑟 ) | 𝑟1<𝑏1
𝑟2 }
(cid:75)
(cid:74)
where <#t(cid:66)<; <#f(cid:66) ≤; 𝑟1 < 𝑟2
(cid:66) ∪𝑚
𝑖=1

𝑣1 ⨿ · · · ⨿ 𝑣𝑚

𝑟 <𝑏2

𝑣𝑖

V

(cid:75)

(cid:75)

(cid:74)
(a) Outcomes

V

(cid:74)

𝑖=1 { (↓ String
𝑠𝑖 ) }
Outcome
𝑠) | ∀𝑖.𝑠 ≠ 𝑠𝑖 }

[Empty]

[FiniteStr]

[FiniteReal]
[Interval]

[Union]

T : Transform → Real → Real
T

T
T

(cid:66) 𝜆𝑟 ′.𝑟 ′; T
Id(𝑥)
(cid:75)
(cid:74)
(cid:66) 𝜆𝑟 ′. |T
Abs(𝑡 )
𝑡
(cid:74)
(cid:74)
(cid:75)
Poly(𝑡 𝑟0 . . . 𝑟𝑚)
(cid:75)
(cid:74)

Reciprocal(𝑡 )
(cid:75)
(cid:74)
(𝑟 ′) |; T
Root(𝑡 𝑛)
(cid:75)
(cid:74)
(cid:75)
(cid:66) 𝜆𝑟 ′. (cid:205)𝑚
𝑟𝑖 (T
𝑡
𝑖=0
(cid:75)
(cid:74)
(b) Transformations (Lst. 17 in Appx. C.1)

(cid:66) 𝜆𝑟 ′.1/(cid:0)T
𝑡
(cid:66) 𝜆𝑟 ′. 𝑛√︃
(cid:74)
(cid:75)
T
(𝑟 ′))𝑖
; . . .

(𝑟 ′)(cid:1) ;
(𝑟 ′);
𝑡
(cid:74)

(cid:75)

𝑥 (cid:66) if (vars 𝑡 ) = {𝑥 } then (preimg 𝑡 𝑣) else ∅ [Contains]

E : Event → Var → Outcomes
E
E
E

(𝑡 in 𝑣)
𝑒1 ⊓ · · · ⊓ 𝑒𝑚
(cid:74)
(cid:75)
𝑒1 ⊔ · · · ⊔ 𝑒𝑚
(cid:74)
(cid:74)

𝑥 (cid:66) (intersection E
𝑥 (cid:66) (union E

(cid:75)
(cid:75)

(cid:75)

e1
𝑥 . . . E
(cid:74)
(cid:75)

e1
(cid:74)
(c) Events

𝑥 . . . E
e𝑚
(cid:74)

e𝑚
𝑥)
(cid:74)
(cid:75)

𝑥)

(cid:75)

(Id(𝑥) in {rs}) in (1 [𝑤 = 0] , 𝑤)
(Id(𝑥𝑖 ) in {rsi}) (cid:66)

: SPE → Event → Natural × [0, ∞)

𝑆
Leaf(𝑥 𝑑 𝜎)
(cid:74)
▷ DistR(𝐹 𝑟1 𝑟2) ⇒ match rs
(cid:75)
(cid:74)

(Id(𝑥) in {rs}) (cid:66) match 𝑑

(cid:75)

▷ 𝑟 ⇒ (1, 1 [𝑟1 ≤ 𝑟 ≤ 𝑟2 ] 𝐹 ′ (𝑟 )/[𝐹 (𝑟2) − 𝐹 (𝑟1) ])
▷ 𝑠 ⇒ (1, 0)
▷ else ⇒ let 𝑤 be P
Leaf(𝑥 𝑑 𝜎)
⊓ℓ
(cid:75)
(cid:74)
(𝑆1 𝑤1) ⊕ · · · ⊕ (𝑆𝑚 𝑤𝑚)
𝑖=1
(cid:0)⊓ℓ
(Id(𝑥𝑖 ) in {rsi})(cid:1)
(cid:74)
(cid:75)
let1≤𝑖 ≤𝑚 (𝑑𝑖, 𝑝𝑖 ) be P0
𝑖=1
in if ∀1≤𝑖 ≤𝑚 . 𝑝𝑖 = 0 then (1, 0)

𝑆𝑖
(cid:74)
else let 𝑑 ∗ be min{𝑑𝑖 | 1 ≤ 𝑖 ≤ 𝑚, 0 < 𝑝𝑖 }

(cid:75)

in (𝑑 ∗, (cid:205)𝑚

𝑖=1 1 [𝑑𝑖 = 𝑑 ∗ ] 𝑤𝑖 𝑝𝑖 )

P0
P0

P0

P0

⊓ℓ
𝑖=1

(Id(𝑥𝑖 ) in {rsi}) (cid:66)

𝑆1 ⊗ · · · ⊗ 𝑆𝑚
let1≤𝑖 ≤𝑚 (𝑑𝑖, 𝑝𝑖 ) be match {𝑥1, . . . , 𝑥𝑚 } ∩ (scope 𝑆𝑖 )
(cid:74)
(Id(𝑥𝑛𝑡 ) in {rst })
𝑆𝑖
(cid:74)

⊓𝑘
𝑡 =1

(cid:75)

(cid:75)

▷ {𝑛1, . . . , 𝑛𝑘 } ⇒ P0
▷ { } ⇒ (0, 1)
𝑑𝑖, (cid:206)𝑚
𝑖=1

𝑝𝑖 )

in ((cid:205)𝑛
𝑖=1
(d) Sum-Product Expressions (Density Semantics)

[Sum]

[Product]

[Conjunction]
[Disjunction]

[Leaf ]

D : Distribution → Outcomes → [0, 1]
DistS((𝑠𝑖 𝑤𝑖 )𝑚
D
𝑖=1
(cid:74)
match (intersection 𝑣 {𝑠1 . . . 𝑠𝑚}#f)

𝑣 (cid:66)

)

(cid:75)

𝑚} | ((𝑏1 𝑟1) (𝑟2 𝑏2)) ⇒ 0

. . . 𝑟 ′
▷ ∅ | {𝑟 ′
1
▷ 𝑣1 ⨿ · · · ⨿ 𝑣𝑚 ⇒ (cid:205)𝑚
𝑖=1
▷ {𝑠′
1

. . . 𝑠′

𝑘 }𝑏 ⇒ let 𝑤 be (cid:205)𝑚

D (cid:113)DistS((𝑠𝑖 𝑤𝑖 )𝑚
)(cid:121) 𝑣𝑖
𝑖=1
𝑗 }𝑘
𝑖=1 (𝑤𝑖 if 𝑠𝑖 ∈ {𝑠′
𝑗 =1 else 0)

in if ¯𝑏 then 𝑤 else 1 − 𝑤

[DistStr]

D

D

𝑣𝑖

)) ⇒

𝑣 (cid:66) match (intersection ((#f 𝑟1) (𝑟2 #f)) 𝑣) [DistReal]

DistR(𝐹 𝑟1 𝑟2)
(cid:75)
(cid:74)
𝑘 }𝑏 ⇒ 0
. . . 𝑠′
𝑚} | {𝑠′
. . . 𝑟 ′
▷ ∅ | {𝑟 ′
1
1
▷ 𝑣1 ⨿ · · · ⨿ 𝑣𝑚 ⇒ (cid:205)𝑚
DistR(𝐹 𝑟1 𝑟2)
D
𝑖=1
(cid:74)
(cid:75)
𝐹 (𝑟 ′
2) − 𝐹 (𝑟 ′
1)
) (𝑟 ′
▷ ((𝑏′
𝑟 ′
𝑏′
1
2
1
2
𝐹 (𝑟2) − 𝐹 (𝑟1)
DistI(𝐹 𝑟1 𝑟2)
𝑣 (cid:66) match (intersection ((#f 𝑟1) (𝑟2 #f)) 𝑣) [DistInt]
(cid:74)
(cid:75)
𝑘 }𝑏 ⇒ 0
. . . 𝑠′
▷ ∅ | {𝑠′
1
▷ 𝑣1 ⨿ · · · ⨿ 𝑣𝑚 ⇒ (cid:205)𝑚
D
DistI(𝐹 𝑟1 𝑟2)
𝑣𝑖
𝑖=1
(cid:75)
(cid:74)
𝑖 ⌋) ∧ (𝑟1 ≤ 𝑟 ′
𝑖 = ⌊𝑟 ′
(cid:20)if (𝑟 ′
𝑖 ≤ 𝑟2)
then 𝐹 (𝑟 ′) − 𝐹 (𝑟 ′ − 1) else 0
𝐹 ( ⌊𝑟2 ⌋) − 𝐹 ( ⌈𝑟1 ⌉ − 1)
1 ⌋ − 1 (cid:2)(𝑟 ′

)) ⇒ let ˜𝑟1 be ⌊𝑟 ′

𝑚} ⇒
𝑏′
) (𝑟 ′
2
2

▷ {𝑟 ′
1
▷ ((𝑏′
1

1 = ⌊𝑟 ′

. . . 𝑟 ′

𝑚
∑︁

𝑟 ′
1

𝑖=1

(cid:21)

(cid:3)
1 ⌋) ∧ ¯𝑏′
1
2 ⌋) ∧ ¯𝑏′
2

(cid:3)

in let ˜𝑟2 be ⌊𝑟 ′

2 ⌋ − 1 (cid:2)(𝑟 ′

2 = ⌊𝑟 ′

in

𝐹 ( ˜𝑟2) − 𝐹 ( ˜𝑟1)
𝐹 ( ⌊𝑟2 ⌋) − 𝐹 ( ⌈𝑟1 ⌉ − 1)

(e) Primitive Distributions

(cid:75)
in P

P : SPE → Event → [0, 1]
P
𝑑
(cid:74)
P

𝑒 (cid:66) D

(E
Leaf(𝑥 𝑑 𝜎)
𝑥)
(subsenv 𝑒 𝜎)
𝑒 (cid:66) let 𝑍 be (cid:205)𝑚
(cid:74)
(cid:75)
(cid:74)
(cid:75)
(𝑆1 𝑤1) ⊕ · · · ⊕ (𝑆𝑚 𝑤𝑚)
𝑖=𝑖 𝑤𝑖
in (cid:205)𝑚
(cid:74)
(cid:75)
𝑖=1 (P
𝑆𝑖
(cid:74)
𝑆1 ⊗ · · · ⊗ 𝑆𝑚
(cid:74)
▷ (𝑡 in 𝑣) ⇒ let 𝑛 be min{1 ≤𝑖 ≤𝑚 | (vars 𝑒) ⊂ (scope 𝑆𝑖 ) }

𝑒 (cid:66) match (dnf 𝑒)

𝑒)𝑤𝑖 /𝑍

(cid:75)

(cid:75)

P

[Leaf ]
[Sum]

[Product]

(cid:75)

𝑒

𝑆𝑛
(cid:74)
▷ (𝑒1 ⊓ · · · ⊓ 𝑒ℓ ) ⇒
match {1 ≤ 𝑗 ≤ ℓ | (vars 𝑒 𝑗 ) ⊂ (scope 𝑆𝑖 ) }


▷ {𝑛1, . . . , 𝑛𝑘 } ⇒ P
(𝑒𝑛1 ⊓ · · · ⊓ 𝑒𝑛𝑘 )


▷ { } ⇒ 1


▷ (𝑒1 ⊔ · · · ⊔ 𝑒ℓ ) ⇒

𝑆𝑖
(cid:74)
(cid:75)
(−1) |𝐽 |−1 P

1≤𝑖 ≤𝑚

(cid:214)

∑︁

(cid:104)

𝑆1 ⊗ · · · ⊗ 𝑆𝑚
(cid:74)

(cid:75)

𝐽 ⊂ [ℓ ]







(⊓𝑖∈𝐽 𝑒𝑖 )

(cid:105)

(f) Sum-Product Expressions (Distribution Semantics)

Listing 1. Syntax and semantics of a core calculus for sum-product expressions and related domains.

of an Event takes a variable 𝑥 and returns the set 𝑣 ∈ Outcomes
of elements that satisfy the predicate along dimension 𝑥,
leveraging the properties of preimg. This domain specifies
measurable sets of an 𝑛-dimensional distribution on variables
{𝑥1, . . . , 𝑥𝑛 } as follows: let 𝜎gen ({𝐴1, 𝐴2, . . . }) be the sigma-
algebra generated by 𝐴1, 𝐴2, . . . , and define B𝑛 (Outcome)
(cid:66) 𝜎gen({(cid:206)𝑛
| ∀1≤𝑖 ≤𝑛. 𝑈𝑖 ∈ B (Outcomes)}). In other
𝑖=1
words, B𝑛 (Outcome) is the 𝑛-fold product sigma-algebra
generated by open rectangles of Outcomes. Any 𝑒 ∈ Event
specifies a measurable set 𝑈 in B𝑛 (Outcome), whose 𝑖th
coordinate 𝑈𝑖 = E
𝑥𝑖 if 𝑥𝑖 ∈ vars 𝑒, and 𝑈𝑖 = Outcomes
otherwise. Any Transform in 𝑒 is solved and any Var that
does not appear in 𝑒 is marginalized, as in the next example.

𝑒
(cid:74)

𝑈𝑖

(cid:75)

Example 3.2. Let {X, Y, Z} be elements of Var. Then
𝑒 (cid:66) Reciprocal(Id(X)) in ((#f 1) (2 #f))

corresponds to the B3(Outcome)-measurable set

{ (cid:0)↓ Real

Outcome

𝑟 (cid:1) | 1/2 ≤ 𝑟 ≤ 1} × Outcomes × Outcomes.

As in Remark 3.1, we may speak about probability distri-
butions on B𝑛 (Outcome) as mappings from Event to [0, 1].

Primitive Distributions Lst. 1e presents the primitive dis-
tributions out of which multivariate distributions are con-
structed. The CDF domain contains cumulative distribution
functions 𝐹 , whose quantile function is denoted 𝐹 −1 and
derivative 𝐹 ′. CDF is in 1-1 correspondence with all distri-
butions and random variables on Real [7, Thms 12.4, 14.1].
The Distribution domain specifies continuous real, atomic

PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

(−∞, 1)

.69

+

.31

[1, ∞)

X ~ normal(0, 2)
if X < 1:
Z ~ -X**3 + X**2 + 6*X
else:
Z ~ 5*sqrt(X) + 11

𝑋 ∼𝑁 (0, 2)

𝑋 ∼𝑁 (0, 2)

𝑍

−𝑋 3+𝑋 2+6𝑋

𝑍
√
𝑋 +11

5

+

[−2.2, −2]

.16

.49

.35

[0, 0.32]

[3.2, 4.8]

condition

Z**2 <= 4
and Z >= 0

𝑋 ∼𝑁 (0, 2)

𝑋 ∼𝑁 (0, 2)

𝑋 ∼𝑁 (0, 2)

𝑍

𝑍

−𝑋 3+𝑋 2+6𝑋

−𝑋 3+𝑋 2+6𝑋

5

𝑍
√

𝑋 +11

(a) Prior Program

(b) Prior Sum-Product Expression

(c) Condition

(d) Conditioned Sum-Product Expression

(e) Prior Marginal Distributions

(f) Conditioned Marginal Distributions
Figure 4. Inference on a stochastic many-to-one transformation of a real random variable in Sppl.

(cid:75)

𝑑
(cid:74)

real (on the integers) and nominal distributions. The deno-
tation D
of a Distribution is a distribution on Outcomes
(recall Remark 3.1). For example, DistR(𝐹 𝑟1 𝑟2) is the restric-
tion of 𝐹 to a positive measure interval [𝑟1, 𝑟2]. The distribu-
tions specified by DistR and DistI can be simulated using
a variant of the integral probability transform (Prop. A.1 in
Appx. A), which also defines their sampling semantics.

Sum-Product Expressions Lsts. 1d and 1f show the proba-
bility density and distribution semantics of the SPE domain,
respectively, whose elements are probability measures. The
following conditions specify well-definedness for SPE:
(C1) ∀ Leaf(𝑥 𝑑 𝜎). 𝑥 ∈ 𝜎 and 𝜎 (𝑥) = Id(𝑥).
(C2) ∀ Leaf(𝑥 𝑑 𝜎). If dom(𝜎) = {𝑥, 𝑥1, . . . , 𝑥𝑚 } for some
𝑚 > 0 then ∀1≤𝑡 ≤𝑚. (vars 𝜎 (𝑥𝑡 )) ⊂ {𝑥, 𝑥1, . . . , 𝑥𝑡 −1}.
(C3) ∀(𝑆1 ⊗ · · · ⊗ 𝑆𝑚). ∀𝑖≠𝑗. (scope 𝑆𝑖 ) ∩ (scope 𝑆 𝑗 ) = ∅.
(C4) ∀(𝑆1 𝑤1) ⊕ · · · ⊕ (𝑆𝑚 𝑤𝑚). ∀𝑖. (scope 𝑆𝑖 ) = (scope 𝑆1).
(C5) ∀(𝑆1 𝑤1) ⊕ · · · ⊕ (𝑆𝑚 𝑤𝑚). 𝑤1 + · · · + 𝑤𝑛 > 0.

In Lst. 1f, the denotation P

For Leaf, (C1) ensures that 𝜎 maps the leaf variable 𝑥 to
the Identity Transform and (C2) ensures there are no cyclic
dependencies or undefined variables in Environment 𝜎. Con-
dition (C3) ensures the scopes of all children of a Product
are disjoint and (C4) ensures the scopes of all children of a
Sum are identical, which together ensure completeness and
decomposability from sum-product networks [47, Defs. 4, 5].
of 𝑆 ∈ SPE is a map from 𝑒 ∈
𝑆
(cid:74)
(cid:75)
Event to its probability under the 𝑛-dimensional distribution
defined by 𝑆, where 𝑛 (cid:66) |scope 𝑆 | is the number of variables
in 𝑆. A terminal node Leaf(𝑥 𝑑 𝜎) is comprised of a Var 𝑥,
Distribution 𝑑, and Environment 𝜎 that maps other variables
to a Transform of 𝑥, e.g., Z ↦→ Poly(Root(Id(X) 2) [11, 5]).
When assessing the probability of 𝑒 at a Leaf, subsenv
(Lst. 13 in Appx. A) rewrites 𝑒 as an Event 𝑒 ′ on one variable
𝑥, so that the probability of Outcomes that satisfy 𝑒 is ex-
actly D
𝑥). The scope function (Lst. 12 in Appx. A)
returns the list of variables in 𝑆. For a Sum, the probability of

𝑒 ′
(cid:74)

𝑑
(cid:74)

(E

(cid:75)

(cid:75)

In Lst. 1d, the denotation P0

𝑒 is a weighted average of the probabilities under each subex-
pression. For a Product, the semantics are defined in terms of
(dnf 𝑒) (Lst. 15 in Appx. B), leveraging inclusion-exclusion.
defines the density se-
mantics of SPE, used for measure zero events such as {𝑋 =
3, 𝑌 = 𝜋, 𝑍 = "foo"} under a mixed-type base measure.
These semantics, which define the density as a pair, adapt
“lexicographic likelihood-weighting”, an approximate infer-
ence algorithm for discrete-continuous Bayes Nets [68], to
exact inference using “lexicographic enumeration” for SPE.

𝑆
(cid:74)

(cid:75)

4 Conditioning Sum-Product Expressions

We next present the main theoretical result for exact infer-
ence on probability distributions defined by an expression
𝑆 ∈ SPE and describe the inference algorithm for condition-
ing on an Event (Lst. 1c) in the core calculus, which includes
transformations and predicates with set-valued constraints.

Theorem 4.1 (Closure under conditioning). Let 𝑆 ∈ SPE and
𝑒 ∈ Event be given, where P
𝑒 > 0. There exists an algo-
𝑆
(cid:74)
rithm which, given 𝑆 and 𝑒, returns 𝑆 ′ ∈ SPE such that, for all
𝑒 ′ ∈ Event, the probability of 𝑒 ′ according to 𝑆 ′ is equal to the
conditional probability of 𝑒 ′ given 𝑒 according to 𝑆, i.e.,

(cid:75)

𝑒 ′ ≡ P

P

𝑆 ′
(cid:74)

(cid:75)

𝑆
(cid:74)

(cid:75)

(𝑒 ′ | 𝑒) (cid:66)

P

𝑆
(cid:75)
(cid:74)
P

(𝑒 ⊓ 𝑒 ′)
𝑒
𝑆
(cid:74)

(cid:75)

.

(5)

Thm. 4.1 is a structural conjugacy property [20] for the
family of probability distributions defined by the SPE domain,
where both the prior and posterior are identified by elements
of SPE. We establish Eq. (5) constructively, by describing
a new algorithm condition : SPE → Event → SPE that
satisfies

P

(condition 𝑆 𝑒)

𝑒 ′ = P

(𝑒 ′ | 𝑒)

(6)

(cid:75)
(cid:74)
for all 𝑒, 𝑒 ′ ∈ Event with P
𝑒 > 0. Refer to Appx. D for the
(cid:75)
proof. Fig. 5 shows a conceptual example of how condition

𝑆
(cid:74)

(cid:75)

𝑆
(cid:74)

−4−2024xX∼Normal(0,2)−4−2024x−5051015zTransformationz=t(x)tif(x)=−x3+x2+6xtelse(x)=−5√x+11−5051015zZ∼t(X)−4−2024xX|(0<Z<2)−4−2024x−5051015zTransformationz=t(x)tif(x)=−x3+x2+6xtelse(x)=−5√x+11z∈[0,2]x∈t−1([0,2])−5051015zZ|(0<Z<2)SPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

×𝑆

𝑍𝑌𝑋

Appx. D

condition
(Lst. 6)

+𝑆′

𝑆1

×

...

𝑆5

×

calls
disjoin (Lst. 5)

Disjoined Region

Conditioning Region

𝑍𝑌𝑋
𝑍𝑌𝑋
Prior SPE
Conditioned SPE
Figure 5. Conditioning a Product 𝑆 on an Event 𝑒 that de-
fines a union of hyperrectangles in Real3. The inference
algorithm partitions the region into a disjoint union, in this
case converting two overlapping regions into five disjoint
regions. The result is a Sum-of-Product, where each child is
the restriction of 𝑆 to one of the disjoint hyperrectangles.

works, where the prior distribution is a Product 𝑆 and the
conditioned distribution is a Sum-of-Product 𝑆’. Fig. 4 shows
an example of the closure property when the expression has
transformed variables (details in Appx. C).

Remark 4.2. Thm. 4.1 refers to a positive probability Event
𝑒. As with sum-product networks, SPE is also closed un-
der conditioning on a Conjunction of possibly measure zero
equality constraints on non-transformed variables, which ap-
pear in many PPL interfaces [17, 41, 51]. Appx. D.3 presents
the condition0 algorithm for inference on such events, lever-
aging the generalized mixed-type density semantics in Lst. 1d.

The next result, Thm. 4.3, states a sufficient requirement
for inference using (condition 𝑆 𝑒) to scale linearly in the size
of 𝑆, which holds for both zero and positive measure events.

Theorem 4.3. The runtime of (condition 𝑆 𝑒) scales linearly
in the number of nodes in the graph representing 𝑆 when-
ever 𝑒 is a single Conjunction (𝑡1 in 𝑣1) ⊓ · · · ⊓ (𝑡𝑚 in 𝑣𝑚) of
Containment constraints on non-transformed variables.

5 Translating Probabilistic Programs to

Sum-Product Expressions

𝑆
(cid:74)

We next present a probabilistic language called Sppl and
show how to translate each program in the language to an
element 𝑆 ∈ SPE that symbolically represents (via P
) the
(cid:75)
probability distribution specified by the program. As in Fig. 1,
𝑆 can then be used to answer queries about an Event 𝑒:
simulate: Samples from the distribution defined by P
prob: Computes the probability of 𝑒, using P
condition: Conditions on 𝑒, using condition (Eq. (6)).
Lst. 2 shows the source syntax of Sppl, which contains stan-
dard constructs of an imperative language such as array
data structures, if-else statements, and bounded for loops.
The switch-case macro is defined in Eq. (4). Random vari-
ables are defined using “sample” (~) and condition(𝐸) can
be used to restrict executions to those for which 𝐸 ∈ Expr
evaluates to #t as part of the prior definition. Lst. 3 de-
fines a relation ⟨𝐶, 𝑆⟩ →SPE 𝑆 ′, which translates a “current”
𝑆 ∈ SPE and 𝐶 ∈ Command into 𝑆 ′ ∈ SPE, where the initial

𝑆
;
(cid:74)
(cid:75)
𝑒 (Lst. 1f);

𝑆
(cid:74)

(cid:75)

𝑥 ∈ Var; 𝑦 ∈ ArrayVar; 𝑛 ∈ Natural; 𝑏 ∈ Boolean; 𝑟 ∈ Real; 𝑠 ∈ String;
𝑜arith ∈ {+, -, *, /, **};
𝑜bool ∈ {and, or};
𝑜rel ∈ {<=, <, >, >=, ==, in}; 𝐷 ∈ {normal, poisson, choice, . . . };
𝐸 ∈ Expr (cid:66) 𝑥 | 𝑛 | 𝑏 | 𝑟 | 𝑠 | 𝑦[𝐸] | 𝐷(𝐸∗) | (𝐸1, . . . , 𝐸𝑚)

𝑜neg ∈ {not};

| 𝐸1 𝑜arith 𝐸2 | 𝑜neg 𝐸 | 𝐸1 𝑜bool 𝐸2 | 𝐸1 𝑜rel 𝐸2

𝐶 ∈ Command (cid:66) 𝑥 = 𝐸 | 𝑦[𝐸1] = 𝐸2 | 𝑥 ~ 𝐸 | 𝑦[𝐸1] ~ 𝐸2 | 𝑦 = array(𝐸)

| skip | 𝐶1;𝐶2 | if 𝐸 {𝐶1} else {𝐶2} | condition(𝐸)
| for 𝑥 in range(𝐸1, 𝐸2) {𝐶} | switch 𝑥1 cases (𝑥2 in 𝐸) {𝐶}

Listing 2. Source syntax of Sppl.

(Sample)
𝐸 ⇓ 𝑑;

where 𝑥 ∉ scope 𝑆

⟨𝑥 ~ 𝐸, 𝑆 ⟩ →SPE 𝑆 ⊗ (𝑥 𝑑 {𝑥 ↦→ Id(𝑥)})

(Transform-Leaf)

𝐸 ⇓ 𝑡 ; where vars 𝑡 ∈ dom(𝜎), 𝑥 ∉ dom(𝜎)
⟨𝑥 = 𝐸, Leaf(𝑥 ′ 𝑑 𝜎)⟩ →SPE Leaf(𝑥 ′ 𝑑 (𝜎 ∪ {𝑥 ↦→ 𝑡 }))

(Transform-Sum)

𝐸 ⇓ 𝑡, ∀1≤𝑖 ≤𝑚 . ⟨𝑥 = 𝐸, 𝑆𝑖 ⟩ →SPE 𝑆′
𝑖
𝑖 𝑤𝑖 )

(𝑆𝑖 𝑤𝑖 ) →SPE ⊕𝑚
𝑖=1

⟨𝑥 = 𝐸, ⊕𝑚
𝑖=1

(𝑆′

(Transform-Prod)
𝐸 ⇓ 𝑡, ⟨𝑥 = 𝐸, 𝑆 𝑗 ⟩ →SPE 𝑆′

𝑗 ; where 𝑗 (cid:66) min{𝑖 | (vars 𝐸) ∈ scope 𝑆𝑖 } > 0
𝑖=1,𝑖≠𝑗 𝑆𝑖 ⊗ 𝑆′
𝑗

𝑆𝑖 ⟩ →SPE ⊗𝑚

⟨𝑥 = 𝐸, ⊗𝑚
𝑖=1

(IfElse)
𝐸 ⇓ 𝑒, ⟨𝐶1, condition 𝑆 𝑒 ⟩ →SPE 𝑆1, ⟨𝐶2, condition 𝑆 (negate 𝑒) ⟩ →SPE 𝑆2

⟨if 𝐸 {𝐶1} else {𝐶2}, 𝑆 ⟩ →SPE (𝑆1 P

(For-Repeat)

𝑆
(cid:74)

𝐸1 ⇓ 𝑛1, 𝐸2 ⇓ 𝑛2;
⟨for 𝑥 in range(𝐸1, 𝐸2) {𝐶}, 𝑆 ⟩

𝑒) ⊕ (𝑆2 (1 − P

(cid:75)
where 𝑛1 < 𝑛2

𝑒))

𝑆
(cid:74)

(cid:75)

→SPE ⟨𝐶 [𝑥/𝑛1 ]; for 𝑥 in range(𝑛1 + 1, 𝐸2) {𝐶}, 𝑆 ⟩

Listing 3. Example rules for translating an Sppl command
𝐶 (Lst. 2) to an element of SPE (Lst. 1f).

step operates on an “empty” 𝑆. (Lst. 8 in Appx. E defines a
semantics-preserving inverse of →SPE). The ⇓ relation eval-
uates 𝐸 ∈ Expr to other domains in the core calculus using
straightforward rules. We briefly describe key rules of →SPE:
(Transform-Leaf) updates the environment 𝜎 at each Leaf.
(Transform-Sum) delegates to all subexpressions.
(Transform-Prod) delegates to the subexpression whose

scope contains the transformed variable.

(For-Repeat) unrolls a for loop into a Command sequence.
(IfElse) returns a Sum with two subexpressions, where the
if branch is conditioned on the test Event and the else
branch is conditioned on the negation of the test Event.
This translation step involves running inference (using
condition, Eq. (6)) on the current 𝑆 ∈ SPE translated so far.
The rule for condition(𝐸) (not shown) calls (condition 𝑆 𝑒)
(Eq. 6) or (condition0 𝑆 𝑒) (Remark 4.2), where 𝐸 ⇓ 𝑒. To en-
sure Sppl programs translate to well-defined element of SPE,
per (C1)–(C5)), each program must satisfy these restrictions:
(R1) Variables 𝑥 in 𝑥 ~ 𝐸 (Sample) and 𝑥 = 𝐸 (Transform-
Leaf) must be fresh (ensures conditions (C1), (C2) and (C3)).
(R2) The branches in an if-else statement must define iden-
tical variables (ensures conditions (C4) and (C5)).

PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

(a) Invalid program (translates to an infinite-sized SPE)

mu ~ beta(a=4, b=3, scale=7)
num_loops ~ poisson(mu)
for i in range(0, num_loops): # invalid (infinite series)

# invalid (real integral)

[... commands ... ]

(b) Valid program (translates to a finite-sized SPE)

mu ~ beta(a=4, b=3, scale=7)
# binspace partitions [0,7] into 10 intervals
switch (mu) cases (m in binspace(0, 7, n=10)):

num_loops ~ poisson(m.mean()) # discretization

condition (num_loops < 50)
switch num_loops cases (n in range(50)):

# truncation

for i in range(0, n):
[... commands ... ]

Listing 4. Examples of valid and invalid Sppl programs.

(R3) Derived random variables are obtained via (many-to-
one) univariate transformations (Lst. 1b).
(R4) Parameters of distributions 𝐷 or range must be either
constants or random variables with finite support.

(R3) is required since the distribution of a multivariate
transform (e.g., 𝑍 = 𝑋 /𝑌 2) is typically intractable and does
not factor into Sum and Product expressions. (R4) is required
to ensure a finite-size SPE: distributional parameters with
infinite support require integrals (uncountable support) or in-
finite series (countable support), which are not in SPE. Lst. 4
shows an example of using switch and condition to work
around these restrictions by discretization and truncation.

5.1 Building Compact Sum-Product Expressions

As discrete Bayesian networks can be encoded as Sppl pro-
grams, it is possible to write programs where exact inference
is NP-Hard [16], which corresponds to an element of SPE that
is exponentially large. It is well known that the complexity of
exact inference in Bayesian networks is worst case exponen-
tial in the treewidth, which is the only structural restriction
that can ensure tractability [12]. As computing treewidth is
NP-Complete [3], for fundamental theoretical reasons we
cannot generally check conditions needed for even simple
Sppl programs, such as those that only use if/else statements
on binary variables, to translate into a “small” expression.

However, many models of interest contain (conditional)
independence relationship [33] that induce a compact factor-
ization of the model into tractable subparts, as in, e.g., Sec. 2.2.
Sppl uses several optimization techniques to improve scala-
bility of translation (Lst. 3) and inference (Eq. (6)) by auto-
matically exploiting independences and repeated structure,
when they exist, to build compact sum-product expressions.

Factorization Using standard algebraic manipulations, a
sum-product expression can be made smaller without chang-
ing its semantics (Lst. 1f) by “factoring out” common terms
(Fig. 6a), provided that the new expression satisfies (C1)–(C5).
Factorization plays a key role in the (IfElse) rule of →SPE:
since all statements before the if-else are shared by the

Original

+

Factorized

×

Original

+

Deduplicated

+

×

×

×

×

×

𝑆

+

𝑋

+

𝑆

𝑌𝑋

𝑋

+

𝑆

𝑆1

𝑆

𝑆 ′
1

𝑆1

𝑆 ′
1

. . .

×

𝑌

𝑆

. . .

×

𝑌

𝑆

×

𝑌𝑋

(a) Factorization

(b) Deduplication

Figure 6. Exploiting independences and repeated structure
during translation of Sppl programs to build compact sum-
product expressions. Blue subtrees are identical components.
Table 1. Measurements of SPE graph size with and without
the factorization and deduplication optimizations in Fig. 6.
Data Compression
Ratio (unopt/opt)

No. of Nodes in Translated SPE

Benchmark

Unoptimized

Optimized

Hiring [2]
Alarm [46]
Grass [46]
Noisy OR [46]
Clinical Trial [46]
Heart Disease [56]
Hierarchical HMM (Sec. 2.2)

33
58
130
783
43761
1041235
29273397577908185

27
45
59
132
4131
6257
1787

1.2x
1.3x
2.2x
4.1x
10.6x
166.4x
16381308101795x

bodies of the if and else branches, statements outside the
branch that are independent of statements inside the branch
often produce subexpressions that can be factored out.

Deduplication When a sum-product expression contains
duplicate subexpressions that cannot be factored out without
violating the definedness conditions, we instead resolve du-
plicates into a single physical representative. Fig. 6b shows
an example where the left and right components of the origi-
nal expression contain an identical subexpression 𝑆 (in blue),
but factorization would lead to an invalid sum-product ex-
pression. The optimizer represents the computation graph
of this expression using a single data structure 𝑆 shared by
the left and right subtrees (see also Figs. 3c–3d).

Memoization While deduplication reduces memory over-
head, memoization is used to reduce runtime overhead. Con-
sider either SPE in Fig. 6b: calling condition on the Sum root
will dispatch the query to the left and right subexpressions
(Lst. 6b). We cache the results of (condition 𝑆 𝑒) or P
𝑒
(cid:75)
when 𝑆 is visited in the left subtree to avoid recomputing the
result when 𝑆 is visited again in the right subtree via a depth-
first traversal. Memoization delivers large runtime gains not
only for solving queries but also for detecting duplicates
returned by condition in the (IfElse) translation step.

𝑆
(cid:74)

Measurements Table 1 shows measurements of performance
gains delivered by the factorization and deduplication opti-
mizations on seven benchmarks. Compression ratios range
between 1.2x to 1.64×1013x and are highest in the presence of
independence or repeated structure. The deduplication and
memoization optimizations together enable fast detection of
duplicate subtrees by comparing logical memory addresses
of internal nodes in 𝑂 (1) time, instead of computing hash
functions that require an expensive subtree traversal.

SPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

Table 2. Runtime measurements and speedup for 15 fairness verification tasks using Sppl, FairSquare [2], and VeriFair [4].
Decision
Program

Population
Model

Fairness
Judgment

Sppl Speedup Factor

Lines
of Code

DT4

DT14

DT16

DT𝛼
16

DT44

Independent
Bayes Net. 1
Bayes Net. 2
Independent
Bayes Net. 1
Bayes Net. 2
Independent
Bayes Net. 1
Bayes Net. 2
Independent
Bayes Net. 1
Bayes Net. 2
Independent
Bayes Net. 1
Bayes Net. 2

15
25
29
32
46
50
36
49
53
62
58
45
93
109
113

Unfair
Unfair
Unfair
Fair
Fair
Fair
Fair
Unfair
Unfair
Fair
Fair
Fair
Fair
Unfair
Unfair

Wall-Clock Runtime (seconds)
VeriFair
16.0
1.27
0.91
105
152
151
13.6
1.58
2.02
2.01
21.6
24.5
23.1
19.8
20.1

FairSquare
1.4
2.5
6.2
2.7
15.5
70.1
4.1
12.3
30.3
5.1
15.4
53.8
15.6
264.1
t/o

Sppl
0.01
0.03
0.03
0.03
0.07
0.08
0.03
0.08
0.08
0.06
0.12
0.12
0.05
0.09
0.09

vs. FairSquare
140x
83x
206x
90x
221x
876x
136x
153x
378x
85x
128x
448x
312x
2934x
—

vs. VeriFair
1600x
42x
30x
3500x
2171x
1887x
453x
19x
25x
33x
180x
204x
462x
220x
223x

6 Evaluation
We implemented a prototype of Sppl2 and evaluated its
performance on benchmark problems from the literature.
Sec. 6.1 compares the runtime of verifying fairness prop-
erties of decision trees using Sppl to FairSquare [2] and
VeriFair [4], two state-of-the-art fairness verification tools.
Sec. 6.2 compares the runtime of conditioning and querying
probabilistic programs using Sppl to PSI [23], a state-of-the-
art tool for exact symbolic inference. Sec. 6.3 compares the
runtime of computing exact rare event probabilities in Sppl
to sampling-based estimation in BLOG [40]. Experiments
were run on Intel i7-8665U 1.9GHz CPU with 16GB RAM.

6.1 Fairness Benchmarks

Characterizing the fairness of classification algorithms is
a growing application area in machine learning [21]. Re-
cently, Albarghouthi et al. [2] precisely cast the problem of
verifying the fairness of a classifier in terms of computing
ratios of conditional probabilities in a probabilistic program
that specifies the data generating and classification processes.
Briefly, if (i) 𝐷 is a decision program that classifies whether
applicant 𝐴 should be hired; (ii) 𝐻 is a population program
that generates random applicants; and (iii) 𝜙m (resp. 𝜙q) is a
predicate on 𝐴 that is true if the applicant is a minority (resp.
qualified), then 𝐷 is 𝜖-fair on 𝐻 (where 𝜖 > 0) if
Pr𝐴∼𝐻 (cid:2)𝐷 (𝐴) | 𝜙m(𝐴) ∧ 𝜙q(𝐴)(cid:3)
Pr𝐴∼𝐻 (cid:2)𝐷 (𝐴) | ¬𝜙m(𝐴) ∧ 𝜙q(𝐴)(cid:3)

> 1 − 𝜖,

(7)

i.e., the probability of hiring a qualified minority applicant is
𝜖-close to that of hiring a qualified non-minority applicant.
In this evaluation, we compare the runtime needed by Sppl
to obtain a fairness judgment (Eq. (7)) for machine-learned
decision and population programs against the FairSquare [2]
and VeriFair [4] solvers. We evaluate performance on the de-
cision tree benchmarks from Albarghouthi et al. [2, Sec. 6.1],

2Available in supplement and online at https://github.com/probcomp/sppl.

which are one-third of the full benchmark set. Sppl cannot
solve the neural network and support-vector machine bench-
marks, as they contain multivariate transforms which do not
have exact tractable solutions and are ruled out by the Sppl
restriction (R3). FairSquare and VeriFair can express these
benchmarks as they have approximate inference.

Table 2 shows the results. The first column shows the
decision making program (DT𝑛 means “decision tree” with
𝑛 conditionals); the second column shows the population
model used to generate data; the third column shows the
lines of code (in Sppl); and the fourth column shows the
result of the fairness analysis (FairSquare, VeriFair, and Sppl
produce the same judgment on all fifteen benchmarks). The
remaining columns show the runtime and speedup factors.
We note that Sppl, VeriFair, and FairSquare are all imple-
mented in Python, which allows for a fair comparison. The
measurements indicate that Sppl consistently obtains proba-
bility estimates in milliseconds, whereas the two baselines
can each require over 100 seconds. The Sppl speedup factors
are up to 3500x (vs. VeriFair) and 2934x (vs. FairSquare). We
further observe that the runtimes in FairSquare and VeriFair
vary significantly. For example, VeriFair uses rejection sam-
pling to estimate Eq. (7) with a stopping rule to determine
when the estimate is close enough, leading to unpredictable
runtime (e.g., >100 seconds for DT14 but <1 second for DT4,
Bayes Net. 2). FairSquare, which uses symbolic volume com-
putation and hyperrectangle sampling to approximate Eq. (7),
is faster than VeriFair in some cases (e.g., DT14), but times
out in others (DT44, Bayes Net. 2). In contrast, Sppl, com-
putes exact probabilities for Eq. (7) and its runtime does not
vary significantly across the various benchmark problems.
The performance–expressiveness trade-off here is that Sppl
computes exact probabilities and is substantially faster on
the decision tree problems that it can express. FairSquare
and VeriFair compute approximate probabilities that enable
them to express more fairness problems, at the cost by of a
higher and less predictable runtime on the decision trees.

PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

sample(vars)
. . .
condition(constraint)
. . .
sample(vars)

Sppl Program
(Prior)

sample(variables)
sample(vars)
. . .
. . .
condition(constraint)
condition(constraint)
condition(data)
condition(data)
. . .
. . .
return query
return query

PSI Program
(Prior+Data+Query)

Sppl Program (Data)

Sppl Program (Query)

condition(data)

query

Sppl
Translator

†

Sum
Product
Expression

Sppl
Inference
Engine

‡

Sum
Product
Expression

★

Sppl
Inference
Engine

(Prior)

(Posterior)

(a) Multi-Stage Workflow in Sppl

0.17123
Interpretable
Result

†‡

PSI

(b) Single-Stage Workflow in PSI

Computer Algebra Expression
(Posterior)

Figure 7. Comparison of multi-stage and single-stage in-
ference workflows. In Sppl, modeling, observing data, and
querying are separated into distinct stages, enabling substan-
tial efficiency gains from computation reuse across multiple
datasets and/or queries, as opposed to single-stage work-
flows such as in PSI which combine all these tasks into one
large symbolic computation (daggers/colors used in Table 4).

6.2 Comparison to Symbolic Integration

We next compare Sppl to PSI [23], a state-of-the-art sym-
bolic inference engine, on benchmark problems that include
discrete, continuous, and transformed random variables. PSI
can express more inference problems than Sppl, as it uses
general computer algebra without having restrictions (R3)
and (R4) in Sppl. As a result, Sppl can solve 14/21 bench-
marks listed in [23, Table 1]. We first discuss key architecture
novelties in Sppl that contribute to its performance gains.

Workflow Comparison In Sppl, the multi-stage modeling
and inference workflow (Fig. 7a) involves three steps that
reflect the key elements of a Bayesian inference problem:
(S1) Translating the model program into a prior SP 𝑆.
(S2) Conditioning 𝑆 on data to obtain a posterior SP 𝑆 ′.
(S3) Querying 𝑆 ′, using, e.g., prob or simulate.
An advantage of this multi-stage workflow is that multiple
tasks can be run at a given stage without rerunning previ-
ous stages. For example, multiple datasets can be observed
in (S2) without translating the prior expression in (S1) once
per dataset; and, similarly, multiple queries can be run in (S3)
without conditioning on data in (S2) once per query. In con-
trast, PSI adopts a single-stage workflow (Fig. 7b), where a
single program contains the prior distribution over variables,
“observe” (i.e., “condition”) statements for conditioning on a
dataset, and a “return” statement for the query. PSI converts
the program into a symbolic expression for the distribution
over the return value: if this expression is “complete” (i.e.,
no unevaluated symbolic integrals) it can be used to obtain
interpretable answers (e.g., for plotting or tabulating); oth-
erwise, the result is “partial” and is too complex to be used
for practical purposes. A consequence of the single-stage
workflow in a system like PSI is that the entire solution is
recomputed from scratch on a per-dataset or per-query basis.

Table 3. Distribution of end-to-end inference runtime for
four benchmarks from Table 4 using PSI [23] and Sppl.
Mean/Std Runtime (sec/sec)

Benchmark

Digit Recognition
Markov Switching
Student Interviews
Clinical Trial

PSI
26.5/1.3
22.5/3.8
539/663
107.3/153.2

Sppl
15.9/0.5
0.1/0.0
7.8/0.2
12.7/0.3

Runtime Comparison Table 4 compares the runtime of
Sppl and PSI on seven benchmarks problems: Digit Recog-
nition [23]; TrueSkill [36]; Clinical Trial [23]; Gamma trans-
forms (described below); Student Interviews [36] (two vari-
ants); and Markov Switching (two variants, from Sec. 2.2);
The second column shows the distributions in each bench-
mark, which include continuous, discrete, and transformed
variables. The third column shows the number of datasets
on which to condition the program. The next three columns
show the time needed to translate the program (stage (S1)),
condition the program on a dataset (stage (S2)), and query
the posterior (stage (S3))—entries in the latter two columns
are written as 𝑛 × 𝑡, where 𝑛 is the number of datasets and 𝑡
the average time per dataset. For PSI: (i) modeling and ob-
serving data are a single stage, shown in the merged gray
cell; and (ii) querying the posterior times out whenever the
system returns a result with unsimplified integrals (⋉). The
last column shows the overall runtime for solving all 𝑛 tasks.
For benchmarks that both systems solve completely, Sppl
realizes speedups between 3x (Digit Recognition) to 3600x
(Markov Switching3). In addition, the measurements show
the advantage of our multi-stage workflow; for example, in
TrueSkill, which uses a Poisson–Binomial distribution, Sppl
translation (3.4 seconds) is more expensive than both con-
ditioning on data (0.7 seconds) and querying (0.1 seconds),
which highlights the benefit of amortizing the translation
cost over several datasets or queries. In PSI, solving TrueSkill
takes 2 × 41.6 seconds, but the solution contains unsimplified
integrals and is thus unusable. The Markov Switching and
Student Interviews benchmarks show that PSI may not per-
form well in the presence of many discrete random variables.
The Gamma Transform benchmark tests the robustness of
many-to-one transforms of random variables (Lst. 1b), where
𝑋 ∼ Gamma(3, 1); 𝑌 = 1/exp 𝑋 2 if 𝑋 < 1 and 𝑌 = 1/ln 𝑋 oth-
erwise; and 𝑍 = − 𝑌 3 + 𝑌 2 + 6𝑌 . Each of the 𝑛 = 5 datasets
specifies a different constraint 𝜙 (𝑍 ) and a query about the
posterior 𝑌 | 𝜙 (𝑍 ), which needs to compute and integrate out
𝑋 | 𝜙 (𝑍 ). PSI reports that there is an error in its answer for
all five datasets, whereas Sppl, using the symbolic transform
solver from Appx. C.2, solves all five problems effectively.

Table 3 compares the runtime variance using Sppl and PSI
for four of the benchmarks in Table 4, repeating one query
over 10 datasets. In all benchmarks, the Sppl variance is
lower than that of PSI, with a maximum standard deviation
𝜎 = 0.5 sec. In contrast, the spread of PSI runtime is high

SPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

Table 4. Runtime comparison of PSI [23] and Sppl.

Benchmark Distribution Datasets ⋄ System

Digit
Recognition

C×B784

TrueSkill

P×Bi2

Clinical
Trial
Gamma
Transforms
Student
Interviews2
Student
Interviews10
Markov
Switching3
Markov
Switching100

B×U3
×B50×B50
G×T
×(T+T)
P×B2×Bi4
×(A+Be)2
P×B10×Bi20
×(A+Be)10
B×B3
×N3×P3
B×B100
×N100×P100

10

2

10

5

10

10

10

10

Sppl
PSI
Sppl
PSI
Sppl
PSI
Sppl
PSI
Sppl
PSI
Sppl
PSI
Sppl
PSI
Sppl
PSI

Translation † Conditioning ‡
10 × 7.7 sec

6.9 sec

9.5 sec

3.4 sec

0.02 sec

2 × 0.7 sec

10 × 2.2 sec

10 × 24.3 sec

2 × 41.60 sec

10 × 107.3 sec

Wall-Clock Runtime of Inference Stages
Querying ★
10 × (<0.01 sec)
10 × (<0.01 sec)
2 × 0.1 sec
⋉
10 × (<0.01 sec)
10 × (<0.01 sec)
5 × 0.03 sec
⋉
10 × 0.2 sec
⋉
10 × 1.2 sec
⊘
10 × (<0.01 sec)
10 × (<0.01 sec)
10 × 0.5 sec
⊘

10 × 540 sec; h/m (35GB)

10 × (<0.01 sec)

5 × 0.68 sec; i/e

10 × 182.9 sec

5 × 0.52 sec

10 × 0.7 sec

10 × 3.9 sec

10 × 6.5 sec

o/m (64GB+)

24.6 sec

0.05 sec

o/m (64GB+)

4.1 sec

4.0 sec

Overall
Time

84 sec
244 sec
4.9 sec
⊘
31 sec
1073 sec
2.8 sec
⊘
13.5 sec
⊘
75 sec
⊘
0.5 sec
1829 sec
74 sec
⊘

Legend

B: Bernoulli

A: Atomic
Bi: Binomial C: Categorical
N: Normal
G: Gamma
T: Transform U: Uniform

Be: Beta

P: Poisson

⋄ Number of distinct datasets on which to
condition the benchmark program.
†, ‡ Runtime of first two phases in Fig. 7;
PSI implements these phases in a single
computation.
★ Runtime of final phase in Fig. 7; same
query used for all datasets of a given bench-
mark program.

h/m High-Memory
o/m Out-of-Memory
Integration Error
i/e
⋉
Unsimplified Symbolic Integrals
⊘
No Value

for Student Interviews (𝜎 = 540 sec, range 64–1890 sec) and
Clinical Trial (𝜎 = 153 sec, range 2.75–470 sec). In PSI, the
symbolic analyses are sensitive to the numeric values in the
dataset, leading to unpredictable runtime across different
datasets, even for a fixed query pattern. In Sppl, the runtime
depends only on the query pattern not the observed data
and therefore behaves predictably across different datasets.
As with the fairness benchmarks in Sec. 6.1, PSI trades
off expressiveness with efficacy on tractable problems, and
our measurements show that its runtime and memory do
not scale well or are unpredictable on benchmarks that Sppl
solves very efficiently. Moreover, the evaluations show that
PSI can return unusable inference results to the user and
that it needs to recompute entire symbolic solutions from
scratch for each new dataset or query, whereas Sppl is less
expressive than PSI but carries neither of these limitations.

6.3 Comparison to Sampling-Based Estimates

We next compare the runtime and accuracy of estimating
probabilities of rare events in a canonical Bayesian net-
work [33] using Sppl and BLOG [40]. As discussed by Koller
and Friedman [33, Sec 12.13], rare events are the rule, not
the exception, in many applications, as the probability of a
predicate 𝜙 (𝑋 ) decreases exponentially with the number of
observed variables in 𝑋 . Small estimation errors can magnify
substantially when, e.g., taking ratios of probabilities.

In Fig. 8, each subplot shows the runtime and probability
estimates for a low-probability predicate 𝜙. In BLOG, the
rejection sampler estimates the probability of 𝜙 by comput-
ing the fraction of times it holds in a size 𝑛 i.i.d. random
sample from the prior. The horizontal red line shows the
“ground truth” probability. The x marker shows the runtime
needed by Sppl to (exactly) compute the probability and the
dots show the estimates from BLOG with increasing run-
time (i.e., more samples 𝑛). Sppl consistently returns an exact
answer in less than 2ms. The accuracy of BLOG estimates
improve as the runtime increases: by the strong law of large
numbers, these estimates converge to the true value, but

√

the fluctuations for any single run can be large (the stan-
𝑛). Each “jump” corresponds to a
dard error decays as 1/
new sample 𝑋 ( 𝑗) that satisfies 𝜙 (𝑋 ( 𝑗) ), which increases the
estimate. Without ground truth, it is hard to predict how
much computation is needed for BLOG to obtain accurate
results: estimates for predicates with log 𝑝 = −12.73 and
log 𝑝 = −17.32 did not converge within the allotted time,
while those for log 𝑝 = −14.48 converged after 180 seconds.

7 Related Work

Sppl is distinguished by being the first system to deliver exact
symbolic inference by translating probabilistic programs to
sum-product expressions, which extend and generalize sum-
product networks. We briefly discuss related approaches.
Symbolic Integration Several systems deliver exact infer-
ence by translating a probabilistic program and observed
dataset into a symbolic expression whose solution is the an-
swer to the query [6, 10, 23, 43, 69]. Our approach to exact
inference, which uses sum-product expressions instead of
general computer algebra, enables effective performance on
a range of models and queries, primarily at the expense of
the expressiveness of the language on continuous priors. The
state-of-the-art solver, PSI [23], can effectively solve many
inference problems that Sppl cannot express due to restric-
tions (R1)–(R4), including higher-order programs [24]. How-
ever, comparisons on benchmarks that Sppl targets (Sec. 6.2)
find PSI has less scalable and higher variance runtime, and
can return partial results with unsimplified symbolic inte-
grals. In contrast, Sppl exploits conditional independences,
when they exist, to improve scalability (Sec. 5.1) and delivers
complete, usable answers to users. Moreover, Sppl’s multi-
stage workflow (Fig. 7) allows expensive computations such
as translation and conditioning to be amortized over multiple
datasets or queries, whereas PSI recomputes the symbolic
solution from scratch each time. Hakaru [43] is a symbolic
solver that delivers exact inference in a multi-stage work-
flow based on program transformations, and can disintegrate
against a variety of base measures [44]. This paper compares

PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

Figure 8. Runtime comparison for computing probabilities using exact inference in Sppl and rejection sampling in BLOG.

against PSI because the reference Hakaru implementation
crashes or delivers incorrect or partial results on several
benchmark problems [23, Table 1], and, as mentioned by
the developers, does not support constructs such as arrays
needed to support dozens or hundreds of observations.
Symbolic Execution and Volume Computation: Previ-
ous work has addressed the problem of computing the prob-
ability of a predicate by integrating a distribution defined
by a program [2, 25, 55, 61]. For example, Geldenhuys et al.
[25] present a probabilistic symbolic execution technique
that uses model counting to compute path probabilities, as-
suming that all program variables are discrete and uniformly
distributed. While Sppl can model a variety of distributions,
due to restriction (R3) it only supports predicates that specify
rectangular regions, whereas several of the aforementioned
systems can (approximately) handle non-rectangular regions.
More specifically, predicates in Sppl may include combina-
tions of nonlinear transforms, each of a single variable, which
are solved into linear expressions that specify unions of dis-
joint hyperrectangles (Appx. C.2). Table 2 shows that Sppl
delivers substantial speedup on the hyperrectangular regions
specified by the important class of decision trees, which are
widely used in interpretable machine learning applications.
Sum-Product Networks: The SPFlow library [41] is an
object-oriented “graphical model toolkit” in Python for con-
structing and querying sum-product networks. Sppl lever-
ages a new and more general sum-product representation
(Lst. 1) and solves probability and conditioning queries that
are not supported by SPFlow (Thm. 4.1), which include mixed
random variables, numeric transforms, and logical predicates
with set-valued constraints. In addition, we introduce a novel
translation strategy (Sec. 5) that allows users to specify mod-
els as generative code in a PPL (using e.g., variables, arrays,
arithmetic and logical expressions, loops, branches) without
having to manually manipulate low-level data structures.
“Factored sum-product networks” [58] have been used as
intermediate representations for converting a probabilistic
program and any functional interpreter into a system of
equations whose solution is the marginal probability of the
program’s return value. These algorithms handle recursive
procedures and leverage dynamic programming, but only
apply to discrete variables, cannot handle transforms, and
require solving fixed-points. Moreover, they have not been
quantitatively evaluated on PPL benchmark problems.

Weighted Model Counting/Integration: A common ap-
proach to probabilistic inference is using algorithmic re-
ductions from probabilistic programs to weighted-model
counting (WMC) or integration (WMI) via knowledge com-
pilation [5, 15, 19, 22, 66]. For example, Symbo [70] leverages
WMI for exact inference in hybrid domains, using sentinel
decision diagrams as the representation and the PSI solver to
symbolically integrate over continuous variables. Dice [30]
leverages WMC for scaling exact inference in discrete prob-
abilistic programs and uses binary decision diagram repre-
sentations that automatically exploit program structure to
factorize inference. The representations in Dice enable sub-
stantial computation reuse for querying and/or conditioning,
such as computing “all-marginal” probabilities by reusing
the same compiled representation multiple times. Sppl also
leverages factorization and computation reuse, but uses a dif-
ferent representation based on sum-product expressions that
handle additional computations such as numeric transforms
and continuous and mixed-type random variables.
Probabilistic Program Synthesis: Existing PPL synthesis
systems for tabular data [13, 53] produce programs in lan-
guages that are subsets of Sppl, which enable automatic
synthesis of full Sppl programs from data. Sppl can also
unify and extend custom PPL query engines used in these
systems for tasks such as similarity search and dependence
detection [49, 50, 52]. It may also be fruitful to use struc-
ture discovery methods for time series [1, 54] or relational
data [32] to synthesize Sppl programs for these domains.

8 Conclusion

We have presented Sppl, a new system that automatically
delivers exact answers to a range of probabilistic inference
queries. A key insight in Sppl is to impose restrictions on
probabilistic programs that enable them to be translated to
sum-product expressions, which are highly effective rep-
resentations for inference. Our evaluation highlights the
efficacy of Sppl on inference tasks in the literature and un-
derscores the importance of key design decisions, including
the multi-stage inference workflow and techniques used to
build compact expressions by exploiting probabilistic struc-
ture. In addition to its efficacy as a standalone language, we
further anticipate that Sppl could be useful as an embed-
ded domain-specific language within more expressive PPLs,
combining the benefits of exact and approximate inference.

05101520Runtime(sec)−10.00−9.75−9.50−9.25EstimateEventLogProb:-9.63TrueValueSPPLBLOG05101520Runtime(sec)−13.5−13.0−12.5EventLogProb:-12.73050100150200Runtime(sec)−15−14−13−12−11EventLogProb:-14.48050100150200Runtime(sec)−18.0−17.5−17.0−16.5−16.0EventLogProb:-17.32SPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

References

[1] Amr Ahmed and Eric Xing. 2008. Dynamic Non-Parametric Mix-
ture Models and The Recurrent Chinese Restaurant Process: With
Applications to Evolutionary Clustering. In SDM 2008: Proc. 2008
SIAM Int. Conf. Data Min. SIAM, Philadelphia, PA, USA, 219–230.
https://doi.org/10.1137/1.9781611972788.20

[2] Aws Albarghouthi, Loris D’Antoni, Samuel Drews, and Aditya V. Nori.
2017. FairSquare: Probabilistic Verification of Program Fairness. Proc.
ACM Program. Lang. 1, OOPSLA, Article 80 (Oct. 2017), 30 pages.
https://doi.org/10.1145/3133904

[3] Stefan Arnborg, Derek G Corneil, and Andrzej Proskurowski. 1987.
Complexity of Finding Embeddings in a k-Tree. SIAM J. Alg. Disc.
Meth. 8, 2 (April 1987), 277–284. https://doi.org/10.1137/0608024
[4] Osbert Bastani, Xin Zhang, and Armando Solar-Lezama. 2019. Prob-
abilistic Verification of Fairness Properties via Concentration. Proc.
ACM Program. Lang. 3, OOPSLA, Article 118 (Oct. 2019), 27 pages.
https://doi.org/10.1145/3360544

[5] Vaishak Belle, Andrea Passerini, and Guy Van den Broeck. 2015. Prob-
abilistic Inference in Hybrid Domains by Weighted Model Integration.
In IJCAI 2015: Proc. 24th Int. Jt. Conf. Artif. Intell. International Joint
Conferences on Artificial Intelligence, Cham, 2770–2776.

[6] Sooraj Bhat, Johannes Borgström, Andrew D. Gordon, and Claudio
Russo. 2013. Deriving Probability Density Functions from Probabilistic
Functional Programs. In TACAS 2013: Proc. 19th Int. Conf. Tools Alg.
Constr. Anal. Syst. Springer, Berlin, 508–522. https://doi.org/10.1007/
978-3-642-36742-7_35

[7] Patrick Billingsley. 1986. Probability and Measure (2nd ed.). John Wiley

& Sons, New York.

[8] Andrew D. Bolton, Martin Haesemeyer, Josua Jordi, Ulrich Schaechtle,
Feras A. Saad, Vikash K. Mansinghka, Joshua B. Tenenbaum, and
Florian Engert. 2019. Elements of a Stochastic 3D Prediction Engine
in Larval Zebrafish Prey Capture. eLife 8 (Nov. 2019), e51975. https:
//doi.org/10.7554/eLife.51975

[9] Stephen P. Brooks and Andrew Gelman. 1998. General Methods
for Monitoring Convergence of Iterative Simulations. J. Comput.
Graph. Stat. 7, 4 (1998), 434–455. https://doi.org/10.1080/10618600.
1998.10474787

[10] Jacques Carette and Chung-chieh Shan. 2016. Simplifying Prob-
abilistic Programs Using Computer Algebra. In PADL 2016: Proc.
18th Int. Sympos. Pract. Asp. Declar. Lang. Springer, Cham, 135–152.
https://doi.org/10.1007/978-3-319-28228-2_9

[11] Bob Carpenter, Andrew Gelman, Matthew Hoffman, Daniel Lee, Ben
Goodrich, Michael Betancourt, Marcus Brubaker, Jiqiang Guo, Peter Li,
and Allen Riddell. 2017. Stan: A Probabilistic Programming Language.
J. Stat. Soft. 76, 1 (Jan. 2017), 1–32. https://doi.org/10.18637/jss.v076.i01
[12] Venkat Chandrasekeran, Nathan Srebro, and Prahladh Harsha. 2008.
Complexity of Inference in Graphical Models. In UAI 2008: Proc. 24th
Annu. Conf. Uncertain. Artif. Intell. AUAI Press, Arlington, VA, USA,
70–78.

[13] Sarah Chasins and Phitchaya M. Phothilimthana. 2017. Data-driven
Synthesis of Full Probabilistic Programs. In CAV 2017: Proc. 29th Int.
Conf. Comput. Aided Verif. Springer, Cham, 279–304. https://doi.org/
10.1007/978-3-319-63387-9_14

[14] Nick Chater, Joshua B. Tenenbaum, and Alan Yuille. 2006. Probabilistic
Models of Cognition: Conceptual Foundations. Trends Cogn. Sci. 10, 7
(July 2006), 287–291. https://doi.org/10.1016/j.tics.2006.05.007
[15] Mark Chavira and Adnan Darwiche. 2008. On Probabilistic Inference
by Weighted Model Counting. Artif. Intell. 172, 6 (April 2008), 772–799.
https://doi.org/10.1016/j.artint.2007.11.002

[16] Gregory F. Cooper. 1990. The Computational Complexity of Proba-
bilistic Inference Using Bayesian Belief Networks. Artif. Intell. 42, 2
(March 1990), 393–405. https://doi.org/10.1016/0004-3702(90)90060-D
[17] Marco F. Cusumano-Towner, Feras A. Saad, Alexander K. Lew, and
Vikash K. Mansinghka. 2019. Gen: A General-Purpose Probabilistic

Programming System with Programmable Inference. In PLDI 2019:
Proc. 40th ACM SIGPLAN Conf. Program. Lang. Des. Impl. ACM, New
York, NY, USA, 221–236. https://doi.org/10.1145/3314221.3314642
[18] Paul Dagum and Michael Luby. 1993. Approximating Probabilistic
Inference in Bayesian Belief Networks is NP-Hard. Artif. Intell. 60, 1
(March 1993), 141–153. https://doi.org/10.1016/0004-3702(93)90036-B
[19] Adnan Darwiche and Pierre Marquis. 2002. A Knowledge Compilation
Map. J. Artif. Intell. Res. 17 (2002), 229–264. https://doi.org/10.1613/
jair.989

[20] Persi Diaconis and Donald Ylvisaker. 1979. Conjugate Priors for Ex-
ponential Families. Ann. Statist. 7, 2 (March 1979), 269–281. https:
//doi.org/10.1214/aos/1176344611

[21] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and
Richard Zemel. 2012. Fairness through Awareness. In ITCS 2012: Proc.
3rd Innov. Theor. Comput. Sci. Conf. ACM, New York, NY, USA, 214–226.
https://doi.org/10.1145/2090236.2090255

[22] Daan Fierens, Guy Van den Broeck, Ingo Thon, Bernd Gutmann, and
Luc De Raedt. 2011. Inference in Probabilistic Logic Programs using
Weighted CNF’s. In UAI 2011: Proc. 27th Annu. Conf. Uncertain. Artif.
Intell. AUAI Press, Arlington, VA, USA, 211–220.

[23] Timon Gehr, Sasa Misailovic, and Martin Vechev. 2016. PSI: Exact
Symbolic Inference for Probabilistic Programs. In CAV 2016: Proc. 28th
Int. Conf. Comput. Aided Verif. Springer, Cham, 62–83. https://doi.org/
10.1007/978-3-319-41528-4_4

[24] Timon Gehr, Samuel Steffen, and Martin Vechev. 2020. 𝜆PSI: Exact
Inference for Higher-Order Probabilistic Programs. In PLDI 2020: Proc.
41st ACM SIGPLAN Conf. Program. Lang. Des. Impl. ACM, New York,
NY, USA, 883–897. https://doi.org/10.1145/3385412.3386006

[25] Jaco Geldenhuys, Matthew B. Dwyer, and Willem Visser. 2012. Proba-
bilistic Symbolic Execution. In ISSTA 2012: Proc. 2012 Int. Sympos. Soft.
Test. Anal. ACM, New York, NY, USA, 166–176. https://doi.org/10.
1145/2338965.2336773

[26] Robert Gens and Pedro Domingos. 2013. Learning the Structure of
Sum-Product Networks. In ICML 2013: Proc. 30th Int. Conf. Mach. Learn.
PMLR, 873–880.

[27] Zoubin Ghahramani. 2015. Probabilistic Machine Learning and Ar-
tificial Intelligence. Nature 521, 7553 (May 2015), 452–459. https:
//doi.org/10.1038/nature14541

[28] Noah Goodman, Vikash Mansinghka, Daniel M. Roy, Keith Bonawitz,
and Joshua B. Tenenbaum. 2008. Church: A Language For Generative
Models. In UAI 2008: Proc. 24th Annu. Conf. Uncertain. Artif. Intell. AUAI
Press, Arlington, VA, USA, 220–229.

[29] Andrew D. Gordon, Thomas A. Henzinger, Aditya V. Nori, and Sri-
ram K. Rajamani. 2014. Probabilistic Programming. In FOSE 2014:
Future Soft. Eng. Proc. ACM, New York, NY, USA, 167–181. https:
//doi.org/10.1145/2593882.2593900

[30] Steven Holtzen, Guy Van den Broeck, and Todd Millstein. 2020.
Scaling Exact Inference for Discrete Probabilistic Programs. Proc.
ACM Program. Lang. 4, OOPSLA, Article 140 (Oct. 2020), 31 pages.
https://doi.org/10.1145/3133904

[31] Frederick Jelinek. 1997. Statistical Methods for Speech Recognition. MIT

Press, Cambridge, MA, USA.

[32] Charles Kemp, Joshua B. Tenenbaum, Thomas L. Griffiths, Takeshi
Yamada, and Naonori Ueda. 2006. Learning Systems of Concepts with
An Infinite Relational Model. In AAAI 2006: Proc. 21st AAAI Conf. Artif.
Intell. AAAI Press, Palo Alto, CA, USA, 381–388.

[33] Daphne Koller and Nir Friedman. 2009. Probabilistic Graphical Models:

Principles and Techniques. MIT Press, Cambridge, MA, USA.

[34] Christopher Krapu and Mark Borsuk. 2019. Probabilistic Programming:
A Review for Environmental Modellers. Environ. Model. Softw 114
(April 2019), 40–48. https://doi.org/10.1016/j.envsoft.2019.01.014
[35] Tejas D. Kulkarni, Pushmeet Kohli, Joshua B. Tenenbaum, and Vikash
Mansinghka. 2015. Picture: A Probabilistic Programming Language
for Scene Perception. In CVPR 2015: Proc. 2015 IEEE Conf. Comput. Vis.

PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

Pattern Recognit. IEEE Press, Piscataway, NJ, USA, 4390–4399.
[36] Jacob Laurel and Sasa Misailovic. 2020. Continualization of Probabilis-
tic Programs with Correction. In ESOP 2020: Proc. 29th Eur. Sympos.
Program. Springer, Cham, 366–393. https://doi.org/10.1007/978-3-030-
44914-8_14

[37] Sang-Woo Lee, Christopher Watkins, and Byoung-Tak Zhang. 2014.
Non-Parametric Bayesian Sum-Product Networks. In LTPM 2014: Work-
shop Learn. Tractable Probab. Model.

[38] Alexander K. Lew, Marco F. Cusumano-Towner, Benjamin Sherman,
Michael Carbin, and Vikash K. Mansinghka. 2020. Trace Types and
Denotational Semantics for Sound Programmable Inference in Proba-
bilistic Languages. Proc. ACM Program. Lang. 4, POPL, Article 19 (Jan.
2020), 32 pages. https://doi.org/10.1145/3371087

[39] Vikash K. Mansinghka, Ulrich Schaechtle, Shivam Handa, Alexey
Radul, Yutian Chen, and Martin Rinard. 2018. Probabilistic Program-
ming with Programmable Inference. In PLDI 2018: Proc. 39th ACM
SIGPLAN Conf. Program. Lang. Des. Impl. ACM, New York, NY, USA,
603–616. https://doi.org/10.1145/3192366.3192409

[40] Brian Milch, Bhaskara Marthi, Stuart Russell, David Sontag, Daniel L.
Ong, and Andrey Kolobov. 2005. BLOG: Probabilistic Models with
Unknown Objects. In IJCAI 2005: Proc. 19th Int. Jt. Conf. Artif. Intell.
International Joint Conferences on Artificial Intelligence, Cham, 1352–
1359.

[41] Alejandro Molina et al. 2020. SPFlow: An Easy and Extensible Library
for Deep Probabilistic Learning using Sum-Product Networks. (Jan.
2020). arXiv:1901.03704

[42] Kevin Murphy and Mark A. Paskin. 2002. Linear-Time Inference in
Hierarchical HMMs. In NIPS 2001: Adv. Neural Inf. Proc. Syst. 14. MIT
Press, Cambridge, MA, USA, 833–840.

[43] Praveen Narayanan, Jacques Carette, Wren Romano, Chung-chieh
Shan, and Robert Zinkov. 2016. Probabilistic Inference by Program
Transformation in Hakaru (System Description). In FLOPS 2016: Proc.
13th Int. Sympos. Func. Log. Program. Springer, Cham, 62–79. https:
//doi.org/10.1007/978-3-319-29604-3_5

[44] Praveen Narayanan and Chung-chieh Shan. 2020. Symbolic Disintegra-
tion with a Variety of Base Measures. ACM Trans. Program. Lang. Syst.
42, 2, Article 9 (May 2020), 60 pages. https://doi.org/10.1145/3374208
[45] Davide Nitti, Tinne De Laet, and Luc De Raedt. 2016. Probabilistic
Logic Programing for Hybrid Relational Domains. Mach. Learn. 103
(April 2016), 407–449. https://doi.org/10.1007/s10994-016-5558-8
[46] Aditya Nori, Chung-Kil Hur, Sriram Rajamani, and Selva Samuel. 2014.
R2: An Efficient MCMC Sampler for Probabilistic Programs. In AAAI
2014: Proc. 28th AAAI Conf. Artif. Intell. AAAI Press, 2476–2482.
[47] Hoifung Poon and Pedro Domingos. 2011. Sum-Product Networks: A
New Deep Architecture. In UAI 2011: Proc. 27th Annu. Conf. Uncertain.
Artif. Intell. AUAI Press, Arlington, VA, USA, 337–346.

[48] Fabrizio Riguzzi. 2018. Foundations of Probabilistic Logic Programming:
Languages, Semantics, Inference and Learning. River Publishers, Delft,
The Netherlands.

[49] Feras Saad, Leonardo Casarsa, and Vikash Mansinghka. 2017. Proba-
bilistic Search For Structured Data Via Probabilistic Programming and
Nonparametric Bayes. (April 2017). arXiv:1704.01087

[50] Feras Saad and Vikash Mansinghka. 2016. Probabilistic Data Analysis
with Probabilistic Programming. (Aug. 2016). arXiv:1608.05347
[51] Feras Saad and Vikash Mansinghka. 2016. A Probabilistic Programming
Approach to Probabilistic Data Analysis. In NIPS 2016: Adv. Neural Inf.
Proc. Syst. 29. Curran Associates, Inc., Red Hook, NY, USA, 2011–2019.
[52] Feras Saad and Vikash Mansinghka. 2017. Detecting Dependencies
in Sparse, Multivariate Databases using Probabilistic Programming
and Non-parametric Bayes. In AISTATS 2017: Proc. 20th Int. Conf. Artif.
Intell. Stat. PMLR, 632–641.

[53] Feras A. Saad, Marco F. Cusumano-Towner, Ulrich Schaechtle, Mar-
tin C. Rinard, and Vikash K. Mansinghka. 2019. Bayesian Synthe-
sis of Probabilistic Programs for Automatic Data Modeling. Proc.

ACM Program. Lang. 3, POPL, Article 36 (Jan. 2019), 32 pages. https:
//doi.org/10.1145/3290350

[54] Feras A. Saad and Vikash K. Mansinghka. 2018. Temporally-reweighted
Chinese Restaurant Process Mixtures for Clustering, Imputing, and
Forecasting Multivariate Time Series. In AISTATS 2018: Proc. 21st Int.
Conf. Artif. Intell. Stat. PMLR, 755–764.

[55] Sriram Sankaranarayanan, Aleksandar Chakarov, and Sumit Gulwani.
2013. Static Analysis for Probabilistic Programs: Inferring Whole
Program Properties from Finitely Many Paths. In PLDI 2013: Proc. 34th
ACM SIGPLAN Conf. Program. Lang. Des. Impl. ACM, New York, NY,
USA, 447–458. https://doi.org/10.1145/2491956.2462179

[56] David J. Spiegelhalter, A. Philip Dawid, Steffen L. Lauritzen, and
Robert G. Cowell. 1993. Bayesian Analysis in Expert Systems. Statist.
Sci. 8, 3 (Aug. 1993), 219–247. https://doi.org/10.1214/ss/1177010888
[57] Siddharth Srivastava, Nicholas Hay, Yi Wu, and Stuart Russell. 2017.
The Extended Semantics for Probabilistic Programming Languages. In
PPS 2017: Workshop Probab. Program. Semant.

[58] Andreas Stuhlmüller and Noah Goodman. 2012. A Dynamic Program-
ming Algorithm for Inference in Recursive Probabilistic Programs.
(Sept. 2012). arXiv:1206.3555

[59] Andrew Thomas. 1994. BUGS: A Statical Modelling Package. RTA/BCS

Modul. Lang. Newsl. 2 (1994), 36–38.

[60] Sebastian Thrun, Wolfram Burgard, and Dieter Fox. 2005. Probabilistic

Robotics. MIT Press, Cambridge, MA, USA.

[61] Neil Toronto, Jay McCarthy, and David Van Horn. 2015. Running
Probabilistic Programs Backwards. In ESOP 2015: Proc. 24th Eur. Sympos.
Program. Springer, Berlin, 53–79. https://doi.org/10.1007/978-3-662-
46669-8_3

[62] Martin Trapp, Robert Peharz, Hong Ge, Franz Pernkopf, and Zoubin
Ghahramani. 2019. Bayesian Learning of Sum-Product Networks. In
NeurIPS 2019: Adv. Neural Inf. Proc. Syst. 32. Curran Associates, Inc.,
Red Hook, NY, USA, 6347–6358.

[63] F. Turbak, D. Gifford, and M.A. Sheldon. 2008. Design Concepts in

Programming Languages. MIT Press, Cambridge, MA, USA.

[64] Antonio Vergari. 2021. A Systematic View of the Literature on Proba-
bilistic Circuits, Computational Graphs Encoding Tractable Probability
Distributions. Retrieved Mar 7, 2021 from https://arranger1044.github.
io/probabilistic-circuits/

[65] Antonio Vergari, Alejandro Molina, Robert Peharz, Zoubin Ghahra-
mani, Kristian Kersting, and Isabel Valera. 2019. Automatic Bayesian
Density Analysis. In AAAI 2019: Proc. 23rd AAAI Conf. Artif. Intell.
AAAI Press, Palo Alto, CA, USA, 5207–5215.

[66] Jonas Vlasselaer, Guy Van den Broeck, Angelika Kimmig, Wannes
Meert, and Luc De Raedt. 2015. Anytime Inference in Probabilistic
Logic Programs with Tp-Compilation. In IJCAI 2015: Proc. 24th Int. Jt.
Conf. Artif. Intell. International Joint Conferences on Artificial Intelli-
gence, Cham, 1852–1858.

[67] David Wingate and Theophane Weber. 2013. Automated Variational
Inference in Probabilistic Programming. (Jan. 2013). arXiv:1301.1299
[68] Yi Wu, Siddharth Srivastava, Nicholas Hay, Simon Du, and Stuart
Russell. 2018. Discrete-Continuous Mixtures in Probabilistic Program-
ming: Generalized Semantics and Inference Algorithms. In ICML 2018:
Proc. 35th Int. Conf. Mach. Learn. PMLR, 5343–5352.

[69] Jieyuan Zhang and Jingling Xue. 2019.

Incremental Precision-
Preserving Symbolic Inference for Probabilistic Programs. In PLDI
2019: Proc. 40th ACM SIGPLAN Conf. Program. Lang. Des. Impl. ACM,
New York, NY, USA, 237–252. https://doi.org/10.1145/3314221.3314623
[70] Pedro Zuidberg Dos Martires, Anton Dries, and Luc De Raedt. 2019.
Exact and Approximate Weighted Model Integration with Probability
Density Functions Using Knowledge Compilation. In AAAI 2019: Proc.
23rd AAAI Conf. Artif. Intell. AAAI Press, Palo Alto, CA, USA, 7825–
7833.

SPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

A Syntax of Core Calculus

C.2 Preimage Computation

The metalanguage in this paper follows that of Turbak et al.
[63, Appx. A]. For completeness, Lst. 9 shows the syntax of
the core calculus whose semantics are given in Lst. 1 from
the main text. Prop. A.1 below establishes that distributions
specified by DistInt and DistReal (Lst. 1e) with CDF 𝐹 can be
sampled using a variant of the integral probability transform.
Proposition A.1. Let 𝐹 be a CDF and 𝑟1, 𝑟2 real numbers
such that 𝐹 (𝑟1) < 𝐹 (𝑟2). Let 𝑈 ∼ Uniform(𝐹 (𝑟1), 𝐹 (𝑟2)) and
define the random variable 𝑋 (cid:66) 𝐹 −1(𝑈 ). Then for all real
numbers 𝑟 ,

˜𝐹 (𝑟 ) (cid:66) Pr[𝑋 ≤ 𝑟 ] =

0
𝐹 (𝑟 ) − 𝐹 (𝑟1)
𝐹 (𝑟2) − 𝐹 (𝑟1)
1





if 𝑟 < 𝑟1
if 𝑟1 ≤ 𝑟 ≤ 𝑟2
if 𝑟2 < 𝑟

(8)

Proof. Immediate from Pr[𝑋 ≤ 𝑟 ] = Pr[𝑈 ≤ 𝐹 (𝑟 )] and the
□
uniformity of 𝑈 on [𝑟1, 𝑟2].

B Definitions of Auxiliary Functions

Sec. 3 refers to the following operations on the Outcomes
domain:

union : Outcomes∗ → Outcomes,
intersection : Outcomes∗ → Outcomes,
complement : Outcomes → Outcomes.

Any implementation satisfies the following invariants:

𝑣1 ⨿ · · · ⨿ 𝑣𝑚 = union 𝑣 ∗

⇐⇒ ∀𝑖 ≠ 𝑗 .intersection 𝑣𝑖 𝑣 𝑗 = ∅,

𝑣1 ⨿ · · · ⨿ 𝑣𝑚 = intersection 𝑣 ∗

⇐⇒ ∀𝑖 ≠ 𝑗 .intersection 𝑣𝑖 𝑣 𝑗 = ∅,

𝑣1 ⨿ · · · ⨿ 𝑣𝑚 = complement 𝑣

⇐⇒ ∀𝑖 ≠ 𝑗 .intersection 𝑣𝑖 𝑣 𝑗 = ∅.

(9)

(10)

(11)

(12)

(13)

(14)

Lst. 10 shows an implementation of the complement func-
tion, which operates separately on the Real and String com-
ponents; union and intersection are implemented similarly.
Lst. 11 shows the vars function, which returns the variables
in a Transform or Event expression. Lst. 14 shows the negate
function, which returns the logical negation of an Event.

C Transforms of Random Variables

This appendix describes the Transform domain in the core
calculus (expanding Lst. 1b), which is used to express numer-
ical transformations of real random variables.

C.1 Valuation of Transforms
Lst. 17 shows the valuation function T which defines each 𝑡
𝑡 is defined
as a Real function on Real. Each real function
on an input 𝑟 ′ if and only if (cid:0)↓ Real
𝑟 ′(cid:1) ∈ (domainof 𝑡).
(cid:75)
Lst. 18 shows the implementation of domainof .

Outcome

𝑇
(cid:74)

Lst. 19 shows the algorithm that implements

preimg : Transform → Outcomes → Outcomes,

(15)

which, as discussed in Sec. 3 of the main text, satisfies

(↓

Real
Outcome
(↓ String
Outcome

𝑟 ) ∈ V

𝑠) ∈ V

preimg 𝑡 𝑣
(cid:74)
preimg 𝑡 𝑣
(cid:74)

(cid:75)

(cid:75)

⇐⇒ T

(𝑟 ) ∈ V

𝑡
(cid:74)

(cid:75)

,

𝑣
(cid:74)

(cid:75)

⇐⇒ (𝑡 ∈ Identity) ∧ (𝑠 ∈ V

𝑣
(cid:74)

).
(cid:75)

The implementation of preimg uses several helper functions:
(Lst. 20) finv: computes the preimage of each 𝑡 ∈ Transform

at a single Real.

(Lst. 21) polyLim: computes the limits of a polynomial at

the infinites.

(Lst. 22) polySolve: computes the set of values at which a
polynomial is equal to a given value (possibly positive
or negative infinity).

(Lst. 23) polyLte: computes the set of values at which a

polynomial is less than or equal a given value.

In addition, we assume access to a general root finding
algorithm roots : Real+ → Real∗ (not shown), that returns
a (possibly empty) list of roots of the degree-𝑚 polynomial
with specified coefficients. In the reference implementation
of Sppl, the roots function uses symbolic analysis for poly-
nomials whose degree is less than or equal to two and semi-
symbolic analysis for higher-order polynomials.

C.3 Example of Exact Inference on a Many-to-One

Random Variable Transformation

√

This appendix shows how Sppl enables exact inference on
many-to-one transformations of real random variables de-
scribed in the previous section, where the transformation is
itself determined by a stochastic branch (Fig. 4 in main text).
Fig. 4a shows an Sppl program that defines a pair of ran-
dom variables (𝑋, 𝑍 ), where 𝑋 is normally distributed; and
𝑍 = −𝑋 3 +𝑋 2 + 6𝑋 if 𝑋 < 1, otherwise 𝑍 = 5
𝑋 + 1. The first
plot of Fig. 4e shows the prior distribution of 𝑋 ; the middle
plot shows the transformation 𝑡 that defines 𝑍 = 𝑡 (𝑋 ), which
is a piecewise sum of 𝑡if and 𝑡else; and the final plot shows
the distribution of 𝑍 = 𝑡 (𝑋 ). Fig. 4b shows the sum-product
expression representing this program, where the root node
is a sum whose left and right children have weights 0.691...
and 0.309..., which corresponds to the prior probabilities of
{𝑋 < 1} and {1 ≤ 𝑋 }. Nodes labeled 𝑋 ∼ 𝑁 (𝜇, 𝜎) with an
incoming directed edge from a node labeled (𝑟1, 𝑟2) denotes
that the random variable is constrained to the interval (𝑟1, 𝑟2)
(and similarly for closed intervals). Deterministic transfor-
mations are denoted by using red directed edges from a leaf
node (i.e., 𝑋 ) to a numeric expression (e.g., 5
𝑋 + 11), with
the name of the transformed variable along the edge (i.e., 𝑍 ).
Fig. 4c shows an Sppl query that conditions the program
on an event {𝑍 2 ≤ 4} ∩ {𝑍 ≥ 0} involving the transformed
variable 𝑍 . The inference engine performs the following

√

PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

analysis on the query:

{𝑍 2 ≤ 4} ∩ {𝑍 ≥ 0}
≡ {𝑍 ∈ [0, 2]}
≡ {𝑋 ∈ 𝑡 −1([0, 2])}
≡ {𝑋 ∈ 𝑡 −1
≡ {−2.174... ≤ 𝑋 ≤ −2} ∪ {0 ≤ 𝑋 ≤ .321...}
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:125)
(cid:123)(cid:122)
constraints from left subtree

if ([0, 2])} ∪ {𝑋 ∈ 𝑡 −1

recall (𝑍 (cid:66) 𝑡 (𝑋 ))
else ([0, 2])}

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

(cid:124)

(16)

(17)

(18)

(19)

(20)

(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)

∪ {81/25 ≤ 𝑋 ≤ 121/25}
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124)
(cid:125)
(cid:123)(cid:122)
constraint from right subtree

Eq. (17) shows the first stage of inference, which solves
any transformations in the conditioning event and yields
{0 ≤ 𝑍 ≤ 2}. The conditional distribution of 𝑍 is shown
in the final plot of Fig. 4f. The next step is to dispatch the
simplified event to the left and right subtrees. Each subtree
will compute the constraint on 𝑋 implied by the event under
the transformation in that branch, as shown in Eq. (19). The
middle plot of Fig. (4f) shows the preimage computation un-
der 𝑡if from the left subtree, which gives two intervals, and
𝑡else from the right subtree, which gives one interval.

The final step is to transform the prior expression (Fig. 4b)
by conditioning each subtree on the intervals in Eq. (20),
which gives the posterior expression (Fig. 4d). The left sub-
tree in Fig. 4b, which originally corresponded to {𝑋 < 1}, is
split in Fig. 4d into two subtrees that represent the events
{−2.174... ≤ 𝑋 ≤ −2} and {0 ≤ 𝑋 ≤ 0.321...}, respectively,
and whose weights 0.159... and 0.494... are the (renormalized)
probabilities of these regions under the prior distribution
(first plot of Fig. 4e). The right subtree in Fig. 4b, which
originally corresponded to {1 ≤ 𝑋 }, is now restricted to
{81/25 ≤ 𝑋 ≤ 121/25} in Fig. 4d and its weight 0.347...
is again the (renormalized) prior probability of the region.
The graph in Fig. 4d represents the distribution of (𝑋, 𝑍 )
conditioned on the query in Eq. (17). The new sum-product
expression be used to run further queries, such as using
simulate to generate 𝑛 i.i.d. random samples {(𝑥𝑖, 𝑧𝑖 )}𝑛
𝑖=1
from the posterior distributions in Fig. 4f or condition to
condition the program on further events.

D Conditioning Sum-Product Expressions

This section presents algorithms for exact inference, that
is, conditioning the distribution defined by an element of
SPE (Lst. 1f). Sec. D.2 focuses on a positive probability Event
(Lst. 1c) and Sec. D.3 focuses on a Conjunction of equality
constraints on non-transformed variables, such as {𝑋 = 3} ∩
{𝑌 = 4} (see also Remark 4.2 in the main text). We will first
prove Thm. 4.1 from the main text, which establishes that
SPE is closed under conditioning on any positive probability
Event. For completeness, we restate the Thm. 4.1 below.

Theorem 4.1 (Closure under conditioning). Let 𝑆 ∈ SPE and
𝑒 ∈ Event be given, where P
𝑒 > 0. There exists an algo-
𝑆
(cid:74)
rithm which, given 𝑆 and 𝑒, returns 𝑆 ′ ∈ SPE such that, for all
𝑒 ′ ∈ Event, the probability of 𝑒 ′ according to 𝑆 ′ is equal to the
conditional probability of 𝑒 ′ given 𝑒 according to 𝑆, i.e.,

(cid:75)

𝑒 ′ ≡ P

P

𝑆 ′
(cid:74)

(cid:75)

𝑆
(cid:74)

(cid:75)

(𝑒 ′ | 𝑒) (cid:66)

P

𝑆
(cid:75)
(cid:74)
P

(𝑒 ⊓ 𝑒 ′)
𝑒
𝑆
(cid:74)

(cid:75)

.

(5)

Thm. 4.1 is a structural conjugacy property [20] for the
family of probability distributions defined by the SPE domain,
where both the prior and posterior are identified by elements
of SPE. In Sec. D.2, we present the domain function condition
(Eq. (6), main text) which proves Thm. 4.1 by construction.
We first discuss several preprocessing algorithms that are
key subroutines used by condition.

D.1 Algorithms for Event Preprocessing

Normalizing an Event The dnf function (Lst. 15) converts
an Event 𝑒 to DNF, which we define below.

Definition D.1. An Event 𝑒 is said to be in disjunctive nor-
mal form (DNF) if and only if one of the following holds:
(D.1.1) 𝑒 ∈ Containment
(D.1.2) 𝑒 = 𝑒1 ⊓ · · · ⊓ 𝑒𝑚 ∈ Conjunction

=⇒ ∀1≤𝑖 ≤𝑚. 𝑒𝑖 ∈ Containment

(D.1.3) 𝑒 = 𝑒1 ⊔ · · · ⊔ 𝑒𝑚 ∈ Disjunction

=⇒ ∀1≤𝑖 ≤𝑚. 𝑒𝑖 ∈ Containment ∪ Conjunction

Terms 𝑒 and 𝑒𝑖 in (D.1.1) and (D.1.2) are called “literals” and
terms 𝑒𝑖 in (D.1.3) are called “clauses”.

We next define the notion of an Event in “solved” DNF.

Definition D.2. An Event 𝑒 is in solved DNF if all the fol-
lowing conditions hold: (i) 𝑒 is in DNF; (ii) all literals within
a clause 𝑒𝑖 of 𝑒 have different variables; and (iii) each literal
(𝑡 in 𝑣) of 𝑒 satisfies 𝑡 ∈ Identity and 𝑣 ∉ Union.

Example D.3. Using informal notation, the solved DNF
form of the event {𝑋 2 ≥ 9} ∩ {|𝑌 | < 1} is a disjunction
with two conjunctive clauses: [{𝑋 ∈ (−∞, −3)} ∩ {𝑌 ∈
(−1, 1)}] ∪ [{𝑋 ∈ (3, ∞)} ∩ {𝑌 ∈ (−1, 1)}].

Lst. 5a shows the normalize operation, which converts an
Event 𝑒 to solved DNF. In particular, predicates with (possibly
nonlinear) arithmetic expressions are converted to predicates
that contain only linear expressions (which is a property of
Transform and preimg; Appx. C); e.g., as in Eqs. (17)–(20).
The next result, Prop. D.4, follows from E
(cid:75)
and denotations of Union (Lst. 1a) and Disjunction (Lst. 1c).

dnf 𝑒
(cid:74)

= E

𝑒
(cid:74)

(cid:75)

Proposition D.4. ∀𝑒 ∈ Event, E

≡ E

(normalize 𝑒)

𝑒
(cid:74)

(cid:75)

(cid:74)

.

(cid:75)

Disjoining an Event Suppose that 𝑒 ∈ Event is in DNF and
has 𝑚 ≥ 2 clauses. A key inference subroutine is to rewrite
𝑒 in solved DNF (Def. D.2) where all the clauses are disjoint.

SPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

Definition D.5. Let 𝑒 ∈ Event be in DNF. Two clauses 𝑒𝑖
and 𝑒 𝑗 of 𝑒 are said to be disjoint if both 𝑒𝑖 and 𝑒 𝑗 are in solved
DNF and at least one of the following conditions holds:

∃𝑥 ∈ (vars 𝑒𝑖 ). E
∃𝑥 ∈ (vars 𝑒 𝑗 ). E

𝑒𝑖𝑥
(cid:74)
𝑒 𝑗𝑥
(cid:74)
𝑒𝑖𝑥 ⊓ 𝑒 𝑗𝑥
(cid:74)

𝑥 ≡ ∅
𝑥 ≡ ∅
𝑥 ≡ ∅

(cid:75)

(cid:75)

∃𝑥 ∈ (vars 𝑒𝑖 ) ∩ (vars 𝑒 𝑗 ). E

(23)
where 𝑒𝑖𝑥 denotes the unique literal of 𝑒𝑖 that contains vari-
able 𝑥 (for each 𝑥 ∈ vars 𝑒𝑖 ), and similarly for 𝑒 𝑗 .

(cid:75)

(21)

(22)

Lst. 16 shows the disjoint? procedure, which given a pair
of clauses 𝑒𝑖 and 𝑒 𝑗 that are in solved DNF (as produced by
normalize), returns true if and only if one of the conditions
in Def. D.5 hold. Lst. 5b presents the main algorithm disjoin,
which decomposes an arbitrary Event 𝑒 into solved DNF
whose clauses are mutually disjoint. Prop. D.6 establishes
the correctness and worst-case complexity of disjoin.

Proposition D.6. Let 𝑒 be an Event with ℎ (cid:66) |vars 𝑒 | vari-
ables, and suppose that 𝑒1 ⊔ · · · ⊔ 𝑒𝑚 (cid:66) (normalize 𝑒) has
exactly 𝑚 ≥ 1 clauses. Put ˜𝑒 (cid:66) (disjoin 𝑒). Then:

(D.6.1) ˜𝑒 is in solved DNF.
(D.6.2) ∀1≤𝑖≠𝑗 ≤ℓ . disjoint? ⟨𝑒𝑖, 𝑒 𝑗 ⟩.
(D.6.3) E
(D.6.4) The number ℓ of clauses in ˜𝑒 satisfies ℓ ≤ (2𝑚 − 1)ℎ.

˜𝑒
(cid:74)

= E

𝑒
(cid:74)

(cid:75)

(cid:75)

.

Proof. Suppose first that (normalize 𝑒) has 𝑚 = 1 clause 𝑒1.
Then ˜𝑒 = 𝑒1, so (D.6.1) holds since 𝑒1 = normalize 𝑒; (D.6.2)
holds trivially; (D.6.3) holds by Prop. D.4; and (D.6.4) holds
since ℓ = (2 − 1)ℎ = 1. Suppose now that (normalize 𝑒) has
𝑚 > 1 clauses. To employ set-theoretic reasoning, fix some
𝑥 ∈ Var and define E′
(cid:66) V
⊂ Outcome, for all
𝑒 ∈ Event. We have

𝑒
(cid:74)

𝑒
(cid:74)

E

𝑥

(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

E′
𝑒1 ⊔ · · · ⊔ 𝑒𝑚
(cid:74)
= ∪𝑚
E′
𝑒𝑖
𝑖=1
(cid:74)
(cid:16)E′
= ∪𝑚
𝑖=1
(cid:16)E′
= ∪𝑚
𝑖=1
= ∪𝑚
𝑖=1

(cid:75)
𝑒𝑖
(cid:74)
𝑒𝑖
(cid:74)
𝑒𝑖
(cid:74)

(cid:0)E′

(cid:75)

(cid:75)

(cid:75)

𝑗=1(E′

∩ ¬ (cid:2)∪𝑖−1

𝑒 𝑗
(cid:74)
∩ (cid:2)∩𝑖−1
𝑗=1(¬E′
𝑒 𝑗
(cid:74)
∩ (cid:2)∩𝑗 ∈𝑘 (𝑖) (¬E′

)(cid:3) (cid:17)
(cid:75)
)(cid:3) (cid:17)
(cid:75)
𝑒 𝑗
(cid:74)

(cid:75)

)(cid:3) (cid:1)

(25)

(26)

(27)

(28)

(29)

where we define for each 𝑖 = 1, . . . , 𝑚,
𝑘 (𝑖) (cid:66) (cid:8)1 ≤ 𝑗 ≤ 𝑖 − 1 | E′

𝑒𝑖
(cid:74)

(cid:75)

∩ E′

𝑒 𝑗
(cid:74)

(cid:75)

≠ ∅(cid:9).

Eq. (29) follows from the fact that for any 𝑖 = 1, . . . , 𝑚 and
𝑗 < 𝑖, we have

𝑗 ∉ 𝑘 (𝑖) =⇒ (cid:2)(cid:0)E′

∩ ¬E′

(cid:1) ≡ E′

(cid:3) .

(30)

𝑒𝑖
(cid:74)

(cid:75)

𝑒 𝑗
(cid:74)

(cid:75)

𝑒𝑖
(cid:74)

(cid:75)

As negate (Lst. 14) computes set-theoretic complement ¬ in
the Event domain and 𝑗 ∉ 𝑘 (𝑖) if and only if (disjoint? 𝑒 𝑗 𝑒𝑖 ),
𝑖 (cid:66) 𝑒𝑖 ⊓ ˜𝑒𝑖 (𝑖 = 2, . . . , 𝑚) in Eq. (24c)
it follows that the Events 𝑒 ′
are pairwise disjoint and are also disjoint from 𝑒1, so that
E
𝑚(cid:121). Thus, if disjoin halts, then all
of (D.6.1)–(D.6.3) follow by induction.

2 ⊔ · · · ⊔ 𝑒 ′

= E (cid:113)𝑒1 ⊔ 𝑒 ′

𝑒
(cid:74)

(cid:75)

We next establish that disjoin halts by upper bounding
the number of clauses ℓ returned by any call to disjoin. Re-
calling that ℎ (cid:66) |vars 𝑒 |, we assume without loss of gener-
ality that all clauses 𝑒𝑖 (𝑖 = 1, . . . , 𝑛) in Eq. (24a) have the
same variables {𝑥1, . . . , 𝑥ℎ }, by “padding” each 𝑒𝑖 with vacu-
ously true literals of the form (Id(𝑥𝑖 ) in Outcomes). Next,
recall that clause 𝑒𝑖 in Eq. (24a) is in solved DNF and has
𝑚𝑖 ≥ 1 literals 𝑒𝑖 𝑗 = (Id(𝑥𝑖 𝑗 ) in 𝑣𝑖 𝑗 ) where 𝑣𝑖 𝑗 ∉ Union
(Def. D.2). Thus, 𝑒𝑖 specifies exactly one hyperrectangle in
ℎ-dimensional space, where 𝑣𝑖 𝑗 is the “interval” (possibly
infinite) along the dimension specified by 𝑥𝑖 𝑗 in literal 𝑒𝑖 𝑗
(𝑖 = 1, . . . , 𝑚; 𝑗 = 1, . . . , 𝑚𝑖 ). A sufficient condition to pro-
duce the worst-case number of pairwise disjoint primitive
sub-hyperrectangles that partition the region 𝑒1 ⊔ · · · ⊔ 𝑒𝑚
is when the previous clauses 𝑒1, . . . , 𝑒𝑚−1 (i) are pairwise
disjoint (Def. D.5); and (ii) are strictly contained in 𝑒𝑚, i.e.,
∀𝑥 . E
, ( 𝑗 = 1, . . . , 𝑚 − 1). If these two con-
ditions hold, then disjoin partitions the interior of the ℎ-
dimensional hyperrectangle specified by 𝑒𝑚 into no more
than 2(𝑚 − 1)ℎ sub-hyperrectangles that do not intersect
one another (and thus, produce no further recursive calls),
□
thereby establishing (D.6.4).

𝑒𝑚
(cid:74)

𝑒 𝑗
(cid:74)

⊊ E

(cid:75)

(cid:75)

Example D.7. The left panel in Fig. 9 shows 𝑚 = 4 rect-
angles in Real × Real. The right panel shows a grid (in red)
with (2𝑚 − 1)2 = 49 primitive rectangular regions that are
pairwise disjoint from one another and whose union over-
approximates the union of the 4 rectangles. In this case, 29
of these primitive rectangular regions are sufficient (but ex-
cessive) to exactly partition the union of the rectangles into
a disjoint union. No more than 49 primitive rectangles are
ever needed to partition any 4 rectangles in Real2, and this
bound is tight. The bound in (D.6.4) generalizes this idea to
hyperrectangles that live in ℎ-dimensional space.

Conditioning Region

Partition into Rectangles

Figure 9. Example illustrating the upper bound (D.6.4) on
the number of disjoint rectangles in a worst-case partition
of a conditioning region in the two-dimensional Real plane.

Remark D.8. When defining ˜𝑒 in Eq (24b) of disjoin, ignor-
ing previous clauses that are disjoint from 𝑒𝑖 is essential for
disjoin to halt, so as to avoid recursing on a primitive sub-
rectangle in the interior. That is, filtering out such clauses
ensures that disjoin makes a finite number of recursive calls.

PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

normalize : Event → Event
normalize (𝑡 in 𝑣) (cid:66) match preimg 𝑡 𝑣

𝑚 ⇒ ⊔𝑚
▷ 𝑣′
1 ⨿ · · · ⨿ 𝑣′
𝑖=1
▷ 𝑣′ ⇒ (Id(𝑥) in 𝑣′),

(Id(𝑥) in 𝑣′
𝑖 )
where {𝑥 } (cid:66) vars 𝑡

normalize (𝑒1 ⊓ · · · ⊓ 𝑒𝑚) (cid:66) dnf ⊓𝑚
normalize (𝑒1 ⊔ · · · ⊔ 𝑒𝑚) (cid:66) dnf ⊔𝑚

𝑖=1 (normalize 𝑒𝑖 )
𝑖=1 (normalize 𝑒𝑖 )

disjoin : Event → Event
disjoin 𝑒 (cid:66) let (𝑒1 ⊔ · · · ⊔ 𝑒𝑚) be normalize 𝑒

in let2≤𝑖 ≤𝑚 ˜𝑒 be

(cid:108)
1≤ 𝑗 <𝑖 | ¬(disjoint? ⟨𝑒 𝑗 ,𝑒𝑖 ⟩)

(negate 𝑒 𝑗 )

in let2≤𝑖 ≤𝑚 ˜𝑒𝑖 be (disjoin (𝑒𝑖 ⊓ ˜𝑒𝑖 ))
in 𝑒1 ⊔ ˜𝑒2 ⊔ · · · ⊔ ˜𝑒𝑚

(a) normalize

(b) disjoin

Listing 5. Event preprocessing algorithms used by condition.

(24a)

(24b)

(24c)

condition Leaf(𝑥 𝑑 𝜎) 𝑒 (cid:66) let 𝑣 be E
▷ DistS((𝑠𝑖 𝑤𝑖 )𝑚
𝑖=1
. . . 𝑠′

) ⇒ match 𝑣
𝑙 }𝑏 ⇒ let1≤𝑖 ≤𝑚 𝑤′

▷ {𝑠′
1

(subsenv 𝑒 𝜎)

(cid:74)

(cid:75)

𝑥 in match 𝑑

𝑖 be if ¯𝑏 then 𝑤𝑖 1[∃1≤ 𝑗 ≤ℓ .𝑠′
else 𝑤𝑖 1[∀1≤ 𝑗 ≤ℓ .𝑠′
𝑗 ≠ 𝑠𝑖 ]
𝑖 )𝑚
) 𝜎)
𝑖=1

𝑗 = 𝑠𝑖 ]

in Leaf(𝑥 DistS((𝑠𝑖 𝑤′

▷ else undefined

▷ DistR(𝐹 𝑟1 𝑟2) ⇒ match (intersection ((#f 𝑟1) (𝑟2 #f)) 𝑣)

) (𝑟 ′
2

𝑏2)) ⇒ Leaf(𝑥 DistR(𝐹 𝑟 ′
1
𝑑
(cid:74)

▷ ∅ | {𝑟1 . . . 𝑟𝑚} ⇒ undefined
▷ ((𝑏1 𝑟 ′
1
▷ 𝑣1 ⨿ · · · ⨿ 𝑣𝑚 ⇒ let1≤𝑖 ≤𝑚 𝑤𝑖 be D
in let {𝑛1, . . . , 𝑛𝑘 } be {𝑛 | 0 < 𝑤𝑛 }
in let1≤𝑖 ≤𝑘 𝑆𝑖 be (condition Leaf(𝑥 𝑑 𝜎) (Id(𝑥) in 𝑣𝑛𝑖 ))
in if (𝑘 = 1) then 𝑆1 else ⊕𝑘
𝑖=1

𝑖 𝑤𝑛𝑖 )

𝑟 ′
2
𝑣𝑖

) 𝜎)

(𝑆′

(cid:75)

▷ DistI(𝐹 𝑟1 𝑟2) ⇒ match (intersection ((#f 𝑟1) (𝑟2 #f)) 𝑣)

▷ {𝑟1 . . . 𝑟𝑚} ⇒ let1≤𝑖 ≤𝑚 𝑤𝑖 be D
𝑑
(cid:74)
in let {𝑛1, . . . , 𝑛𝑘 } be {𝑛 | 0 < 𝑤𝑛 }
in let1≤𝑖 ≤𝑘 𝑆𝑖 = (𝑥 DistI(𝐹 (𝑟𝑛𝑖 −1/2) 𝑟𝑛𝑖 ) 𝜎)
in if (𝑘 = 1) then 𝑆1 else ⊕𝑘
𝑖=1
▷ else // same as last two cases forDistR

𝑖 𝑤𝑛𝑖 )

{𝑟𝑖 }

(𝑆′

(cid:75)

𝑒)

condition ((𝑆1 𝑤1) ⊕ · · · ⊕ (𝑆𝑚 𝑤𝑚)) 𝑒 (cid:66)
let1≤𝑖 ≤𝑚 𝑤′
𝑖 be 𝑤𝑖 (P
𝑆𝑖
(cid:74)
in let {𝑛1, . . . , 𝑛𝑘 } be {𝑛 | 0 < 𝑤′
in let1≤𝑖 ≤𝑘 𝑆′
𝑖 be (condition 𝑆𝑛𝑖 𝑒)
1 else ⊕𝑘
(𝑆′
in if (𝑘 = 1) then 𝑆′
𝑖=1
(b) Conditioning Sum

𝑖 𝑤′
𝑛𝑖

𝑛 }

)

(cid:75)

condition (𝑆1 ⊗ · · · ⊗ 𝑆𝑚) 𝑒 (cid:66)
match disjoin 𝑒
▷ 𝑒1 ⊓ · · · ⊓ 𝑒ℎ ⇒ //one ℎ-dimensional hyperrectangle








match {1 ≤ 𝑗 ≤ ℎ | (vars 𝑒 𝑗 ) ⊂ (scope 𝑆𝑖 ) }

▷ {𝑛1, . . . , 𝑛𝑘 }







⇒ condition 𝑆𝑖 (𝑒𝑛1 ⊓ · · · ⊓ 𝑒𝑛𝑘 )

▷ { } ⇒ 𝑆𝑖

1≤𝑖 ≤𝑚

(cid:204)

▷ 𝑒1 ⊔ · · · ⊔ 𝑒ℓ ⇒ //ℓ ≥ 2 disjoint hyperrectangles
let1≤𝑖 ≤ℓ 𝑤𝑖 be P
𝑒𝑖
𝑆1 ⊗ · · · ⊗ 𝑆𝑚
(cid:74)
in let {𝑛1, . . . , 𝑛𝑘 } be {𝑛 | 0 < 𝑤𝑛 }
in let1≤𝑖 ≤𝑘 𝑆′
in if (𝑘 = 1) then 𝑆′

𝑖 be (condition (𝑆1 ⊗ · · · ⊗ 𝑆𝑚) 𝑒𝑛𝑖 )
1 else ⊕𝑘
𝑖=1

𝑖 𝑤𝑛𝑖 )

(𝑆′

(cid:75)

(a) Conditioning Leaf

(c) Conditioning Product

Listing 6. Implementation of condition for Leaf, Sum, and Product expressions using distribution semantics in Lst. 1e.

D.2 Algorithms for Conditioning Sum-Product

Expressions on Positive Measure Events

Having established the key background details, we now
prove Thm. 4.1 from the main text, which establishes the
closure under conditioning property of the SP domain.

Proof of Theorem. 4.1. We establish Eq. (5) by defining

condition : SPE → Event → SPE

(31)

which satisfies

P

(condition 𝑆 𝑒)

(cid:74)

𝑒 ′ =

P

𝑆
(cid:75)
(cid:74)
P

(cid:75)

(cid:75)
for all 𝑒 ′ ∈ Event and 𝑒 ∈ Event for which P

(𝑒 ⊓ 𝑒 ′)
𝑒
𝑆
(cid:74)

𝑆
(cid:74)

(cid:75)

(32)

𝑒 > 0.

We will define condition separately for each of the three
constructors Leaf, Sum, and Product from Lst. 9f. The proof
is by structural induction, where Leaf is the base case and
Sum and Product are the recursive cases.

Conditioning Leaf Lst. 6a shows the base cases of condition.
The case of 𝑑 ∈ DistStr is straightforward. For 𝑑 ∈ DistReal,
if the intersection (defined in second line of Lst. 6a) of 𝑣
with the support of 𝑑 is an interval ((𝑏 ′
)), then
1
it suffices to return a Leaf restricting 𝑑 to the interval. If the
intersection is a Union 𝑣1 ⨿ · · · ⨿ 𝑣𝑚 (recall fro Eq. (13) that

) (𝑟 ′
2

, 𝑏 ′
2

𝑟 ′
1

intersection ensures the 𝑣𝑖 are disjoint), then the conditioned
SPE is a Sum, whose 𝑖th child is obtained by recursively call-
ing condition on 𝑣𝑖 and 𝑖th (relative) weight is the probability
of 𝑣 under 𝑑, since, for any new 𝑣 ′ ∈ Outcomes, we have

D

𝑑
(cid:74)

(cid:75)

(intersect 𝑣 ′ (𝑣1 ⨿ · · · ⨿ 𝑣𝑚))
D
(𝑣1 ⨿ · · · ⨿ 𝑣𝑚)
(cid:75)
𝑑
(cid:74)

𝑑
(cid:74)
D

(33)

.

(cid:75)

=

D

⨿𝑚
𝑖=1(intersect 𝑣 ′ 𝑣𝑖 )
(cid:205)𝑚
𝑣𝑖
𝑑
𝑖=1
(cid:74)
Eq. (33) follows from the additivity of D
. The plots of 𝑋
(cid:75)
in Figs. 4e and 4f illustrate the identity in Eq. (33), where
conditioning the unimodal normal distribution results in a
mixture of three restricted normals whose weights are given
by the relative prior probabilities of the three regions.

𝑑
(cid:74)

(cid:75)

For 𝑑 ∈ DistInt, if the positive probability Outcomes are
{𝑟1 . . . 𝑟𝑚}, then the conditioned SPE is a Sum of “delta”-CDFs
whose atoms are located on the integers 𝑟𝑖 and weights are
the (relative) probabilities D
{𝑟𝑖 } (𝑖 = 1, . . . , 𝑚). Since
𝑑
(cid:74)
the atoms of 𝐹 for DistInt are integers, it suffices to restrict
𝐹 to the interval (𝑟𝑖 − 1/2, 𝑟𝑖 ), for each 𝑟𝑖 with a positive
weight. Correctness again follows from Eq. (33), since finite
sets are unions of disjoint singleton sets. For other positive
probability Outcomes, the conditioning procedure DistInt
is the same as that for DistReal.

(cid:75)

SPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

(cid:75)

(cid:74)

P

=

𝑤𝑖 P

(𝑒 ⊓ 𝑒 ′)
𝑒

Conditioning Sum Lst. 6b shows condition for 𝑆 ∈ Sum.
Recalling the denotation P
for 𝑆 ∈ Sum in Lst. 1f, the
𝑆
(cid:74)
correctness follows from the following properties:
(𝑆1 𝑤1) ⊕ · · · ⊕ (𝑆𝑚 𝑤𝑚)
(cid:75)
(𝑆1 𝑤1) ⊕ · · · ⊕ (𝑆𝑚 𝑤𝑚)
P
(cid:74)
(cid:75)
(cid:205)𝑚
(𝑒 ⊓ 𝑒 ′)
𝑆𝑖
𝑖=1
(cid:75)
(cid:74)
𝑤𝑖 P
𝑆𝑖
𝑒
(cid:74)
(cid:75)
𝑒)P
𝑆𝑖
(cid:74)
(cid:75)
(cid:205)𝑚
𝑖=1

(cid:74)
𝑤𝑖 P
((condition 𝑆𝑖 𝑒) , 𝑤𝑖 P
(37)
where Eq. (36) has applied Eq. (32) inductively for each 𝑆𝑖 .
Eqs. (35)–(36) assume for simplicity that P
𝑒 > 0 for each
𝑖 = 1, . . . , 𝑚, whereas Lst. 6a does not make this assumption.

(condition 𝑆𝑖 𝑒)
𝑒

(cid:205)𝑚
𝑖=1
𝑤𝑖 (P

𝑒)
(cid:75)

(cid:205)𝑚
𝑖=1

⊕𝑚
𝑖=1

𝑆𝑖
(cid:74)

𝑆𝑖
(cid:74)

𝑆𝑖
(cid:74)

= P

(35)

(34)

(36)

𝑒 ′,

𝑒 ′

=

(cid:74)

(cid:75)

(cid:75)

(cid:75)

(cid:75)

Conditioning Product Lst. 6c how condition operates on
𝑆 ∈ Product. The first step is to invoke disjoin to rewrite
(dnf 𝑒) as ℓ ≥ 1 disjoint clauses 𝑒 ′
ℓ (recall from
Prop. D.6 that disjoin is semantics-preserving). The first pat-
tern in the match statement corresponds ℓ = 1, and the result
is a new Product, where the 𝑖th child is conditioned on the
literals of 𝑒1 whose variables are contained in scope 𝑆𝑖 (if any).
The second pattern returns a Sum of Product, since

1 ⊔ · · · ⊔ 𝑒 ′

(𝑒 ⊓ 𝑒 ′)
𝑒
((𝑒1 ⊔ · · · ⊔ 𝑒ℓ ) ⊓ 𝑒 ′)
(𝑒1 ⊔ · · · ⊔ 𝑒ℓ )

((𝑒1 ⊓ 𝑒 ′) ⊔ · · · ⊔ (𝑒ℓ ⊓ 𝑒 ′))
𝑆1 ⊗ · · · ⊗ 𝑆𝑚
(cid:74)
(𝑒𝑖 ⊓ 𝑒 ′)
𝑒𝑖

𝑒𝑖

(cid:75)

(cid:75)

P

𝑆1 ⊗ · · · ⊗ 𝑆𝑚
(cid:74)
P
P

(cid:75)

(cid:75)

(cid:75)
(cid:75)

𝑆1 ⊗ · · · ⊗ 𝑆𝑚
(cid:74)

𝑆1 ⊗ · · · ⊗ 𝑆𝑚
(cid:74)
𝑆1 ⊗ · · · ⊗ 𝑆𝑚
(cid:74)
P
𝑆1 ⊗ · · · ⊗ 𝑆𝑚
(cid:75)
(cid:74)
P
𝑖=1
𝑆1 ⊗ · · · ⊗ 𝑆𝑚
(cid:74)
P
𝑖=1
P
𝑆
(cid:74)

𝑆1 ⊗ · · · ⊗ 𝑆𝑚
(cid:74)
𝑒𝑖 P
(cid:205)ℓ

P
(cid:205)ℓ

(cid:205)ℓ

𝑖=1

𝑖=1

P

(cid:205)ℓ

(cid:205)ℓ

=

=

=

=

(cid:75)

(cid:75)

(condition (𝑆1 ⊗ · · · ⊗ 𝑆𝑚) 𝑒𝑖 )
(cid:74)
P
𝑖=1
((condition 𝑆 𝑒𝑖 ) P

𝑆1 ⊗ · · · ⊗ 𝑆𝑚
(cid:74)
𝑆
(cid:74)

𝑒𝑖
(cid:75)
𝑒𝑖 )(cid:121) 𝑒 ′.

(cid:75)

𝑒 ′

(cid:75)

= P (cid:113)⊕ℓ

𝑖=1

Eq (42) follows from the induction hypothesis Eq. (32) and
(disjoin 𝑒𝑖 ) ≡ 𝑒𝑖 (idempotence), so that (disjoin 𝑒𝑖 ⊓ 𝑒 ′) ≡
(disjoin 𝑒𝑖 ) ⊓ (disjoin 𝑒 ′) ≡ 𝑒𝑖 ⊓ (disjoin 𝑒 ′).

Thm. 4.1 is thus established.

(38)

(39)

(40)

(41)

(42)

(43)

□

Fig. 5 in the main text shows an example of the closure
property from Thm. 4.1, where conditioning on a hyperrect-
angle changes the structure of the SPE from a Product into a
Sum-of-Product. The algorithms in this section are the first
to describe probabilistic inference and closure properties for
conditioning an SPE on a query that involves transforms of
random variables and predicates with set-valued constraints.
We next establish Thm. 4.3 from the main text, which gives
a sufficient condition for the runtime of condition (Lst. 6) to
scale linearly in the number of nodes in 𝑆; identical results
hold for computing Event probabilities (P
𝑒, Lst. 1f) and
probability densities (P0

𝑒, Lst. 1d).

𝑆
(cid:74)

(cid:75)

𝑆
(cid:74)

(cid:75)

Theorem 4.3. The runtime of (condition 𝑆 𝑒) scales linearly
in the number of nodes in the graph representing 𝑆 when-
ever 𝑒 is a single Conjunction (𝑡1 in 𝑣1) ⊓ · · · ⊓ (𝑡𝑚 in 𝑣𝑚) of
Containment constraints on non-transformed variables.

Proof. First, if 𝑆 is a Sum (Lst. 6b) with 𝑚 children then
(condition 𝑆 𝑒) makes no more than 𝑚 subcalls to condition
(one for each child), and if 𝑆 is a Leaf (Lst. 6a) then there are
zero subcalls, independently of 𝑒. Since each node has ex-
actly one parent, we can can conclude that each node in 𝑆 is
visited exactly once by showing that the hypothesis on 𝑒 im-
plies that for any 𝑆 ∈ Product (Lst. 6c) with 𝑚 children, there
are makes at most 𝑚 subcalls to condition from which we
(each node has exactly one parent). Suppose that (disjoin 𝑒)
returns a single Conjunction. Then the first pattern of the
match statement in Lst. 6c is matched (one ℎ-dimensional
rectangle), resulting in 𝑚 subcalls to condition. Thus, each
node in 𝑆 is visited (at most) once by condition. To complete
the proof, note that the hypothesis that 𝑒 specifies a single
Conjunction (𝑡1 in 𝑣1) ⊓ · · · ⊓ (𝑡𝑚 in 𝑣𝑚) of Containment
constraints on non-transformed variables is sufficient for
□
(disjoin 𝑒) to return a single Conjunction.

D.3 Conditioning Sum-Product Expressions on

Measure Zero Equality Constraints

Recall from Remark 4.2 in the main text that SPE is also
closed under conditioning on a Conjuction of possibly mea-
sure zero equality constraints of non-transformed variable,
such as {𝑋 = 3, 𝑌 = 𝜋, 𝑍 = "foo"}. In this section, we
describe the conditioning algorithm for this case, which is
implemented by

condition0 : SPE → Event → SPE,
(44)
where 𝑒 ∈ Event satisfies the follows requirements with
respect to 𝑆 ∈ SPE:

1. Either 𝑒 ≡ (Id(𝑥) in {rs}) or 𝑒 is a Conjunction of
such literals, where ≡ here denote syntactic (not se-
mantic) equivalence.

2. Every Var 𝑥 in each literal of 𝑒 is a non-transformed
variable; i.e., for each Leaf expression 𝑆 such that 𝑥 ∈
scope 𝑆, we have 𝑆 ≡ Leaf(𝑥 𝑑 𝜎), for some 𝑑 and 𝜎.
With these requirements on 𝑒, Lst. 7 presents the imple-
mentation of condition0, leveraging the generalized density
semantics from Lst. 1d in the main text. The inference rules
closely match those for standard sum-product networks, ex-
cept for the fact that a density from P0
is a pair, whose
first entry is the number of continuous distributions partici-
pating in the weight of the Event 𝑒 which must be correctly
accounted for by condition0. In the reference implementa-
tion of Sppl, condition invokes condition and constrain
invokes condition0. Analogously to the prob query, which
returns probabilities using the distribution semantics P in
Lst. 9e, Sppl also includes the density query, which returns
densities using the generalized semantics P0 in Lst. 1d.

𝑆
(cid:74)

(cid:75)

PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

(Leaf)

𝑑 ⇑ 𝐷(𝐸), 𝑡1 ⇑ 𝐸1, . . . , 𝑡𝑚 ⇑ 𝐸𝑚

(𝑥 𝑑 {𝑥 ↦→ Id(𝑥), 𝑥1 ↦→ 𝑡1, . . . , 𝑥𝑚 ↦→ 𝑡𝑚 })

→Sppl 𝑥 ~ 𝐷(𝐸);𝑥1 = 𝐸1; . . . ;𝑥𝑚 = 𝐸𝑚

(Product)
𝑆1 →Sppl 𝐶1, . . . , 𝑆𝑚 →Sppl 𝐶𝑚

⊗𝑚
𝑖=1

𝑆𝑖 →Sppl 𝐶1; . . . ;𝐶𝑚

(Sum)

𝑆1 →Sppl 𝐶1, . . . , 𝑆𝑚 →Sppl 𝐶𝑚; where 𝑏 is a fresh Var

⊕𝑚
𝑖=1

(𝑆𝑖 𝑤𝑖 ) →Sppl

𝑏 ~ choice({'1':𝑤1, . . . ,'𝑚':𝑤𝑚})




if (𝑏 == '1') {𝐶1}




elif . . .






elif (𝑏 == '𝑚') {𝐶𝑚}





Listing 8. Translating an element of SPE (Lst. 9f) to an Sppl
command 𝐶 (Lst. 2).

Instead, it can be shown that →Sppl is a semantics-preserving
inverse of →SPE, in the sense that for all 𝑒 ∈ Event
((𝐶 →∗
𝑒 = P

𝑆 ′ =⇒ P

𝐶 ′) →∗

𝑆) →∗

𝑒.(46)

SPE

Sppl

SPE

𝑆
(cid:74)

(cid:75)

𝑆 ′
(cid:74)

(cid:75)

Eq. (46) establish a formal semantic correspondence between
the Sppl language and the class of sum-product expressions:
each Sppl program admits a representation as an SPE, and
each valid element of SPE that satisfies conditions (C1)–(C5)
expression corresponds to some Sppl program.

Thus, in addition to synthesizing full Sppl programs from
data using the PPL synthesis systems [13, 53] mentioned
in Sec. 7, it is also possible with the translation strategy in
Lst. 8 to synthesize Sppl programs using the wide range
of techniques for learning the structure and parameters of
sum-product networks [26, 37, 62, 65]. With this approach,
Sppl (i) provides users with a uniform representation of
existing sum-product networks as generative source code in
a formal PPL (Lst. 2); (ii) allows users to extend these baseline
programs with modeling extensions supported by the core
calculus (Lst. 1), such as predicates for decision trees and
numeric transformations; and (iii) delivers exact answers
to an extended set of probabilistic inference queries (Sec. 4)
within the modular and reusable workflow from Fig. 1.

condition0 Leaf(𝑥 𝑑 𝜎) (Id(𝑥) in {rs}) (cid:66) match 𝑑

▷ DistR(𝐹 𝑟1 𝑟2) ⇒ match rs

▷ 𝑟 ⇒ match (P0

Leaf(𝑥 𝑑 𝜎)
▷ (1, 0) ⇒ undefined
(cid:75)
(cid:74)
▷ else let ˜𝐹 be (cid:0)𝜆𝑟 ′. 1 (cid:2)𝑟 ≤ 𝑟 ′(cid:3) (cid:1) in DistI( ˜𝐹 (𝑟 − 1/2) 𝑟 )

(Id(𝑥) in {rs}))

▷ 𝑠 ⇒ undefined

▷ else ⇒ condition Leaf(𝑥 𝑑 𝜎) (Id(𝑥) in {rs})

(a) Conditioning Leaf

condition0 ((𝑆1 𝑤1) ⊕ · · · ⊕ (𝑆𝑚 𝑤𝑚)) (cid:0)⊓ℓ
let1≤𝑖 ≤𝑚 (𝑑𝑖, 𝑝𝑖 ) be P0
in if ∀1≤𝑖 ≤𝑚 . 𝑝𝑖 = 0 then undefined

𝑖=1
(Id(𝑥𝑖 ) in {rsi})(cid:1)

𝑆𝑖
(cid:74)

(cid:0)⊓ℓ

𝑖=1

(cid:75)

(Id(𝑥𝑖 ) in {rsi})(cid:1) (cid:66)

else let1≤𝑖 ≤𝑚 𝑤′

𝑖 be 𝑤𝑖 𝑝𝑖

in let 𝑑 ∗ be min{𝑑𝑖 | 1 ≤ 𝑖 ≤ 𝑚, 0 < 𝑝𝑖 }
in let{𝑛1, . . . , 𝑛𝑘 } be {𝑛 | 0 < 𝑤′
𝑖 be (cid:0)condition0 𝑆𝑛𝑖
in let1≤𝑖 ≤𝑘 𝑆′
1 else ⊕𝑘
in if (𝑘 = 1) then 𝑆′
𝑖=1
(b) Conditioning Sum

𝑛, 𝑑𝑖 = 𝑑 ∗ }
(cid:0)⊓ℓ
𝑖=1
𝑖 𝑤𝑛𝑖 )

(𝑆′

(Id(𝑥𝑖 ) in {rsi})(cid:1)(cid:1)

condition0 (𝑆1 ⊗ · · · ⊗ 𝑆𝑚) ⊓ℓ

let1≤𝑖 ≤𝑚 𝑆′

𝑖=1
𝑖 be match {𝑥1, . . . , 𝑥𝑚 } ∩ (scope 𝑆𝑖 )

(Id(𝑥𝑖 ) in {rsi}) (cid:66)

▷ {𝑛1, . . . , 𝑛𝑘 } ⇒ condition0 𝑆𝑖 ⊓𝑘
▷ { } ⇒ 𝑆𝑖
1 ⊗ · · · ⊗ 𝑆′
𝑚

𝑡 =1

in 𝑆′

(Id(𝑥𝑛𝑡 ) in {rst })

(c) Conditioning Product
Listing 7. Implementation of condition0 for Leaf, Sum, and
Product expressions using density semantics in Lst. 1d.

E Translating Sum-Product Expressions to

Sppl Programs

Lst. 3 in Sec. 5 presents the relation →SPE, that translates
𝐶 ∈ Command (i.e., Sppl source syntax, Lst. 2) to a sum-
product expression 𝑆 ∈ SPE in the core language (Lst. 9).
Lst. 8 defines a relation →Sppl
that reverses the →SPE re-
lation: it converts expression 𝑆 ∈ SPE to 𝐶 ∈ Command.
Briefly, (i) a Product is converted to a sequence Command;
(ii) a Sum is converted to an if-else Command; and (iii) a
Leaf is converted to a sequence of sample (~) and transform
(=). The symbol ⇑ (whose definition is omitted) in the (Leaf)
rule converts semantic elements such as 𝑑 ∈ Distribution
and 𝑡 ∈ Transform from the core calculus (Lst. 1) to an Sppl
expression 𝐸 ∈ Expr (Lst. 2) in a straightforward way, e.g.,

(Poly(Id(X) 1 2 3)) ⇑ (1 + 2*X + 3*X**2).

(45)

It is easy to see that chaining →SPE (Lst. 3) and →Sppl
(Lst. 8) for a given Sppl program does not preserve either
Sppl or core syntax, that is3

((𝐶 →∗
((𝐶 →∗

SPE

SPE

𝑆) →∗
𝑆) →∗

Sppl

Sppl

𝐶 ′)
𝐶 ′) →∗

SPE

𝑆 ′

does not imply 𝐶 = 𝐶 ′
does not imply 𝑆 = 𝑆 ′.

3The symbol 𝐶 →∗
𝑆 means ⟨𝐶, 𝑆∅ ⟩ translates to 𝑆 in zero or more steps
of →SPE, where 𝑆∅ is an “empty” SP used for the initial translation step,
and similarly for →∗

SPE

Sppl .

SPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

𝑥 ∈ Var
𝑛 ∈ Natural
𝑏 ∈ Boolean (cid:66) {#t, #f}
𝑢 ∈ Unit (cid:66) {#u}
𝑤 ∈ [0, 1]
𝑟 ∈ Real ∪ {−∞, ∞}
𝑠 ∈ String (cid:66) Char∗

(a) Basic Sets

rs ∈ Outcome (cid:66) Real + String
𝑣 ∈ Outcomes

(cid:66) ∅
| {𝑠1 . . . 𝑠𝑚 }𝑏
| {𝑟1 . . . 𝑟𝑚 }
| ((𝑏1 𝑟1) (𝑟2 𝑏2))
| 𝑣1 ⨿ · · · ⨿ 𝑣𝑚

[Empty]
[FiniteStr]
[FiniteReal]
[Interval]
[Union]

𝑡 ∈ Transform
(cid:66) Id(𝑥)
| Reciprocal(𝑡 )
| Abs(𝑡 )
| Root(𝑡 𝑛)
| Exp(𝑡 𝑟 )
| Log(𝑡 𝑟 )
| Poly(𝑡 𝑟0 . . . 𝑟𝑚 )
| Piecewise((𝑡1 𝑒1)

. . .
(𝑡𝑚 𝑒𝑚 ))

[Identity]
[Reciprocal]
[AbsValue]
[Radical]
[Exponent]
[Logarithm]
[Polynomial]
[Piecewise]

(c) Transformations

𝑒 ∈ Event

(cid:66) (𝑡 in 𝑣)
| 𝑒1 ⊓ · · · ⊓ 𝑒𝑚
| 𝑒1 ⊔ · · · ⊔ 𝑒𝑚

[Containment]
[Conjunction]
[Disjunction]

(b) Outcomes

(d) Events

Listing 9. Core calculus.

𝐹 ∈ CDF ⊂ Real → [0, 1]

(cid:66) Norm(𝑟1, 𝑟2) | Poisson(𝑟 ) | Binom(𝑛, 𝑤) . . .
where 𝐹 is càdlàg;
𝐹 (𝑟 ) = 1;

lim
lim
𝑟 →−∞
𝑟 →∞
and 𝐹 −1 (𝑢) (cid:66) inf {𝑟 | 𝑢 ≤ 𝐹 (𝑟 ) }.

𝐹 (𝑟 ) = 0;

𝑑 ∈ Distribution

(cid:66) DistR(𝐹 𝑟1 𝑟2)
| DistI(𝐹 𝑟1 𝑟2)
| DistS((𝑠1 𝑤1) . . . (𝑠𝑚 𝑤𝑚 ))

[DistReal]
[DistInt]
[DistStr]

(e) Primitive Distributions

𝜎 ∈ Environment (cid:66) Var → Transform
𝑆 ∈ SPE

(cid:66) Leaf(𝑥 𝑑 𝜎)
| (𝑆1 𝑤1) ⊕ · · · ⊕ (𝑆𝑚 𝑤𝑚 )
| 𝑆1 ⊗ · · · ⊗ 𝑆𝑚

[Leaf ]
[Sum]
[Product]

(f) Sum-Product

complement {𝑠1 . . . 𝑠𝑚}𝑏 (cid:66) {𝑠1 . . . 𝑠𝑚}¬𝑏

complement ((𝑏1 𝑟1) (𝑟2 𝑏2)) (cid:66) ((#f −∞) (𝑟1 ¬𝑏1)) ⨿ ((¬𝑏2 𝑟2) (∞ #f))

complement {𝑟1 . . . 𝑟𝑚} (cid:66) ((#f −∞) (𝑟1 #t))

((#t 𝑟 𝑗−1) (𝑟 𝑗 #t))(cid:3)

⨿ (cid:2)⨿𝑚
𝑗=2
⨿ ((#t 𝑟𝑚) (∞ #f))

complement ∅ (cid:66) {}#t ⨿ ((#f −∞) (∞ #f))

Listing 10. Implementation of complement on the sum domain Outcomes.

vars : (Transform + Event) → P (Vars)

vars te = match te

▷ 𝑡 ⇒ match 𝑡

▷ Id(𝑥) ⇒ {𝑥 }
▷ Root(𝑡 ′ 𝑛) | Exp(𝑡 ′ 𝑟 ) | Log(𝑡 ′ 𝑟 ) | Abs(𝑡 ′)
| Reciprocal(𝑡 ′) | Poly(𝑡 ′ 𝑟0 . . . 𝑟𝑚)
⇒ vars 𝑡 ′
▷ Piecewise((𝑡𝑖 𝑒𝑖 )𝑚
𝑖=1

) ⇒ ∪𝑚

𝑖=1((vars 𝑡𝑖 ) ∪ (vars 𝑒𝑖 ))

▷ (𝑡 in 𝑣) ⇒ vars 𝑡
▷ (𝑒1 ⊓ · · · ⊓ 𝑒𝑚) | (𝑒1 ⊔ · · · ⊔ 𝑒𝑚) ⇒ ∪𝑚

𝑖=1vars 𝑒𝑖

Listing 11. Implementation of vars, which returns the variables in a Transform or Event.

scope : SPE → P (Var)
scope (𝑥 𝑑 𝜎) (cid:66) dom(𝜎)
scope (𝑆1 ⊗ · · · ⊗ 𝑆𝑚) (cid:66) ∪𝑚
scope ((𝑆1 𝑤1) ⊕ · · · ⊕ (𝑆𝑚 𝑤𝑚)) (cid:66) (scope 𝑆1)

𝑖=1(scope 𝑆𝑖 )

Listing 12. Implementation of scope, which returns the set of variables in an element of SPE.

PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

subsenv : Event → Environment → Event
subsenv 𝑒 𝜎 (cid:66) let {𝑥, 𝑥1, . . . , 𝑥𝑚 } = dom(𝜎)
in let 𝑒1 be subs 𝑒 𝑥𝑚 𝜎 (𝑥𝑚)
. . .
in let 𝑒𝑚 be subs 𝑒𝑚−1 𝑥1 𝜎 (𝑥1)
in 𝑒𝑚

Listing 13. Implementation of subsenv, which rewrites 𝑒 as an Event 𝑒 ′ on one variable 𝑥.

negate : Event → Event
negate (𝑡 in 𝑣) (cid:66) match (complement 𝑣)

▷ 𝑣1 ⨿ · · · ⨿ 𝑣𝑚 ⇒ (𝑡 in 𝑣1) ⊔ · · · ⊔ (𝑡 in 𝑣𝑚)
▷ 𝑣 ⇒ (𝑡 in 𝑣)
negate (𝑒1 ⊓ · · · ⊓ 𝑒𝑚) (cid:66) ⊔𝑚
negate (𝑒1 ⊔ · · · ⊔ 𝑒𝑚) (cid:66) ⊓𝑚

𝑖=1(negate 𝑒𝑖 )
𝑖=1(negate 𝑒𝑖 )

Listing 14. Implementation of negate, which applies De Morgan’s laws to an Event.

dnf : Event → Event
dnf (𝑡 in 𝑣) (cid:66) (𝑡 in 𝑣)
dnf 𝑒1 ⊔ · · · ⊔ 𝑒𝑚 (cid:66) ⊔𝑚
𝑖=1(dnf 𝑒𝑖 )
dnf 𝑒1 ⊓ · · · ⊓ 𝑒𝑚 (cid:66) let1≤𝑖 ≤𝑚 (𝑒 ′
𝑚
(cid:108)
𝑖=1

(cid:196)

in

𝑒 ′
𝑖,𝑗𝑖

1≤ 𝑗1 ≤𝑘1
...
1≤ 𝑗𝑚 ≤𝑘𝑚

𝑗1 ⊓ · · · ⊓ 𝑒 ′
𝑗,𝑘𝑖

) be dnf 𝑒𝑖

Listing 15. dnf converts and Event to DNF (Def. D.1).

disjoint? : Event × Event → Boolean
disjoint? ⟨𝑒1, 𝑒2⟩ (cid:66) match ⟨𝑒1, 𝑒2⟩
𝑖=1(Id(𝑥1,𝑖 ) in 𝑣1,𝑖 ), ⊓𝑚2
▷ ⟨⊓𝑚1
⇒ (cid:2)∃1≤𝑖 ≤2.∃1≤ 𝑗 ≤𝑚𝑖 .𝑣𝑖 𝑗 = ∅)(cid:3) ∨
▷ else ⇒ undefined

𝑖=1(Id(𝑥2,𝑖 ) in 𝑣2,𝑖 )⟩

(cid:20)let {⟨𝑛1𝑖, 𝑛2𝑖 ⟩}𝑘
in (∃1≤𝑖 ≤𝑘 .(intersection 𝑣1,𝑛1,𝑖 𝑣2,𝑛2,𝑖 ) = ∅)

𝑖=1 be {⟨𝑖, 𝑗⟩ | 𝑥1,𝑖 = 𝑥2,𝑗 }

(cid:21)

Listing 16. disjoint? returns #t if two Events are disjoint (Def. D.5).

SPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

T : Transform → (Real → Real)

T

T

T

T

Id(𝑥)
(cid:75)
(cid:74)
Reciprocal(𝑡)
(cid:75)
(cid:74)
Abs(𝑡)
(cid:75)
(cid:74)
Root(𝑡 𝑛)
(cid:75)
Exp(𝑡 𝑟 )
(cid:75)
Log(𝑡 𝑟 )
(cid:75)
(cid:74)
Poly(𝑡 𝑟0 . . . 𝑟𝑚)
(cid:75)
)
(cid:75)

T
Piecewise((𝑡𝑖 𝑒𝑖 )𝑚
𝑖=1

(cid:74)
T

T

(cid:74)

(cid:74)

T

(cid:74)

(𝑟 ′)

(cid:75)
(𝑟 ′))

(𝑟 ′))

(cid:75)
𝑡
(cid:74)
𝑡
(cid:75)
(cid:74)

𝑡
(cid:74)
(cid:75)
(𝑟 ′)|

(cid:66) 𝜆𝑟 ′. 𝑟 ′
(cid:66) 𝜆𝑟 ′. 1/(T
(cid:66) 𝜆𝑟 ′. |T
𝑡
(cid:74)
(cid:66) 𝜆𝑟 ′. 𝑛√︃
T
(cid:66) 𝜆𝑟 ′. 𝑟 (T
(𝑟 ′))
(cid:66) 𝜆𝑟 ′. log𝑟 (T
(cid:66) 𝜆𝑟 ′. (cid:205)𝑚
𝑡
𝑖=0
(cid:74)
(cid:75)
(cid:66) 𝜆𝑟 ′. if (cid:2)(↓ Real
Outcome
else if . . .
else if (cid:2)(↓ Real
else undefined

𝑡
(cid:74)
(cid:75)
𝑟𝑖 (T

(iff 0 < 𝑟 )
(iff 0 < 𝑟 )

E

𝑥

𝑒1
(cid:74)

(cid:75)

(cid:75)

(cid:74)

(cid:3) then T

𝑟 ′

𝑡1
(cid:74)

(cid:75)

(𝑟 ′))𝑖
𝑟 ′) ∈ V

Outcome

𝑟 ′) ∈ V

E

(cid:74)

𝑥

𝑒𝑚
(cid:74)

(cid:75)

(cid:75)

(cid:3) then T

𝑟 ′

𝑡𝑚
(cid:74)

(cid:75)

(iff (vars 𝑡1) = . . . = (vars 𝑡𝑚)

= (vars 𝑒1) = · · · = (vars 𝑒𝑚) (cid:67) {𝑥 }

Listing 17. Semantics of Transform.

domainof : Transform → Outcomes

domainof Id(𝑥) (cid:66) ((#f −∞) (∞ #f))

domainof Reciprocal(𝑡) (cid:66) ((#f 0) (∞ #f))

domainof Abs(𝑡) (cid:66) ((#f −∞) (∞ #f))

domainof Root(𝑡 𝑛) (cid:66) ((#f 0) (∞ #f))
domainof Exp(𝑡 𝑟0) (cid:66) ((#f −∞) (∞ #f))
domainof Log(𝑡 𝑟0) (cid:66) ((#f 0) (∞ #f))

domainof Poly(𝑡 𝑟0 . . . 𝑟𝑚) (cid:66) ((#f −∞) (∞ #f))

domainof Piecewise((𝑡𝑖 𝑒𝑖 )𝑚
𝑖=1

) (cid:66) union [(intersection (domainof 𝑡𝑖 ) (E
where {𝑥 } (cid:66) vars 𝑡1

𝑒
(cid:74)

(cid:75)

𝑥)]𝑚
𝑖=1

Listing 18. domainof returns the Outcomes on which a Transform is defined.

PLDI ’21, June 20–25, 2021, Virtual, Canada

Feras A. Saad, Martin C. Rinard, and Vikash K. Mansinghka

preimg 𝑡 𝑣 (cid:66) preimage′ 𝑡 (intersection (domainof 𝑡) 𝑣)
preimage′ Id 𝑣 (cid:66) 𝑣
preimage′ 𝑡 ∅ (cid:66) ∅
preimage′ 𝑡 (𝑣1 ⨿ · · · ⨿ 𝑣𝑚) (cid:66) union (preimg 𝑡 𝑣1) . . . (preimg 𝑡 𝑣𝑚)
preimage′ 𝑡 {𝑟1 . . . 𝑟𝑚} (cid:66) preimg 𝑡 ′ (union (finv 𝑡 𝑟1) . . . (finv 𝑡 𝑟𝑚))
preimage′ 𝑡 ((𝑏left 𝑟left) (𝑟right 𝑏right)) (cid:66) match 𝑡
▷ Radical(𝑡 ′ 𝑛) | Exp(𝑡 ′ 𝑟 ) | Log(𝑡 ′ 𝑟 ) ⇒ let {𝑟 ′
left
in let {𝑟 ′
right
in preimg 𝑡 ′ ((𝑏left 𝑟 ′
left

} be finv 𝑡 𝑟left

} be finv 𝑡 𝑟right
) (𝑟 ′

right

𝑏right))

▷ Abs(𝑡 ′) ⇒ let 𝑣 ′

pos be ((𝑏left 𝑟left) (𝑟right 𝑏right))

in let 𝑣 ′
in preimg 𝑡 ′ (union 𝑣 ′

neg be ((𝑏right −𝑟right) (−𝑟left 𝑏left))
𝑣 ′
neg)

pos

▷ Reciprocal(𝑡 ′) ⇒ let ⟨𝑟 ′
left

, 𝑟 ′

right⟩ be if (0 ≤ 𝑟left < 𝑟right)

then ⟨if (0 < 𝑟left) then 1/𝑟left else ∞,

if (𝑟right < ∞) then 1/𝑟right else 0⟩

else ⟨if (−∞ < 𝑟left) then 1/𝑟left else 0,

in preimg 𝑡 ′ ((𝑏right 𝑟 ′

right

if (𝑟right < 0) then 1/𝑟right else −∞⟩
) (𝑟 ′

𝑏left))

left

▷ Polynomial(𝑡 𝑟0 . . . 𝑟𝑚) ⇒ let 𝑣 ′

left be polyLte ¬𝑏left 𝑟left 𝑟0 . . . 𝑟𝑚

▷ Piecewise((𝑡𝑖 𝑒𝑖 )𝑚
𝑖=1

) ⇒ let1≤𝑖 ≤𝑚 𝑣 ′

right be polyLte 𝑏right 𝑟right 𝑟0 . . . 𝑟𝑚

in let 𝑣 ′
in preimg 𝑡 ′ (intersection 𝑣 ′

right (complement 𝑣 ′
𝑖 be preimg 𝑡𝑖 ((𝑏left 𝑟left) (𝑏right 𝑟right))

left))

in let1≤𝑖 ≤𝑚 𝑣𝑖 be intersection 𝑣 ′
in union 𝑣1 . . . 𝑣𝑚

𝑖 (E
𝑒𝑖
(cid:74)
where {𝑥 } (cid:66) vars 𝑡1

𝑥),

(cid:75)

Listing 19. preimg computes the generalized inverse of a many-to-one Transform.

finv : Transform → Real → Outcomes

finv Id(𝑥) 𝑟 (cid:66) {𝑟 }

finv Reciprocal(𝑡) 𝑟 (cid:66) if (𝑟 = 0) then {−∞ ∞}else {1/𝑟 }

finv Abs(𝑡) 𝑟 (cid:66) {−𝑟 𝑟 }

finv Root(𝑡 𝑛) 𝑟 (cid:66) if (0 ≤ 𝑟 ) then {𝑟 𝑛} else ∅
finv Exp(𝑡 𝑟0) 𝑟 (cid:66) if (0 ≤ 𝑟 ) then {log𝑟0
finv Log(𝑡 𝑟0) 𝑟 (cid:66) {𝑟 𝑟
0

}

(𝑟 )} else ∅

finv (Polynomial 𝑡 𝑟0 . . . 𝑟𝑚) 𝑟 (cid:66) polySolve 𝑟 𝑟0 𝑟1 . . . 𝑟𝑚

finv (Piecewise (𝑡𝑖 𝑒𝑖 )𝑚
𝑖=1

) (cid:66) union [(intersection (finv 𝑡𝑖 𝑟 ) (E
where {𝑥 } (cid:66) vars 𝑡1

𝑒𝑖
(cid:74)

(cid:75)

𝑥))]𝑚
𝑖=0

,

Listing 20. finv computes the generalized inverse of a many-to-one transform at a single Real.

SPPL: Probabilistic Programming with Fast Exact Symbolic Inference

PLDI ’21, June 20–25, 2021, Virtual, Canada

polyLim : Real+ → Real2
polyLim 𝑟0 (cid:66) ⟨𝑟0, 𝑟0⟩
polyLim 𝑟0 𝑟1 . . . 𝑟𝑚 (cid:66)

let 𝑛 be max{ 𝑗 | 𝑟 𝑗 > 0}
in if (even 𝑛) then (if (𝑟𝑛 > 0) then ⟨∞, ∞⟩ else ⟨−∞, −∞⟩)
else (if (𝑟𝑛 > 0) then ⟨−∞, ∞⟩ else ⟨∞, −∞⟩)

Listing 21. polyLim computes the limits of a polynomial limits at the infinities.

polySolve : Real → Real+ → Set
polySolve : 𝑟 𝑟0 . . . 𝑟𝑚 (cid:66) match 𝑟

▷ (∞ | −∞) ⇒ let ⟨𝑟neg, 𝑟pos⟩

in let f
in let 𝑣neg
in let 𝑣pos
in

be polyLim 𝑟0 . . . 𝑟𝑚
be 𝜆𝑟 ′. if (𝑟 = ∞) then (𝑟 ′ = ∞) else (𝑟 ′ = −∞)
be if (f 𝑟neg) then {−∞} else ∅
be if (f 𝑟pos) then {∞} else ∅
union 𝑣pos 𝑣neg

▷ else

⇒ (roots (𝑟0 − 𝑟 ) 𝑟1 . . . 𝑟𝑚)

Listing 22. polySolve computes the set of values at which a polynomial is equal to a specific value 𝑟 .

polyLte : Boolean → Real → Real+ → Outcomes
polyLte 𝑏 𝑟 𝑟0 . . . 𝑟𝑚 (cid:66) match 𝑟

▷ −∞ ⇒ if 𝑏 then ∅ else (polySolve 𝑟 𝑟0 . . . 𝑟𝑚)
▷ ∞ ⇒ if ¬𝑏 then ((#t −∞) (∞ #t))

else let ⟨𝑟left, 𝑟right⟩ be polyLim 𝑟0 . . . 𝑟𝑚

in let ⟨𝑏left, 𝑏right⟩ be ⟨𝑟left = ∞, 𝑟right = ∞⟩
in ((𝑏left −∞) (∞𝑏right))
▷ else ⇒ let [𝑟s,𝑖 ]𝑘
𝑖=1 be roots (𝑟0 − 𝑟 ) 𝑟1 . . . 𝑟𝑚
right,𝑖 ⟩]𝑘
left,𝑖, 𝑟 ′
in let [⟨𝑟 ′
in let fmid be 𝜆𝑟𝑟 ′. if

(𝑟 = −∞)
(𝑟 ′ = ∞)
(𝑟 + 𝑟 ′)/2
𝑡 ′ be Poly(Id(x) (𝑟0 − 𝑟 ) 𝑟1 . . . 𝑟𝑚)

elseif

else

then 𝑟 ′
then 𝑟

𝑖=0 be [⟨−∞, 𝑟s,0⟩, ⟨𝑟s,1, 𝑟s,2⟩, . . . , ⟨𝑟s,𝑘−1, 𝑟s,𝑘 ⟩, ⟨𝑟s,𝑘, ∞⟩]

in union

(cid:34)if T

𝑡 ′
(cid:74)

(cid:75)

(fmid 𝑟 ′

left,𝑖 𝑟 ′

right,𝑖 )

then ((𝑏 𝑟 ′
else ∅

left,𝑖 ) (𝑟 ′

right,𝑖 𝑏))

(cid:35)𝑘

𝑖=0

Listing 23. polyLte computes the set of values at which a polynomial is less than a given value 𝑟 .

