Generalized Policy Iteration for Optimal Control in
Continuous Time

Jingliang Duan, Shengbo Eben Li*, Zhengyu Liu, Monimoy Bujarbaruah, and Bo Cheng

1

9
1
0
2

p
e
S
1
1

]

Y
S
.
s
s
e
e
[

1
v
2
0
4
5
0
.
9
0
9
1
:
v
i
X
r
a

Abstractâ€”This paper proposes the Deep Generalized Policy
Iteration (DGPI) algorithm to ï¬nd the inï¬nite horizon optimal
control policy for general nonlinear continuous-time systems with
known dynamics. Unlike existing adaptive dynamic programming
algorithms for continuous time systems, DGPI does not require
the â€œadmissibilityâ€ of initialized policy, and input-afï¬ne nature
of controlled systems for convergence. Our algorithm employs
the actor-critic architecture to approximate both policy and
value functions with the purpose of
iteratively solving the
Hamilton-Jacobi-Bellman equation. Both the policy and value
functions are approximated by deep neural networks. Given
any arbitrary initial policy, the proposed DGPI algorithm can
eventually converge to an admissible, and subsequently an
optimal policy for an arbitrary nonlinear system. We also relax
the update termination conditions of both the policy evaluation
and improvement processes, which leads to a faster convergence
speed than conventional Policy Iteration (PI) methods, for the
same architecture of function approximators. We further prove
the convergence and optimality of the algorithm with thorough
Lyapunov analysis, and demonstrate its generality and efï¬cacy
using two detailed numerical examples.

Index Termsâ€”Dynamic programming, Reinforcement learn-

ing, Optimal control.

I. INTRODUCTION

D YNAMIC programming offers a theoretical and sys-

tematic way to solve Continuous Time (CT) inï¬nite
horizon optimal control problems with known dynamics for
unconstrained linear systems, by employing the principle
of Bellman optimality via the solution of the underlying
Hamilton-Jacobi-Bellman (HJB) equation [1]. This yields the
celebrated Linear Quadratic Regulator, where the optimal
control policy is an afï¬ne state feedback [2]. However, if
the system is subject to operating constraints, or is modeled
by nonlinear dynamics, solving an inï¬nite horizon optimal
control problem analytically is a challenging task. This is
due to the fact that it is difï¬cult to get an analytical solution
of the HJB equation (typically nonlinear partial differential
equation [3]) by applying traditional DP, as the computation
grows exponentially with increase in the dimensionality of the
system, summarized by the phrase curse of dimensionality [4].

All correspondences should be sent to S. Li with email: lisb04@gmail.com.
J. Duan, S. Li, Z. Liu and B., Cheng are with State Key Lab of Automotive
Safety and Energy, School of Vehicle and Mobility, Tsinghua University,
Beijing, 100084, China. They are also with Center for Intelligent Connected
Vehicles and Transportation, Tsinghua University, Beijing, China. Email:
(djl15, liuzheng17)@mails.tsinghua.edu.cn;
(lishbo, chengbo)@tsinghua.edu.cn.

To ï¬nd a suboptimal approximation to the optimal control
policy for nonlinear dynamics, Werbos deï¬ned a family of
actor-critic algorithms, which he termed Adaptive Dynamic
Programming (ADP) algorithms [5], [6]. Another well-known
name for this kind of algorithms, especially in the ï¬eld of
machine learning, is Reinforcement Learning (RL) [7], [8].
A distinct feature of the ADP method is that it employs a
critic parameterized function, such as a Neural Network (NN)
for value function approximation and an actor parameterized
function for policy approximation. For the sake of ï¬nding
suitable approximation of both value function and policy,
most ADP methods adopt an iterative technique, called Policy
Iteration (PI) [9]. PI refers to a class of algorithms built
as a two-step iteration: 1) policy evaluation, in which the
value function associated with an admissible control policy
is evaluated, and 2) policy improvement, in which the policy
is updated to optimize the corresponding value function, using
Bellmanâ€™s principle of optimality.

Over the last few decades, numerous ADP (or RL) methods
and the inherent analyses have appeared in literature for
controlling autonomous systems [10]â€“[17], including for CT
systems. Some of these algorithms for CT systems are also
called approximate DP or neuro-DP [18]â€“[21]. Abu-Khalaf
and Lewis (2005) proposed an ADP algorithm to ï¬nd nearly
optimal constrained control state feedback laws for general
nonlinear systems by introducing a non-quadratic cost function
[22]. The value function represented by a linear combination
of artiï¬cially designed basis functions is trained by least-
squares method at the policy evaluation step, while the policy
is directly derived from the value function. Utilizing the same
single approximator scheme, Dierks and Jagannathan (2010)
derived a novel online parameter tuning law that not only
ensures the optimal value function and policy are achieved,
but also ensures the system states remain bounded during
the online learning process [23]. Vamvoudakis and Lewis
(2012) proposed a synchronous PI algorithm implemented
as actor-critic architecture for nonlinear CT systems without
control constraints. Both the value function and policy are
approximated by linear methods and tuned simultaneously
online [24]. Furthermore, Vamvoudakis (2014) presented an
event-triggered ADP algorithm that reduces the computation
cost by updating the policy only when an event-triggered
condition was violated [25]. Dong et al. (2017) extended this
idea to nonlinear systems with saturated actuators [26]. In
addition, the ADP method has also been widely applied in
the optimal control of incompletely known dynamic systems
[27]â€“[31] and multi-agent systems [32], [33].

J. Duan and M. Bujarbaruah are with Department of Mechanical Engineer-
ing, University of California Berkeley, Berkeley, CA 94720, USA. Email:
It should be pointed out that most existing ADP techniques
monimoyb@berkeley.edu.
Â©2019 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including
reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or
reuse of any copyrighted component of this work in other works.

 
 
 
 
 
 
for CT systems are valid on the basis of one or both of the
following two assumptions:

â€¢ A1: Admissibility of Initial Policy: The inï¬nite horizon
value function can be evaluated only in the case of
stabilizing control policies. Hence, the initial policy must
be â€œadmissibleâ€, that is, it has to stabilize the system
(detailed in Deï¬nition 1). However, in practical situations,
especially for complex systems, it is often difï¬cult to
obtain an admissible policy.

â€¢ A2: Input-Afï¬ne Nature of System: Most ADP methods
are subject to input-afï¬ne systems. This is due to the fact
that these methods require that the optimal policy needs
to be directly represented by the value function. Which
means that the minimum point of the Hamilton function
could be solved analytically, when the value function is
given. For non input-afï¬ne systems, directly solving the
optimal policy in this way is often intractable.

In this paper, we propose a Deep Generalized Policy
Iteration (DGPI) algorithm with proof of convergence and
optimality, for solving optimal control problems of general
nonlinear CT systems with known dynamics to overcome the
limitation of the above two central assumptions. Both the
actor and critic are approximated by deep NNs which build
a map from the system states to action and value function
respectively. Our main contributions can be summarized as
follows:

1) Given any arbitrary initial policy, the proposed DGPI al-
gorithm is proven to converge to an admissible policy by
continuously minimizing the square of the Hamiltonian.
This relaxes the requirement of A1.

2) We prove faster convergence speeds of DGPI than
corresponding PI methods, due to novel update ter-
mination conditions of both the policy evaluation and
improvement processes. The policy network is updated
by directly minimizing the associated Hamiltonian, and
the tuning rules are generalized to arbitrary nonlinear,
non input-afï¬ne dynamics. This relaxes the requirement
of A2.

The paper is organized as follows. In Section II, we provide
the formulation of the optimal control problem, followed by
the general description of PI and DPI algorithm. In Section III,
we describe the DGPI algorithm and analyze its convergence
and optimality. In Section IV, we present simulation exam-
ples that show the generality and effectiveness of the DGPI
algorithm for CT system. Section V concludes this paper.

II. MATHEMATICAL PRELIMINARIES

A. HJB Equation of
Problem

the Continuous-time Optimal Control

Consider the general time-invariant dynamical system given

by

Ë™x(t) = f (x(t), u(t)),

(1)

with state x(t) âˆˆ Rn, control
input u(t) âˆˆ Rm, and
f : Rn Ã— Rm â†’ Rn. We assume that f (x(t), u(t)) is
Lipschitz continuous on a compact set â„¦ that contains the
origin, and that the system is stabilizable on â„¦, i.e., there exists

2

a continuous policy Ï€(x), where u(t) = Ï€(x(t)), such that the
system is asymptotically stable on â„¦. The system dynamics
f (x(t), u(t)) is assumed to be known, it can be nonlinear and
input non-afï¬ne analytic functions, Neural Networks (NNs),
or even a MATLAB/Simulink model (only if âˆ‚f
âˆ‚u is available).
Moreover, the system input u(t) can be either constrained or
unconstrained. Given the policy Ï€(x), deï¬ne its associated
inï¬nite horizon value function
(cid:90) âˆ

V Ï€(x(t)) =

l(x(Ï„ ), Ï€(x(Ï„ ))dÏ„,

(2)

t

where l(x, u) : Rn Ã— Rm â†’ R is positive deï¬nite, i.e. x (cid:54)=
0 âˆ¨ u (cid:54)= 0 â†’ l(x, u) > 0, and x = 0 âˆ§ u = 0 â†’ l(x, u) = 0.
For dynamics in (1) with the value function in (2), introduce
the associated Hamiltonian

H(x, u,

âˆ‚V Ï€(x)
âˆ‚x

) = l(x, u) +

âˆ‚V Ï€(x)
âˆ‚x(cid:62) f (x, u).

(3)

Deï¬nition 1. (Admissible Policy). A control policy Ï€(x) is
deï¬ned as admissible with respect to (2) on â„¦, denoted by
Ï€(x) âˆˆ Î¨(â„¦), if Ï€(x) is continuous on â„¦, Ï€(0) = 0, u(x) =
Ï€(x) stabilizes (1) on â„¦, and V (x) is ï¬nite x âˆˆ â„¦ [22].

For any control policy Ï€(x) âˆˆ Î¨(â„¦),

the differential
equivalent to (2) on â„¦ is a sort of a Lyapunov equation for
systems in (1)

H(x, Ï€(x),

âˆ‚V Ï€(x)
âˆ‚x

) = 0,

V Ï€(0) = 0.

(4)

Therefore, given a policy Ï€(x) âˆˆ Î¨(â„¦), the value function in
(2) associated with the system (1) can be found by solving
the Lyapunov equation. Then the optimal control problem for
continuous-time (CT) system can now be formulated as ï¬nding
a policy Ï€(x) âˆˆ Î¨(â„¦) such that the value function associated
with systems in (1) is minimized. The minimized or optimal
value function V âˆ—(x(t)) deï¬ned by
(cid:90) âˆ

V âˆ—(x(t)) = min

l(x(Ï„ ), Ï€(x(Ï„ ))dÏ„,

Ï€(x)âˆˆÎ¨(â„¦)

t

satisï¬es the Hamilton-Jacobi-Bellman (HJB) equation [3]
âˆ‚V âˆ—(x)
âˆ‚x

min
Ï€(x)âˆˆÎ¨(â„¦)

V âˆ—(0) = 0.

H(x, u,

) = 0,

(5)

Meanwhile, the optimal control Ï€âˆ—(x) for every state x âˆˆ â„¦

can be derived as

Ï€âˆ—(x) = arg min

u

H(x, u,

âˆ‚V âˆ—(x)
âˆ‚x

).

(6)

Inserting this optimal control policy Ï€âˆ—(x) and optimal value
function V âˆ—(x) in the Lyapunov equation, we obtain the
formulation of the HJB equation in terms of âˆ‚V âˆ—(x)
and uâˆ—
[3]

âˆ‚x

l(x, uâˆ—) +

V âˆ—(0) = 0.

âˆ‚V âˆ—(x)
âˆ‚x(cid:62) f (x, uâˆ—) = 0,
Existence and uniqueness of the value function has been shown
in [34]. In order to ï¬nd the optimal policy for CT systems
one only needs to solve the HJB equation (5) for the value
function and then substitute the solution into (6) to obtain
the optimal control. However, due to the nonlinear nature of
the HJB equation, ï¬nding its solution is generally difï¬cult or
impossible.

B. Policy Iteration

The proposed algorithm for CT system used in this paper is
motivated by Policy Iteration (PI) [7] . Therefore in this section
we describe PI. PI is an iterative method of reinforcement
learning (RL) for solving optimal policy of CT or discrete-
time systems, and involves computation cycles between policy
evaluation based on (4) and policy improvement based on (6).
The pseudo-code of PI is shown in Algorithm 1.

Algorithm 1 PI algorithm

Initial with policy Ï€0(x) âˆˆ Î¨(â„¦)
Given an arbitrarily small positive (cid:15) and set i = 0
while (cid:107)V i+1(x) âˆ’ V i(x)(cid:107) â‰¤ (cid:15) do

1. Solve value function V i(x) for âˆ€x âˆˆ â„¦ using

l(x, Ï€i(x)) +

âˆ‚V i
âˆ‚x(cid:62) f (x, Ï€i(x)) = 0,

V i(0) = 0

(7)

2. Solve new policy Ï€i+1(x) for âˆ€x âˆˆ â„¦ using

Ï€i+1(x) = arg min

[l(x, u) +

u

âˆ‚V i
âˆ‚x(cid:62) f (x, u)]

(8)

end while

As Algorithm 1 shows, the ï¬rst step of PI is to ï¬nd an initial
policy Ï€0(x) âˆˆ Î¨(â„¦) because the associated value function
V 0(x) is ï¬nite only when the system is asymptotically stable.
Algorithm 1 will iteratively converge to the optimal control
policy Ï€âˆ—(x) âˆˆ Î¨(â„¦) and value function V âˆ—(x). Proofs of
convergence and optimality have been given in [22].

C. Value Function and Policy Approximation

In previous adaptive dynamic programming (ADP) re-
searches for CT systems, the value function V i(x) and policy
Ï€i(x) are usually approximated by linear methods, which
requires a large number of artiï¬cially designed basis func-
tions [31]. In recent years, deep NNs are favored in many
ï¬elds such as RL and machine learning due to their better
generality and higher ï¬tting ability [35], [36]. In our work,
both the value function and policy are approximated by deep
NNs, called respectively the value network (or critic network)
V (x; Ï‰) (VÏ‰(x) for short) and the policy network (or actor
network) Ï€(x; Î¸) (Ï€Î¸(x) for short), where w and Î¸ are network
parameters. These two networks directly build a map from
the raw system states to the approximated value function and
control inputs respectively; in this case, no hand-crafted basis
function is needed.

Inserting the value and policy network in (3), we obtain the
formulation of approximate Hamiltonian in terms of w and Î¸

H(x, Ï‰, Î¸) = l(x, Ï€(x; Î¸)) +

âˆ‚V (x; Ï‰)

âˆ‚x(cid:62) f (x, Ï€(x; Î¸)).

We refer to the algorithm combining PI and deep NN approx-
imators as Deep PI (DPI), which involves alternatively tuning
each of the two networks to ï¬nd optimal parameters Ï‰âˆ— and
Î¸âˆ— such that V âˆ—(x) = V (x; Ï‰âˆ—), Ï€âˆ—(x) = Ï€(x; Î¸âˆ—).

The policy evaluation process of DPI proceeds by tuning
the value network by solving (7). Given any policy Ï€(x; Î¸) âˆˆ

3

Î¨(â„¦), it is desired to ï¬nd parameters w to minimize the critic
loss function

Lc(Ï‰, Î¸) = Exâˆˆâ„¦

(cid:2)H(x, Ï‰, Î¸)2(cid:3).

(9)

Noted that V (x; Ï‰) â‰¡ 0 can be easily guaranteed by selecting
proper activation function ÏƒV (Â·) for the value network. Based
on (8), the policy improvement process is carried out by tuning
the policy network to minimize expectation of Hamiltonian in
each state, which is also called actor loss function here

La(Ï‰, Î¸) = Exâˆˆâ„¦

(cid:2)H(x, Ï‰, Î¸)(cid:3).

(10)

Many off-the-shelf NN optimization methods can be used to
tune these two NNs, such as Stochastic Gradient Descent
(SGD), RMSProp, Levenberg Marquardt or Adam [37]. In
fact, the value network and policy network usually require
multiple updating iterations to make (7) and (8) hold respec-
tively. Therefore, compared with the PI algorithm mentioned
above,
two inner updating loops would be introduced to
update value network and policy network respectively until
convergence. Taking the SGD optimization method as an
example, the pseudo-code of DPI is shown in Algorithm 2.

Algorithm 2 DPI algorithm

Initial with Î¸0 such that Ï€Î¸0(x) âˆˆ Î¨(â„¦) and arbitrary Ï‰0
Choose the appropriate learning rates Î±Ï‰ and Î±Î¸
Given an arbitrarily small positive (cid:15) and set i = 0
while (cid:107)VÏ‰i+1(x) âˆ’ VÏ‰i(x)(cid:107) â‰¤ (cid:15) do

1. Estimate VÏ‰i+1(x) using Ï€Î¸i(x)

Ï‰i+1 = Ï‰i
repeat

Ï‰i+1 = Ï‰i+1 âˆ’ Î±Ï‰

dLc(Ï‰i+1, Î¸i)
dÏ‰i+1

(11)

until Lc(Ï‰i+1, Î¸i) â‰¤ (cid:15)

2. Find improved policy Ï€Î¸i+1(x) using VÏ‰i+1(x)

Î¸i+1 = Î¸i
repeat

La,old = La(Ï‰i+1, Î¸i+1)

Î¸i+1 = Î¸i+1 âˆ’ Î±Î¸

dLa(Ï‰i+1, Î¸i+1)
dÎ¸i+1
until |La(Ï‰i+1, Î¸i+1) âˆ’ La,old| â‰¤ (cid:15)

(12)

end while

III. DEEP GENERALIZED POLICY ITERATION ALGORITHM

Algorithm 2 proceeds by alternately updating the value
and policy network by minimizing (9) and (10) respectively.
Note that while one NN is being tuned, the other is held
constant. Besides, each NN usually requires multiple updating
iterations to satisfy the terminal conditions, which is the
so-called protracted iterative computation problem [7]. This
problem usually leads to the admissibility requirement because
the initial policy network needs to satisfy Ï€Î¸0 (x) âˆˆ Î¨(â„¦)
to have a ï¬nite and converged value function VÏ‰1(x). Many

4

previous studies used trials and errors process to obtain the
range of the initial weights for the policy network to keep the
stability of the system [8], [25]. However, this method usually
takes a lot of time, especially for complex systems. On the
other hand, the protracted problem also often results in slower
learning [7].

A. Description of the DGPI Algorithm

Inspired by the idea of generalized PI framework, which is
typically utilized in discrete-time dynamic RL problems [7],
we present the Deep Generalized PI (DGPI) algorithm for CT
systems to relax the requirement A1 (from Introduction) and
improve the learning speed by truncating the inner loops (re-
laxing the requirement A2) of Algorithm 2 without losing the
convergence guarantees. The pseudo-code of DGPI algorithm
shown in Algorithm 3.

Algorithm 3 DGPI algorithm

Initial with arbitrary Î¸0 and Ï‰0
Choose the appropriate learning rates Î±, Î±Ï‰ and Î±Î¸
Given an arbitrarily small positive (cid:15) and set i = 0
Phase 1: Warm-up

while maxxâˆˆâ„¦ H(x, Ï‰i, Î¸i) â‰¤ 0 do

Update Ï‰ and Î¸ using:

Assumption 2. If both the value network VÏ‰ and policy
network Ï€Î¸ are over-parameterized, the global minimum of
the critic loss function in (9) and actor loss function in (10)
can be found respectively using an appropriate optimization
algorithm such as SGD.

Next, the convergence property of Algorithm 3 will be
established. As the iteration index i tends to inï¬nity, we will
show that the optimal value function and optimal policy can be
achieved using Algorithm 3. Before the main theorem, some
lemmas are necessary at this point.

Lemma 1. (Universal Approximation Theorem). For any con-
tinuous function F (x) on a compact set â„¦, there exists a
feed-forward NN, having only a single hidden layer, which
uniformly approximates F (x) and its gradient to within arbi-
trarily small error (cid:15) âˆˆ R+ on â„¦ [40].

Lemma 1 allows us to ignore the NN approximation errors

when proving convergence of Algorithm 3.

Lemma 2. Consider the CT dynamic optimal control problem
for (1) and (2). Suppose V Ï€(x) âˆˆ C 1 : Rn â†’ R is a smooth
positive deï¬nite solution to the HJB in (5). The control policy
Ï€(x) is given by (6). Then we have that V Ï€(x) = V âˆ—(x) and
Ï€(x) = Ï€âˆ—(x) [3].

{Ï‰i+1, Î¸i+1} = {Ï‰i, Î¸i} âˆ’ Î±

dLc(Ï‰i, Î¸i)
d{Ï‰i, Î¸i}

(13)

The following lemma shows how Algorithm 3 can be used
to obtain a policy Ï€(x; Î¸) âˆˆ Î¨(â„¦) given any initial policy
Ï€(x; Î¸0).

end while

Phase 2: PI with relaxed termination conditions

while (cid:107)VÏ‰i+1(x) âˆ’ VÏ‰i(x)(cid:107) â‰¤ (cid:15) do

1. Estimate VÏ‰i+1(x) using Ï€Î¸i(x)

Ï‰i+1 = Ï‰i
repeat

Update Ï‰ using (11)

until H(x, Ï‰i, Î¸i) â‰¤ H(x, Ï‰i+1, Î¸i) â‰¤ 0, âˆ€x âˆˆ â„¦

2. Find improved policy Ï€Î¸i+1(x) using VÏ‰i(x)

Î¸i+1 = Î¸i
repeat

Update Î¸ using (12)

until maxxâˆˆâ„¦ H(x, Ï‰i+1, Î¸i+1) â‰¤ 0

end while

B. Convergence and Optimality Analysis

The solution to (7) may not be smooth for general nonlinear
non input-afï¬ne systems. However, in keeping with other work
in the literature [24] we make the following assumption.

Assumption 1. The solution to (7) is smooth if Ï€(x) âˆˆ Î¨(â„¦),
i.e. V Ï€(x) âˆˆ C 1(â„¦) [22], [24].

In recent years, many experimental results and theoretical
proofs have shown simple optimization algorithms such as
SGD can ï¬nd global minima on the training objective of deep
NNs in polynomial time if the network is over-parameterized
(i.e., the number of hidden neurons is sufï¬ciently large) [38],
[39]. Based on this fact, our second assumption is:

i.e.,

Lemma 3. Consider the CT dynamic optimal control problem
for (1) and (2). The value function V (x; Ï‰) and policy Ï€(x; Î¸)
are represented by over-parameterized NNs. The parameters
the initial policy
w and Î¸ are initialized randomly,
Ï€(x; Î¸0) can be inadmissible. These two NNs are updated with
Algorithm 3. Let Assumption 1 and 2 hold, and suppose all the
hyper-parameters (such as Î±, Î±w and Î±Î¸) and NN optimization
method are properly selected. The NN approximation errors
are ignored according to Lemma 1. Suppose all the activation
functions ÏƒV (Â·) and biases bV of the value network V (x; Ï‰)
are set to ÏƒV (0) = 0 and bV â‰¡ 0, and the output layer
activation function ÏƒVout satisï¬es ÏƒVout (Â·) â‰¥ 0. We have that:
âˆƒNa âˆˆ Z+, if i â‰¥ Na, then Ï€(x; Î¸i) âˆˆ Î¨(x) for systems (1)
on â„¦.

Proof. According to (4) and Lemma 1, if Ï€Î¸ âˆˆ Î¨(â„¦), there
exists parameters (Ï‰, Î¸), such that H(x, Ï‰, Î¸) = 0 for all x âˆˆ
â„¦. It follows that

min
Ï‰,Î¸

H(x, Ï‰, Î¸) â‰¤ 0,

âˆ€x âˆˆ â„¦,

which implies that the global minima of loss function Lc is
equal to 0, corresponding to the Hamiltonian vanishing for all
states x âˆˆ â„¦. From Lemma 1, utilizing the fact that global
H(x, Ï‰, Î¸)2(cid:105)
minima of Lc = Exâˆˆâ„¦
can be obtained, one has

(cid:104)

min
Ï‰,Î¸

Lc(Ï‰, Î¸) = min
Ï‰,Î¸

Exâˆˆâ„¦

(cid:104)

H(x, Ï‰, Î¸)2(cid:105)

= 0.

Since Algorithm 3 updates Ï‰ and Î¸ using (13) to continuously
minimize Lc(Ï‰, Î¸) in Phase 1 if maxxâˆˆâ„¦ H(x, Ï‰i, Î¸i) > 0,

according to Assumption 2, there exists Na âˆˆ Z+, such that

H(x, Ï‰Na , Î¸Na ) â‰¤ 0,

âˆ€x âˆˆ â„¦.

(14)

Take the time derivative of V (x; Ï‰) to obtain

dV (x; Ï‰)
dt

âˆ‚V (x; Ï‰)

=

âˆ‚x(cid:62) f (x, Ï€(x; Î¸)),
= H(x, Ï‰, Î¸) âˆ’ l(x, Ï€(x; Î¸)).

(15)

Using (14) and (15), one has

dV (x; Ï‰Na )
dt

â‰¤ âˆ’l(x, Ï€(x; Î¸Na )),

âˆ€x âˆˆ â„¦.

As the utility function l(x, Ï€(x; Î¸)) is positive deï¬nite,
follows

it

dV (x; Ï‰Na )
dt

< 0,

âˆ€x âˆˆ â„¦\{0}.

(16)

Since ÏƒV (0) = 0, bV â‰¡ 0 and ÏƒVout(Â·) â‰¥ 0, we have

(cid:40)

V (x; Ï‰) â‰¥ 0, âˆ€x âˆˆ â„¦\{0} âˆ§ âˆ€Ï‰,

V (x; Ï‰) â‰¡ 0, x = 0 âˆ§ âˆ€Ï‰.

From (16) and (17), we have

V (x; Ï‰Na ) > min
zâˆˆâ„¦

V (z; Ï‰Na ) = 0,

âˆ€x âˆˆ â„¦\{0}.

(18)

From (17) and (18), we infer that the V (x; Ï‰Na ) is positive
deï¬nite. Then, according to (16), V (x; Ï‰Na ) is a Lyapunov
function for closed loop dynamics obtained from (1) when
policy Ï€(x; Î¸Na ) is used. Therefore, the policy Ï€(x; Î¸Na ) âˆˆ
Î¨(â„¦) for the system in (1) on â„¦ [41], that is, it is a stabilizing
admissible policy.

At this point, Algorithm 3 enters Phase 2. According to (4),

one has

Lc(Ï‰, Î¸Na ) = 0.

min
Ï‰

So, from Assumption 2 and Lemma 1, we can always ï¬nd
Ï‰Na+1 by continuously applying (11), such that

H(x, Ï‰Na , Î¸Na ) â‰¤ H(x, Ï‰Na+1, Î¸Na ) â‰¤ 0,

âˆ€x âˆˆ â„¦.

Again, from Lemma 1, utilizing the fact that global minima
of La = Exâˆˆâ„¦

can be obtained, we get

(cid:104)
H(x, Ï‰, Î¸)

(cid:105)

La(Ï‰Na+1, Î¸) = Exâˆˆâ„¦

min
Î¸

(cid:104)

min
Î¸

(cid:105)
H(x, Ï‰Na+1, Î¸)

.

This implies that Hamiltonian H(x, Ï‰Na+1, Î¸) can be taken to
global minimum, for any value of x, by minimizing over Î¸.
Then, we can also ï¬nd Î¸Na+1 through (12), such that

H(x, Ï‰Na+1, Î¸Na+1) â‰¤ H(x, Ï‰Na+1, Î¸Na ) â‰¤ 0,

âˆ€x âˆˆ â„¦.

This implies that like the case with V (x; Ï‰Na ), V (x; Ï‰Na+1) is
also a Lyapunov function. So, Ï€(x; Î¸Na+1) âˆˆ Î¨(â„¦). Extending
this for all subsequent time steps, V (x; Ï‰i) is a Lyapunov
function for all i â‰¥ Na, and it is obvious that

H(x, Ï‰i, Î¸i) â‰¤ H(x, Ï‰i+1, Î¸i) â‰¤ 0,

âˆ€i â‰¥ Na âˆ§ âˆ€x âˆˆ â„¦,

and

Ï€(x; Î¸i) âˆˆ Î¨(â„¦),

âˆ€i â‰¥ Na.

(19)

(20)

5

This proves Lemma 3. We have thus proven that starting from
any arbitrary initial policy, the DGPI algorithm in Algorithm 3
converges to an admissible policy. As claimed previously, this
relaxes the requirement A1, which is typical to most other
ADP algorithms.

We now present our main result. It is shown in the following
theorem that the value function VÏ‰(x) and policy Ï€Î¸(x) con-
verge to optimum uniformly by applying DGPI Algorithm 3.

Deï¬nition 2. (Uniform Convergence). A sequence of functions
{fn} converges uniformly to f on a set K if âˆ€(cid:15) > 0, âˆƒN ((cid:15)) âˆˆ
Z+ : n > N â†’ supxâˆˆK |fn(x) âˆ’ f (x)| < (cid:15).
Theorem 1. For arbitrary V (x; Ï‰0) and Ï€(x; Î¸0), if these
two NNs are updated with Algorithm 3, V (x; Ï‰i) â†’ V âˆ—(x),
Ï€(x; Î¸i) â†’ Ï€âˆ—(x) uniformly on â„¦ as i goes to âˆ.

Proof. From Lemma 3, it can be shown by induction that the
policy Ï€(x; Î¸i) âˆˆ Î¨(â„¦) for system in (1) on â„¦ when i â‰¥ Na.
Furthermore, according to (15) and (19),

(17)

dV (x; Ï‰i)
dt

â‰¤

dV (x; Ï‰i+1)
dt

â‰¤ 0,

âˆ€x âˆˆ â„¦ âˆ§ i â‰¥ Na.

(21)

From Newton-Leibniz formula,

V (x(t); Ï‰) = V (x(âˆ); Ï‰) âˆ’

According to (17) and (20),

(cid:90) âˆ

t

dV (x(Ï„ ); Ï‰)
dÏ„

dÏ„.

(22)

V (x(âˆ); Ï‰) = V (0; Ï‰) = 0,

i â‰¥ Na âˆ§ âˆ€Ï‰.

(23)

So, from (17), (21), (22) and (23), it follows that

0 â‰¤ V (x; Ï‰i+1) â‰¤ V (x; Ï‰i),

âˆ€x âˆˆ â„¦ âˆ§ i â‰¥ Na.

As such, V (x; Ï‰i) is pointwise convergent as i â†’ âˆ. We can
write limiâ†’âˆ V (x; Ï‰i) = V (x; Ï‰âˆ). Because â„¦ is compact,
then uniform convergence follows immediately from Dinis
theorem [42].

From Deï¬nition 2, given arbitrarily small (cid:15)V > 0, âˆƒNc â‰¥

Na, such that

sup
xâˆˆâ„¦

|V (x; Ï‰i)âˆ’V (x; Ï‰i+1)|,

â‰¤ sup
xâˆˆâ„¦
< (cid:15)V ,

(cid:12)
(cid:12)V (x; Ï‰i) âˆ’ V (x; Ï‰âˆ)(cid:12)
(cid:12) ,

âˆ€i â‰¥ Nc.

According to (15) and (22), one has

V (x; Ï‰i) âˆ’ V (x; Ï‰i+1)

=

=

(cid:90) âˆ

t
(cid:90) âˆ

t

d(V (x(Ï„ ); Ï‰i+1) âˆ’ V (x(Ï„ ); Ï‰i))
dÏ„

dÏ„,

(cid:104)

(cid:105)
H(x(Ï„ ), Ï‰i+1, Î¸i) âˆ’ H(x(Ï„ ), Ï‰i, Î¸i)

dÏ„.

Since limiâ†’âˆ supxâˆˆâ„¦ |V (x; Ï‰i) âˆ’ V (x; Ï‰i+1)| = 0, we have

lim
iâ†’âˆ

sup
xâˆˆâ„¦

|H(x, Ï‰i+1, Î¸i) âˆ’ H(x, Ï‰i, Î¸i)| = 0.

From Lemma 1, (4) and (20),

Lc(Ï‰, Î¸i) = 0,

âˆ€i â‰¥ Nc.

min
Ï‰

So, it is true that

6

lim
iâ†’âˆ

H(x, Ï‰i, Î¸i) = lim
iâ†’âˆ

H(x, Ï‰i+1, Î¸i) = 0,

âˆ€x âˆˆ â„¦.

(24)
Therefore, V (x; Ï‰âˆ) and Ï€(x; Î¸âˆ) are the solution of the
Lyapunov equation (4), and it follows that

V (x; Ï‰âˆ) = V Ï€Î¸âˆ (x).

Policy Ï€Î¸i âˆˆ Î¨(â„¦) for i â‰¥ Na, therefore the state trajec-
tories generated by it is unique due to the locally Lipschitz
continuity assumption on the dynamics [22]. Since V (x; Ï‰i)
converges uniformly to V (x; Ï‰âˆ), this implies that the system
trajectories converge for all x âˆˆ â„¦. Therefore, Ï€(x; Î¸i) also
converges uniformly to Ï€(x; Î¸âˆ) on â„¦. From (24), it is also
obvious that

lim
iâ†’âˆ

min
Î¸

H(x, Ï‰i, Î¸) = 0,

âˆ€x âˆˆ â„¦.

(25)

it

(25) and Lemma 2,

According to (24),
follows that
limiâ†’âˆ VÏ‰i(x) = V âˆ—(x) and limiâ†’âˆ Ï€Î¸i(x) = Ï€âˆ—(x).
Therefore, we can conclude that VÏ‰i (x) â†’ VÏ‰âˆ— (x) and
Ï€Î¸i(x) â†’ Ï€Î¸âˆ— (x) uniformly on â„¦ as i goes to âˆ. Thus we
have proven that the DGPI Algorithm 3 converges uniformly,
to V âˆ—(x), to the optimal policy Ï€âˆ—(x). As claimed previously,
this also relaxes the requirement A2.

Remark 1. Since the state x is continuous, it is usually
intractable to check the H(x, Ï‰, Î¸) value of every x âˆˆ â„¦.
Therefore, in practical applications, we usually use the ex-
pected value of H(x, Ï‰, Î¸) to judge whether each termination
condition in Algorithm 3 is satisï¬ed. So, the DGPI Algorithm 3
can also be formulated as Algorithm 4. Fig. 1 shows the
frameworks of DPI Algorithm 2 and DGPI Algorithm 4.

Algorithm 4 DGPI algorithm: Tractable Relaxation

Initial with arbitrary Î¸0 and Ï‰0
Choose the appropriate learning rates Î±, Î±Ï‰ and Î±Î¸
Given an arbitrarily small positive (cid:15) and set i = 0
Phase 1: Warm-up

while La(Ï‰i, Î¸i) â‰¤ 0 do

Update Ï‰ and Î¸ using (13)

end while

Phase 2: PI with relaxed termination conditions

while (cid:107)VÏ‰i+1(x) âˆ’ VÏ‰i(x)(cid:107) â‰¤ (cid:15) do

Update w using (11)
Update Î¸ using (12)

end while

Remark 2. In previous analysis, the l(x, u) is limited to a
positive deï¬nite function, i.e., the equilibrium state (denoted
by xe) of the system must be xe = 0. If we take x âˆ’ xe as the
input of value network V (x; Ï‰), the DGPI Algorithm 4 can
be extended to problems with non-zero xe, where l(x, u) = 0
only when x = xe âˆ§ u = 0. The corresponding convergence
and optimality analysis is similar to the problems of xe = 0.

Fig. 1: DPI and DGPI algorithm framework diagram.

networks, we propose another effective method that drives the
V (xe; Ï‰) to gradually approach 0 by adding an equilibrium
term to the critic loss function (9)

Lc

(cid:48)(Ï‰, Î¸) = Exâˆˆâ„¦

(cid:2)H(x, Ï‰, Î¸)2(cid:3) + Î·V (xe; Ï‰),

where Î· is the hyper-parameter that trades off the importance
of the Hamiltonian term and equilibrium term.

IV. RESULTS

To support the proposed DGPI Algorithm 4, we offer two
simulation examples, one with linear, and the other one with a
nonlinear non input-afï¬ne system. We apply Algorithm 4 and
Algorithm 2 to solve the optimal policy and value function
for these two systems. The simulation results show that our
algorithm performs better than Algorithm 2 in both cases.

A. Example 1: Linear Time Invariant System

1) Problem Description: Consider the CT aircraft plant
control problem used in [24], [25], [43], which can be for-
mulated as
(cid:90) âˆ

(x(cid:62)Qx + u(cid:62)Ru)

ï£®

ï£°

0.90506 âˆ’0.00215
âˆ’1.01887
0.82225 âˆ’1.07741 âˆ’0.17555
0

âˆ’1

0

ï£¹

ï£» x +

ï£¹

ï£» u,

ï£®
0
0
ï£°
1

min
u

0

s.t.

Ë™x =

Remark 3. According to Lemma 3, all activation functions ÏƒV
and biases bV of V (x; Ï‰) are set to ÏƒV (0) = 0 and bV â‰¡ 0 to
ensure V (xe; Ï‰) â‰¡ 0. To remove these restrictions for value

where Q and R are identity matrices of appropriate di-
mensions. In this linear case, the optimal analytic strategy
Ï€âˆ—(x) = 0.1352x1 + 0.1501x2 âˆ’ 0.4329x3 and optimal value

DGPI Warm-up(,ğœ”,ğœƒ)2 Arbitrary 0 Arbitrary ğœ”0 â‰¤0 =+1  Policy Evaluation  (;ğœ”) ğœ‹(;ğœƒ) Policy Improvement =+1 ğœ”+1=ğœ” ğœƒ+1=ğœƒ  âˆ† â‰¤Îµ PI with relaxed termination conditionValue Networkstate state Policy NetworkPolicy Improvement DPI PI (;ğœ”) ğœ‹(;ğœƒ) =+1 Admissible 0 Arbitrary ğœ”0 â‰¤Îµ  âˆ† â‰¤Îµ ğœ”+1=ğœ”  âˆ† â‰¤Îµ ğœƒ+1=ğœƒ Policy Evaluation function V âˆ—(x) = x(cid:62)P x can be easily found by solving the
algebraic Riccati equation, where
ï£®

ï£¹

P =

ï£°

1.4245
1.1682
âˆ’0.1352 âˆ’0.1501

1.1682 âˆ’0.1352
1.4349 âˆ’0.1501
0.4329

ï£» .

2) Algorithm Details: This system is very special, in par-
ticular, if the parameters Î¸ of the policy network is randomly
initialized around 0, which is a very common initialization
method, then the initialized policy Ï€Î¸0 (x) âˆˆ Î¨(â„¦). Therefore,
to compare the learning speed of Algorithm 2 and Algorithm 4,
both algorithms are implemented to ï¬nd the optimal policy
and value function. The value function and policy are repre-
sented by 3-layer fully-connected NNs, which have the same
architecture except for the output layers. For each network, the
input layer is composed of the states, followed by 2 hidden
layers using exponential
linear units (ELUs) as activation
functions with 28 units per layer. The outputs of the value
and policy network are V (s; Ï‰) and Ï€(s; Î¸), using softplus
unit and linear unit as activation functions respectively. The
training set consists of 28 states which are randomly selected
from the compact set â„¦ at each iteration. The learning rate Î±Ï‰
and Î±Î¸ are both set to 0.01 and the Adam update rule is used
to minimize the loss functions.

3) Result Analysis: Each algorithm was run 20 times and
the mean and 95% conï¬dence interval of the training per-
formance are shown in Fig. 2. We plot the policy error eÏ€
and value error eV of Algorithm 2 and Algorithm 4 at each
iteration, which are solved by

eÏ€ = ExâˆˆX

ï£®

ï£°

Ï€(x; Î¸) âˆ’ Ï€âˆ—(x)
Ï€âˆ—(x) âˆ’ min
xâˆˆX

max
xâˆˆX

Ï€âˆ—(x)

ï£¹

ï£» ,

7

Fig. 2: DGPI vs DPI performance comparison: Example 1.

B. Example 2: Nonlinear and Non Input-Afï¬ne System

1) Problem Description: Consider the vehicle trajectory
tracking control problem with non input-afï¬ne nonlinear ve-
hicle system derived as in [44], [45]. The desired velocity is
12 m/s and the desired vehicle trajectory is shown in Fig.
4. The system states and control inputs of this problem are
listed in Table I, and the vehicle parameters are listed in Table
II. The vehicle is controlled by a saturating actuator, where
Î´ âˆˆ [âˆ’0.35, 0.35] rad and ax âˆˆ [âˆ’3, 3] m/s2. The dynamics
of the vehicle along with detailed state and input descriptions
is given as

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£»

, u =

(cid:21)

(cid:20) Î´
ax

, f (x, u) =

ï£®

ï£¯
ï£¯
ï£¯
ï£¯
ï£°

vy
r
vx
Ï†
y

ï£®

Fyf cos Î´+Fyr
m

âˆ’ vxr

aFyf cos Î´âˆ’bFyr
ï£¯
ï£¯
Iz
ï£¯
ax + vyr âˆ’ Fyf sin Î´
ï£¯
ï£¯
r
ï£°
vx sin Ï† + vy cos Ï†

m

ï£¹

ï£º
ï£º
ï£º
ï£º
ï£º
ï£»

,

eV = ExâˆˆX

ï£®

ï£°

V (x; Ï‰) âˆ’ V âˆ—(x)
V âˆ—(x) âˆ’ min
xâˆˆX

max
xâˆˆX

V âˆ—(x)

ï£¹

ï£» ,

x =

where X is the test set which contains 500 states randomly
selected from the compact set â„¦ at the beginning of each
simulation. We also draw violin plots in different iterations to
show the precision distribution and 4-quartiles. Noted that one
iteration of Fig. 2 corresponds to one NN update.

It is clear from Fig. 2 that both two algorithms can make
the value and policy network approximation errors (eÏ€ and
eV ) fall with iteration. And after 105 iterations, both er-
rors of Algorithm 4 are less than 1%. This indicates that
Algorithm 4 has the ability to converge value function and
policy to optimality. In addition, the t-test results in Fig. 2
show that both eÏ€ and eV of Algorithm 4 are signiï¬cantly
smaller than that of Algorithm 2 (p < 0.001) under the same
number of iterations. From the perspective of convergence
speed, Algorithm 4 requires only about 104 iterations to make
both approximation errors less than 0.03, while Algorithm 2
requires about 105 steps. Based on this, Algorithm 4 is about
10 times faster than Algorithm 2. To summarize, Algorithm 4
can converge to the optimal value function and policy, and the
convergence speed of Algorithm 4 is signiï¬cantly higher than
that of Algorithm 2.

where Fyf and Fyr are the lateral tire forces of the front
and rear tires respectively. The lateral tire forces are usually
approximated according to the Fiala tire model:

Fyâ€  = âˆ’sgn(Î±â€ ) âˆ— min

(cid:110)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Câ€  tan Î±â€ 

(cid:16) C 2

â€  (tan Î±â€ )2
27(Âµâ€ Fzâ€ )2 âˆ’

Câ€  |tan Î±â€ |
3Âµâ€ Fzâ€ 

+ 1

(cid:17)

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

, |Âµâ€ Fzâ€ |

(cid:111)
,

where Î±â€  is the tire slip angle, Fzâ€  is the tire load, Âµâ€  is the
lateral friction coefï¬cient, and the subscript â€  âˆˆ {f,r} repre-
sents the front or rear tires. The slip angles can be calculated
from the geometric relationship between the front/rear axle
and the center of gravity (CG):

Î±f = arctan(

vy + ar
vx

) âˆ’ Î´, Î±r = arctan(

vy âˆ’ br
vx

).

Assuming that the rolling resistance is negligible, the lateral
friction coefï¬cient of the front/rear wheel is:
(cid:112)(ÂµFzâ€ )2 âˆ’ (Fxâ€ )2
Fzâ€ 

Âµâ€  =

,

0246810 (Ã—104)Iteration3.02.52.01.51.00.50.0Logarithmic error[p=3Ã—10âˆ’27[p=10âˆ’31[p=2Ã—10âˆ’27[p=7Ã—10âˆ’27[p=9Ã—10âˆ’27[p=3Ã—10âˆ’19[p=5Ã—10âˆ’15[p=4Ã—10âˆ’12[p=9Ã—10âˆ’11NetworkPolicy Ï€Value VMethodDGPIDPIwhere Fxf and Fxr are the longitudinal tire forces of the front
and rear tires respectively, calculated as

8

Fxf =

(cid:40) 0,

max
2

ax â‰¥ 0

, ax < 0

, Fxr =

(cid:40) max,
max
2

ax â‰¥ 0

.

, ax < 0

The loads on the front and rear tires can be approximated by

Fzf =

b
a + b

mg, Fzr =

a
a + b

mg.

The control objective is to minimize the output tracking errors.
Hence, the optimal control problem is given by
(cid:20)280
0

0.4(vx âˆ’ 12)2 + 80y2 + u(cid:62)

0
0.3

min
u

(cid:90) âˆ

dt

(cid:16)

(cid:17)

u

(cid:21)

0

s.t.

Ë™x = f (x, u).

TABLE I
State and control input

state

input

Lateral velocity
Yaw rate at center of gravity (CG)
Longitudinal velocity
Yaw angle between vehicle & trajectory
Distance between CG & trajectory
Front wheel angle
Longitudinal acceleration

vy
r
vx
Ï†
y
Î´
ay

[m/s]
[rad/s]
[m/s]
[rad]
[m]
[rad]
[m/s2]

TABLE II
Vehicle parameters

Front wheel cornering stiffness
Rear wheel cornering stiffness
Distance from CG to front axle
Distance from CG to rear axle
Mass
Polar moment of inertia at CG
Tire-road friction coefï¬cient

Cf
Cr
a
b
m
Iz
Âµ

88000 [N/rad]
94000 [N/rad]
1.14 [m]
1.40 [m]
1500 [kg]
2420 [kgÂ·m2]
1.0

2) Algorithm Details: We use the 6-layer fully-connected
NNs to approximate V (s; Ï‰) and Ï€(s; Î¸), and the state input
layer of each NN is followed by 5 fully-connected hidden
layers, 25 units per layer. The selection of activation function
is similar to that of Example 1, except that the output layer
of the policy network is set as a tanh(Â·) layer with two
units, multiplied by the vector [0.35, 3] to confront bounded
controls. Inspired by the ideas used in multi-threaded variants
of Deep RL, the training set consists of the current states of
28 parallel independent vehicles with different initial states,
thereby obtaining a more realistic state distribution [46]. We
use Adam method to update two NNs, while the learning
to 8âˆ’4
rate of value network and policy network are set
and 2âˆ’4 respectively. Besides, we use Î· = 0.1 to trade off
the Hamiltonian term and equilibrium term of the critic loss
function (Remark 3).

3) Result Analysis: Fig. 3 shows the evolution of the
average absolute Hamiltonian |H| of 28 random states and the
training performance of 20 different runs. The shaded area rep-
resents the 95% conï¬dence interval. The policy performance
at each iteration is measured by the accumulated cost function
in 20s time domain

C =

(cid:90) 20

0

l(x(Ï„ ), u(Ï„ ))dÏ„,

Fig. 3: DGPI vs DPI performance comparison: Example 2.

where initial state x(0) is randomly selected for each run.
Since the initial policy is not admissible, that is, Ï€Î¸0 (x) /âˆˆ
Î¨(â„¦), Algorithm 2 can never make |H| close to 0, hence the
terminal condition of policy evaluation can never be satisï¬ed.
Therefore, the ï¬nite horizon cost C has no change during the
entire learning process, i.e., Algorithm 2 can never converge
to an admissible policy if Ï€Î¸0(x) /âˆˆ Î¨(â„¦).

On the other hand,

|H| of Algorithm 4 can gradually
converge to 0, while the ï¬nite horizon cast C is also reduced
to a small value during the learning process. Fig. 4 shows the
state trajectory controlled by one of the trained DGPI policies.
The learned policy can make the vehicle reach the equilibrium
state very quickly, which takes less than 0.5s for the case in
Fig. 4. The results of Example 2 show that Algorithm 4 can
solve the CT dynamic optimal control problem for general non
input-afï¬ne nonlinear CT systems with saturated actuators and
handle inadmissible initial policies.

Fig. 4: State trajectory.

In conclusion, these two examples demonstrate that the
proposed DGPI algorithm can converge to the optimal policy
and value function for general nonlinear and non input-afï¬ne

02468(Ã—104)Iteration20406080100Average absolute HamiltonianValue|H|lgCMethodDGPIDPI01234567Finite horizon cost02468101214161820time (s)20020Statevx [m/s]r [Â° / s]vy [m/s]Ï† [Â°]y[dm]0100200300400Longitudinal position (m)-3.0-2.01.000.01.0Lateral position (m)Reference TrajectoryActual TrajectoryCT systems without reliance on initial admissible policy. In
addition, if the initial policy Ï€Î¸0(x) âˆˆ Î¨(â„¦), the learning
speed of Algorithm 4 is also faster than that of Algorithm 2.

V. CONCLUSION

The paper presented the Deep Generalized Policy Iteration
(DGPI) Algorithm 4, along with proof of convergence and
optimality, for solving optimal control problems of general
nonlinear CT systems with known dynamics. The proposed
algorithm can circumvent the requirements of â€œadmissibilityâ€
and input-afï¬ne system dynamics (described in A1 and A2 of
Introduction), quintessential to previously proposed counter-
part algorithms. As a result, given any arbitrary initial policy,
the DGPI algorithm is shown to eventually converge to an
admissible and optimal policy, even for general nonlinear non
input-afï¬ne system dynamics. The convergence and optimal-
ity were mathematically proven by using detailed Lyapunov
analysis. We further demonstrated the efï¬cacy and theoretical
accuracy of our algorithm via two numerical examples, which
yielded faster learning speed of the optimal policy starting
from an admissible initialization, as compared to conventional
Deep Policy Iteration (DPI) algorithm (Algorithm 2).

VI. ACKNOWLEDGMENT

We would like to acknowledge Prof. Francesco Borrelli, Ms.
Ziyu Lin, Dr. Yiwen Liao, Dr. Xiaojing Zhang and Ms. Jiatong
Xu for their valuable suggestions throughout this research.

REFERENCES

[1] D. P. Bertsekas, Dynamic Programming and Optimal Control. Athena

Scientiï¬c Belmont, MA, 2005.

[2] T. Pappas, A. Laub, and N. Sandell, â€œOn the numerical solution of
the discrete-time algebraic riccati equation,â€ IEEE Transactions on
Automatic Control, vol. 25, no. 4, pp. 631â€“641, 1980.

[3] F. L. Lewis, D. Vrabie, and V. L. Syrmos, Optimal control. John Wiley

& Sons, 2012.

[4] F. Wang, H. Zhang, and D. Liu, â€œAdaptive dynamic programming:
An introduction,â€ IEEE Computational Intelligence Magazine, vol. 4,
pp. 39â€“47, May 2009.

[5] P. Werbos, â€œBeyond regression: New tools for prediction and analysis in
the behavioral sciences,â€ Ph. D. dissertation, Harvard University, 1974.
[6] P. Werbos, â€œApproximate dynamic programming for realtime control and
neural modelling,â€ Handbook of Intelligent Control: Neural, Fuzzy and
Adaptive Approaches, pp. 493â€“525, 1992.

[7] R. S. Sutton and A. G. Barto, Reinforcement Learning: An Introduction.

MIT press, 2018.

[8] D. Liu, Q. Wei, D. Wang, X. Yang, and H. Li, Adaptive Dynamic
Programming with Applications in Optimal Control. Springer, 2017.

[9] R. A. Howard, â€œDynamic programming and markov processes,â€ 1964.
[10] K. Doya, â€œReinforcement learning in continuous time and space,â€ Neural

Computation, vol. 12, no. 1, pp. 219â€“245, 2000.

[11] P. Abbeel and A. Y. Ng, â€œApprenticeship learning via inverse reinforce-
ment learning,â€ in Proceedings of the 21st International Conference on
Machine Learning, (Banff, Alberta, Canada), pp. 1â€“, ACM, 2004.
[12] J. Peters and S. Schaal, â€œNatural actor-critic,â€ Neurocomputing, vol. 71,

no. 7-9, pp. 1180â€“1190, 2008.

[13] S. Levine and V. Koltun, â€œGuided policy search,â€ in Proceedings of the
30th International Conference on Machine Learning, vol. 28, (Atlanta,
Georgia, USA), pp. 1â€“9, PMLR, 17â€“19 Jun 2013.

[14] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
â€œDeterministic policy gradient algorithms,â€ in Proceedings of the 31st
International Conference on Machine Learning, vol. 32, (Bejing, China),
pp. 387â€“395, PMLR, 22â€“24 Jun 2014.

9

[15] Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel, â€œBench-
marking deep reinforcement learning for continuous control,â€ in Pro-
ceedings of the 33rd International Conference on International Confer-
ence on Machine Learning, vol. 48, (New York, NY, USA), pp. 1329â€“
1338, PMLR, 2016.

[16] B. Recht, â€œA tour of reinforcement learning: The view from continuous
control,â€ Annual Review of Control, Robotics, and Autonomous Systems,
vol. 2, pp. 253â€“279, 2019.

[17] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, â€œTrust
region policy optimization,â€ in Proceedings of the 32nd International
Conference on Machine Learning, vol. 37, (Lille, France), pp. 1889â€“
1897, PMLR, 07â€“09 Jul 2015.

[18] W. B. Powell, Approximate Dynamic Programming: Solving the Curses

of Dimensionality. John Wiley & Sons, 2007.

[19] A. Al-Tamimi, F. L. Lewis, and M. Abu-Khalaf, â€œDiscrete-time nonlin-
ear hjb solution using approximate dynamic programming: convergence
proof,â€ IEEE Transactions on Systems, Man, and Cybernetics, Part B
(Cybernetics), vol. 38, no. 4, pp. 943â€“949, 2008.

[20] D. P. Bertsekas and J. N. Tsitsiklis, â€œNeuro-dynamic programming: an
overview,â€ in Proceedings of the 34th IEEE Conference on Decision and
Control, vol. 1, (New Orleans, LA, USA), pp. 560â€“564, IEEE, 1995.

[21] R. Kamalapurkar, P. Walters, and W. E. Dixon, â€œModel-based rein-
forcement learning for approximate optimal regulation,â€ in Control of
Complex Systems, pp. 247â€“273, Elsevier, 2016.

[22] M. Abu-Khalaf and F. L. Lewis, â€œNearly optimal control

laws for
nonlinear systems with saturating actuators using a neural network hjb
approach,â€ Automatica, vol. 41, no. 5, pp. 779â€“791, 2005.

[23] T. Dierks and S. Jagannathan, â€œOptimal control of afï¬ne nonlinear
continuous-time systems,â€ in American Control Conference (ACC),
2010, (Baltimore, MD, USA), pp. 1568â€“1573, IEEE, 2010.

[24] K. G. Vamvoudakis and F. L. Lewis, â€œOnline actor critic algorithm
to solve the continuous-time inï¬nite horizon optimal control problem,â€
Automatica, vol. 46, no. 5, pp. 878â€“888, 2010.

[25] K. G. Vamvoudakis, â€œEvent-triggered optimal adaptive control algorithm
for continuous-time nonlinear systems,â€ IEEE/CAA Journal of Automat-
ica Sinica, vol. 1, no. 3, pp. 282â€“293, 2014.

[26] L. Dong, X. Zhong, C. Sun, and H. He, â€œEvent-triggered adaptive
dynamic programming for continuous-time systems with control con-
straints,â€ IEEE Transactions on Neural Networks and Learning Systems,
vol. 28, no. 8, pp. 1941â€“1952, 2017.

[27] H. Modares, F. L. Lewis, and M.-B. Naghibi-Sistani, â€œAdaptive optimal
control of unknown constrained-input systems using policy iteration and
neural networks,â€ IEEE Transactions on Neural Networks and Learning
Systems, vol. 24, no. 10, pp. 1513â€“1525, 2013.

[28] X. Yang, D. Liu, and Q. Wei, â€œOnline approximate optimal control
for afï¬ne non-linear systems with unknown internal dynamics using
adaptive dynamic programming,â€ IET Control Theory & Applications,
vol. 8, no. 16, pp. 1676â€“1688, 2014.

[29] D. Vrabie, K. Vamvoudakis, and F. Lewis, â€œAdaptive optimal controllers
based on generalized policy iteration in a continuous-time framework,â€
in 17th Mediterranean Conference on Control and Automation, (Thes-
saloniki, Greece), pp. 1402â€“1409, IEEE, 2009.

[30] D. Vrabie, O. Pastravanu, M. Abu-Khalaf, and F. L. Lewis, â€œAdaptive
optimal control for continuous-time linear systems based on policy
iteration,â€ Automatica, vol. 45, no. 2, pp. 477â€“484, 2009.

[31] Y. Jiang and Z.-P. Jiang, â€œGlobal adaptive dynamic programming for
continuous-time nonlinear systems,â€ IEEE Transactions on Automatic
Control, vol. 60, no. 11, pp. 2917â€“2929, 2015.

[32] K. G. Vamvoudakis, F. L. Lewis, and G. R. Hudas, â€œMulti-agent
differential graphical games: Online adaptive learning solution for syn-
chronization with optimality ,â€ Automatica, vol. 48, no. 8, pp. 1598â€“
1611, 2012.

[33] J. Li, H. Modares, T. Chai, F. L. Lewis, and L. Xie, â€œOff-policy rein-
forcement learning for synchronization in multiagent graphical games,â€
IEEE Transactions on Neural Networks and Learning Systems, vol. 28,
no. 10, pp. 2434â€“2445, 2017.

[34] S. Lyashevskiy, â€œConstrained optimization and control of nonlinear
systems: new results in optimal control,â€ in Proceedings of 35th IEEE
Conference on Decision and Control, vol. 1, (Kobe, Japan), pp. 541â€“546,
IEEE, 1996.

[35] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski,
S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran,
D. Wierstra, S. Legg, and D. Hassabis, â€œHuman-level control through
deep reinforcement learning,â€ Nature, vol. 518, pp. 529â€“533, Feb. 2015.
[36] Y. LeCun, Y. Bengio, and G. E. Hinton, â€œDeep learning,â€ Nature,

vol. 521, pp. 436â€“444, 2015.

10

[37] S. Ruder, â€œAn overview of gradient descent optimization algorithms,â€

arXiv preprint arXiv:1609.04747, 2016.

[38] Z. Allen-Zhu, Y. Li, and Z. Song, â€œA convergence theory for deep learn-
ing via over-parameterization,â€ in Proceedings of the 36th International
Conference on Machine Learning, vol. 97, (Long Beach, California,
USA), pp. 242â€“252, PMLR, 09â€“15 Jun 2019.

[39] S. Du, J. Lee, H. Li, L. Wang, and X. Zhai, â€œGradient descent
ï¬nds global minima of deep neural networks,â€ in Proceedings of the
36th International Conference on Machine Learning (K. Chaudhuri
and R. Salakhutdinov, eds.), vol. 97, (Long Beach, California, USA),
pp. 1675â€“1685, PMLR, 09â€“15 Jun 2019.

[40] K. Hornik, M. Stinchcombe, and H. White, â€œUniversal approximation of
an unknown mapping and its derivatives using multilayer feedforward
networks,â€ Neural Networks, vol. 3, no. 5, pp. 551â€“560, 1990.

[41] A. M. Lyapunov, â€œThe general problem of the stability of motion,â€

International Journal of Control, vol. 55, no. 3, pp. 531â€“534, 1993.

[42] R. G. Bartle and D. R. Sherbert, Introduction to Real Analysis. Hoboken,

NJ: Wiley, 2011.

[43] B. L. Stevens and F. L. Lewis, Aircraft control and simulation. New

Jersey: John Willey, 2003.

[44] J. Kong, M. Pfeiffer, G. Schildbach, and F. Borrelli, â€œKinematic and
dynamic vehicle models for autonomous driving control design,â€ in
2015 IEEE Intelligent Vehicles Symposium (IV), (Seoul, South Korea),
pp. 1094â€“1099, IEEE, 2015.

[45] R. Li, Y. Li, S. Li, E. Burdet, and B. Cheng, â€œDriver-automation
indirect shared control of highly automated vehicles with intention-aware
authority transition,â€ in 2017 IEEE Intelligent Vehicles Symposium (IV),
(Los Angeles, CA, USA), pp. 26â€“32, IEEE, 2017.

[46] V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Harley, T. P. Lil-
licrap, D. Silver, and K. Kavukcuoglu, â€œAsynchronous methods for
deep reinforcement learning,â€ in Proceedings of the 33rd International
Conference on International Conference on Machine Learning - Volume
48, (New York, NY, USA), pp. 1928â€“1937, PMLR, 2016.

