1
2
0
2

v
o
N
9
2

]

G
L
.
s
c
[

1
v
6
5
7
4
1
.
1
1
1
2
:
v
i
X
r
a

AUTOMATED BENCHMARK-DRIVEN
DESIGN AND EXPLANATION OF HYPERPARAMETER OPTIMIZERS

A PREPRINT

Julia Moosbauer*1, Martin Binder*1,
Lennart Schneider1, Florian Pfisterer1, Marc Becker1, Michel Lang1, Lars Kotthoff2, Bernd Bischl1

1 Department of Statistics
Ludwig-Maximilians-Universität München
[first.last]@stat.uni-muenchen.de

2 Department of Computer Science
University of Wyoming
larsko@uwyo.edu

November 30, 2021

ABSTRACT

Automated hyperparameter optimization (HPO) has gained great popularity and is an important ingredient
of most automated machine learning frameworks.
The process of designing HPO algorithms, however, is still an unsystematic and manual process: Limitations
of prior work are identified and the improvements proposed are – even though guided by expert knowledge
– still somewhat arbitrary. This rarely allows for gaining a holistic understanding of which algorithmic
components are driving performance, and carries the risk of overlooking good algorithmic design choices.
We present a principled approach to automated benchmark-driven algorithm design applied to multifidelity
HPO (MF-HPO): First, we formalize a rich space of MF-HPO candidates that includes, but is not limited to
common HPO algorithms, and then present a configurable framework covering this space. To find the best
candidate automatically and systematically, we follow a programming-by-optimization approach and search
over the space of algorithm candidates via Bayesian optimization. We challenge whether the found design
choices are necessary or could be replaced by more naive and simpler ones by performing an ablation
analysis. We observe that using a relatively simple configuration, in some ways simpler than established
methods, performs very well as long as some critical configuration parameters have the right value.

1 Introduction

Machine learning (ML) is, in many regards, an optimization problem, and many ML methods can be expressed as algorithms
that perform loss minimization with respect to a given objective function. The higher-level task of selecting the ML method
and its configuration is often framed as an optimization problem as well, sometimes referred to simply as hyperparameter
optimization (HPO) [1] or combined algorithm selection and hyperparameter optimization (CASH) problem [2]. Successfully
addressing this problem can lead to large performance gains compared to simply using a default set of parameters. Automated
methods to perform this meta-optimization, in the form of automated machine learning (AutoML), can make ML methods
more accessible to non-experts. Because of their potential benefits to ML performance and usability, it is of particular interest
to find optimization algorithms that perform well for this meta-optimization.

Optimization problems arise in many fields of science and engineering, but as the no-free-lunch theorem states, there is no
one optimization algorithm that solves all problems equally well [3]. To design suitable optimizers, it is therefore important
to understand the characteristics of HPO:

*These authors contributed equally to this work.

 
 
 
 
 
 
Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

• Black-box: The objective usually provides no analytical information [4] – such as a gradient – thus rendering the

application of many traditional optimization methods, such as BFGS, inappropriate or at least dubious.

• Complex Search Space: The search space of the optimization problem is often high-dimensional and may contain
continuous, integer-valued and categorical dimensions. Often, there are dependencies between dimensions or even
specific parameter values [5].

• Expensive: A single evaluation of the objective function may take hours or days. Thus, the total number of possible

function evaluations is often severely limited [4].

• Low-fidelity approximations possible: An approximation of the true objective value at lower expense can often

be obtained, for example through a partial evaluation [6].

• Low effective dimensionality: The landscape of the objective function can usually be approximated with high

fidelity as a function of a small subset of all dimensions [7].

These characteristics have been a primary focus in recent HPO and AutoML research. Many approaches tackle HPO by
estimating a local or global structure of the objective landscape by some form of statistical or ML model. This introduces
additional overhead and increases the overall resource requirements of the method compared to e.g. random search.

Bayesian optimization (BO) [8] and frameworks based on BO are usually based on the global optimization of a non-linear
regression model, e.g., a Gaussian process or random forest. They have yielded significant improvements in performance [9]
by putting less emphasis on unpromising parts of the search space, but carry a significant overhead. BO is somewhat difficult
to parallelize due to its sequential nature, although many variants exists e.g. [10–14], for which superiority is unclear.

Multifidelity HPO (MF-HPO) algorithms aim to accelerate the optimization process by exploiting cheaper proxy functions of
the objective function itself (e.g., by training ML models on a smaller subsample of the available training data, or by running
fewer training iterations). Bandit-based algorithms like Hyperband (HB) [15] have become particularly popular because
of their good trade-off between optimization performance and simplicity. In more recent works, the expensive nature of the
optimization problem is addressed by algorithms designed with both parallel execution and multifidelity evaluations [15–17]

Progress in the field of MF-HPO often consists of iterative improvements of established algorithms. Considerable work has
been done, for example, to improve the limitations of HB: ASHA [16] proposes a sophisticated way to make efficient use of
parallel resources, BOHB [18] improves performance during later parts of a run by incorporating surrogate assistance into HB,
and A-BOHB [19] unites a bandit-based optimization scheme using model-based guidance with asynchronous parallelization.

While these conceptual extensions of HPO carry all their respective merit, it is often somewhat overlooked that the simplicity
of an optimization algorithm heavily influences its adoption in practice, i.e. how difficult modifications and extensions are
and on how many dependencies a system relies [20]. For example, random search (RS) still enjoys great popularity, as it
is extremely simple to implement and parallelize, has almost no overhead, and is able to take advantage of the aforementioned
low-effective dimensionality [7]. Furthermore, algorithmic developments build on prior research that identifies and addresses
limitations, but rarely questions core algorithmic choices that have been made in the original implementation. Many
multifidelity algorithms, for example, are extensions and further developments of HB that take the fixed successive halving
schedule for granted. The process of designing a good MF-HPO optimizer in practice, and many other algorithmic solutions
in science in general, can therefore often feel somewhat like a “manual stochastic local search on the meta level”. The
drawback of this manual process is that the design space of all HPO algorithms is not searched systematically and parts of it
excluded by prior algorithmic decisions. This carries the risk that well-performing algorithms are overlooked, as “established”
algorithmic choices are not questioned, and it is often hard to determine which components of an algorithm make a difference.
In particular, it is possible that overly complicated algorithms with many components are developed by extending “established”
designs, only some of which contribute meaningfully to performance gains. We design an HPO algorithm following the
programming-by-optimization paradigm [21]. We formalize the space of potential HPO algorithms to be able to apply an
automatic search procedure to find the best designs of optimization algorithm. Furthermore, as any HPO algorithm is going to
be applied in a diverse set of application scenarios that are very different (e.g. in terms of dimensionality and complexity of the
search space), we require an expressive and representative benchmark on which to perform the optimization and evaluation.

1.1 Contributions

We demonstrate in a principled manner how HPO algorithm design can be performed systematically and automatically in
a benchmark-driven approach. In particular, the contributions of this work are:

• Formalization: We formalize the design space of MF-HPO algorithms and demonstrate how common MF-HPO

algorithms represent instances within this space.

• Framework: Based on this formalization, we present a rich, configurable framework for MF-HPO algorithms,

whose software implementation is called SMASHY (Surrogate Model Assisted HYperband).

2

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

• Configuration: Based on the formalization and framework, we follow an empirical approach to design an MF-HPO
algorithm by optimization, given a 2 large benchmarks suits. This configuration procedure does not only consider
performance, but also e.g. the simplicity of the design.

• Explanation: For the resulting MF-HPO system, we systematically assess and explain the effect of different
design choices on overall algorithmic performance. Furthermore, We investigate the behavior of algorithmic design
components in the context of specific problem scenarios; i.e., we investigate which algorithmic components lead
to performance improvements for simple HPO with numeric hyperparameters, AutoML pipeline configuration,
and neural architecture search.

We stress that the design of a new algorithm for algorithm configuration is not in the scope of this paper. We are building
on prior work and use state-of-the-art configuration algorithms, and apply them to our novel MF-HPO framework.

2 Related Work

AutoML is a relatively new field, enabled by recent advances in computational resources, the availability of data and datasets
for ML on a large scale, and driven by an ever-growing need for ML expertise that cannot be met by human experts. However,
the basic problem of choosing the most suitable approach for solving a problem goes back much further than that, formalized
as the Algorithm Selection Problem in its most basic form [22]. Before the advent of large-scale computing and data, so-called
meta-learning methods were developed to make the best use of the sparse experience of running ML on specific tasks. There
are many approaches; one of the most popular identifies approaches that performed well on tasks similar to the one given
and applies them (see, e.g., [23–25]). There are more recent approaches to meta-learning [26] for deep neural networks. The
interested reader is referred to a recent survey on meta-learning [27]. Arguably, one of the most successful and popular tools for
current AutoML is HPO [1] and its recent variant MF-HPO. As we reference both extensively in Section 1 and the remainder
of the paper deals exclusive with this topic, we abstain from a further discussion here. Although new, AutoML has become
a large field, and a full review is beyond the scope of this paper, the interested reader is referred to [28] for more information.

There are several other approaches that provide a high-level language for the configuration of a software system that allows ex-
pression of common solution approaches for a particular problem, in particular in AI. Multi-TAC [29] provides high-level LISP
constructs to solve constraint satisfaction problems (CSP), allowing expression of different heuristics and types of CSPs to be
solved. The KIDS system [30] provides similar functionality and in addition reformulates the problem to solve it more efficiently.
The Dominion architecture [31] allows generating C++ CSP solvers that are specialized to a particular type of problem based on
code templates. Aeon [32] allows for the flexible construction of solvers for scheduling problems. The Heuristic Search Frame-
work [33] provides building blocks for heuristic search that can be assembled based on the problem to be solved. PetaBricks [34]
is not specific to any particular type of problem and allows expressing and configuring algorithmic choices more generally.

SATenstein [35] provides components for solvers for the satisfiability problem that do not have to be assembled explicitly, but
are selected by setting hyperparameters of the software. The authors use the SMAC [5] hyperparameter tuner to automatically
tailor SATenstein to particular sets of problems. Similarly, [36] use irace [37] to automatically configure an ant colony
optimization algorithm with multiple competing objectives. We take a similar approach here in that the algorithmic choices
are exposed as hyperparameters that can be tuned without the need for an approach that is specialized to our setting.

One of the problems with automatically designing software from building blocks is that not all combinations of building
blocks are necessarily valid or solve the target problem. Common solutions to this problem are to make all combinations
valid, or to automatically identify and prohibit/penalize invalid combinations. A particularly elegant solution is presented
in [38] in the context of search heuristics, where declarative constraints specify what components can be combined and how
they can be combined. Every solution to the combinatorial problem defined by these constraints represents a valid design.
In this paper, we take a similar approach – all configurations are a valid design.

The general approach of allowing algorithmic choices in a software system instead of fixing them at implementation time
has been termed “Programming by Optimization” [21] (PBO). HPO can be seen as an instance of PBO in a certain sense,
but we are not aware of many cases where PBO is applied to designing HPO systems themselves. There are many approaches
that focus on individual algorithmic choices (e.g. the choice of surrogate model for Bayesian optimization [39]) without
the flexibility of our approach, or optimizing a meta-algorithmic process, e.g. [40] investigate the impact of tuning the
hyperparameters of a Bayesian optimization process and [41] configure irace on a set of surrogate benchmarks.

Most existing approaches do not analyze the algorithmic choices of an optimized system, and the ones that do perform
relatively straightforward analyses. For example, [29] compare the designs their approach finds automatically to the designs
expert humans generated. [42] evaluate the contribution of one component at a time to the change between a default and
an optimized design in order to automatically identify the most important change. [43] perform ANOVA and non-parametric
Friedman tests to investigate in detail the effects that algorithmic choices have for ant colony optimization algorithms.

3

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

3 HPO and Multifidelity HPO

3.1 Supervised Machine Learning
Supervised ML typically deals with a dataset (which is, mathematically speaking, a tuple) D =(cid:0)(x(i),y(i))(cid:1)∈(X ×Y)n of
n observations, assumed to be drawn i.i.d. from a data-generating distribution Pxy. An ML model is a function ˆf :X →Rg
that assigns a prediction to a feature vector from X . ˆf is itself constructed by an inducer function I, i.e., the model-fitting
algorithm. The inducer I :(D,λ)(cid:55)→ ˆf uses training data D and a vector of hyperparameters λ that govern its behavior. The
overall goal of supervised ML is to derive a model ˆf from a data set D, so that ˆf predicts data sampled from Pxy best. The
quality of a prediction is measured as the discrepancy between predictions and ground truth. This is operationalized by the loss
function L:Y ×Rg →R+
0 , which is to be minimized during model fitting. The expectation of the loss value of predictions
made for (unseen) data samples drawn from Pxy is the generalization error

GE :=E(x,y)∼Pxy

(cid:105)
(cid:104)
L(y,ˆf(x))

(1)

which can not be computed directly if Pxy is not known beyond the available data D. One therefore often uses so-called
resampling techniques [44] that fit models on Niter subsamples D[Jj] and evaluate them on complements D[−Jj] of these
subsets to obtain an estimate of the generalization error

(cid:100)GE(I,λ,J)=

1
Niter

Niter(cid:88)

j=1

L(cid:0)y[−Jj],I(D[Jj],λ)(x[−Jj])(cid:1).

(2)

Depending on the resampling method, the inducer I, and the quantity of data in D, evaluating the resampling objective in
Equation 2 can require large amounts of computational resources.

3.2 Hyperparameter Optimization (HPO)

The goal of HPO is to identify a hyperparameter configuration that performs well in terms of the (estimated) generalization error
in Eq. 2. Often, optimization only concerns a subspace of available hyperparameters because some hyperparameters might be set
based on prior knowledge or due to other constraints. Formally, one can split up the space of hyperparameters Λ into a subspace
of hyperparameters influencing the performance ΛS and the remaining hyperparameters ΛC. We define the HPO problem as:

λ∗

S ∈argmin
λS∈ΛS

c(λS)=argmin
λS∈ΛS

(cid:100)GE(I,(λS,λC),J).

(3)

Here, λ∗

S denotes the theoretical optimum and c(λS) is a shorthand for the estimated generalization error in Eq. (2).
Hyperparameters can be either continuous, discrete, or categorical, and search spaces are often a mix of the different types.
The search space may be hierarchical, i.e., some (child) hyperparameters can only be set in a meaningful way if another
(parent) hyperparameter takes a certain value. Therefore, the search space ΛS might be mixed and hierarchical. In particular,
many AutoML frameworks perform optimization over a hierarchical hyperparameter space that represents the components
of a complex ML pipeline [1, 2, 28].

Many HPO algorithms can be characterized by how they handle two different trade-offs: a) The exploration vs. exploitation
trade-off refers to how much budget an optimizer spends on either trying to directly exploit the currently available knowledge
base by evaluating very close to the currently best candidates (e.g., local search) or whether it explores the search space to
gather new knowledge (e.g., random search). b) The inference vs. search trade-off refers to how much time and overhead is
spent to induce a model from the currently available archive data in order to exploit past evaluations as much as possible. Other
relevant aspects that HPO algorithms differ in are: Parallelizability, i.e. how many configurations a tuner can (reasonably)
propose at the same time; global vs. local behavior of the optimizer, i.e. if updates are always quite close to already evaluated
configurations; noise handling, i.e., if the optimizer takes into account that the estimated generalization error is noisy;
multifidelity, i.e., if the tuner uses cheaper evaluations, for example on smaller subsets of the data, to infer performance on
the full data; search space complexity, i.e., if and how hierarchical search spaces can be handled.

3.3 Multifidelity HPO

The HPO problem defined in (3) is challenging. One of the reasons is that the value of the objective function c(λS) is
expensive to evaluate. Optimization methods for HPO attempt to overcome this in two ways: by being very sample-efficient
and attempting to extract as much information from previous objective evaluations as possible, and by using multifidelity
methods that evaluate cheaper proxy functions of (cid:100)GE.

4

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

Multifidelity methods make use of the fact that the resampling procedure in Equation (2) can be modified in multiple ways to
make evaluation cheaper: one can (i) reduce the training sizes |Jj| via subsampling, as model evaluation complexity is often at
least linear in training set size, or (ii) change some components in λ in a way that makes model fits cheaper, such as reducing the
overall number of training cycles performed by a neural network fitting process, or reducing the number of base learner fits in a
bagging or boosting method. These modifications can both increase the variance of (cid:100)GE and also introduce an (often pessimistic)
bias, as models trained on smaller datasets or with values of λ that make fitting cheaper often have worse generalization error.

We introduce a fidelity hyperparameter r ∈ (0,1] that influences the resource requirements of the evaluation of (cid:100)GE and
thus the overall budget spent for the optimization. It can influence the size of the training dataset or the λ to trade off
between evaluation efficiency and information gained about the generalization error. In the latter case, r can only influence
hyperparameters separate from λS, i.e., λC. Typically, r only affects one of these aspects at a time, and if it affects λC, it
only affects a single hyperparameter dimension. We define

c(λS;r):=(cid:100)GE(I,(λS,λC(r)),J).

(4)

If the assumption holds that a higher fidelity r returns a better model in terms of the generalization error (e.g., if r controls
the size of the training set via sub-sampling), we are interested in finding the configuration λS for which the inducer will
return the best model given the full budget r = 1. Under this assumption, we define c(λS) := c(λS;1), see e.g. [45], and
define the optimization problem as in Equation (3). In practice, an HPO algorithm would return a configuration λ∗
S. A
final model might be trained for λ∗
S for the full fidelity r = 1 after optimization (if not yet done by the optimizer). This
assumption may be violated in some scenarios and model performance could worsen for a higher fidelity (e.g., a neural
network, which may overfit on a small dataset if trained for too many epochs). In this case, we define the optimization
problem as (λ∗

S,r∗)∈arg minλS∈ΛS,r∈(0,1]c(λS;r).

The resource requirements of evaluating c(λ;r) can have a complicated relationship with λ and r; in practice, r is chosen
such that it has an overwhelming and linear influence on resource demand. The overall cost of optimization up to a point
in time is therefore assumed to be the cumulative sum of the values of r of all evaluations of c(λ;r) up to that point. We
can also interpret r as the fraction of the overall budget that has been spent in this way.

Given the definition of the HPO problem, we present a (multifidelity) hyperparameter optimization algorithm for a single
worker in its most generic form (see Alg. 1). Until a pre-determined budget is exhausted, such an algorithm decides in every
iteration (a) which configuration(s) λS to evaluate next and (b) which fraction of the budget r to allocate (non-multifidelity
algorithms set this to r =1 as default). The algorithm makes use of an archive A, a database recording previously proposed
hyperparameter configurations and, if available, their evaluation results.

Algorithm 1 A generic HPO algorithm

,i=1,...,k, based on archive A

3:

4:

1: while budget is not exhausted do
S ,r(i)(cid:17)
(cid:16)
λ(i)

2:

Propose
Write proposals into a shared archive A
S ;r(i)(cid:17)
(cid:16)
λ(i)
Evaluate configuration(s) c
Write results into shared archive A

5:
6: end while
7: Wait for workers to synchronize
8: Return best configuration in archive A

The optimization process can be accelerated by making efficient use of parallel resources. We distinguish between synchronous
and asynchronous scheduling. The former starts multiple evaluations synchronously at the same time. To be more precise,
a number of k >1 configurations is proposed in line 2 and evaluated in parallel in line 4, all within the inner loop of Alg. 1.
Given K available parallel resources, it should be ensured that the number k of configurations that are scheduled in parallel
is not significantly smaller than K, and that the evaluation runtimes amongst these k configurations do not differ significantly
in order to avoid idling single parallel resources. In contrast, for asynchronous scheduling, Alg. 1 is individually run on a
set of K different worker resources. Given a shared archive that is synchronized amongst the workers, every worker can
independently schedule evaluations as soon as they are free.

Common established HPO algorithms are instances of Alg. 1 as described below and summarized in Table 1.

5

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

3.3.1 Random Search

Configurations are drawn (uniformly) at random, and every configuration is evaluated with full fidelity r =1. Parallelization
is straightforward, as configurations are drawn independently.

3.3.2 Bayesian optimization

In Bayesian optimization (BO), the configuration that maximizes an acquisition function a(λ) (e.g., expected improvement,
EI [4], or the lower confidence bound, LCB) is proposed and evaluated with the full fidelity r = 1. a(λ) is based on a
surrogate model trained on the archive A. BO can be parallelized by either using methods that can propose multiple points
at the same time using a single surrogate model, or alternatively by fitting a surrogate model on the anticipated outcome
of configurations that were proposed but not yet evaluated [11].

3.3.3 FABOLAS

Fabolas [46] is a continuous multi-fidelity BO method based on BO, where the conditional validation error is modelled as
a Gaussian process using a complex kernel capturing covariance with the training set fraction r ∈(0,1] to allow for adaptive
evaluation at different resource levels.

3.3.4 Successive Halving

Successive halving (SH) [47] is a simple multi-fidelity optimization algorithm that combines random sampling of
configurations with a fixed schedule for r. At the beginning, a batch of µ configurations is sampled randomly and evaluated
with an initial fidelity rmin < 1. This is followed by repeated “halving” steps, where the top fraction η−1 of configurations
is kept and evaluated after r is increased by a factor of η, until the maximum fidelity value is reached. The schedule is chosen
to keep the total sum of all evaluated r constant in each batch.

3.3.5 Hyperband

Similar to SH, Hyperband [15] uses a fixed schedule for the fidelity parameter r, but it augments SH by using multiple
brackets b of SH runs starting at different rmin(b) and with different µ(b).
The number of brackets is set to s=(cid:98)−logη(rmin)(cid:99)+1, which coincides with the number of fidelity steps that can be performed
on a geometric scale on the interval [rmin,1]. In bracket b∈{1,2,...,s}, a number of µ(b)=(cid:100)s· ηs−b
s−b+1(cid:101) are initially sampled and
evaluated with initial fidelity r =ηs−b. µ(b) is chosen such that each bracket approximately spends a similar overall budget.

3.3.6 Asynchronous Successive Halving (ASHA) and Asynchronous Hyperband

Hyperband, as well as SH, have the drawback that batch sizes decrease throughout the stages of an SH run, preventing efficient
utilization of parallel resources. ASHA [16] is an effective method to parallelize SH by an asynchronous parallelization
scheme. A shared archive across a number of different workers is maintained. Instead of waiting until all n configurations
of a batch have been evaluated for fidelity r, every free worker queries the shared archive A for promotable configurations
(i.e., configurations that belong to the fraction of top η−1 configurations evaluated with the same fidelity). Asynchronous
Hyperband works similarly.

3.3.7 Bayesian Optimization Hyperband (BOHB)

Model-based methods outperform Hyperband when a relatively large amount of budget is available and many objective function
evaluations can be performed. BOHB [18] was created to overcome this drawback. It iterates through successive halving
brackets like Hyperband, but, instead of sampling new configurations randomly, it uses information from the archive to propose
points that are likely to perform well. Ns total configurations are proposed for evaluation; ρ are sampled at random and the rest
are chosen based on Bayesian optimization with a surrogate model induced on the evaluated configurations in A. The models
used by BOHB are a pair of kernel density estimators of the top and bottom configurations in A, similar to the process in [48].

3.3.8 A-BOHB

[17] propose A-HBOB, an asynchronous extension of BOHB where configurations are sampled from a joint Gaussian Process,
explicitly capturing correlations across fidelities. In contrast to ASHA [16] and asynchronous versions of BOHB implemented
by [18], A-HBOB does not perform synchronization after each stage but instead uses a stopping rule [49] to asynchronously
determine whether a configuration should be promoted to the next rung (and immediately scheduled) or terminated.

6

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

4 Formalizing a broad class of MF-HPO ALgorithms

We aim to find an HPO algorithm that performs particularly well in the multifidelity setting. To design an algorithm by
optimization, we propose a framework and search space of HPO algorithm candidates that covers a large class of possible
algorithms as in Alg. 1, and focus on a subclass of algorithms similar to Hyperband because of their favorable properties. This
subclass is limited to multifidelity algorithms that use a pre-defined schedule of geometrically increasing fidelity evaluations,
and therefore contains fixed-fidelity-schedule HPO algorithms like BOHB and Hyperband.

The basis of this framework is presented as Algorithm 2, which has configuration parameters that allow it to be tailored
to specific benchmarks by combining algorithmic building blocks in novel ways. The main difference to Alg. 1 is that the
Propose part is specified more explicitly. At its core, Alg. 2 consists of two parts: (i) sampling new configurations at low
fidelities (lines 2–7) and (ii) increasing the fidelity for existing configurations (lines 8–14). In contrast to Alg. 1, Alg. 2 makes
use of state variables t, b, and r to account for optimization progress. However, these variables are only shown in Alg. 2
for clarity and can, in principle, be inferred from the archive A. As argued in Section 3, every single worker instance of
Alg. 1 can in principle be scheduled asynchronously, but we do not consider this in this work.

First, Alg. 2 uses a SAMPLE-subroutine to initialize the initial batch C of µ solution candidates. The fidelity of the evaluation
of the proposed configurations is refined iteratively; when all configurations in the batch have been evaluated with given
fidelity r, the top 1/ηsurv fraction of configurations is evaluated with a fidelity that is increased by a factor of ηfid. When
the fidelity cannot be further increased for a batch because all of its configurations were evaluated at full fidelity r =1, they
are set aside, and a new batch of configurations is sampled.

The SAMPLE subroutine creates new configurations to be evaluated, possibly using information from the archive to propose
points that are likely to perform well, similar to BOHB. Instead of using a pair of kernel density estimators, we allow that
any inducer Ifsur that produces a surrogate model fsur can be used for model-assisted sampling1. The subroutine works by
at first sampling a number of points from a given generating distribution Pλ(A). The performance of these points is then
predicted using the surrogate model, and points with unfavorable predictions are discarded a process which we refer to as
filtering. This process is repeated until the requested number µ of points that were not discarded is obtained. Ns and ρ have
the same function as in [18] (see Section 3.3.7), with Ns controlling the number of sampled points needed for each of the µ
points returned, and ρ controlling the fraction of points that are not filtered. Thus the configuration space of sampling methods
also includes purely random sampling, as in Hyperband, by setting ρ=1. The influence of the surrogate model on sampled
candidates is larger when (i) the number of sampled configurations Ns is large, or (ii) the fraction of candidates sampled
at random ρ is small. We present two slightly different SAMPLE algorithms (Algorithms 3 and 4) based on this principle.

s

s

s )(n−i)/µ·(N 1

for different points λ(i)
s and N 1

samples, we draw nper_tournament points from nper_tournament·N (i)

Algorithm SAMPLETOURNAMENT (pseudocode in Appendix A, Alg. 3) diversifies the set of points proposed through an
extension that uses different values of N (i)
S ,i ∈ {1,...,µ} sampled at each invocation of SAMPLE.
We parameterize this using the configuration parameters N 0
s ; the effective value of Ns for each point is interpolated
s )i/µ(cid:7)
geometrically. This means that, when n points are sampled, the ith point is chosen from a set of (cid:4)(N 0
randomly sampled points. We furthermore make it possible to draw more than one point from the same independent random
search sample: instead of drawing one point from N (i)
samples.
Besides the sampling method described above, we propose an alternative method, which we name SAMPLEPROGRESSIVE
(pseudocode in Appendix A, Alg. 4): instead of sampling Ns(i) points independently for each configuration λ(i)
S , we sample
a single ordered pool P of µ·max(N 0
S is then selected as
the point with the best surrogate-predicted performance from the first µ·Ns(i) points in P that was not already selected before.
While hyperparameters λS are proposed by one of the two SAMPLE methods, the fidelity hyperparameter r follows a fixed
schedule similar to SH and Hyperband, with a few extensions. For one, the fraction of survivors ηsurv can be a different value
from the fidelity scaling factor ηfid. Furthermore, the algorithm allows two scheduling modes, controlled by batch_method:
The HB mode evaluates brackets, as performed by Hyperband. µ(b) is then adjusted to make total budget expenditure
approximately equal between all brackets, and depends on both ηsurv and ηfid in a more complex way than in Hyperband.
On the other hand, the equal batch_method uses equal batch sizes for every evaluation. Individuals that perform badly
at low fidelity are removed, as in SH, but new individuals are sampled to fill up batches to the original size. Because new
individuals are added to the batches at all fidelity steps, it is not necessary to use brackets with different initial fidelities, and
therefore only a single repeating bracket b=1 is used.

s ) random points once at the beginning of SAMPLE. Each λ(i)

s ,N 1

s

If the exploration-exploitation tradeoff is not balanced properly, optimization progress can either stagnate or functions
evaluation are wasted due to too much exploration of uninteresting regions of the search space. However, the relative
importance of exploration and exploitation can change throughout the course of optimization, where exploration performed later
during the optimization is not as useful as during the beginning. The given configuration space makes it possible to make the

1BOHB itself uses an inducer that produces a function to calculate the ratio of kernel densities, an unusual kind of regression model.

7

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

Algorithm 2 SMASHY algorithm

Configuration Parameters: batch size schedule µ(b), number of fidelity stages s, survival rate ηsurv, fidelity rate ηfid,
SAMPLE method (either SAMPLETOURNAMENT or SAMPLEPROGRESSIVE), batch_method (either equal or HB), total
budget B; further configuration parameters of SAMPLE: Ifsurr, Pλ(A), ρ(t), (cid:0)N 0

s (t)(cid:1), ntrn.

s ,N 1

State Variables: Expended budget fraction t←0, bracket counter b←1 (remains 1 for batch_method = equal), current

fidelity r ←1, batch of proposed configurations C ←∅

1: while t<1 do

(cid:46) Generate new batch of configurations

(cid:1)

2:
3:
4:

5:
6:
7:
8:
9:
10:
11:
12:
13:

14:
15:

if r =1 then

r ←(ηfid)b−s
C ← SAMPLE(cid:0) A,µ(b),r;Ifsur,Pλ(A),
ρ(t),(cid:0)N 0

s (t),N 1

s (t)(cid:1),ntrn

if batch_method = HB then

b←(bmods)+1

end if

else

r ←r·ηfid
C ← SELECT_TOP(C,|C|/ηsurv)
if batch_method = equal then

˜µ←µ(b)−|C|
C ←C ∪SAMPLE(cid:0)A,˜µ,r;Ifsur,Pλ(A),

ρ(t),(cid:0)N 0

s (t),N 1

s (t)(cid:1),ntrn

end if

end if

Evaluate configuration(s) c(λS;r) for all λS ∈C
Write results into shared archive A
t←t+r·|C|/B

16:
17:
18:
19: end while

* possibly adapted during the course of optimization

(cid:1)

(cid:46) Progress fidelity

(cid:46) Update budget spent

s (t)(cid:1)
exploration-exploitation tradeoff dependent on optimization progress by providing the option to make ρ(t) and (cid:0)N 0
dependent on the proportion of exhausted total budget at every configuration proposal step. It is likely that large values of
ρ(t) / small values of Ns(t) perform better when t is small, and conversely, small ρ(t) / large Ns(t) work well for large t.

s (t),N 1

Common HPO algorithms (RS, BO, SH, HB, BOHB) can be instantiated within this framework (see Table 1).

Table 1: RS, BO, HB, BOHB as instances of Algorithm 2. η, ρ, Ns are configuration parameters of the respective algorithms.
“—” denotes that the value has no influence on the algorithm in this configuration.
*: BO and BOHB use inducers that produce non-standard model functions, which do not aim to predict the actual outcome
of configurations, and instead calculate the value of an acquisition function such as EI [4] (for BO) or the ratio of two KDE
models (for BOHB).
†: In a small departure from BOHB, Alg. 2 uses the KDE estimate of good points for all sampled points, even when randomly
interleaved. BOHB does random interleaving from a uniform distribution.

Algorithm

µ(b)

s

RS
BO
HB

BOHB

—
1
(cid:100)s· ηs−b
s−b+1(cid:101)
(cid:100)s· ηs−b
s−b+1(cid:101)

1
1
(cid:98)−logη(rmin)(cid:99)+1
(cid:98)−logη(rmin)(cid:99)+1

ηsurv
—
—
η

η

ρ Ns
1 —
ρ Ns
1 —
ρ Ns

batch_mode Pλ(A)
uniform
uniform
uniform
KDE†

—
—
HB

HB

Ifsur
—

ηbudget
—
— e.g. GP+EI*
η

—

η

TPE*

8

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

5 Automatic Algorithm Design and Analysis

5.1 Algorithm Design

Our goal is to configure a new HPO algorithm based on a superset of design choices included in previously published HPO
methods. We are interested in finding a configuration (or making design choices) based on a set of training instances that
works across a broad set of future problem instances. This problem is called algorithm configuration [5, 50]. It is quite
similar to HPO, while a major usual difference is that algorithm configuration optimizes the configuration of an arbitrary
algorithm over a diverse set of often heterogeneous instances for optimal average performance, HPO performs a per-instance
configuration of an ML inducer for a single data set. We introduce the following notation for consistency with the relevant
literature: γ denotes configuration-parameters controlling our optimizer A, while λ denotes hyperparameters optimized by
our optimizer (controlling our inducer I). The problem can be formally stated as follows: Given an algorithm A:Ω×Γ→Λ
parametrized by γ ∈Γ and a distribution PΩ over problem instances Ω together with a cost metric ζ, we must find a parameter
setting γ∗ that minimizes the expected ζ(A) over PΩ:

γ∗ ∈argmin

Eω∼PΩ[ζ(A(ω,γ))].

γ∈Γ

(5)

In our example, Γ corresponds to the space of possible components of our HPO method and Ω to a class of HPO problems
(i.e. ML methods and datasets on which they are evaluated) for which their configuration should be optimal. Based on
a training set of representative instances {ωi} drawn from PΩ, a configuration γ∗ that minimizes c across these instances
should be chosen through optimization. Here we refer to this process as meta-optimization if necessary to distinguish it from
the optimization performed by the HPO algorithm A.

A variety of racing-based strategies have been proposed for algorithm configuration problems, such as F-RACE [51] and
IRACE [37], along with non-parametric alternatives like ParamILS [52] as well as genetic algorithm-based variants (e.g.
GGA [53]) and sequential model-based optimization (SMBO) variants based on Bayesian optimization (such as SMAC [5]).
We describe two common algorithm configuration methods (SMAC, IRACE) below.

SMAC [28] extends the SMBO paradigm described in Section 3.3.2 to an algorithm configuration setting. This is achieved
through the use of an intensification procedure that governs across how many problem instances each configuration is
evaluated, trading off computational cost against confidence regarding the superiority of a given configuration. Furthermore,
instance features if(ω) describing properties of ω are used to train the empirical performance model (EPM) if(ω)×γ (cid:55)→ ˆζ
predicting the performance ζ(A(ω,γ) of a configuration γ on a new problem instance ω. IRACE [37] does not make use
of an EPM, but instead iteratively repeats the following steps: After sampling new candidate configurations according to
a sampling distribution, a set of elite configurations are selected through a racing procedure [54]. These configurations are
used to update the sampling distribution in order to focus exploration around the current set of elite configurations. During
racing, statistical tests are used to preemptively terminate unpromising configurations with high cost ζ across instances.

5.2 Algorithm Analysis

The design of well-performing algorithms is often the focus of research. However, gaining an understanding of the effect
of the design choices that achieve this performance is crucial for multiple reasons. First, this understanding of such effects
is directly interesting in itself. Second, design by optimization carries the risk of producing overly complex algorithms; if
algorithmic elements have only minor or no effect on algorithm performance they may not be necessary. Third, sophisticated
algorithmic components might be praised as drivers of algorithmic performance, while the actual importance of minor
modifications is not properly highlighted. Hence, it is crucial for research to detect and analyse the effects of all components
to be able to direct future efforts at improvement.

The field of sensitivity analysis (SA) – which generally deals with assessing which inputs (to a mathematical model) determine
the variance of an output – comprises a multitude of methods to assess importance of input factors on the output of a
mathematical model [55]. Commonly used methods are analysis of variance (ANOVA) and functional ANOVA (fANOVA)
methods, which decompose the response of a (mathematical) model or function into lower-order components [42]. In the
field of ML, fANOVA is a popular way of analyzing the importance of hyperparameters [56].

Performing a sensitivity analysis can be computationally expensive, as it usually requires evaluation of the mathematical
model (in our case, running an HPO algorithm) on some experimental design. We want to limit the costs of performing
such a sensitivity analysis, either by approximating the mathematical model to be investigated by a surrogate [57], or by
choosing a method that keeps the resource requirements of the experimental design reasonably low, like for example a
one-factor-at-a-time (OFAT) [58] analysis. Those methods have their own limitations; for example, any interactions between
inputs cannot be detected by an OFAT analysis, and parts of the input space remain hidden.

9

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

A popular way of performing SA in practice are ablation studies. An ablation study involves measuring the performance
of a mathematical model (e.g. the performance of a deep neural network) when removing one or more of its components (e.g.
layers) to understand the relative contribution of the ablated components to overall performance [58]. There are different ways
of performing an ablation analysis; probably the most common approach is Leave-One-Component-Out (LOCO) ablation [58].

In the context of algorithm configuration, [42] propose an ablation approach that links a source configuration (e.g. the default)
to a target (e.g. the optimized configuration) through an ablation path.

6 Experimental Setup

6.1 Benchmark Setup

Given the formalization of the generic MF-HPO algorithm in Section 4, our goal is to (1) find the best representative (out of
this class of algorithms) via optimization, and (2) explain the role of specific algorithmic components in a benchmark-driven
approach. To allow for meaningful conclusions on both aspects, a benchmark problem must be carefully selected to represent
a relevant set of optimization instances on which each candidate algorithm is run and evaluated.

We rely on benchmark scenarios of the YAHPO Gym benchmark suite [59], each of which provides a number of related
instances of optimization problems. Benchmarks in the YAHPO Gym are implemented as surrogate model benchmarks, where
a Wide & Deep [60] neural network was fitted to a set of pre-evaluated performance values of hyperparameter configurations.
These have the advantage that they can be evaluated quickly, making them well-suited for meta-optimization, with a more
realistic objective landscape than synthetic functions or benchmarks based on lookup tables.

Benchmarks are useful both for (meta-)optimization and for comparison of different methods. However, when comparing
the performance of an algorithm found by meta-optimization with other methods, it is necessary to make this comparison on
instances that differ from the instances used for meta-optimization. This is to avoid an optimistic bias caused by “overfitting”
to random peculiarities of the particular instances used for optimization that are not representative of the class of optimization
problems as a whole. YAHPO Gym provides a set of dedicated “training” instances for meta-optimization, and a set of “test”
instances for evaluation of algorithmic performance.

The benchmark collections we have chosen cover three important application areas of AutoML: Hyperparameter optimization,
AutoML pipeline configuration, and neural architecture search (NAS). These classes of problems do not only represent
common and relevant tasks for researchers and practitioners in the field; as presented in Table 2, they are also quite different
regarding: (1) the dimensionality of the search space, (2) the types of the hyperparameters (categorical, integer, continuous)
in the search space, and (3) whether there are hierarchical dependencies between hyperparameters in the search space. While
the underlying data for lcbench and nb301 have been previously used in publications [61, 62], rbv2_super is a novel task
that has not been investigated previously in literature.

HPO on a neural network: The first set of problems, lcbench [61], covers hyperparameter optimization (HPO) on a relatively
small and numeric search space. The neural network (more precisely, a funnel-shaped multilayer perceptron) that is tuned
has a total of 7 numerical hyperparameters. The fidelity of an evaluation can be controlled by setting the number of epochs
over which the neural network is trained. The instances belonging to this scenario represent HPO performed on 35 different
classification tasks taken from OpenML [63]. As a target metric, we choose the cross entropy loss on the validation set.

AutoML pipeline configuration: Second, we investigate the problem of configuring an AutoML pipeline. Here, a learning
algorithm needs to be selected first from the following candidates: approximate k nearest neighbours [64], elastic net
linear models [65], random forests [66], decision trees [67], support vector machines [68], and gradient boosting [69]. The
hyperparameters of each learner are chosen conditioned on this learner being active, i.e. there are hierarchical hyperparameter
dependencies. The fidelity of a single evaluation can be controlled by choosing the size of the training data set that is used
to train the respective learner. The automated optimization of the pipeline is performed for 89 different classification tasks [70],
again taken from OpenML. As a target metric, we opt for the log loss.

Neural architecture search: As third problem scenario we consider is neural architecture search. The search space of
architectures is given by the darts search space [71] and architectures were trained and evaluated on CIFAR-10 [72]: A
convolutional neural network is constructed by stacking so-called normal and reduction cells that each can be represented
as a directed acyclic graph consisting of an ordered sequence of vertices (nodes) resembling feature maps and each directed
edge being associated with an operation that transforms the input node. A tabular representation can be derived using 34
categorical parameters with 24 dependencies. Each architecture can be trained for 1 to 98 epochs, allowing again for lower
fidelity evaluations. The target metric is validation accuracy.

10

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

Table 2: Three Benchmark Collections of YAHPO Gym used in our Benchmark.

Hyperparameter Types

Scenario

Target Metric

d Cont.

lcbench: HPO of a neural network
rbv2_super: AutoML pipeline configuration
nb301: Neural architecture search

cross entropy loss
log loss
validation accuracy

7
38
34

6
20
0

6.2 Algorithm Design via Optimization

Integer Categ. Hierarchical
(cid:55)
(cid:88)
(cid:88)

1
11
0

0
7
34

# Instances

# Training Set

35
89
1

8
30
—

We perform experiments both to meta-optimize Alg. 2 and to compare the resulting configuration to the performance of other
(MF-)HPO algorithms. Meta-optimization is done on the provided training subset of instances, and performance comparisons
are done using all other instances. To investigate to what degree optimal configurations differ between different scenarios,
we run optimization for lcbench and rbv2_super separately and compare the results. There is only one instance of the nb301
optimization task, which is therefore exclusively used as a test set and not for optimization.

To meta-optimize the given algorithm, and to draw conclusions about its performance in comparison with other algorithms,
a budget endpoint needs to be chosen at which optimization performance is measured. We chose a budget of 30·d, with
d the dimension of the problem instance. Algorithm configurations are evaluated by running Alg. 2 on a benchmark instance
until this budget is exhausted, and the best value reached until then is used as overall outcome.

Each problem instance is evaluated once for each configuration evaluation and the average loss value across instances is
used to approximate the expected value in (5). The number of problem instances is relatively small, and we chose to evaluate
performance by running surrogate optimization in parallel on a single machine, as opposed to using more sophisticated racing
approaches that could potentially save some evaluations. Meta-optimization is done via Bayesian optimization, using the
LCB infill criterion and interleaved random meta-configurations every 3 evaluations.

The meta-optimization is performed three times for each scenario, and the resulting performance values are combined into
a single dataset to get the overall resulting configuration. This results in two configurations for our optimizer, γ∗lcbench and
γ∗rbv2_super optimized on training problems in lcbench and rbv2_super respectively.

6.2.1 Optimization Search Space

The search space of configurations for Alg. 2 used is shown in Appendix B, Table 4.

While the batch size µ is constant in the equal batch_method, it is changing for every bracket when batch_method is HB.
The batch sizes of µ(2),µ(3),... are constructed from µ(1) dynamically such that the total budget spent on each batch is
approximately constant, similar to Hyperband’s method [15] but extended to the cases where ηsurv (cid:54)= ηfid. We try several
surrogate learners: Random forests [73] (RF), K-nearest-neighbors with k set to 1 (KNN1), kernelized K-nearest-neighbors
with “optimal” weighting [74] (KKNN7), and the ratio of density predictions of good and bad points, similar to tree parzen
estimators [48] without a hierarchical structure as done in BOHB [18] (TPE). For the pre-filtering sample distribution Pλ(A),
we try both uniform sampling (uniform), and sampling from the estimated density of good points, as done in BOHB [18]
(KDE). ρrandom determines whether the surrogate model makes predictions assuming the highest fidelity value r observed (TRUE),
as opposed to assuming the fidelity of the points being sampled; in the framework of Algs. 3 and 4, this relates to preprocessing
done by Ifsurr. Note that the maximum number of fidelity steps per batch s is not part of the searchspace: Instead, it is inferred
automatically from ηfid and the ratio of the maximal and minimal bounds set for r as part of the optimization problem.

6.3 Algorithm Analysis

Our goal in this work is not only to determine a configuration of Alg. 2 that performs well on the given HPO problems, but
also to determine what effect individual components have on performance. To achieve this, we ran experiments on the “test”
instances provided by YAHPO Gym, both with configurations of our algorithm, and with configurations of our algorithm
that re-implement popular, published methods (c.f. Table 1). Every evaluation, i.e., a complete HPO run on a problem instance,
is repeated 30 times (with different seeds) to allow for statistical analysis.

We aim to answer the following research questions:

RQ1: How does the optimal configuration differ between problem scenarios, i.e., do different problem scenarios benefit

from different HPO algorithms?

RQ2: How does the optimized algorithm compare to other established HPO algorithms?
RQ3: Does the successive-halving fidelity schedule have an advantage over the (simpler) equal-batch-size schedule?

11

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

RQ4: What is the effect of using multi-fidelity methods in general?

RQ5a: Does changing SAMPLE configuration parameters throughout the optimization process give an advantage?

RQ5b: Does (more complicated) surrogate-assisted sampling in SAMPLE provide an advantage over using simple random

sampling?

RQ6: What effect do different surrogate models (or using no model at all) have on the final performance?

A simple method to answer many of these questions is to take the optimized configuration of Alg. 2 and swap out components
of it for generally simpler components, thereby performing a one-factor-at-a-time analysis or an ablation study. However,
the optimal values of some components may have a strong influence on the choice of other components. We therefore run
separate optimizations that are constrained in some ways to reflect some of the research questions. See Table 3 for the different
values of γ we generate. For each value of γ, we run the respectively configured HPO algorithm on both the lcbench and the
rbv2_super setting, and, unless stated otherwise, once each for batch_method being equal and HB. We refer to an optimized
configuration that was obtained on the lcbench scenario with batch_method set to equal as γ∗lcbench[equal], and to the
overall optimum (i.e. the better of γ∗lcbench[equal] and γ∗lcbench[HB]) as γ∗lcbench; similar for rbv2_super. In the following,
we describe each question, and how we intend to answer it, in more detail.

RQ1 – Variation between scenarios: We investigate the difference in the values that γ∗lcbench and γ∗rbv2_super take, and how
large these differences are compared to the natural uncertainty of the optimization outcome. We also investigate how their
performance behaves. This gives an idea how robust the results are, and how reliably they perform in new environments.

RQ2 – Optimized algorithm compared to other algorithms: We evaluate several well-known HPO algorithms in their
default-configuration on the same benchmark instances: for BOHB [18] we use the implementation found in HpBandSter2
version 0.7.4, for HB [15] we use mlr3hyperband3 version 0.1.2, for SMAC [5] we use the SMACv3 package4 version
1.0.1. SMAC does Bayesian optimization with a random forest model, so we included mlrMBO 5 version 1.1.5 for Gaussian
process based BO (GPBO) [4]; however, mlrMBO only uses a GP for numerical search spaces and defaults to a random
forest for categorical spaces, so we only evaluate GPBO on lcbench. Note that GPBO, SMAC and RS are not multi-fidelity
algorithms and therefore always evaluate points with maximum fidelity 1.

RQ3 – Advantage of SH fidelity scheduling: It is likely that the relationship between many configuration parameters and
algorithm performance depends to a large degree on whether equal or HB fidelity scheduling is used. We therefore perform
the overall optimization with both settings separately, and compare their resulting performances.

RQ4 – Effect of using multi-fidelity methods: By setting ηfid to ∞, and therefore the number of fidelity stages s to 1, we
ensure that only evaluations with maximum fidelity 1 are evaluated.

s and N 1
s

RQ5a and RQ5b – Changing configuration parameters during optimization, and within SAMPLE evaluations: We
optimized γ2, where all configuration parameters that could otherwise depend on t were restricted to being constant, i.e.
to have their beginning and end point be equal (RQ5a). In addition to this, we ran another optimization where we further
restricted N 0
to be equal, ntrn to be 1, and only the tournament filter_method be used (RQ5b). The “test”
performance of the resulting configurations gives an indication for the performance that is lost for the gain in simplicity.
RQ6 – Effect of surrogate models: We evaluate the overall result γ∗[equal] with Ifsur set to each of the inducers in the origi-
nal search space (see Tbl. 4). Furthermore, γ∗[equal] is evaluated with ρ set to 1 (all points are randomly sampled, independent
of the filtering model), and finally, with ρ=1 and Pλ(A)=uniform (all points are sampled completely model-free).
RQ7 – Performance with parallel resources: As optimization of expensive machine learning methods is often run in
parallel settings, we evaluate the performance of our method, as well as other methods, in a (simulated) parallel setting.
We evaluate our λ∗[equal] with µ set to 32, and with an optimization budget of 30·4·d, where d is the dimension of the
optimization problem. We compare it to GPBO with qLCB [10] with 32 objective evaluations in parallel and we simulate
parallel execution of RS by running enough random evaluations to fill up 30 · 4 · d evaluations. Both BOHB and SMAC
offer parallelized versions, but the YAHPO Gym benchmark package does not yet provide support for asynchronous parallel
evaluation [59]. However, since HB and BOHB propose evaluations in batches, we compared HB and BOHB by accounting
for submitted batches of objective evaluations in increments of 32, essentially simulating a single HB/BOHB optimizer
sending objective evaluation tasks to 32 parallel workers and waiting for their completion in a synchronous fashion.

2https://github.com/automl/HpBandSter
3https://cran.r-project.org/package=mlr3hyperband
4https://github.com/automl/SMAC3
5https://cran.r-project.org/package=mlrMBO

12

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

6.4 Reproducibility and Open Science

The implementation of the framework of Alg. 2 as well as reproducible scripts for the algorithm configuration and analysis
are made available via public repositories6. All data that was generated by those analyses is made available as well.

Table 3: Summary of Experiment. Shown are the various optimizer configurations γ that were obtained for our experiments.
“Name”: The name by which we refer to the configuration in the text. “RQ”: The research question that mainly relates to the
configuration. “Optimize”: Whether the given configuration was obtained by conducting a (possibly constrained) optimization
((cid:88)), or by substituting values into the global optimum γ∗.

Optimize Design Modification

Signifier RQ
γ∗
γ1
γ2
γ3
γ4
γ5
γ6
γ7

1, 2, 3 (cid:88)
(cid:55)
4
(cid:88)
5a
(cid:88)
5b
(cid:55)
6
(cid:55)
6
(cid:55)
6
(cid:55)
7

s (0)=N 0

none (global optimization)
ηfid →∞
ntrn(0)=ntrn(1), N 0
filter_method→ tournament, ntrn →1, N 0
batch_method→ equal, Ifsur →∗
batch_method→ equal, ρ→0
batch_method→ equal, ρ→0, Pλ(A)→uniform
batch_method→ equal, µ→32, quadruple budget

s (0)=N 1

s (1), N 1

s (1), ρ(0)=ρ(1)
s (1)=N 1

s (0)=N 0

s (0)=N 1

s (1), ρ(0)=ρ(1)

7 Results

7.1 General Results

Table 5 in Appendix C shows the configuration parameters that were selected, for each benchmark scenario, search space
restriction, and for both batch_method variants. For both benchmark scenarios the configurations with the HB batch method, i.e.,
γ∗[HB], were returned as best on the training instances, although the difference is small. Figure 1 shows the configuration values
of the top 80 evaluated points according to their surrogate predicted performance. The implied ranges covered by the bee swarms
are again an indicator of approximate ranges of configurations that can be expected to work well. Figure 4 shows the final perfor-
mance at 30·d full-budget evaluations of all optimization runs that were performed. The standard error shown for lcbench and
rbv2_super reflects the variation between the calculated benchmark-instance-wise performance means, representing uncertainty
about the “true” performance mean if an infinite number of benchmark instances of the given class of problems were available.

RQ1 – Variation between scenarios: As can be seen in Table 5 and in Figure 1, many of the selected components of the
γ∗ are relatively close to each other across the two scenarios on which they were optimized. It is therefore interesting to
note the differences: Ifsurr is restricted to KNN1 on rbv2_super, but can also use KKNN7 on lcbench, which in fact seems to
be slightly preferred. Pλ(A) may take any of the two values for rbv2_super but is restricted to KDE in lcbench. Finally,
ρ(0) is close to 1 in the beginning on rbv2_super, and closer to 0 (although still greater than ρ(1)) for lcbench.
The degree to which the differences in γ∗ make a difference can be observed in Figure 4. The optimized results seem to
perform well in the test sets of the same scenarios as they were configured on. This is although the optimization has happened
on a set of benchmark instances distinct from the test instances. It can therefore be said that we are observing an effect of a
latent property of the benchmark scenarios, such as their dimensionality or their landscapes. Comparing the absolute outcome
values and their variance, however, the advantage is relatively modest.

RQ2 – Optimized algorithm compared to other algorithms: The performance curves for the mean normalized regret is
shown in Figure 2, and the final performance values at 30·d full fidelity evaluations is shown in Figure 4. A critical difference
plot and test can be seen in Figure 3b. The behavior of RS, HB, BOHB and SMAC is not surprising: Initially RS and SMAC
perform the same, because SMAC evaluates an initial random design, after which its performance improves fast. HB and
BOHB initially perform the same, and perform better than RS or SMAC because of their multi-fidelity evaluations. After
a while, BOHB starts to outperform HB because of its surrogate-based sampling. Therefore, BOHB performs well for most
budgets, often being the best optimizer for a budget of 1 full evaluation and for 100 full evaluations. Given its multifidelity
characteristics, Hyperband (HB) is a good choice for low budgets, while SMAC is well suited for larger optimization budgets.

Our algorithm is very competitive on both lcbench and rbv2_super, but gets outperformed by SMAC on nb301. We assume
this is because the search space of nb301 is a purely categorical search space with many components, for which SMACs
random forest based surrogate model appears to give it an advantage.

6https://github.com/mlr-org/miesmuschel/tree/smashy_ex,

https://github.com/compstat-lmu/paper_2021_benchmarking_special_issue

13

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

Figure 1: Beeswarm plot of best configurations according to surrogate model over meta-optimization archive of γ∗. Shown are
the top 80 configuration points, according to surrogate model predicted performance, that were evaluated during optimization.
Discrete parameters have their levels shown. Most numeric parameters are on a log-scale (left axis), except for ρ(0), ρ(1),
which are on a linear scale (right axis). Instead of showing both N 0
s (t), their geometric mean, named Ns(t), is
shown. The highlighted large points are γ∗[HB] and γ∗[EQUAL] that were found on both benchmark scenarios.

s (t) and N 0

Although our algorithm was only optimized for its performance at 30 · d evaluations, it still has strong performance after
fewer evaluations, as seen in Figure 3b.

RQ3 – Advantage of SH fidelity scheduling: In both scenarios, the batch method HB is ultimately selected for the optimum
γ∗, although Figures 3b and 3b show that the difference to batch size equal is not statistically significant at α=1%. We
observe that the equal fidelity scheduling mode has several advantages: For one, it is much simpler than HB, as it does not
need to keep track of SH-brackets, and does not need to adapt µ(b) to make the budget expense at each bracket approximately
equal. As another benefit, it allows for easy parallel scheduling of evaluations. This is because it always schedules the same
number of function evaluations at a time, which can therefore be done synchronously.

RQ4 – Effect of using multi-fidelity methods: Our results support the superiority of multi-fidelity HPO methods as
compared to HPO methods that do not make use of lower fidelity approximations. Fig. 3a suggests that multi-fidelity methods
are significantly better than their non multi-fidelity counterparts if optimization is stopped at an intermediate overall budget
corresponding to 100 full model evaluations. To be more precise, it can be seen that BOHB as well as both optimized variants
γ∗[equal] and γ∗[HB] (optimized for the respective scenario, respectively) significantly outperform SMAC under this strict
budget constraint. In line with [15], HB significantly outperforms random search under this budget. On the other hand, Fig. 3b
provides evidence that multifidelity methods can achieve final performance on the same level as the state-of-the-art method
that is not making use of low fidelity approximations (SMAC). It can be concluded that the effect of a properly designed
multifidelity mechanism provides significant improvements on anytime performance, while not lowering, but potentially also
not raising final performance significantly. In our opinion, the gain in anytime performance justifies the additional algorithmic
complexity that is introduced by using multi-fidelity methods.

RQ5a, RQ5b – Changing configuration parameters during optimization, and within SAMPLE evaluations: The
observations made for γ2 (forbidding change over time) and γ3 (forbidding change over time and within each batch) are
slightly contradictory. In particular, the nb301 performance of γlcbench
[HB] is a visible outlier with regard to optimization
performance. There is no obvious explanation available from inspecting the configuration parameters of γlcbench
[HB], but
it is possible that it is an accidental “good fit” of configuration parameters to the specific landscape of nb301.

2

2

On lcbench and rbv2_super, the impact of restricting the search space is less and within the possible bounds of random error.
We note, however, that both changing configuration parameters over time and within each batch sample introduces significant
complexity to the algorithm, therefore making the restricted optimization results obvious favorites over γ∗.

RQ6 – Effect of surrogate models: Surprisingly, the simple k-nearest-neighbors algorithm seems to be chosen systematically
by the algorithm configuration for both lcbench and rbv2_super (see Fig. 1), either with a value of k =1 or k =7. Our ablation
experiments suggest that the performance of the optimizer is on average best when using this surrogate learner, even though the

14

11000TPEKNN1KKNN7RF01KDEuniformProgressiveTournament0111000HBequal11000110001100010010110210301/32/31batch_methodμ(1)ηﬁdηsurvﬁlter_methodIfsurrNs(0)Ns(1)ρ(0)ρ(1)Pλ(A)ParameterValueScenarioLCbenchrbv2_super      γ* batch_methodHBequalBest Conﬁguration ParametersAutomated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

differences do not seem to be significant. Under the aspect of algorithmic complexity, it seems that the KNN1 is a reasonable
alternative to more complex surrogate learners like the TPE-based method proposed by the original BOHB algorithm.

RQ7 – Performance with parallel resources: The results shown in Fig. 2 show that our algorithm is competitive with GPBO,
a state-of-the-art synchronously parallel optimization algorithm, when evaluated with 32x parallel resources. This result
also shows the main advantage that the equal fidelity schedule has over scheduling like HB: synchronously parallelizing
HB or BOHB puts them at a great disadvantage over even RS; for these it is necessary to use asynchronously parallelized
methods [16, 17] or use an archive shared between multiple workers [18] to get competitive results. However, synchronous
objective evaluations are much easier to implement in many environments than asynchronous communication between
workers, making the advantage of the simplicity of the equal schedule even more pronounced.

8 Conclusion

We presented a principled approach to benchmark-driven algorithm design by applying it to generic multi-fidelity HPO. For
this, we formalized the search space for our rich and configurable optimization framework. Given the search space, we applied
Bayesian optimization for meta-optimization of our framework on two different problem scenarios within the field of AutoML.
The configured algorithms are evaluated and compared to BOHB, Hyperband, SMAC, and a simple random search as reference.
We performed an extensive analysis to evaluate the effect of algorithmic components on performance against the additional al-
gorithmic complexity they introduce, observing that we achieve decent performance compared to widely used HPO-algorithms.

We conclude that the additional algorithmic complexity introduced by handling of multi-fidelity evaluations provides significant
benefits. However, based on our experiments we argue that design choices made by established multifidelity optimizers like
BOHB can be replaced by simpler choices: For example, the (more complex) successive halving schedule is not significantly
better than a schedule using equal batch sizes. Switching to equal batch sizes can simplify parallelization drastically by offering
the opportunity of efficient synchronous parallelization while staying competitive with other parallelized optimization methods.

An aspect of BOHB that was consistently chosen by the optimizer to be included in the search space is the kernel-density
estimator-based sampling of new proposed points, whether filtered by a surrogate model or not. This detail, which is not usually
presented as the main feature of BOHB, seems to have an unexpectedly large impact. On the other hand our optimization
results suggest a surprisingly simple surrogate learner (knn, k =1), even outperforming the choices made in BOHB.

Some components of our search space with potential algorithmic complexity have not shown much benefit. Optimization
on rbv2_super did choose time-varying random interleaving, and overall there was a slight favoring of more agressive filtering
late during an optimization run (Ns(1) > Ns(0)), but the results did not consistently outperform a configuration obtained
from a restricted optimization that excluded time-varying configuration parameters.

Our analysis of the set of best observed performances during optimization indicates there is a large agreement between
benchmark scenarios about what the optimal γ∗ configuration should be, the most notable distinction being parameters that
control (model-based) sampling and the surrogate model. This suggests there may be a set of configuration parameters that
are either generally good in many machine machine learning problems, or do have little influence on performance and can
therefore be set to the simplest value. There seems to be a remaining set of configuration parameters that should be adapted
to the properties of the optimization problem. The meta-optimization framework presented in this work could be used in
future work to investigate the relationship between features of optimization problems and related optimal configuration. The
knowledge gained could ultimately used for exploratory landscape analysis to set configuration parameters automatically [76].

Other future work indicated because of some of the drawbacks of the benchmarks used: We were not able to optimize,
or compare to, methods that use asynchronous proposals of evaluations. This is a major drawback, because asynchronous
methods are important in the current age where parallel resources are plentiful, but current widely-used surrogate-based
benchmarks do not allow for the easy evaluation of asynchronous optimization performance. Suggested methods, such
as waiting with a sleep-timer for an appropriate amount [18], are impractical for meta-optimization. A method based on
synchronizing objective evaluation return times by the benchmark suite has been suggested [59] and, when available, would
make optimization of asynchronous algorithms possible.

Acknowledgment

The authors of this work take full responsibilities for its content. This work was supported by the German Federal Ministry
of Education and Research (BMBF) under Grant No. 01IS18036A. LK is supported by NSF grant 1813537.

15

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

Figure 2: Optimization progress (mean normalized regret) of serial evaluation on each benchmark scenario (“test” instances
only), as well as 32x parallel evaluation on lcbench. “γ∗(lcbench bm equal)” is the configuration obtained from optimizing on
lcbench with batch_method equal etc. Shown is the mean over 30 evaluations, averaged over all available test benchmark
instances for each of the three scenarios, the uncertainty bands show standard error over test instances. Note log scale on
x axis. Regret is calculated as the difference from the so far best evaluation performance to the overall smallest value found on
each benchmark instance throughout all experiments, and then normalized so that the value 1 coincides with the median of the
performance of all randomly sampled full fidelity evaluations. Performance is calculated as the best performance observed by
the optimizer and therefore depends on evaluation fidelity; this is the reason for the initially “slow” convergence of algorithms
that make their first full-fidelity evaluation late. Note on the parallel plot that µ of γ∗[equal] was set to 32 and HB and BOHB
were only naively parallelized to simulate a synchronous “single optimizer, multiple workers” environment.

16

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

(a) Intermediate optimization budget of 100 full evaluations

(b) Full evaluation budget (final performance)

Figure 3: Critical difference plot [75] to compare the performance of multiple algorithms across all instances and scenarios.
Lower ranks are better. Horizontal bold bars indicate that there is no significant difference between algorithms (α = 1%).
GPBO, which was not evaluated on all scenarios, is not included.

Figure 4: Mean normalized regret of final performance on “test” benchmark instances for the configuration shown in Table 3.
Shown is the mean over 30 evaluations, averaged over all available test benchmark instances for each of the three scenarios,
the uncertainty bands show standard error over instance means. Regret is calculated as the difference from the so far best
evaluation performance to the overall smallest value found on each benchmark instance throughout all experiments, and then
normalized so that the value 1 coincides with the median of the performance of all randomly sampled full fidelity evaluations.

17

23456CDgamma*[HB]BOHBgamma*[EQUAL]HBSMACRS23456CDgamma*[EQUAL]SMACgamma*[HB]BOHBHBRSAutomated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

References

[1] B. Bischl, M. Binder, M. Lang, T. Pielok, J. Richter, S. Coors, J. Thomas, T. Ullmann, M. Becker, A. Boulesteix,
D. Deng, and M. Lindauer, “Hyperparameter optimization: Foundations, algorithms, best practices and open challenges,”
CoRR, vol. abs/2107.05847, 2021.

[2] L. Kotthoff, C. Thornton, H. H. Hoos, F. Hutter, and K. Leyton-Brown, “Auto-weka: Automatic model selection and
hyperparameter optimization in weka,” in Automated Machine Learning: Methods, Systems, Challenges, F. Hutter,
L. Kotthoff, and J. Vanschoren, Eds. Cham: Springer International Publishing, 2019, pp. 81–95.

[3] D. H. Wolpert and W. G. Macready, “No free lunch theorems for optimization,” IEEE Transactions on Evolutionary

Computation, vol. 1, no. 1, pp. 67–82, 1997.

[4] D. R. Jones, M. Schonlau, and W. J. Welch, “Efficient global optimization of expensive black-box functions,” J. Glob.

Optim., vol. 13, no. 4, pp. 455–492, 1998.

[5] F. Hutter, H. H. Hoos, and K. Leyton-Brown, “Sequential model-based optimization for general algorithm configuration,”
in Learning and Intelligent Optimization, C. A. C. Coello, Ed. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011,
pp. 507–523.

[6] K. Swersky, J. Snoek, and R. P. Adams, “Freeze-thaw bayesian optimization,” CoRR, vol. abs/1406.3896, 2014.
[7] J. Bergstra and Y. Bengio, “Random search for hyper-parameter optimization,” J. Mach. Learn. Res., vol. 13, pp.

281–305, 2012.

[8] J. Snoek, H. Larochelle, and R. P. Adams, “Practical bayesian optimization of machine learning algorithms,” in
Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2, ser. NIPS’12.
Red Hook, NY, USA: Curran Associates Inc., 2012, p. 2951–2959.

[9] R. Turner, D. Eriksson, M. McCourt, J. Kiili, E. Laaksonen, Z. Xu, and I. Guyon, “Bayesian optimization is superior to
random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020,” in
NeurIPS 2020 Competition and Demonstration Track, 6-12 December 2020, Virtual Event / Vancouver, BC, Canada, ser.
Proceedings of Machine Learning Research, H. J. Escalante and K. Hofmann, Eds., vol. 133. PMLR, 2020, pp. 3–26.
[10] F. Hutter, H. H. Hoos, and K. Leyton-Brown, “Parallel algorithm configuration,” in Learning and Intelligent Optimization,
ser. Lecture Notes in Computer Science, Y. Hamadi and M. Schoenauer, Eds. Berlin, Heidelberg: Springer Berlin
Heidelberg, 2012, no. 7219, pp. 55–70.

[11] B. Bischl, S. Wessing, N. Bauer, K. Friedrichs, and C. Weihs, “MOI-MBO: multiobjective infill for parallel model-based
optimization,” in Learning and Intelligent Optimization - 8th International Conference, Lion 8, Gainesville, FL, USA,
February 16-21, 2014. Revised Selected Papers, ser. Lecture Notes in Computer Science, P. M. Pardalos, M. G. C.
Resende, C. Vogiatzis, and J. L. Walteros, Eds., vol. 8426. Springer, 2014, pp. 173–186.

[12] J. González, Z. Dai, P. Hennig, and N. D. Lawrence, “Batch bayesian optimization via local penalization,” in Proceedings
of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain, May 9-11,
2016, ser. JMLR Workshop and Conference Proceedings, A. Gretton and C. C. Robert, Eds., vol. 51.
JMLR.org,
2016, pp. 648–657.

[13] C. Chevalier and D. Ginsbourger, “Fast computation of the multi-points expected improvement with applications in

batch selection,” in Learning and Intelligent Optimization. Springer Berlin Heidelberg, 2013, pp. 59–69.

[14] M. Balandat, B. Karrer, D. R. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy, “BoTorch: A
framework for efficient monte-carlo bayesian optimization,” in Advances in Neural Information Processing Systems,
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds., vol. 33, 2020. [Online]. Available:
https://proceedings.neurips.cc/paper/2020/hash/f5b1b89d98b7286673128a5fb112cb9a-Abstract.html

[15] L. Li, K. G. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar, “Hyperband: A novel bandit-based approach

to hyperparameter optimization,” J. Mach. Learn. Res., vol. 18, pp. 185:1–185:52, 2017.

[16] L. Li, K. G. Jamieson, A. Rostamizadeh, E. Gonina, J. Ben-tzur, M. Hardt, B. Recht, and A. Talwalkar, “A system
for massively parallel hyperparameter tuning,” in Proceedings of Machine Learning and Systems 2020, MLSys 2020,
Austin, TX, USA, March 2-4, 2020, I. S. Dhillon, D. S. Papailiopoulos, and V. Sze, Eds. mlsys.org, 2020.

[17] L. C. Tiao, A. Klein, C. Archambeau, and M. W. Seeger, “Model-based asynchronous hyperparameter optimization,”

CoRR, vol. abs/2003.10865, 2020.

[18] S. Falkner, A. Klein, and F. Hutter, “BOHB: robust and efficient hyperparameter optimization at scale,” in Proceedings
of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July
10-15, 2018, ser. Proceedings of Machine Learning Research, J. G. Dy and A. Krause, Eds., vol. 80. PMLR, 2018,
pp. 1436–1445.

18

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

[19] L. C. Tiao, A. Klein, C. Archambeau, and M. W. Seeger, “Model-based asynchronous hyperparameter optimization,”

CoRR, vol. abs/2003.10865, 2020.

[20] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, M. Young, J. Crespo, and D. Dennison,
“Hidden technical debt in machine learning systems,” in Advances in Neural Information Processing Systems 28: Annual
Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada,
C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, Eds., 2015, pp. 2503–2511.

[21] H. H. Hoos, “Programming by Optimization,” Communications of the Association for Computing Machinery (CACM),

vol. 55, no. 2, pp. 70–80, Feb. 2012.

[22] J. R. Rice, “The Algorithm Selection Problem,” Advances in Computers, vol. 15, pp. 65–118, 1976.

[23] B. Pfahringer, H. Bensusan, and C. G. Giraud-Carrier, “Meta-Learning by Landmarking Various Learning Algorithms,”
in Proceedings of the Seventeenth International Conference on Machine Learning, ser. ICML ’00. San Francisco,
CA, USA: Morgan Kaufmann Publishers Inc., 2000, pp. 743–750.

[24] Q. Sun and B. Pfahringer, “Pairwise meta-rules for better meta-learning-based algorithm ranking,” Machine Learning,

vol. 93, no. 1, pp. 141–161, 2013.

[25] C. Soares, P. B. Brazdil, and P. Kuba, “A Meta-Learning Method to Select the Kernel Width in Support Vector

Regression,” Mach. Learn., vol. 54, no. 3, pp. 195–209, Mar. 2004.

[26] C. Finn, P. Abbeel, and S. Levine, “Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,” in
Proceedings of the 34th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research,
D. Precup and Y. W. Teh, Eds., vol. 70. PMLR, Aug. 2017, pp. 1126–1135.

[27] J. Vanschoren, “Meta-learning: A survey,” 2018.

[28] F. Hutter, L. Kotthoff, and J. Vanschoren, Eds., Automated Machine Learning: Methods, Systems, Challenges, 1st ed.,

ser. The Springer Series on Challenges in Machine Learning. Springer, Cham, 2019.

[29] S. Minton, “Automatically Configuring Constraint Satisfaction Programs: A Case Study,” Constraints, vol. 1, pp. 7–43,

1996.

[30] S. J. Westfold and D. R. Smith, “Synthesis of Efficient Constraint Satisfaction Programs,” Knowl. Eng. Rev., vol. 16,

no. 1, pp. 69–84, 2001.

[31] D. Balasubramaniam, L. de Silva, C. A. Jefferson, L. Kotthoff, I. Miguel, and P. Nightingale, “Dominion: An
Architecture-driven Approach to Generating Efficient Constraint Solvers,” in 9th Working IEEE/IFIP Conference on
Software Architecture, Jun. 2011, pp. 228–231.

[32] J.-N. Monette, Y. Deville, and P. van Hentenryck, “Aeon: Synthesizing Scheduling Algorithms from High-Level Models,”

in Operations Research and Cyber-Infrastructure, 2009, pp. 43–59.

[33] R. Dorne and C. Voudouris, “HSF: A Generic Framework to Easily Design Meta-Heuristic Methods,” in 4th

Metaheuristics International Conference (MIC 2001).

John Wiley & Sons, 2001, pp. 423–428.

[34] J. Ansel, C. Chan, Y. L. Wong, M. Olszewski, Q. Zhao, A. Edelman, and S. Amarasinghe, “PetaBricks: A Language

and Compiler for Algorithmic Choice,” SIGPLAN Not., vol. 44, no. 6, pp. 38–49, Jun. 2009.

[35] A. R. KhudaBukhsh, L. Xu, H. H. Hoos, and K. Leyton-Brown, “SATenstein: automatically building local search SAT
solvers from components,” in Proceedings of the 21st International Joint Conference on Artifical Intelligence. San
Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 2009, pp. 517–524.

[36] M. López-Ibáñez and T. Stützle, “The automatic design of multi-objective ant colony optimization algorithms,” IEEE

Transactions on Evolutionary Computation, vol. 16, no. 6, pp. 861–875, 2012.

[37] M. López-Ibáñez, J. Dubois-Lacoste, L. P. Cáceres, M. Birattari, and T. Stützle, “The irace package: Iterated racing

for automatic algorithm configuration,” Operations Research Perspectives, vol. 3, pp. 43–58, 2016.

[38] J. Seipp, F. Pommerening, and M. Helmert, “New Optimization Functions for Potential Heuristics,” in Twenty-Fifth
International Conference on International Conference on Automated Planning and Scheduling, ser. ICAPS’15. AAAI
Press, 2015, pp. 193–201.

[39] G. Malkomes and R. Garnett, “Automating bayesian optimization with bayesian optimization,” Advances in Neural

Information Processing Systems, vol. 31, pp. 5984–5994, 2018.

[40] M. Lindauer, M. Feurer, K. Eggensperger, A. Biedenkapp, and F. Hutter, “Towards assessing the impact of bayesian

optimization’s own hyperparameters,” 2019.

19

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

[41] N. Dang, P. De Causmaecker, L. Cáceres, and T. Stützle, “Configuring irace using surrogate configuration benchmarks,”
in GECCO 2017 - Proceedings of the 2017 Genetic and Evolutionary Computation Conference, ser. GECCO 2017 -
Proceedings of the 2017 Genetic and Evolutionary Computation Conference. Association for Computing Machinery, Inc,
Jul. 2017, pp. 243–250, publisher Copyright: © 2017 ACM.; null ; Conference date: 15-07-2017 Through 19-07-2017.
[42] C. Fawcett and H. H. Hoos, “Analysing differences between algorithm configurations through ablation,” J. Heuristics,

vol. 22, no. 4, pp. 431–458, 2016.

[43] M. López-Ibáñez and T. Stützle, “An experimental analysis of design choices of multi-objective ant colony optimization

algorithms,” Swarm Intelligence, vol. 6, pp. 207–232, 2012.

[44] B. Bischl, O. Mersmann, H. Trautmann, and C. Weihs, “Resampling methods for meta-model validation with
recommendations for evolutionary computation,” Evolutionary Computation, vol. 20, no. 2, pp. 249–275, 2012.
[45] A. Klein, L. C. Tiao, T. Lienart, C. Archambeau, and M. Seeger, “Model-based asynchronous hyperparameter and

neural architecture search,” arXiv preprint arXiv:2003.10865, 2020.

[46] A. Klein, S. Falkner, S. Bartels, P. Hennig, and F. Hutter, “Fast bayesian optimization of machine learning

hyperparameters on large datasets,” in Artificial Intelligence and Statistics. PMLR, 2017, pp. 528–536.

[47] K. G. Jamieson and A. Talwalkar, “Non-stochastic best arm identification and hyperparameter optimization,” in
Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, AISTATS 2016, Cadiz, Spain,
May 9-11, 2016, ser. JMLR Workshop and Conference Proceedings, A. Gretton and C. C. Robert, Eds., vol. 51.
JMLR.org, 2016, pp. 240–248.

[48] J. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl, “Algorithms for hyper-parameter optimization,” Advances in neural

information processing systems, vol. 24, 2011.

[49] D. Golovin, B. Solnik, S. Moitra, G. Kochanski, J. Karro, and D. Sculley, “Google vizier: A service for black-box
optimization,” in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data
mining, 2017, pp. 1487–1495.

[50] M. Birattari and J. Kacprzyk, Tuning metaheuristics: a machine learning perspective. Springer, 2009, vol. 197.
[51] M. Birattari, Z. Yuan, P. Balaprakash, and T. Stützle, “F-race and iterated f-race: An overview,” Experimental methods

for the analysis of optimization algorithms, pp. 311–336, 2010.

[52] F. Hutter, H. H. Hoos, K. Leyton-Brown, and T. Stützle, “Paramils: an automatic algorithm configuration framework,”

Journal of Artificial Intelligence Research, vol. 36, pp. 267–306, 2009.

[53] C. Ansótegui, M. Sellmann, and K. Tierney, “A gender-based genetic algorithm for the automatic configuration of
Springer, 2009,

algorithms,” in International Conference on Principles and Practice of Constraint Programming.
pp. 142–157.

[54] O. Maron and A. W. Moore, “The racing algorithm: Model selection for lazy learners,” in Lazy learning. Springer,

1997, pp. 193–225.

[55] A. Saltelli, “Sensitivity analysis for importance assessment,” Risk Analysis, vol. 22, no. 3, pp. 579–590, 2002.
[56] F. Hutter, H. H. Hoos, and K. Leyton-Brown, “An efficient approach for assessing hyperparameter importance,” in
Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014,
ser. JMLR Workshop and Conference Proceedings, vol. 32.

JMLR.org, 2014, pp. 754–762.

[57] K. Cheng, Z. Lu, C. Ling, and S. Zhou, “Surrogate-assisted global sensitivity analysis: an overview,” Structural and

Multidisciplinary Optimization, vol. 61, no. 3, pp. 1187–1213, 2020.

[58] S. Sheikholeslami, M. Meister, T. Wang, A. H. Payberah, V. Vlassov, and J. Dowling, “Autoablation: Automated
parallel ablation studies for deep learning,” in EuroMLSys@EuroSys 2021, Proceedings of the 1st Workshop on Machine
Learning and Systemsg Virtual Event, Edinburgh, Scotland, UK, 26 April, 2021, E. Yoneki and P. Patras, Eds. ACM,
2021, pp. 55–61.

[59] F. Pfisterer, L. Schneider, J. Moosbauer, M. Binder, and B. Bischl, “YAHPO Gym – Design Criteria and a new

Multifidelity Benchmark for Hyperparameter Optimization,” arXiv:2109.03670 [cs, stat], 2021, arXiv: 2109.03670.

[60] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye, G. Anderson, G. Corrado, W. Chai, M. Ispir
et al., “Wide & deep learning for recommender systems,” in Proceedings of the 1st workshop on deep learning for
recommender systems, 2016, pp. 7–10.

[61] L. Zimmer, M. Lindauer, and F. Hutter, “Auto-pytorch tabular: Multi-fidelity metalearning for efficient and robust
autodl,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 43, no. 9, pp. 3079 – 3090, 2021.
[62] J. Siems, L. Zimmer, A. Zela, J. Lukasik, M. Keuper, and F. Hutter, “Nas-bench-301 and the case for surrogate

benchmarks for neural architecture search,” 2020.

20

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

[63] J. Vanschoren, J. N. van Rijn, B. Bischl, and L. Torgo, “Openml: networked science in machine learning,” SIGKDD

Explor., vol. 15, no. 2, pp. 49–60, 2013.

[64] Y. A. Malkov and D. A. Yashunin, “Efficient and robust approximate nearest neighbor search using hierarchical navigable
small world graphs,” IEEE transactions on pattern analysis and machine intelligence, vol. 42, no. 4, pp. 824–836, 2018.
[65] J. Friedman, T. Hastie, and R. Tibshirani, “Regularization paths for generalized linear models via coordinate descent,”

Journal of Statistical Software, vol. 33, no. 1, pp. 1–22, 2010.

[66] M. N. Wright and A. Ziegler, “ranger: A fast implementation of random forests for high dimensional data in C++ and

R,” Journal of Statistical Software, vol. 77, no. 1, pp. 1–17, 2017.

[67] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, Classification And Regression Trees. Routledge, Oct. 2017.
[68] B. E. Boser, I. Guyon, and V. Vapnik, “A training algorithm for optimal margin classifiers,” in Proceedings of the Fifth
Annual ACM Conference on Computational Learning Theory, COLT 1992, Pittsburgh, PA, USA, July 27-29, 1992,
D. Haussler, Ed. ACM, 1992, pp. 144–152.

[69] T. Chen and C. Guestrin, “XGBoost: A scalable tree boosting system,” in Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, ser. KDD ’16. New York, NY, USA: ACM,
2016, pp. 785–794.

[70] M. Binder, F. Pfisterer, and B. Bischl, “Collecting empirical data about hyperparameters for data driven automl,” in

Proceedings of the 7th ICML Workshop on Automated Machine Learning (AutoML 2020), 2020.

[71] H. Liu, K. Simonyan, and Y. Yang, “DARTS: Differentiable Architecture Search,” in Proceedings of the International

Conference on Learning Representations, 2019.

[72] A. Krizhevsky, “Learning multiple layers of features from tiny images,” University of Toronto, Tech. Rep., 2009.
[73] L. Breiman, “Random forests,” Mach. Learn., vol. 45, no. 1, pp. 5–32, 2001.
[74] R. J. Samworth, “Optimal weighted nearest neighbour classifiers,” The Annals of Statistics, vol. 40, no. 5, Oct. 2012.
[75] J. Demsar, “Statistical comparisons of classifiers over multiple data sets,” J. Mach. Learn. Res., vol. 7, pp. 1–30, 2006.
[76] O. Mersmann, B. Bischl, H. Trautmann, M. Preuss, C. Weihs, and G. Rudolph, “Exploratory landscape analysis,” in

Proceedings of the 13th annual conference on Genetic and evolutionary computation, 2011, pp. 829–836.

21

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

A Sample Algorithms

The pseudocode of both SAMPLE algorithms is presented here for clarity.

Algorithm 3 SAMPLETOURNAMENT algorithm

Input: Archive A, number of points to generate µ, current fidelity r
Configuration Parameters: Surrogate learner Ifsur, generating distribution Pλ(A), random interleave fraction ρ, sample

filtering rates (N 0

s ,N 1

s ), points to sample per tournament round ntrn.

State Variables: Batch of proposed configurations C ←∅

1: Use ρ to decide how many points nrandom_interleave to sample without filter
2: C ← Sample nrandom_interleave configurations from Pλ(A)
3: µ←µ−nrandom_interleave
4: n←(cid:100)µ/ntrn(cid:101)
5: fsur ←Ifsur(A)
6: for i←1 to n do
(cid:106)
(N 0
s )
7:

i−1
n−1

n−i

(cid:109)

n−1 ·(N 1
nsample ←
s )
C0 ← Sample nsample configurations from Pλ(A)
Predict performances of points in C0 using fsur
C ←C ∪SELECT_TOP(C0,min(ntrn,µ−|C|))

8:
9:
10:
11: end for
12: return C

(cid:46) Numter of tournament rounds
(cid:46) Surrogate model

Algorithm 4 SAMPLEPROGRESSIVE algorithm

Input: Surrogate learner Ifsur, Archive A, number of points to generate µ, current fidelity r, random interleave fraction

ρ, sample filtering rates (N 0

s ,N 1

s ), generating distribution Pλ(A)

State Variables: Batch of proposed configurations C ←∅, (ordered) pool of sampled points to select from P

1: Use ρ to decide how many points nrandom_interleave to sample without filter
2: C ← Sample nrandom_interleave configurations from Pλ(A)
3: µ←µ−nrandom_interleave
4: npool ←µ·max(N 0
s ,N 1
s )
5: P ← Sample npool configurations from Pλ(A)
6: fsur ←Ifsur(A)
7: Predict performances of points in P using fsur
8: for i←1 to µ do
(cid:106)
(N 0
s )

µ−1 ·(N 1
s )

µ−1
µ−1

9:

µ−i

(cid:109)

noptions ←
Poptions ← first noptions elements of P
S ← SELECT_TOP(Poptions,1)
C ←C ∪S
P ←P −S

10:
11:
12:
13:
14: end for
15: return C

(cid:46) Surrogate model

B Meta-Optimization Search Space

The full optimization space used for optimization of γ∗ is presented here in Table 4; Other γ results have the restrictions
applied to them as shown in Tbl. 3 in Section 6.

C All γ-Values

A table of all optimization γ-values is given in Tbl. 5.

22

Automated Benchmark-Driven Design and Explanation of Hyperparameter Optimizers

A PREPRINT

Table 4: Meta-optimization search space used to configure Alg. 2. Some configuration parameters are optimized on a
non-linear scale, meaning e.g. the optimizer optimizes a value of logµ(1) ranging from log2 to log200.

Parameter

Meaning

Range

µ(1)
batch_method
ηfid
ηsurv
filter_method
Pλ(A)
Ifsurr
ntrn(0)
ntrn(1)
N 0
s (0)
N 0
s (1)
N 1
s (0)
N 1
s (1)
ρ(0)
ρ(1)
filter_mb
ρrandom

(first bracket) batch size
batch method
fidelity rate
survival rate
SAMPLE method
SAMPLE generating distribution
surrogate learner
filter sample per tournament round at t=0
filter sample per tournament round at t=1
filtering rate of first point in batch at t=0
filtering rate of first point in batch at t=1
filtering rate of last point in batch at t=0
filtering rate of last point in batch at t=1
random interleave fraction at t=0
random interleave fraction at t=1
surrogate prediction always with maximum r
random interleave the same number in every batch

{2,...,200}
{equal, HB}
[21/4,24]
[1,∞)
{SAMPLETOURNAMENT, SAMPLEPROGRESSIVE}
{uniform, KDE}
{KNN1, KKNN7, TPE, RF}
{1, . . . , 10}
{1, . . . , 10}
[1,1000]
[1,1000]
[1,1000]
[1,1000]
[0,1]
[0,1]
{TRUE, FALSE}
{TRUE, FALSE}

Scale

logµ(1)

loglogηfid
1/ηsurv

logntrn(0)
logntrn(1)
logN 0
s (1)
logN 0
s (1)
logN 1
s (1)
logN 1
s (1)

Table 5: Optimized configuration parameters, under some constraints. Top: restricted to batch_method HB, bottom: equal.
γ2, γ3 are further restricted as described in Table 3 in Section 6. Shown is the overall result. Square brackets show range
(for numeric parameters) or list (for discrete parameters) of values found in individual optimization runs when not aggregated,
as a rough indicator of uncertainty. “(!)” indicates the parameter was forced to the value by a restriction.

Parameter
Scenario

γ∗
lcbench

γ∗
rbv2_super

γ2
lcbench

γ2
rbv2_super

γ3
lcbench

γ3
rbv2_super

Optimized with batch_method HB:

µ(1)
ηfid
ηsurv
filter_method
Pλ(A)
Ifsurr
ntrn(0)
ntrn(1)
N 0
s (0)
N 0
s (1)
N 1
s (0)
N 1
s (1)
ρ(0)
ρ(1)
filter_mb
ρrandom

5 [5, 23]
3.11 [1.25, 3.11]
2.22 [2.22, 6.1]
PROG [TRN]
KDE
KKNN7 [KNN1]
2 [2, 8]
1 [1, 5]
101 [9.19, 124]
312 [56.3, 817]
28.9 [4.84, 144]
8 [8, 654]
0.21 [0.12, 0.85]
0.08 [0.08, 0.55]
TRUE [FALSE]
FALSE [TRUE]

2 [2, 52]
1.97 [1.97, 6.73]
6.09 [1.65, 6.09]
PROG [TRN]
uniform [KDE]
KNN1
2 [1, 5]
5 [1, 5]
226 [2.03, 226]
495 [57.1, 533]
256 [7.19, 256]
99.7 [46.4, 890]
0.86 [0.68, 0.86]
0.06 [0.01, 0.25]
FALSE [TRUE]
TRUE [FALSE]

Optimized with batch_method equal:

µ(1)
ηfid
ηsurv
filter_method
Pλ(A)
Ifsurr
ntrn(0)
ntrn(1)
N 0
s (0)
N 0
s (1)
N 1
s (0)
N 1
s (1)
ρ(0)
ρ(1)
filter_mb
ρrandom

3 [2, 4]
2.71 [1.77, 12.2]
2.5 [1.23, 3.36]
TRN [PROG]
KDE
KKNN7 [KNN1]
1 [1, 4]
2 [1, 2]
21.5 [1.63, 309]
941 [58.2, 991]
35.4 [7.8, 280]
264 [5, 474]
0.32 [0.09, 0.68]
0.16 [0.06, 0.29]
TRUE
TRUE [FALSE]

15 [11, 15]
1.25 [1.22, 1.43]
18.8 [8.74, 18.8]
PROG
KDE [uniform]
KNN1
2 [1, 3]
9 [1, 9]
39.5 [2.42, 39.5]
18.1 [11.5, 408]
6.65 [5.43, 391]
925 [25.4, 925]
0.83 [0.49, 0.83]
0.03 [0.03, 0.5]
TRUE
FALSE

126 [10, 126]
2.19 [1.68, 10.2]
3.42 [2.58, 9.19]
PROG [TRN]
KDE
KKNN7 [KNN1]

8 [8, 114]
4.4 [2.04, 4.4]
3.26 [3.26, 5]
PROG [TRN]
uniform [KDE]
KNN1

5 [3, 68]
14.6 [1.45, 14.6]
1.15 [1.15, 3.07]
TRN (!)
KDE
KKNN7 [KNN1]

2 [2, 52]
5.19 [2.24, 5.19]
1.20 [1.03, 1.62]
TRN (!)
uniform
KNN1

5 [1, 8]

5 [1, 6]

1 (!)

1 (!)

39.6 [10.5, 76.7]

125 [125, 163]

31.4 [18.2, 74.6]

481 [480, 563]

570 [73, 570]

155 [155, 561]

0.37 [0.2, 0.49]

0.71 [0.49, 0.71]

0.38 [0.12, 0.54]

0.34 [0.34, 0.45]

TRUE
FALSE [TRUE]

TRUE
TRUE [FALSE]

TRUE [FALSE]
TRUE

FALSE
TRUE [FALSE]

5 [2, 7]
2.63 [2.27, 6.01]
1.87 [1.84, 5.49]
TRN [PROG]
KDE
KNN1 [KNN7]

5 [2, 5]
8 [1.28, 8]
3.45 [3.45, 5.53]
PROG [TRN]
uniform [KDE]
KNN1

2 [2, 6]
2.59 [1.46, 2.59]
3.53 [1.29, 4.86]
TRN (!)
KDE
KNN1 [KNN7]

85 [3, 93]
2.3 [1.36, 12.7]
6.5 [5.34, 11.3]
TRN (!)
uniform [KDE]
KNN1

5 [1, 5]

3 [1, 3]

1 (!)

1 (!)

169 [43.4, 191]

212 [49.5, 212]

4.76 [2.34, 273]

1.71 [1.71, 4.21]

81.3 [24.8, 111]

583 [295, 777]

0.34 [0.09, 0.37]

0.34 [0.34, 0.53]

0.27 [0.03, 0.27]

0.96 [0.38, 0.96]

TRUE
TRUE [FALSE]

TRUE
FALSE [TRUE]

TRUE [FALSE]
TRUE

TRUE
TRUE [FALSE]

23

