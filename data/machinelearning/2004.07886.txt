Maximizing Determinants under Matroid Constraints

Vivek Madan∗

Aleksandar Nikolov†

Mohit Singh‡

Uthaipon Tantipongpipat§

April 20, 2020

R

M

= ([n],

∈
such that det

Given a set of vectors v1, . . . , vn

Abstract
d and a matroid
), we study the problem
i∈S viv⊤
is maximized. This problem appears in a
of ﬁnding a basis S of
i
diverse set of areas, such as experimental design, fair allocation of goods, network design, and
machine learning. The current best results include an e2k-estimation for any matroid of rank
k [AGV18] and a (1 + ǫ)d-approximation for a uniform matroid of rank k
ǫ [MSTX19],
where the rank k
d denotes the desired size of the optimal set. Our main result is a new
approximation algorithm for the general problem with an approximation guarantee that depends
only on the dimension d of the vectors, and not on the size k of the output set. In particular, we
show an (O(d))
-approximation for any matroid, giving a signiﬁcant
improvement over prior work when k

-estimation and an (O(d))
d.

d + d

(cid:0)P

M

≥

≥

d3

I

(cid:1)

d

Our result relies on showing that there exists an optimal solution to a convex program-
ming relaxation for the problem which has sparse support ; in particular, no more than O(d2)
variables of the solution have fractional values. The sparsity results rely on the interplay be-
tween the ﬁrst order optimality conditions for the convex program and matroid theory. We
believe that the techniques introduced to show sparsity of optimal solutions to convex programs
will be of independent interest. We also give a randomized rounding algorithm that, given
a sparse fractional solution to the convex program, returns a feasible integral solution to the
original problem. To show the approximation guarantee, we utilize recent works on strongly
log-concave polynomials [AGV18, ALGV19] and show new relationships between diﬀerent con-
vex programs [NS16, AG17] studied for the problem. We remark that sparsity is crucial to the
algorithm and that all previous approaches will necessarily fail to achieve such an improved
guarantee. Finally, we show how to use the estimation algorithm to give an eﬃcient deter-
ministic approximation algorithm. Once again, the algorithm crucially relies on sparsity of the
fractional solution to guarantee that the approximation factor depends solely on the dimension
d.

≫

0
2
0
2

r
p
A
6
1

]
S
D
.
s
c
[

1
v
6
8
8
7
0
.
4
0
0
2
:
v
i
X
r
a

∗Amazon.

This work was done while the author was at Georgia Institute of Technology.

Email:

vmadan7@gatech.edu

†University of Toronto. Email: anikolov@cs.toronto.edu. Supported by NSERC Discovery Grant.
‡Georgia Institute of Technology. Email: mohit.singh@isye.gatech.edu. Supported by NSF- AF:1910423 and

NSF-AF:1717947.

§Georgia Institute of Technology. Email: tao@gatech.edu. Supported by NSF- AF:1910423 and NSF-AF:1717947.

 
 
 
 
 
 
1

Introduction

Choosing a diverse representative set of items from a large corpus is a common problem studied
in a variety of areas, including machine learning, information retrieval, statistics, and optimiza-
tion [KT12, CK06, CEZ17, Puk06]. For example, consider the problem of choosing a subset from
a large data set to train a machine learning algorithm; or of displaying a small set of images out of
a large set of relevant images to a search query. In these contexts, one aims to choose a small and
diverse representative set of items from a large data set. Diversity here can be modeled in many
diﬀerent ways, and the choice of a diversity measure can signiﬁcantly aﬀect both practical perfor-
mance and the algorithmic complexity of ﬁnding a diverse set. Both general and application-speciﬁc
diversity criteria have been proposed in the past [GCGS14, CKS+18, CEZ17, ZCL15, CG98].

In this work, we focus on a popular geometric model of the problem above. While it naturally
captures problems in data retrieval and statistics, we show that it also encompasses problems in
fair allocation of goods, network design, counting, and optimization. We assume that data are
represented as points in the d-dimensional Euclidean space, so that choosing a subset of items
corresponds to selecting a subset of d-dimensional vectors. A number of natural diversity measures
can be formulated in terms of functions of the eigenvalues of the matrix given by the sum of
outerproducts of the selected vectors. Some examples are the determinant, the trace, the harmonic
mean of the eigenvalues, and the minimum eigenvalue. In this work, we focus on the determinant as
the diversity measure. We study the determinant maximization problem with general combinatorial
constraints which makes the model rich enough to include many of the problems mentioned above.
In particular, we consider matroid constraints, which capture cardinality constraints, partition
constraints, and many more as special cases. This allows modeling constraints imposed by, e.g.,
budget, feasibility, or fairness considerations.

In an instance of the Determinant Maximization problem (under a general matroid con-
) with set of

Rd and a matroid

straint), we are given a set of n vectors v1, . . . , vn ∈
bases

, and our goal is to ﬁnd a set S

that maximizes det

= ([n],
, i.e.

I

M
i∈S viv⊤
i

B

∈ B

max

det

(

viv⊤
i

: S

!

∈ B)

(cid:0)P
.

Xi∈S
, which is the size of all the bases in

(cid:1)

(1)

We denote by k the rank of the matroid
M
combinatorial optimization problem (1) by D-OPT and its optimum value by OPT.

B

. We denote the

A number of special cases of D-OPT have been studied, in which either the choice of vectors or
the matroid is restricted [Wel82, BGS10, WYS16, ALSW17a, SX18]. We highlight two illustrative
examples. Under cardinality constraints, in which
consists of all subsets of [n] of size k, the
problem is hard to approximate to a factor better than (1 + c)d for some c > 0 when k = d [Kou06,
C¸ M13, DSEFM15], and Nikolov [Nik15] gave an ed-approximation for k
d.1 Interestingly, when
k > d, improved guarantees are known [WYS16, ALSW17a, SX18] with the current best (1 + ǫ)d-
approximation when k

d + d

≤

B

ǫ [MSTX19].

≤

For general matroids, a series of works [NS16, AG17, SV17b, AGV18] have focused on the
d, and the latest results of Anari, Oveis-Gharan, and Vinzant [AGV18] imply an
case when k
e2k-estimation algorithm. These results were ﬁrst proved for the special case when the generating
polynomial for the matroid is a real stable polynomial [AG17]. Recent and exciting advances on
completely log-concave polynomials [AGV18] (and the equivalent notion of Lorentzian polynomi-
als [BH19]) allow the techniques of [AG17] to be generalized to all matroids. While these results

≥

1For k < d, the objective is naturally replaced by the product of the k highest eigenvalues of the matrix, rather

than the determinant, which is the product of all d eigenvalues

1

 
are not stated when k > d, the analysis naturally yields an e2k-estimation algorithm even in that
case. Such a dependence on k is often exorbitant since k can be much larger than d in many
applications. Moreover, the hardness result mentioned above only shows that the approximation
factor needs to depend exponentially on d, but not necessarily on k.2 A starting point for this
work is a result showing that these existing techniques are incapable of removing the dependence
on k for general matroid constraints. More formally, we show in Appendix F that any algorithm
which solves a convex relaxation and rounds the fractional solution without using the structure of
the vectors yields an approximation factor necessarily dependent on k even when d = 2.

1.1 Our Results and Contributions

Our main result is an algorithm that estimates the objective of the Determinant Maximization
problem under a general matroid constraint.

Theorem 1.1 There is an eﬃciently computable convex program whose objective value estimates
the objective of the Determinant Maximization problem under a general matroid constraint
within a multiplicative factor of (O(d))d.

As outlined earlier, an approximation factor depending only on d cannot be obtained by round-
ing an arbitrary optimal solution to any of the known convex relaxations of the problem. Our
work introduces two key ideas to bypass this bottleneck. First, we show that there always exists
an optimal sparse fractional solution to a particular convex programming relaxation. In particular,
we show that there always exists an optimal solution with no more than O(d2) fractional variables
out of a total of n variables. The proof of this fact relies crucially on the ﬁrst order optimality
conditions of the convex program. A straightforward presentation of the ﬁrst order optimality
conditions leads to a system of (exponentially many) non-linear constraints over an exponential
number of variables. We interpret these constraints using matroid theory and reformulate them as
a system of (exponentially many) linear inequalities. Then, we apply combinatorial optimization
techniques such as uncrossing in order to show that any basic feasible solution to the system of
inequalities must be sparse, again using the inherent matroid structure of the linear constraints.

Second, we give a new randomized algorithm that rounds such a sparse solution for any ma-
troid, giving the desired result. Our algorithm crucially uses the near-integral structure of optimal
solutions, and thus diﬀers signiﬁcantly from previous rounding algorithms, which are oblivious to
any such structure. The main challenge in the design of the algorithm is that the non-linearity
of the objective function implies that even an integral variable cannot be included in the solution
with probability 1. Our rounding proceeds in two phases: we ﬁrst randomly round the fractional
variables, and then we randomly choose which of the integral variables to include in a solution,
while maintaining feasibility. We again rely on matroid theory to show that the random solution
obtained has large objective value in expectation.

This combination of techniques from convex optimization and matroid theory, which we use in
order to ﬁnd a sparse optimal solution of a convex program with exponentially many constraints,
appears to be novel and may be of independent interest.

We also consider the special case of partition matroids due to its signiﬁcant applications and
note that an improved approximation algorithm can be obtained for this case. We observe that
the roadblock in achieving an approximation factor independent of k for general matroids does not
appear in the case of partition matroids. Thus, the standard randomized rounding algorithm also

2Since the objective is the determinant of d × d matrices, and the determinant is homogeneous of degree d,

exponential dependence on d is an appropriate scaling.

2

achieves eO(d)-approximation by generalizing the results on Nash Social Welfare in [AMGV18]. We
include the proof in Theorem G.1 in the Appendix for completeness.

Deterministic Algorithms. A challenge for the Determinant Maximization problem under
a general matroid constraint has been the lack of true approximation algorithms that achieve
the same guarantees as the estimation algorithms. Most results [NS16, AG17, AGV18, ALGV19,
SV17b] give randomized algorithms whose guarantees hold in expectation and are not known to
hold with high probability or deterministically. The few existing eﬃcient algorithms with high
probability or deterministic guarantees either work only for restricted classes of matroids, such
as uniform matroids [Nik15, ALSW17b, SX18] or partition matroids with a constant number of
parts [CDK+17], or rely on special structure of the input vectors (or both) [AGSS16, CDG+17,
CKS+18, BKV18]. Ebrahimi, Straszak and Vishnoi [ESV17] gave the most general algorithmic
results that apply to all regular matroids, but the approximation factors they achieved depend on
the size of the ground set and not just the dimension of vectors, as aimed in our work.

We utilize the existence of sparse optimal solutions to our convex programming relaxation to
give an eﬃcient deterministic algorithm achieving an approximation factor that only depends on
the dimension d of the vectors, and not on the size k of the output set or the size n of the input.

Theorem 1.2 There is a polynomial time deterministic algorithm for the Determinant Maxi-
mization problem that gives an (O(d))d3
-approximation.

The above result is achieved by using the optimal objective value of the convex program as
an estimate of the value of an optimal solution, and reducing the search problem of ﬁnding an
approximately optimal solution to estimation. We have shown that some optimal solution to the
convex program has at most O(d2) fractional variables, and, therefore, has support of size k+O(d2).
Then, producing a feasible solution (which has size k) requires ﬁnding O(d2) elements of the support
of the optimal solution to exclude from the solution: the remaining k elements form the output.
Thus, the sparsity allows us to argue that the estimation problem needs to be recursively solved
only O(d2) times, which is crucial in guaranteeing an approximation factor that depends only on
d.

We remark the guarantee is worse than is achieved (in expectation) by the randomized algo-
rithm. Obtaining true approximation algorithms that match the performance of the estimation
algorithms remains a challenging open problem for the Determinant Maximization problem
under a general matroid constraint, even in the case of a partition constraint.

1.2 Applications

As mentioned earlier, Determinant Maximization models problems in many diﬀerent areas and
our results imply new approximations for many of these problems. We give details for some of them
below.

∈

Experimental Design.
In the optimal experimental design problem for linear models, the goal is
Rd from a possible set of linear measurements of the form yi = v⊤
i θ⋆ + ηi.
to infer an unknown θ⋆
Rd are known vectors, and η1, . . . , ηn are independent Gaussian noises with
Here, v1, . . . , vn ∈
mean 0 and variance 1. In some settings, performing all of the n measurements might be infeasible,
and combinatorial constraints such as matroid constraints can be used to deﬁne the feasible sets of
θ for θ∗ is obtained via solving
measurements. Given a set S
[n] of measurements, an estimator
θ⋆ is distributed as a
the least squares regression problem minθ∈Rd

i θ)2. The error
v⊤

⊆

θ

i∈S(yi −

P

3

b

−

b

. Minimizing the volume of the conﬁdence ellipsoid,
d-dimensional Gaussian N
or equivalently the determinant of the covariance matrix of the error, is referred to as D-optimal
design in statistics [Puk06]. Our results directly imply improved approximability for D-optimal
design under a general matroid constraint.

(cid:0)P

0,

(cid:17)

(cid:16)

(cid:1)

i∈S viv⊤
i

−1

Nash Social Welfare.
In the indivisible goods allocation problem the goal is to allocate, i.e.
partition, m goods among d agents so that some notion of social welfare and/or fairness is achieved.
[m], and if Si are the goods assigned to agent i, then her
Each agent i has utility ui(j) for good j
ui(j). A well studied objective in this context is Nash social welfare (NSW),
utility is ui(Si) =

∈

j∈Si

1/d

B

P

(cid:16)Q

d
i=1 ui(Si)
(cid:17)

whose bases
p

consist of all sets S

which asks to maximize
. This objective interpolates between maximally eﬃcient
and maximally egalitarian allocations – see [Mou04, CKM+16] for more extensive background.
Maximizing the NSW can be formulated as an instance of Determinant Maximization under
a partition constraint, as observed in [AGSS16]. For each agent i and good j, we create a vector
ui(j)ei, where ei is the i-th standard basis vector of Rd, and form a partition matroid
v(i,j) =
[m].
[m] such that
M
corresponds to an allocation of the goods, and the determinant
Then, a feasible solution S
(i,j)∈S v(i,j)v⊤
det
is equal to the NSW objective. Our results recover those in [AGSS16] and
further allow us to give an O(d)-estimation algorithm when the allocation (S1, . . . , Sd) is required to
satisfy additional matroid constraints. For example, the works [GMT13, GMT14, GM19] considered
′. We can model this setting by deﬁning our
allocations such that
constraint matroid
= 1 for
|{
′. Our results then imply an O(d)-estimation
j :
all j
[m] and
{
algorithm and an O(d)d2
-approximation algorithm for maximizing NSW subject to these general
matroid constraints.

d
i=1 Si is a basis of a matroid
[d]
so that S
M
S
i s.t. (i, j)
∃

×
is a basis of

[m] is a basis of

if and only if

= 1 for all j

⊆
S
∈

i : (i, j)

i : (i, j)

(cid:16)P

∈ B

M

M

M

[d]

(i,j)

⊆

×

|{

}|

}|

∈

∈

∈

∈

S

S

(cid:17)

}

Network Design Problems.
In general, the goal in network design problems is to pick a subset
F of the edges of an undirected graph G = (V , E) with non-negative edge weights w such that
the subgraph H = (V , F ) is well-connected. One measure of connectivity is to maximize the total
weight of spanning trees in H = (V , F ), where the weight of a tree is deﬁned as the product of
the weights of its edges (see [LPYZ19] and references therein for other applications). This natural
network design problem is a special case of the Determinant Maximization problem. For each
V with (v(i,j))i = √w(i,j), (v(i,j))j =
√w(i,j),
1
(i, j)
}
and the rest of the coordinates set to zero. Observe that
e is exactly the Laplacian of
H = (V , F ), and the determinant of the Laplacian3 gives the number of spanning trees in H. Our
)|V |-estimation algorithm, and O(
-approximation algorithm for this
results imply an O(
V
V
|
|
|
problem under a general matroid constraint.

E, we introduce a vector v(i,j) ∈ {

e∈F vev⊤

P
)|V |3
|

0, 1,

−

−

∈

1.3 Technical Overview

Our starting point is a variant of the convex relaxation introduced in [NS16] for the partition
matroid. Let the set of input vectors be V =
),
the set of all independent sets of size s. We denote
we denote by
) :=
, which is the convex hull of the indicator vectors
(
by
M

P
3We remark that the Laplacian is always singular, but we can ﬁrst project the vectors ve orthogonal to the all-ones

= s
) the matroid base polytope of

v1, . . . , vn} ⊂
{

Rd. For a matroid

}
M

= ([n],

Is(

S
|

∈ I

M

M

S

I

{

|

:

vector and take the determinant in d − 1 dimensions.

4

i∈S zi. We let

P

(2)

of the bases. For any vector z
), z(S)

Rn :

:=

S

∈ Id(

M

∀

z
{

∈

Z

Rn and a subset S
. Our convex relaxation is
0
}

⊆

[n], we let z(S) :=

∈
≥

sup
x∈P(M)

inf
z∈Z

g(x, z) := log det

xieziviv⊤

i 

.







Xi∈[n]

For ease of notation, we deﬁne f (x) := inf z∈Z g(x, z), the inner inﬁmum of (2).

Similar but somewhat diﬀerent convex programs have been studied by [AGSS16, AG17, SV17b,
SV17a]. (The relationship of our convex program to these also plays a crucial role in our analysis:
see below.) The estimation algorithms in these works rely on a simple randomized algorithm to
round a fractional optimal solution x⋆. The analysis of the algorithm relies on a positive correlation
property: the algorithm outputs a random solution such that all elements of an independent set S
of size d are included with probability at least 1
i , where α is some function of k. This
α ·
property, combined with inequalities for real stable and completely log-concave polynomials, leads
Q
eO(d)-estimation algorithm. We show that there exist fractional optimal solutions x⋆ such
to an α
that no rounding scheme has this positive correlation property for any α which is a function of d
and independent of k. So, the dependence on k is inherent to all the previous algorithms which
round an arbitrary optimal solution x⋆ and do not consider the structure of the vectors to obtain
some structure on the optimal x⋆.

i∈S x∗

·

Our ﬁrst technical result is to show that there always exists an optimal solution that has at
most O(d2) fractional variables. We brieﬂy describe how to obtain such a sparse optimal solution.
Let x⋆ denote an optimal solution to the convex program (similar reasoning works for near optimal
solutions as well). We ﬁrst show that, using a series of careful preprocessing steps, we can assume
that there exists a z⋆ attaining the inﬁmum in f (x⋆) = infz∈Z g(x⋆, z). We then use ﬁrst order
optimality conditions that give a suﬃcient condition for another solution x to be optimal (i.e.,
to have f (x) = f (x⋆)). These conditions, however, present two signiﬁcant obstacles: ﬁrst, the
conditions are not linear in x, and, second, they ask for the existence of an exponentially sized dual
solution as a certiﬁcate of optimality. We address the ﬁrst problem by noticing that insisting that
does not change when x⋆ changes to x leads to the optimality
the entire matrix
conditions becoming a system of linear equations in exponentially many variables. We then use
the simple, yet elegant fact from matroid theory that minimum weight bases of a matroid under
a linear weight function form the base set of another matroid. We use this combinatorial fact to
observe that the existence of the exponentially sized dual solution is equivalent to insisting that
a vector, whose coordinates are linear functions of x, is in the base polytope of a new matroid.
Putting all of this together reduces the search for the new optimal solution x to solving a system
of exponentially many linear inequalities. Now, in the familiar territory of matroid polytopes, we
apply standard uncrossing methods and show that every extreme point solution of the system of
these linear inequalities has only O(d2) fractional variables.

i eziviv⊤
i

i∈[n] x⋆

(cid:16)P

(cid:17)

Finally, we give a new randomized algorithm that gives an O(d)d-estimation algorithm in the
presence of O(d2) fractional variables. Since the objective is non-linear, we cannot just pick all
variables set to 1 and apply a randomized algorithm to fractional elements. Indeed, the variables
set to 1 must also be dropped from the ﬁnal solution with certain probability. We show that given
a solution x with at most O(d2) fractional values, our rounding scheme outputs a random solution
such that for any independent set S of size d, all elements of S are picked with probability at least
(O(d))−d
i∈S xi. To show that this property implies the random solution output by the algorithm
achieves an O(d)d approximation in expectation, we utilize recent and exciting work on strongly
log-concave polynomials [AGV18, ALGV19] and the equivalent notion of Lorentzian polynomi-
als [BH19]. While the analysis using strongly log-concave distributions naturally utilizes a diﬀerent

Q

5

convex programming relaxation introduced in [AG17], the aforementioned sparsity result is not ap-
plicable to these convex programs. To this end, we show that the convex programming relaxation
considered in our work is stronger than the convex programming relaxation from [AG17]. The
relationship between the various convex programs for this problem and their respective strengths
and weaknesses outlined by our results may be of independent interest.

1.4 Related Work

Below we given an overview of prior work on the Determinant Maximization problem, which
has been studied in many special cases.

n
k

Uniform Matroid: Determinant Maximization is NP-hard even for a uniform matroid
[Wel82]. Koutis [Kou06] showed that there exists a constant c > 0 such that it is NP-hard to
achieve approximation better than a factor of (1 + c)d when k
βd for some constant β < 1, and
Di Summa et al. [DSEFM15] extended this hardness result to k = d. Bouhtou et al. [BGS10] gave
d-approximation algorithm based on rounding the solution of a natural convex relaxation.
an
Nikolov [Nik15] improved the result to an ek-approximation when k
d. Wang et al. [WYS16]
d2
improved the approximation ratio to (1 + ǫ)d when k
ǫ . Allen-Zhu et al. [ALSW17a] improved
the bound on k to give (1+ǫ)d-approximation when k = Ω
and showed the existence of a sparse
optimal solution for the standard convex relaxation. This was improved by Singh and Xie [SX18]
who gave a (1 + ǫ)d-approximation when k = Ω
. Recently, this was improved by
Madan et al. [MSTX19] who gave a (1 + ǫ)d-approximation when k

(cid:1)
(cid:0)
ǫ2 log 1
ǫ

ǫ + 1

d
ǫ2

≤

≥

≤

(cid:0)

(cid:1)

d

(cid:0)

(cid:1)

d + d
ǫ .

≥

General Matroid: Nikolov and Singh [NS16] gave an ed-estimation algorithm for Determinant
Maximization under a partition matroid of rank d. Straszak and Vishnoi [SV17b] gave an O(en)-
estimation (where n is the size of the ground set), and Anari and Gharan [AG17] gave an e2k-
estimation when the generating polynomial for the matroid is real-stable. This corresponds to
Strongly Rayleigh matroids which include uniform and partition matroids. These results were
generalized by Anari, Gharan, and Vinzant [AGV18] who gave an e2k-estimation for a general
matroid.4 Algorithms in [NS16, AG17, AGV18] estimate the optimum value within a certain
approximation factor, but they do not yield an approximate solution with high probability in
polynomial time. For partition and regular matroids of rank k
d, Ebrahimi, Straszak, and
Vishnoi [ESV17], using anti-concentration inequalities, gave eﬃcient approximation algorithms with
high probability guarantees. These are the most general algorithmic approximation results known
for the Determinant Maximization problem. Their guarantees on the approximation factor,
however, are worse than the estimation algorithms and depend on the size of the ground set.

≤

Experimental Design:
In the experimental design literature, several diﬀerent objective func-
tions are studied, which lead to diﬀerent optimization problems. Apart from D-optimal design, two
of the most notable problems are A-optimal design and E-optimal design. In A-optimal design,
. In
the objective is to minimize the trace of the covariance matrix: minS∈B tr
E-optimal design, the objective is to minimize the maximum eigenvalue of the covariance matrix:
. There have been a series of works on both of these problems in the
minS∈B λmax
uniform matroid setting [AB13, WYS16, NST19, ALSW17a, MSTX19]. The current best results

i∈S viv⊤
i

i∈S viv⊤
i

(cid:16)(cid:0)P

−1

−1

(cid:17)

(cid:1)

(cid:16)(cid:0)P

(cid:17)

(cid:1)

4While the result in [AGV18] is not stated for k > d, it can be easily deduced from the analysis.

6

are (1+ǫ)-approximation for A-design when k
for E-design when k

[ALSW17a].

Ω

d
ǫ2

≥

d

ǫ + 1

ǫ2 log 1

ǫ

Ω

≥

(cid:0)

[NST19] and (1+ǫ)-approximation

(cid:1)

(cid:0)

(cid:1)

Nash Social Welfare: Cole and Gkatzelis [CG15] gave the ﬁrst constant factor approximation
algorithm for the Nash Social Welfare problem, achieving an approximation factor of (2e1/e). This
result was subsequently improved in a series of papers [AGSS16, CDG+17, BKV18] with the current
best approximation ratio being 1.45.

Completely Log Concave Polynomials: The theory of completely log concave polynomials
introduced in [AGV18, ALGV19] (see also [Gur09, BH19]) plays an important role in the analysis
of algorithms for the Determinant Maximization problem. These results build on the use of
stable polynomials in the analysis of algorithms in [NS16, AGSS16, AG17, SV17b], themselves
building on the results by Gurvits [Gur06].

Sparsity and Fractionality in Convex Programs. Bounding the number of fractional vari-
ables, the sparsity of optimal solutions of convex programs, and, in particular, of convex relaxations
of discrete problems is a powerful technique which appears in many diﬀerent contexts. In com-
binatorics and geometry, early examples can be found in the proof of the Beck-Fiala theorem in
discrepancy theory [BF81] and in work of Barany, Grinberg, and Sevastyanov [Sev78, GS80, BG81].
A survey of these results is given by Barany [B´08]. In approximation algorithms, an early example
is the Karmakar-Karp approximation algorithm for the bin packing problem [KK82]. Bounding
the sparsity and fractionality of optimal basic feasible solutions to linear programs is the basis
of the iterative rounding method in approximation algorithms, introduced by Jain [Jai01]. The
book [LRS11] gives many results derived from this method. Bounding the sparsity of basic feasible
solutions is also key to the linear programming approach in compressed sensing [CT05a]. Related
results are known for the matrix completion problem, where sparsity is deﬁned in terms of ma-
trix rank and the corresponding optimization problem is non-linear [CT10]. Sparsity of optimal
solutions of non-linear convex programs appears to be, however, underexplored in general.

1.5 Organization

In Section 2, we discuss our convex relaxation, some technical issues in solving the relaxation, our
main technical result, and the ﬁrst order optimality conditions for the relaxation. In Section 3,
we show the existence of an optimal solution with at most O(d2) fractional values. In Section 4,
we give the randomized algorithm to round a solution of the relaxation with few fractional values.
In Section 5, we give our deterministic approximation algorithm that gives a guarantee that only
depends on d. In Appendix A, we discussed some of the deﬁnitions and preliminaries related to
matroids, log-concavity, and real stability. In Appendix B, we discuss the preprocessing of a given
instance so that the convex relaxation is solvable and the inner inﬁmum is achieved. In Appendix C,
we derive optimality conditions for our convex relaxation. In Appendices D and E, we give missing
proofs from Sections 3 and 4, respectively. In Appendix F, we show an example proving that none
of the previous approaches can achieve an approximation factor independent of k. In Appendix G,
we give an improved approximation algorithm for Determinant Maximization under a partition
matroid.

7

2 Convex Program and Optimality Conditions

Our algorithm for Determinant Maximization under a general matroid constraint is based on
solving a convex relaxation and rounding an optimal solution of the convex relaxation to an integral
solution. In this section, we formulate this convex relaxation, show that it is eﬃciently solvable,
and prove some of its properties which are crucial for the rounding algorithm.

2.1 Formulation of the Convex Program

be input vectors. For a matroid

the set of all independent sets of size s. We denote by

v1, . . . , vn}
) :=
Is(
Let V =
{
) the matroid base
= s
S
:
S
}
{
|
|
∈ I
, which is the convex hull of all of the bases. We include some basic preliminaries
polytope of
on matroids in Appendix A.1. For any vector z
[n], we let
z(S) :=
. We introduce the optimization
0
}
problem

Rn of real numbers and a subset S

i∈S zi. We let

), we denote by

∈
∈ Id(

= ([n],

), z(S)

(
M

Rn :

z
{

M

M

M

M

:=

≥

⊆

Z

P

∈

S

∀

I

P

sup
x∈P(M)

inf
z∈Z

g(x, z) := log det

xieziviv⊤

i 

.

(3)





Xi∈[n]



For ease of notation, we also let f (x) := inf z∈Z g(x, z), the inner inﬁmum of (3). The above
program is a convex relaxation, as shown in Nikolov and Singh [NS16]. We include a proof for
completeness in Lemma A.21 in the Appendix. Unfortunately, it is not clear whether the outer
supremum and inner inﬁmum are attained at some x⋆ and ﬁnite z⋆. While the supremum over x
can be approximated, our approach relies crucially on the inner inﬁmum being achieved exactly
at some ﬁnite z⋆. We ﬁrst show the following technical lemma that gives a suﬃcient condition for
the inﬁmum to be achieved based on KKT conditions and Slater’s qualiﬁcation of constraints. We
Rd are in general position if any subset of size d is linearly
say that the vectors
independent.

vi : i
{

} ⊆

[n]

∈

Lemma 2.1 Let x
∈ P
suppose that the vectors
over z

at some z∗

(
M
vi : i
{
.
∈ Z

∈ Z

[n]
}

∈

) be such that maxi∈[n] xi < 1 and f (x) = inf z∈Z g(x, z) is ﬁnite, and
are in general position. Then, g(x, z) attains its inﬁmum

In general, an instance of our problem may not satisfy the conditions of the lemma: the given
vectors need not be in general position, and every optimal x may have value 1 on some coordinates.
We outline a preprocessing step in Appendix B to show that both of these assumptions can be made
with a slight loss in optimality by modifying the input instance. This is achieved by modifying
the matroid by introducing two parallel copies of each element as well as perturbing the vectors
slightly to put them in general position. From here on, we assume that these modiﬁcations have
and V to denote the resulting matroid and vectors, respectively.
been carried out, and we use
These reductions allow us to formulate the following stronger convex program where we place

M

an additional upper bound on the coordinates of x:

sup
x∈P(M)∩[0, 1

2 ]n

inf
z∈Z

g(x, z) := log det

xieziviv⊤

i 

(4)





Xi∈[n]



We denote the convex program (4) by CP, its optimum value by OPTCP, and an optimal solution
by (x⋆, z⋆). We denote by OPT, the optimal value of the Determinant Maximization problem.
Based on the discussion above, we show the following lemma where we also outline the polynomial
time solvability of the convex program. The proof of the lemma appears in Appendix B.

8

Lemma 2.2 For any ǫ > 0, there is a polynomial time algorithm that returns x⋆
such that inf z∈Z g(x⋆, z)
inf z∈Z g(x⋆, z).

ǫ. Moreover, there exists z⋆ attaining the inﬁmum in
(cid:3)

log (OPT)

)
(
M

∈ P

0, 1
2

−

≥

∩

(cid:2)

n

Our main algorithmic result is to show that the value of the convex program OPTCP gives a
good approximation of the optimal value OPT of the Determinant Maximization problem. The
theorem below immediately implies Theorem 1.1.

Theorem 2.3 The optimum value OPTCP of the convex program gives a (2e5d)d-approximation
to the value of the optimum, i.e.,

log (OPT)

ǫ

OPTCP ≤

≤

−

log (OPT) + O(d log d).

(5)

Moreover, there is a polynomial time algorithm that, given x⋆ attaining OPTCP and z⋆ attaining
the inﬁmum in infz∈Z g(x⋆, z), returns a random set S

such that

∈ I

E

det

"

vivT
i

!# ≥

Xi∈S

(2e5d)−d (OPT) .

We now outline the ideas behind proving Theorem 2.3. First, we obtain the KKT optimality
conditions of inf z∈Z g(x, z) in Section 2.2. In Section 3, we show that the KKT conditions can
be related to a new matroid deﬁned by minimum weight bases of the original matroid under the
weight function z⋆. We then apply uncrossing methods on matroids to show that there is always an
optimal sparse solution – in particular, one with at most O(d2) fractional variables. In Section 4,
we give a rounding algorithm that uses the fact that number of fractional variables is bounded,
and we prove Theorem 2.3 building on inequalities proved in [AG17] and [AGV18] for stable and
completely log concave polynomials, respectively.

2.2 Optimality Conditions

Recall the notation f (x) = inf z∈Z g(x, z). In the following result, we state a suﬃcient condition that
) satisﬁes f (ˆx) = f (x⋆), where x⋆ is an (approximately) optimal
some feasible solution ˆx
solution to CP as returned by the algorithm in Lemma 2.2. The result is obtained by applying
the general KKT conditions to the optimization problem infz∈Z g(x, z). For completeness, we
give a detailed description of the general KKT conditions and Slater’s constraint qualiﬁcation in
Appendix A.2.

(
M

∈ P

Lemma 2.4 Suppose x⋆
over
Z
exists λ

∩
in CP is achieved, and let z⋆
such that

)
(
M

∈ P

Id(M)
≥0

R

(cid:2)
∈

(cid:3)

n

0, 1
2
arg minz∈Z g(x⋆, z). For any ˆx

is a feasible solution for CP such that the inﬁmum
), suppose that there

(
M

∈ P

) with z⋆(S)
∈ Id(
[n], we have ˆxiez⋆

M

i v⊤

= 0, we have λS = 0,
i X−1vi =

S∈Id(M):i∈S λS where X =

n
i=1 x⋆

i ez⋆

i viv⊤

i , and

3.

n
i=1 x⋆

i viv⊤

i =
Then, f (ˆx) = f (x⋆). Moreover, there exists λ
P
with ˆx = x⋆.

P

i viv⊤
i .

i=1 ˆxiez⋆

n

P

∈

R

Id(M)
≥0

P

such that the above three conditions hold

We remark that the above criteria ask for the existence of exponentially sized vector λ in order
to certify that ˆx is optimal. In the next section, we show that the above condition is equivalent to
showing a certain vector is in the base polytope of another matroid derived from

.
M

9

∈
1. for all S

2. for all i

∈
i ez⋆

 
6
3 Small Support Solutions to CP

3.1 Preserving the Value of a Solution

In this section, we show that there is always an optimal solution to CP that has small number of
fractional components. Indeed, given any solution x such that the inner inﬁmum of CP is attained,
we show how to obtain a sparse solution whose objective is no worse.

Theorem 3.1 (Sparsity of an optimal solution) Let x⋆ be a solution to CP such that the in-
ner inﬁmum of CP is attained. Then there exists a solution ˆx

) such that

(
M

∈ P

1. f (ˆx) = f (x⋆), and

2.

i
|{

∈

[n] : 0 < ˆxi < 1

}| ≤

2

d+1
2

+ d

.

(cid:16)(cid:0)

(cid:1)

(cid:17)

Moreover, such a solution ˆx can be found in polynomial time.

Proof: Given x⋆, a solution to CP, we let z⋆ be an optimal solution to infz∈Z g(x⋆, z). Also, let
X =
i = 0, we can
update the instance by deleting these elements. Observe that this does not eﬀect the optimality
(restricted to supp(x⋆)) of z⋆ (see Lemma D.1 in the Appendix for details).

i . We assume that supp(x⋆) =

since for any i with x⋆

1, . . . , n
{

i∈[n] x⋆

i viv⊤

i ez⋆

P

}

We ﬁrst give a simpler description than Lemma 2.4 for a solution ˆx to have an objective better

than f (x⋆). This relies on the following basic lemma.

⋆ =
⋆). Additionally, if

Lemma 3.2 Let
([n],
I
oracle.

B

S
{
M

∈ Id : z⋆(S) = 0
}
admits an independent oracle, then

⋆ is a basis of another matroid

⋆ =
⋆ also admits an independent

. Then,

M

B

M

0 for all S

⋆ are the minimum weight
Proof: Since z⋆(S)
bases under the weight function z. Minimum weight bases of a matroid form the bases of another
matroid, and the independence oracle can be implemented in polynomial time (see Lemma D.2 in
✷
the Appendix for details).

∈ Id, the basis of

Id included in

≥

I

We now have the following simpler description for ˆx to be optimal building on Lemma 2.4. Let
⋆ be the matroid in Lemma 3.2 and let r⋆ : 2[n]

Z+ denote the rank function of

⋆.

M
Lemma 3.3 Let x⋆ be a solution of CP and z⋆

→
arg minz∈Z g(x⋆, z). Let ˆx

M

R[n] be such that

∈

∈

1. ˆx

),

(
M

∈ P
2. the vector w
where X =

R[n] deﬁned as wi = ˆxiez⋆
i viv⊤
i ,

i ez⋆

∈
i∈[n] x⋆

i v⊤

i X−1vi for each i

[n] satisﬁes w

∈

⋆),

(
M

∈ P

3.

i∈[n] ˆxiez⋆

P
i viv⊤

i =

i∈[n] x⋆

i ez⋆

i viv⊤

i , and

4. supp(ˆx)

P

⊆

supp(x⋆).

P

Then f (ˆx) = f (x⋆).

Proof: We show that the above conditions imply that the conditions of Lemma 2.4 are satis-
R[n] is
ﬁed. Indeed, we only need to show the existence of λ
∈
R[n] is the indicator vector of set S and
S∈B(M⋆) µSχS where χS ∈
in

RId(M) as claimed. Since w

⋆), we have w =

(
M

P

∈

P

10

s.t.

Xi∈S

Xi∈[n]

min 0

xi ≤

r(S)

Xi∈S
x([n]) = r([n]) = k

∀ ∅

xiez⋆

i v⊤

xiez⋆

i v⊤

r⋆(S)

i X−1vi ≤
i X−1vi = r⋆([n]) = d

∀ ∅

( S ( [n]

( S ( [n]

n

Xi=1

xiez⋆

i viv⊤

i =

xi ≥

n

Xi=1
0

i ez⋆
x⋆

i viv⊤
i

[n]

i

∀

∈

(6)

(7)

(8)

(9)

(10)

(11)

(12)

Figure 1: Linear program to obtain a sparse solution.

∈ B

∈ B

(
M

for S
P

⋆) and λS = 0 for all other sets in

S∈B(M⋆) µS = 1. Observe that for each S

⋆), we have z⋆(S) = 0. Thus, setting λS = µS
✷

(
M
Id(
Now the above conditions can be formulated as a feasibility system over the following linear con-
straints as given in Figure 1, and we call the formulated linear program LPx-OPT. Here, constraints
⋆).
(7)-(8) insist that x
Constraints (11) insist that the matrix X does not change when the solution changes to x from x⋆.
For ease of notation, we let wx be the vector (xiez⋆

) and (9)-(10) insist that the vector (xiez⋆

) satisﬁes the conditions of Lemma 2.4.

i viX−1vi)i∈[n] ∈ P

i viX−1vi)i∈[n].

(
M

(
M

∈ P

M

From basic uncrossing methods we obtain the following lemma characterizing any extreme point
, we
of the above linear program. Recall that a collection
have A
A. The proof of the lemma appears in Appendix D. Again, we focus on
supp(x) since x remains extreme after removing coordinates with xi = 0. Thus, we assume that
[n] = supp(x).

of sets is a chain if for all A, B

B or B

∈ C

⊆

⊆

C

Lemma 3.4 If x is an extreme point of the linear program LPx-OPT, then there exist chains
2[n] and P

[d] such that

[d]

C1,

C2 ⊆

⊆

×
1. x(S) = r(S) for each S
i ez⋆

n
i=1 x⋆

i viv⊤

(

i )jk for each (j, k)

P ,

∈

2. the linear constraints corresponding to sets in

P
and

3.

supp(x)
|
|

=

|C1|

+

|C2|

+

P
|

.
|

∈ C1, wx(S) = r⋆(S) for each S

∈ C2, and (

n

i=1 xiez⋆

i viv⊤

i )jk =

P
C2 and pairs in P are linearly independent,

C1,

Let x be an extreme point of the linear program LPx-OPT. Such an x can be found in polynomial
Sl. Then, we have x(Si) = r(Si). Since
where S1 ⊂
S1, . . . , Sl}
time. Let
C1 =
{
k and from the integrality
xi > 0 for all i
[n], we have 1
≤
∈
≤
d, and clearly
of the rank function, we obtain that
|C2| ≤
≤
d symmetric matrices. Therefore,
i and
P
|
| ≤
supp(x)
(cid:0)

k. Similarly,
i are d
. In what follows we argue all but 2

⊂
r(S1) < r(S2) . . . < r(Sl)
= l
|C1|
n
i=1 x⋆

coordinates are set to 1.

≤
i ez⋆
i viv⊤

i=1 xiez⋆

k + d +
(cid:1)

r⋆([n])

i viv⊤

×
d +

S2 . . .

since

d+1
2

d+1
2

d+1
2

n

P

P

≤

(cid:0)

(cid:1)

(cid:16)

(cid:0)

(cid:1)(cid:17)

11

For ease of notation, we let S0 =
= Sj \
I
|

Sj \
|
Sj \
|
variables set to 1. But since every set Sj with j /
∈

Sj−1, then xi = x(Sj)
i
{
}
Since xi > 0, we obtain that xi = 1. Let I =
at least
Sj \

. Observe that if
∅

|
Sj−1, we have

x(Sj−1) = r(Sj)

−
k :

1
{

≤

≤

−

j

supp(x)
|

I
| ≥ |

+ 2(l

I
− |

).
|

|

l, say
= 1 for any 1
Sj−1|
r(Sj−1) which is an non-negative integer.
. Observe that there are
= 1
Sj−1|
}
I contains at least two elements in

≤

≤

j

But from Lemma 3.4, we have

supp(x)
|

| ≤

l + d +

d + 1
2

(cid:19)

.

(cid:18)

Combining the two inequalities, we get

d
−
the number of fractional variables is at most supp(x)

| ≥

I
|

l

d+1
2

−
I
(cid:0)
| ≤
− |

≥ |
2(d +
(cid:1)

supp(x)
d+1
).
2

| −

2(d +

d+1
2

(cid:0)

(cid:1)

). Hence,
✷

(cid:0)

(cid:1)

4 Randomized Rounding Algorithm

In this section, we give our randomized rounding algorithm and prove the guarantee on its perfor-
mance claimed in Theorem 2.3.

Throughout this section, we assume that the algorithm receives an input x

) such that

(
M

∈ P

i : 0 < xi < 1

}| ≤

|{

2

d + 1
2

(cid:19)

+ d

.

(cid:19)

(cid:18)(cid:18)

We ﬁrst describe the rounding algorithm, presented in Algorithm 1. It is obvious that Algo-

rithm 1 runs in polynomial time.

Algorithm 1 Rounding Algorithm

1: Input: a matroid
2: Output: a set S
3: procedure Rounding(x,
4:

M
.
∈ I

i : 0 < xi < 1
}

R1 ← {
T
← ∅
for i in R1 do

= ([n],

), x

).

(
M

∈ P

I

)
I
, R2 ← {

i : xi = 1
}

5:

6:

7:

8:

9:

10:

11:

12:

13:

if T
T

i
} ∈ I
i
T
}
∪ {

∪ {
←

then

for i in R2 do

with probability 1
d

if T
T

i
} ∈ I
i
T
}
∪ {

∪ {
←

then

if T is not a basis then

with probability 1
2

Extend T to a basis (e.g. by going through each element in [n]

remains independent until T is a basis)

return T

T and add it to T if T

\

For ease of notation we denote γ = (2e3d)−d and

independent subset S of R1 ∪
γ. The claim can only be true if the ground set R1 ∪
of x, is small.

). We ﬁrst claim that every
Id(
R2 of size d is contained in the output set with probability at least
R2, which has been restricted to the support

Id =

M

12

Lemma 4.1 Let T denote the random set returned by Algorithm 1. Then, for any set S
such that S

R1 ∪

R2

⊆

∈ Id, we have

P[S

T ]

γ.

≥

⊆

Lemma 4.1 implies a lower bound on the expected objective value of the solution returned.

Lemma 4.2 Algorithm 1 returns an independent set T

with expected objective value

∈ I

E

det

"

viv⊤
i

γ

det

xiviv⊤
i

!# ≥

.

!

Xi∈T

XS∈Id
Next, we relate this lower bound to the objective of the convex relaxation CP in a two-step
procedure. Building on results by [AGV18], the lower bound on the expected objective of the
algorithm can be bounded in terms of objective of a diﬀerent convex relaxation as described in
Lemma 4.3. Proof of the lemma is inferred from the inequality proved in [AGV18] on completely log-
i yiviv⊤
concave polynomials by observing that the polynomials (in y and z variables) det
i
αi
yiwi
z[n]\S are completely log-concave. Here we use the notation
.
αi

n
i=1 x⋆
n
(cid:0)P
i=1

Xi∈S

yw
α

and

S∈Id

:=

(cid:1)

α

P

Lemma 4.3 For any x⋆

0,

≥

det

XS∈Id

Xi∈S

i viv⊤
x⋆
i

e−2d

sup
α∈P (Id)

inf
y,w>0

! ≥

(cid:0)

(cid:1)

Q

(cid:16)

(cid:17)

det

n
i=1 x⋆

i yiviv⊤
i
α
yw
(cid:1) (cid:16)P
α

(cid:0)P

wS

S∈Id

.

(cid:17)

To ﬁnish the proof of Theorem 2.3, we show that the convex relaxation CP is stronger than the

(cid:0)

(cid:1)

convex relaxation studied in [AGV18].

Lemma 4.4 For any x⋆

0,

≥

sup
α∈P (Id)

inf
y,w>0

det

n
i=1 x⋆

i yiviv⊤
i
α
yw
(cid:1) (cid:16)P
α

(cid:0)P

wS

S∈Id

(cid:17)

≥

det

inf
z∈Z

n

Xi=1

i eziviv⊤
x⋆
i

.

!

(cid:0)

(cid:1)

Note that we cannot directly use the convex relaxation of [AGV18] and avoid the two-step
procedure for our problem. Algorithm 1 and the proof of Lemma 4.1 require that the solution x is
sparse, and we do not know if such property holds true for the convex relaxation of [AGV18].

Before we prove these lemmas, we use them to prove the main result of our paper.

Proof of Theorem 2.3: We ﬁrst show (5). Recall that f (x) = infz∈Z log det
.
Let (x⋆, z⋆) be an optimal solution to CP, so we have f (x⋆) = OPTCP. The ﬁrst inequality of (5)
(cid:17)
follows from Lemma 2.2. It remains to show the second inequality.

(cid:16)P

i∈[n] xieziviv⊤
i

By Theorem 3.1, there exists ˆx
d+1
2

. Let T

+ d

2

∈ I

(cid:16)(cid:0)

(cid:1)

(cid:17)

}| ≤
be the random solution returned by Algorithm 1 given an input x = ˆx.

∈ P

∈

|

) such that f (ˆx) = f (x⋆) and

[n]

0 < ˆxi < 1

(
M

i
|{

13

 
 
 
 
We apply Lemmas 4.2, 4.3, and 4.4 successively and in this order to get

E

det

"

viv⊤
i

!# ≥

Xi∈T

(2e3d)−d

det

XS∈Id

Xi∈S

ˆxiviv⊤
i

!

(2e3d)−de−2d

det

(cid:0)P

inf
y,w>0

sup
α∈P (Id)
n

(2e5d)−d inf
z∈Z

det

ˆxieziviv⊤
i

!

≥

≥

n
i=1 ˆxiyiviv⊤
i
α
(cid:1) (cid:16)P

yw
α

S∈Id

wS

(cid:17)

(cid:0)

(cid:1)

= (2e5d)−d inf
z∈Z

det

i eziviv⊤
x⋆
i

!

= (2e5d)−d

OPTCP

·

Xi=1
n

Xi=1

where the ﬁrst of the two equalities follows from Theorem 3.1.
i∈T viv⊤
i

On the other hand, for any T

, we have det

∈ I

det

E

"

Xi∈T

viv⊤
i

(cid:0)P
!# ≤

(cid:1)
OPT.

Combining (13) and (14) proves the second inequality of (5).

OPT, and therefore

≤

(13)

(14)

Given a solution x⋆ and z⋆ attaining the inﬁmum in infz∈Z det

, the eﬃciency
of the randomized algorithm that satisﬁes (13) follows from the eﬃciency of obtaining a sparse
✷
solution (by Theorem 3.1) and of the rounding Algorithm 1.

i eziviv⊤
i

(cid:0)P

(cid:1)

n
i=1 x⋆

∩
S, we have S1 ∈ I
R2 \
S2 ∈ I

Now we prove Lemmas 4.1 and 4.2. The proofs of Lemmas 4.3 and 4.4 appears in Appendix E.

Proof of Lemma 4.1: We need to prove that for any S

⊆
(2e3d)−d.

P[S

T ]

⊆

≥

R1 ∪

R2 such that S

∈ Id,

Let S1 = S
Since S

∈ I

R1 and S2 = S
∩
and S1 ⊆

R2. Since xi = 1 for any i

R2 and x
. We ﬁrst claim the following.

∈

(
M

), we have R2 ∈ I

.

∈ P

Y )

Y
|

S1|
If

| ≤ |
.

S2 such that

and R2 ∈ I

Claim 4.5 There exists Y

(R2 \
⊆
Proof: Recall that S = S1 ∪
S2|
the condition. Else, by the deﬁnition of matroids, there exists an element i
S2 ∪ {
that S1 ∪
i
} ∈ I
S2|
S1 ∪
R2| − |
|
W ), then S1 ∪
(S2 ∪
If Y = R2 \
R2| −
)
S2|
S1 ∪
R2| − |
(
has size
|
|
S2 be a set such that S1 ∪
R2 \

and S1 ∪
S1 ∪
, then Y = R2 satisﬁes
R2| ≤ |
|
(S1 ∪
R2 \
S2) such
∈
S2. Repeating this process for
R2 \
, we get that i
∈
∅
W
S2 ∪
such that S1 ∪
.
S2|
S1 ∪
R2| − |
S2 of size
R2 \
|
∈ I
, Y
S2|
S1 ∪
R2| − |
. Since W has size
W
S2 ∪
Y ) = S1 ∪
|
∈ I
✷
.
S1|
S2| ≤ |
S2| − |
S1 ∪
=
|
. Next, we prove a lower bound on P[S
Y )
(R2 \

. Since S1 ∩
times, we obtain a set W
(R2 \
S2|
− |

R2 =

Let Y

∈ I

∈ I

T ].

⊆

⊆

⊆

.

Note that S is a disjoint union of S1 and S2. Hence,

≥

T ]

P[S

⊆

T ] = P[S1 ⊆
P[T
∩
P[T
≥
= P[T

T and S2 ⊆
R1 = S1 and S2 ⊆
and T
Y =
P[T

∅
]
∩
∅
Next, we lower bound each of the probabilities.

R1 = S1 and S2 ⊆
∩
Y =
T
R1 = S1 |

Y =

T ]

∩

∩

∩

·

14

T ]

]
∅

P[S2 ⊆

·

T

|

T

∩

Y =

, T
∅

∩

R1 = S1]

 
 
 
 
 
1. P[T

∩

Y =

]: Consider the event that T
∅

Y , i is not
added to T during the execution of the algorithm. Let T ′ be the set T before the iteration
considering i. If T ′
, i is not
i
} 6∈ I
∪ {
added to T with probability 1/2. Hence, for each i
T , i is not added to T with probability
at least 1/2. Probability that none of the elements of Y are added to T is therefore at least

, i is not added to T with probability 1. If T ′

. It happens if for each i
∅

i
} ∈ I
∪ {

Y =

∈

∈

∩

|Y |

. Since

1
2

Y
|

,
S1|

| ≤ |

(cid:0)

(cid:1)

P[Y

T =

]
∅

≥

∩

1
2

(cid:18)

|S1|

.

(cid:19)

2. P[T

]: Since all elements of R1 are considered before the elements of
∅
R2 (and hence Y ), we have

R1 = S1 |

Y =

∩

∩

T

To get T

∩

T

∩

P[T

R1 = S1 |
R1 = S1, we must have (R1 \
(R1 \
R1 = S1] = P[T
P[T

∩

∩

∩
S1)

∩
S1) =

T =

]
∅

·

∩
and S1 ⊆
∅
T
T
P[S1 ⊂

|

Y =

] = P[T
∅

R1 = S1].

T . Hence,

(R1 \

∩

S1) =

].
∅

(15)

(16)

As argued above, for any element i
other elements) is at least 1
since S1 ⊆

−
R1.

|R1|−|S1|

1
d

1

R1 \
1
d . Hence, P[T

S1, the probability that i is not in T (regardless of
1
which is equal to
d

|R1\S1|

S1)]

∈

1

(R1 \

∩

≥

−

(cid:0)

(cid:1)

∈ I

and S1 ⊆

−
Since S
(cid:1)
(cid:0)
the set T before the algorithm processes the element i. If no element of R1 \
S1. Hence, T ′
then T ′
i
} ⊂
∪ {
Hence, if no element of R1 \
1
d . This implies that

S1 and the set T ′ being
S1 is picked,
, and the probability that the element i is picked is 1
d .
S1 is picked, then every element of S1 is picked with probability

S, we have S1 ∈ I
i
} ∈ I
∪ {

. Consider an element i

∈

P[S1 ⊆

T

|

T

(R1 \

∩

S1) =

] =
∅

1
d

(cid:19)

(cid:18)

Combining (15)-(17), we get

|S1|

.

(17)

P[T

R1 = S1 |

∩

T

∩

Y =

]
∅

≥

1
d

1

−

(cid:18)

(cid:19)

|R1|−|S1|

|S1|

.

1
d

(cid:19)

(cid:18)

|

T

T

Y =

, T
3. P[S2 ⊆
∅
∩
just before the algorithm considers the element i.
T ′
Y ). By Claim 4.5, S1 ∪
i
(R2 \
S1 ∪
} ⊆
∪ {
. Then, T ′
T ′
i
Y =
} ∈ I
∪ {
∅
∩

∩

(R2 \

If T ′
Y )

R1 = S1]: Consider an element i

∩
∈ I

∈

S2. Let T ′ be the set T
Y =
, then
∅
R1 = S1 and

R1 = S1 and T ′
. Hence, if T ′

∩
∩

, and i is added to T with probability 1/2. Therefore,

P[S2 ⊆

T

|

T

∩

Y =

, T
∅

∩

R1 = S1] =

|S2|

.

1
2

(cid:19)

(cid:18)

Combining the bounds on the three probabilities, we get

P[S

T ]

⊆

1
2

≥

(cid:18)

|S1|

(cid:19)

(cid:18)

1
d

1

−

(cid:19)

|R1|−|S1|

1
d

(cid:18)

|S1|

(cid:19)

(cid:18)

|S2|

.

1
2

(cid:19)

15

Since

S
|

|

assumption of the theorem,

= d and S is a disjoint union of S1 and S2, we have
d+1
2

. Hence,

+ d

2

R1| ≤
|

S1|
|

+

S2|
|

= d. Also, by the

P[S

T ]

⊆

1
2

≥

(cid:18)

Since

S1| ≤
|

d, we have

(cid:16)(cid:0)
d
1

(cid:19)

(cid:18)

(cid:17)
2((d+1

2 )+d)

(cid:1)
1
d

−

(cid:19)

1
(cid:18)

−

1
d

(cid:19)

−|S1|

|S1|

.

1
d

(cid:19)

(cid:18)

2((d+1

2 )+d)

d(d+1)+d

1
d

1

−

(cid:18)

−d

(cid:19)

(cid:18)

d

1
d

(cid:19)

P[S

T ]

⊆

≥

=

(cid:18)

(cid:18)

1
d

1

−

1
(cid:18)

−

(cid:19)
1
d

(cid:19)

d

1
2

(cid:18)
d

(cid:19)
1
2d
(cid:19)
e− 3

d+2 . Hence,

For d

≥

2, we have 1

1
d ≥

−

e− 1.5

d

≥
P[S

T ]

≥

⊆

(2d)−d e−3d =

2e3d

−d

ﬁnishing the proof of Lemma 4.1

(cid:0)

(cid:1)

Proof of Lemma 4.2: By Lemma 4.1, for any S
T ]

−d. The rounding Algorithm 1 returns a solution T of expected value

R2 such that S

R1 ∪

2e3d

⊆

∈ Id, we have P[S

≥

✷

⊂

(cid:0)

(cid:1)

E

det

"

viv⊤
i

!#

Xi∈T

= E 

det

viv⊤
i

XS∈(T
d)



Xi∈S

!






XS⊆[n]:|S|=d
where we apply the Cauchy-Binet formula to obtain the ﬁrst equality. Since we only pick elements
of R1 ∪

R2 which form an independent set, we have

Xi∈S

=

P [S

T ] det

⊆

viv⊤
i

!

det

E

"

viv⊤
i

!#

Xi∈T

=

≥

XS⊆R1∪R2:S∈Id
−d
2e3d

P [S

⊆

T ] det

viv⊤
i

!

Xi∈S

det

viv⊤
i

.

!

XS⊆R1∪R2:S∈Id

Xi∈S

For each i

∈

[n], we have 0

≤

(cid:0)

(cid:1)

1. Hence,

xi ≤
viv⊤
i

!# ≥

det

E

"

−d

2e3d

det

xiviv⊤
i

.

!

For S
S

∈ Id such that S
R1 ∪

6⊆
i∈S xiviv⊤

R2,

6⊆

Xi∈T
(cid:0)
R1 ∪
R2, there exists i
i has rank at most d

(cid:1)

XS⊆R1∪R2:S∈Id
S such that xi = 0. Hence, for S
i∈S xiviv⊤
i

∈
1 and det

Xi∈S

∈ Id such that

= 0. Therefore,

−

P

E

det

"

Xi∈T
ﬁnishing the proof of Lemma 4.2.

viv⊤
i

!# ≥

−d

2e3d

(cid:0)P
det

(cid:0)

(cid:1)

XS∈Id

Xi∈S

(cid:1)
xiviv⊤
i

!

✷

16

 
 
 
 
 
 
 
 
 
 
5 Deterministic Algorithm

In this section, we prove Theorem 1.2 and give the deterministic algorithm achieving the claimed
guarantee. The algorithm reduces the ground set in each iteration until the ground set is itself
I|V ) denote the matroid obtained by
[n], we let
an independent set. Given any V
. Moreover, we let CP(V) denote the convex program when
deleting all elements not in V from
the ground set and the matroid are V and
M|V , respectively, and we consider only vectors indexed
by V . We let OPTCP(V) denote optimal value of the convex program CP(V). We denote by r(V )
the rank of the matroid

M|V = (V ,

⊆
M

We ﬁrst describe the deterministic rounding algorithm, presented in Algorithm 2.

M|V .

Algorithm 2 Deterministic Algorithm

1: Input: a matroid
M
2: Output: a basis S
∈ I
3: procedure Rounding
4:

.

= ([n],

).

I

Let x be optimal solution to CP such that

returned by Theorem 3.1.

5:

6:

7:

8:

Let V
i
← {
while V /

∈
∈ I

.
[n] : 0 < xi}
do

arg maxj∈V :r(V \{j})=r(V ) OPTCP(V
i
V
V
return V

i
}
\ {

←
←

i
|{

[n] : 0 < xi}| ≤

∈

k + 2

d+1
2

+ d

as

(cid:16)(cid:0)

(cid:1)

(cid:17)

j
\ {

) (breaking a tie arbitrarily)
}

Observe that V is initialized to a set of size at most k + 2

along with r(V ) = k.
Moreover, OPTCP(V ) = OPTCP initially, since we just remove all elements with xi = 0 from the
ground set.

+ d

(cid:16)(cid:0)

(cid:17)

(cid:1)

d+1
2

In each iteration of the while loop, we decrease the size of V by one, and thus there can be at
most 2
iterations of the while loop. In each iteration, we do not decrease the rank of
V from k, so the ﬁnal output, by construction, is an independent set of size k and hence feasible.
To prove the guarantee, we show that in each iteration,

d+1
2

+ d

(cid:16)(cid:0)

(cid:17)

(cid:1)

≥
where β = (2e5d)−d = O(d)−d. Also, the relaxation is exact after the last iteration because V is a
basis after the while loop terminates. Thus, the objective value of the returned solution is at least

·

OPTCP(V

)
i
}
\ {

β

OPTCP(V )

(18)

giving an approximation factor O(d)2d((d+1

2 )+d) = O(d)d3

O(1)3d2 log d = O(d)d3

, as claimed.

It only remains to prove (18). From the guarantee of the randomized algorithm given in Theo-

·

β2((d+1

2 )+d)

OPTCP,

·

rem 2.3, there exists a basis S

∈ I|V with S

⊆

V such that

det



vjv⊤

j 

β

·

≥

OPTCP(V ).

(19)

Xj∈S

\

∈

Let j
V
)
j
We have OPTCP(V
}
\ {
) of value det
solution to CP(V
j
(cid:0)P
}
\ {
maximize OPTCP(V
) over j s.t. r(V
j
(cid:0)P
}
\ {
proof of Theorem 1.2.



) = r(S) = k since S is a basis.
S where j must exist since V /
}
∈ I
e∈S vev⊤
, because the indicator vector x of S
is a
e
e∈S vev⊤
. Together with (19), and because i is chosen to
e
(cid:1)
) = k, we have established (18). This completes the
j
}
\ {

. Then r(V

j
\ {

j
\ {

det

⊆

≥

V

}

(cid:1)

17

References

[AB13]

[AG17]

Haim Avron and Christos Boutsidis. Faster subset selection for matrices and applica-
tions. SIAM Journal on Matrix Analysis and Applications, 34(4):1464–1499, 2013.

Nima Anari and Shayan Oveis Gharan. A generalization of permanent inequalities and
applications in counting and optimization. In Proceedings of the 49th Annual ACM
SIGACT Symposium on Theory of Computing, pages 384–396. ACM, 2017.

[AGSS16] Nima Anari, Shayan Oveis Gharan, Amin Saberi, and Mohit Singh. Nash social
welfare, matrix permanent, and stable polynomials. In Proceedings of Conference on
Innovations in Theoretical Computer Science, 2016.

[AGV18]

Nima Anari, Shayan Oveis Gharan, and Cynthia Vinzant. Log-concave polynomials,
entropy, and a deterministic approximation algorithm for counting bases of matroids.
In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS),
pages 35–46. IEEE, 2018.

[ALGV19] Nima Anari, Kuikui Liu, Shayan Oveis Gharan, and Cynthia Vinzant. Log-concave
polynomials ii: high-dimensional walks and an fpras for counting bases of a matroid. In
Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing,
pages 1–12. ACM, 2019.

[ALSW17a] Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal design
of experiments via regret minimization. In Doina Precup and Yee Whye Teh, editors,
Proceedings of the 34th International Conference on Machine Learning, ICML 2017,
Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine
Learning Research, pages 126–135. PMLR, 2017.

[ALSW17b] Zeyuan Allen-Zhu, Yuanzhi Li, Aarti Singh, and Yining Wang. Near-optimal discrete
optimization for experimental design: A regret minimization approach. arXiv preprint
arXiv:1711.05174, 2017.

[AMGV18] Nima Anari, Tung Mai, Shayan Oveis Gharan, and Vijay V Vazirani. Nash social
welfare for indivisible items under separable, piecewise-linear concave utilities. In Pro-
ceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms,
pages 2274–2290. SIAM, 2018.

[B+97]

[B´08]

[BF81]

[BG81]

[BGS10]

Keith Ball et al. An elementary introduction to modern convex geometry. Flavors of
geometry, 31:1–58, 1997.

Imre B´ar´any. On the power of linear dependencies. In Building bridges, volume 19 of
Bolyai Soc. Math. Stud., pages 31–45. Springer, Berlin, 2008.

J´ozsef Beck and Tibor Fiala. integer-making theorems. Discrete Applied Mathematics,
3(1):1–8, 1981.

I. B´ar´any and V. S. Grinberg. On some combinatorial questions in ﬁnite-dimensional
spaces. Linear Algebra Appl., 41:1–9, 1981.

Mustapha Bouhtou, Stephane Gaubert, and Guillaume Sagnol. Submodularity and
randomized rounding techniques for optimal experimental design. Electronic Notes in
Discrete Mathematics, 36:679–686, 2010.

18

[BH19]

[BKV18]

Petter Br¨and´en and June Huh.
arXiv:1902.03719, 2019.

Lorentzian polynomials.

arXiv preprint

Siddharth Barman, Sanath Kumar Krishnamurthy, and Rohit Vaish. Finding fair and
eﬃcient allocations. In Proceedings of the 2018 ACM Conference on Economics and
Computation, pages 557–574. ACM, 2018.

[BV04]

Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university
press, 2004.

[CDG+17] Richard Cole, Nikhil Devanur, Vasilis Gkatzelis, Kamal Jain, Tung Mai, Vijay V Vazi-
rani, and Sadra Yazdanbod. Convex program duality, ﬁsher markets, and nash social
welfare. In Proceedings of the 2017 ACM Conference on Economics and Computation,
pages 459–460. ACM, 2017.

[CDK+17] L. Elisa Celis, Amit Deshpande, Tarun Kathuria, Damian Straszak, and Nisheeth K.
In
Vishnoi. On the complexity of constrained determinantal point processes.
APPROX-RANDOM, volume 81 of LIPIcs, pages 36:1–36:22. Schloss Dagstuhl -
Leibniz-Zentrum f¨ur Informatik, 2017.

[CEZ17]

[CG98]

[CG15]

[CK06]

[CKM+16]

[CKS+18]

[C¸ M13]

[CT05a]

Alfonso Cevallos, Friedrich Eisenbrand, and Rico Zenklusen. Local search for max-sum
diversiﬁcation. In Philip N. Klein, editor, Proceedings of the Twenty-Eighth Annual
ACM-SIAM Symposium on Discrete Algorithms, SODA 2017, Barcelona, Spain, Hotel
Porta Fira, January 16-19, pages 130–142. SIAM, 2017.

Jaime Carbonell and Jade Goldstein. The use of mmr, diversity-based reranking for
reordering documents and producing summaries. In Proceedings of the 21st annual
international ACM SIGIR conference on Research and development in information
retrieval, pages 335–336. ACM, 1998.

Richard Cole and Vasilis Gkatzelis. Approximating the nash social welfare with in-
divisible items. In ACM symposium on Theory of computing, pages 371–380. ACM,
2015.

Harr Chen and David R Karger. Less is more: probabilistic models for retrieving fewer
relevant documents. In Proceedings of the 29th annual international ACM SIGIR con-
ference on Research and development in information retrieval, pages 429–436. ACM,
2006.

Ioannis Caragiannis, David Kurokawa, Herv´e Moulin, Ariel D. Procaccia, Nisarg Shah,
and Junxing Wang. The unreasonable fairness of maximum nash welfare. In EC, 2016.

L. Elisa Celis, Vijay Keswani, Damian Straszak, Amit Deshpande, Tarun Kathuria,
and Nisheeth K. Vishnoi. Fair and diverse dpp-based data summarization. In ICML,
volume 80 of Proceedings of Machine Learning Research, pages 715–724. PMLR, 2018.

Ali C¸ ivril and Malik Magdon-Ismail. Exponential inapproximability of selecting a
maximum volume sub-matrix. Algorithmica, 65(1):159–176, 2013.

Emmanuel J. Candes and Terence Tao. Decoding by linear programming. IEEE Trans.
Inform. Theory, 51(12):4203–4215, 2005.

19

[CT05b]

[CT10]

Richard Caron and Tim Traynor. The zero set of a polynomial. WSMR Report, pages
05–02, 2005.

Emmanuel J. Cand`es and Terence Tao. The power of convex relaxation: near-optimal
matrix completion. IEEE Trans. Inform. Theory, 56(5):2053–2080, 2010.

[DSEFM15] Marco Di Summa, Friedrich Eisenbrand, Yuri Faenza, and Carsten Moldenhauer. On
largest volume simplices and sub-determinants. In SODA, pages 315–323. SIAM, 2015.

[ESV17]

Javad B Ebrahimi, Damian Straszak, and Nisheeth K Vishnoi. Subdeterminant max-
imization via nonconvex relaxations and anti-concentration. In 2017 IEEE 58th An-
nual Symposium on Foundations of Computer Science (FOCS), pages 1020–1031. Ieee,
2017.

[FS05]

Eva Maria Feichtner and Bernd Sturmfels. Matroid polytopes, nested sets and
bergman fans. Portugaliae Mathematica, 62(4):437–468, 2005.

[GCGS14] Boqing Gong, Wei-Lun Chao, Kristen Grauman, and Fei Sha. Diverse sequential subset
In Advances in Neural Information

selection for supervised video summarization.
Processing Systems, pages 2069–2077, 2014.

[GM19]

Laurent Gourv`es and J´erˆome Monnot. On maximin share allocations in matroids.
Theor. Comput. Sci., 754:50–64, 2019.

[GMT13]

[GMT14]

[GS80]

[Gur06]

[Gur09]

[Jai01]

[KK82]

[Kou06]

Laurent Gourv`es, J´erˆome Monnot, and Lydia Tlilane. A matroid approach to the
worst case allocation of indivisible goods.
In IJCAI, pages 136–142. IJCAI/AAAI,
2013.

Laurent Gourv`es, J´erˆome Monnot, and Lydia Tlilane. Near fairness in matroids. In
ECAI, volume 263 of Frontiers in Artiﬁcial Intelligence and Applications, pages 393–
398. IOS Press, 2014.

V. S. Grinberg and S. V. Sevastjanov. Value of the Steinitz constant. Funktsional.
Anal. i Prilozhen., 14(2):56–57, 1980.

Leonid Gurvits. Hyperbolic polynomials approach to van der waerden/schrijver-valiant
like conjectures: Sharper bounds, simpler proofs and algorithmic applications. In ACM
symposium on Theory of computing, STOC ’06, pages 417–426, 2006.

Leonid Gurvits. On multivariate Newton-like inequalities. In Advances in combinato-
rial mathematics, pages 61–78. Springer, Berlin, 2009.

Kamal Jain. A factor 2 approximation algorithm for the generalized steiner network
problem. Combinatorica, 21(1):39–60, 2001.

Narendra Karmarkar and Richard M. Karp. An eﬃcient approximation scheme for the
one-dimensional bin-packing problem. In 23rd Annual Symposium on Foundations of
Computer Science, Chicago, Illinois, USA, 3-5 November 1982, pages 312–320, 1982.

Ioannis Koutis. Parameterized complexity and improved inapproximability for com-
puting the largest j-simplex in a v-polytope. Information Processing Letters, 100(1):8–
13, 2006.

20

[KT12]

Alex Kulesza and Ben Taskar. Determinantal point processes for machine learning.
Foundations and Trends R
(cid:13)

in Machine Learning, 5(2–3):123–286, 2012.

[LPYZ19] Huan Li, Stacy Patterson, Yuhao Yi, and Zhongzhi Zhang. Maximizing the number
of spanning trees in a connected graph. IEEE Transactions on Information Theory,
2019.

[LRS11]

Lap Chi Lau, Ramamoorthi Ravi, and Mohit Singh. Iterative methods in combinatorial
optimization, volume 46. Cambridge University Press, 2011.

[Mou04]

Herv´e Moulin. Fair division and collective welfare. MIT press, 2004.

[MSTX19] Vivek Madan, Mohit Singh, Uthaipon Tantipongpipat, and Weijun Xie. Combinatorial
algorithms for optimal design. In Conference on Learning Theory, pages 2210–2258,
2019.

[Nik15]

[NS16]

[NST19]

Aleksandar Nikolov. Randomized rounding for the largest simplex problem. In Pro-
ceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing,
pages 861–870. ACM, 2015.

Aleksandar Nikolov and Mohit Singh. Maximizing determinants under partition con-
straints. In ACM symposium on Theory of computing, pages 192–201, 2016.

Aleksandar Nikolov, Mohit Singh, and Uthaipon Tao Tantipongpipat. Proportional
volume sampling and approximation algorithms for a-optimal design. Proceedings of
SODA 2019, 2019.

[Puk06]

Friedrich Pukelsheim. Optimal design of experiments. SIAM, 2006.

[Roc97]

[Sch98]

[Sch03]

[Sev78]

R. Tyrrell Rockafellar. Convex analysis. Princeton Landmarks in Mathematics. Prince-
ton University Press, Princeton, NJ, 1997. Reprint of the 1970 original, Princeton
Paperbacks.

Alexander Schrijver. Theory of linear and integer programming. John Wiley & Sons,
1998.

Alexander Schrijver. Combinatorial optimization: polyhedra and eﬃciency, volume 24.
Springer Science & Business Media, 2003.

S. V. Sevastjanov. Approximate solution of some problems of scheduling theory.
Diskret. Analiz, (32 Metody Diskret. Analiza v Sinteze Upravl. Sistem):66–75, 96–
97, 1978.

[Sio58]

Maurice Sion. On general minimax theorems. Paciﬁc J. Math., 8:171–176, 1958.

[SV17a]

[SV17b]

Damian Straszak and Nisheeth K Vishnoi. Belief propagation, bethe approximation
and polynomials. In Communication, Control, and Computing (Allerton), 2017 55th
Annual Allerton Conference on, pages 666–671. IEEE, 2017.

Damian Straszak and Nisheeth K Vishnoi. Real stable polynomials and matroids: op-
timization and counting. In Proceedings of the 49th Annual ACM SIGACT Symposium
on Theory of Computing, pages 370–383. ACM, 2017.

21

[SX18]

[Wel82]

Mohit Singh and Weijun Xie. Approximate positive correlated distributions and ap-
proximation algorithms for D-optimal design. In Proceedings of SODA, 2018.

William J Welch. Algorithmic complexity: three np-hard problems in computational
statistics. Journal of Statistical Computation and Simulation, 15(1):17–25, 1982.

[Wel10]

Dominic JA Welsh. Matroid theory. Courier Corporation, 2010.

[Whi35]

Hassler Whitney. On the abstract properties of linear dependence. American Journal
of Mathematics, 57(3):509–533, 1935.

[WYS16]

Yining Wang, Adams Wei Yu, and Aarti Singh. On computationally tractable selection
of experiments in regression models. arXiv preprint arXiv:1601.02068, 2016.

[ZCL15]

ChengXiang Zhai, William W. Cohen, and John D. Laﬀerty. Beyond independent rele-
vance: Methods and evaluation metrics for subtopic retrieval. SIGIR Forum, 49(1):2–9,
2015.

A Preliminaries

In this section, we introduce deﬁnitions and theorems from convex duality and the polyhedral
theory of matroids that we use in this paper.

Theorem A.1 (Cauchy-Binet Formula) For any set of vectors v1, . . . , vn ∈

Rd,

det

n

Xi=1

viv⊤
i

=

!

det

XS∈([n]
d )

Xi∈S

viv⊤
i

!

where

[n]
d

denotes the set of subsets of [n] of size d.

(cid:1)
A.1 Matroids

(cid:0)

For basic preliminaries on matroids, we refer readers to Chapter 39 of [Sch03]. Here we include the
deﬁnitions and basic facts that are used in our proofs in this paper.

Deﬁnition A.2 (Matroids) A matroid
set E and a non-empty collection

) is a structure consisting of a ﬁnite ground
of independent subsets of E satisfying the following conditions:

= (E,

M

I

I

If S

⊆
If S, T

T and T

and

∈ I

∈ I
T
|

|

•

•

, then S

.

∈ I

>

S
|

, then there exists an element i
|

∈

T

\

S such that S

i
} ∈ I
∪ {

.

Deﬁnition A.3 (Rank Function of Matroids) For a matroid
denoted by r(
r : 2E

), is the size of the largest independent set. For any S

R, denoted by r(S), is the size of maximal independent sets contained in S.

= (E,

M

M

⊆

,
), the rank of
I
M
E, the rank function

→

It is known that all maximal sets in a subset S
well-deﬁned and equals to the size of these maximal sets.

⊆

E have the same size, and therefore r(

) is

M

Deﬁnition A.4 (Basis of Matroids) An independent set with the largest cardinality of a ma-
. The set of all bases of
troid

, is called a basis of

, i.e. with the size equals to the rank of
and is denoted by

is called a base set of

M

M

M

M

M
BM.

22

 
 
Lemma A.5 (Strong Basis Exchange Property) (Theorem 39.12 of [Sch03]) Let B1, B2 be
distinct bases of a matroid
B2 such that
y, B2 + y
B1 + x

B1, there exists y

. Then for all x

B2 \

B1 \

∈

∈

x

−

M
∈ BM.

−

Deﬁnition A.6 (Matroid Base Polytope) For a matroid
indicator vector 1S of S is a binary vector in
otherwise. The matroid base polytope
of

E the
⊆
n whose ith coordinate is 1 if i
S and 0
0, 1
}
{
) is the convex hull of indicator vectors of all the bases

) and a subset S

(
M

= (E,

M

P

∈

I

.
M

Lemma A.7 (Characterization of Base Polytope) (Corollary 40.2d of [Sch03]) For a ma-
troid

, the matroid base polytope

) is characterized by

M

(
M

P

) =

(
M

P

x

(

∈

RE :

Xe∈S

xe ≤

r(S),

S

∀

⊆

[n] and

xe = r(E)

)

.

Xe∈E

(20)

Therefore, for any matroid

= (E,

I

M

) of rank r and x

), we have

(
M

∈ P

e∈E xe = r.

Lemma A.8 (Submodularity of the Rank Function) (Theorem 39.8 of [Sch03]) For a ma-
troid

is submodular. That is, for all sets A, B

), the rank function r of

= (E,

E,

P

M

I

M

⊆

r(A) + r(B)

r(A

∪

≥

B) + r(A

B).

∩

Deﬁnition A.9 (Basis Generating Polynomial of Matroids) For a matroid
BM, the basis generating polynomial of matroid
with the base set
zi.
gM(z1, . . . , zn) =

M

is

(21)

= ([n],

)

I

M

XB∈BM Yi∈B

A.2 Optimality Conditions for Convex Programming

In this section, we recall Slater’s constraint qualiﬁcation as a suﬃcient condition for strong duality
to hold for a convex program. Since all constraints in the optimization problems we consider in
this paper are aﬃne, we will state the relevant results for this special case.

Consider an optimization problem

inf

,
}
where h is a convex function deﬁned on a non-empty convex domain
of dimensions, respectively, m
or ℓ to be 0. Then, the Lagrangian associated with (22) is

h(z) : z
{

b, Cz = d

n, and b

n and ℓ

Rm, d

, Az

∈ D

≤

×

×

∈

∈

(22)

Rn, A and C are matrices
Rℓ are vectors. We allow either m

D ⊆

L(z, λ) := h(z) +

m

Xi=1

λi(Az

−

ℓ

b)i +

λm+i(Cz

d)i,

−

Xi=1
=

deﬁned for z
It is easy to see that if z
L(z, λ)
h(z). Moreover,

∈ D

∈ D

and Lagrange multipliers λ

, where

E
is feasible, i.e., satisﬁes Az

∈ E

Rm+ℓ : λi ≥
λ
{
b and Cz = d, and if λ

∈

∀

0

i = 1, . . . , m

.
}
, then

∈ E

≤

≤

L(z, λ) =

sup
λ∈E

h(z) Az

b, Cz = d

≤
otherwise

.

(

∞

23

Therefore, we have

sup
λ∈E

inf
z∈D

L(z, λ)

≤

≤

sup
λ∈E
inf

L(z, λ) : Az

inf
z∈D{

h(z) : z
{

, Az

∈ D

b, Cz = d

}

b, Cz = d

= inf
z∈D

sup
λ∈E

}

L(z, λ).

≤

≤

The next theorem, which is classical, gives a suﬃcient condition for these inequalities to hold with
equality.

Theorem A.10 (Strong Duality) Suppose that h in (22) is convex with non-empty convex do-
, and that there exists some z in the relative
main
interior of

−∞
b and Cz = d. Then there exists a vector λ⋆

. Suppose that the inﬁmum in (22) is not

such that Az

such that

D

D

≤
L(z, λ⋆) = inf

inf
z∈D

h(z) : z
{

∈ D

, Az

≤

b, Cz = d

.
}

∈ E

Furthermore, the same assumptions also imply that the KKT optimality conditions are neces-

sary and suﬃcient.

Theorem A.11 (KKT Conditions) Suppose that the assumptions of Theorem A.10 hold and
. Then, z⋆ achieves the inﬁmum in
that z⋆
b, Cz⋆ = d, and λ⋆
(22) and λ⋆ achieves inf z∈D L(z, λ⋆) = h(z⋆) if and only if

is feasible, i.e., Az⋆

∈ D

∈ E

≤

a. λ⋆

i (Az⋆

−

b)i = 0 for all i

1, . . . , m

∈ {

, and
}

b. 0

∈

∂h(z⋆) + M⊤λ,

where ∂h(z⋆) is the subgradient of f at z⋆ and M is the matrix M =

C
D

.
(cid:19)

(cid:18)

For proofs of Theorem A.10 and Theorem A.11, see e.g. Theorems 28.2 and 28.3 in [Roc97].

A.3 Complete Log-Concavity

α
|
|

P

In this section, we review some of the results by Anari, Gharan, and Vinzant [AGV18] on log-concave
i=1 ypi
polynomials and their implications for our problem. For vectors y, p
i
and y1−p :=
. We deﬁne a vector multiplied and divided by another vector or a scalar
coordinate-wise. For a vector α

Rn, we let yp :=

Rn, we also denote

i=1 y1−pi

:=

Q

∈

n

n

i

n
i=1 αi.

Q

∈

Deﬁnition A.12 (Log-Concave Polynomials) A polynomial g
coeﬃcients is log-concave if log(g) is concave over Rn
two vectors v, w

[0, 1], we have

Rn

≥0 and λ

R[z1, . . . , zn] with non-negative
>0. Equivalently, g is log-concave if for any

∈

∈

∈

Lemma A.13 (Proposition 2.2 in [AGV18])

g(λv + (1

λ)w)

−

g(v)λ

·

≥

g(w)1−λ.

•

•

For any two log-concave polynomials g, h, the polynomial g

h is log-concave.

·
For any log-concave polynomial g(z1, . . . , zn), the polynomial c
g(λ1z1, . . . , λnzn) is log-concave
·
if c, λ1, . . . , λn ≥

0.

24

Deﬁnition A.14 (Completely Log-Concave Polynomial) A polynomial g
completely log-concave if for every k
≥
and log-concave as a function over Rn
>0, where

0 and nonnegative matrix V

Rn×k

∈

R[z1, . . . , zn] is
≥0 , DVg(z) is nonnegative

∈

DVg(z) =

Πk

j=1

n

Xi=1

Vij∂i

!

g(z).

Lemma A.15 (Theorem 4.2 in [AGV18]) For any matroid
gM(z) is completely log-concave over the positive orthant.

, the basis generating polynomial

M

Lemma A.16 (Corollary 7.2 in [AGV18]) For any completely log-concave multi-aﬃne polynomial
g

[0, 1]n, the following inequality holds:

R[y1, . . . , yn, z1, . . . , zn] and p

∈

∈

(Πn

i=1(∂yi + ∂zi)) g(y, z)
y=z=0 ≥
|

p
e2

(cid:16)

p

(cid:17)

inf
y,z∈Rn

>0

g(y, z)
ypz1−p .

Lemma A.17 (Corollary 1.8 in [ALGV19]) For any set of vectors v1, . . . , vn ∈
i ) is a completely log-concave polynomial in x.
det(

n
i=1 xiviv⊤

Rd, the polynomial

P

Lemma A.18 (Follows from Theorem 5.3 and Corollary 5.5 in [BH19]) For any two completely
log-concave homogenous polynomials g
h is completely
log-concave.

R[y1, . . . , yn] and h

R[z1, . . . , zm], g

∈

∈

·

Lemma A.19 (Implied by Theorem 2.10 in [AGV18]) Let ζ =
be the
[n]
S1, . . . , St ⊆
}
{
convex closure of 1S1, . . . , 1St . Then, for any point p strictly inside
, there exists λ1, . . . , λn > 0
P
λSi and pi = PS∼µ[i
S].
and a distribution µ over Si’s such that µ(Si)
∈

and

∝

P

For a distribution µ : 2[n]
S⊆[n] µ(S)Πi∈Szi. We also call PS∼µ[i

→

R+, the generating polynomial of µ is deﬁned as gµ(z) =
S] the marginal probability of an element i of the distri-

bution µ. A distribution µ is called log-concave if the generating polynomial of µ is log-concave.
P
Lemma A.20 (Theorem 5.2 in [AGV18]) For any log-concave distribution µ : 2[n]
marginal probabilities µ1, . . . , µn ≥

0, we have

→

R+ with

∈

H

(µ) :=

µ(S) log

XS⊆[n]

1

µ(S) ≥

n

Xi=1

µi log

1
µi

.

A.4 Convex Relaxation

Here, we show that the convex program (3) is a relaxation of Determinant Maximization.

Lemma A.21 The optimization (3):

sup
x∈P(M)

inf
z∈Z

g(x, z) := log det



xieziviv⊤

i 

Xi∈[n]





is a relaxation of Determinant Maximization problem (1):

max

det

(

Xi∈S

viv⊤
i

: S

.

∈ B)

!

25

 
 
More speciﬁcally, OPT

exp(OPTCP).

≤

Proof: Let S⋆
⊆
indicator vector of S⋆. We have

[n] denote an optimal set for Determinant Maximization and x⋆ denote the

OPTCP = sup

x∈P(M)

g(x, z)

inf
z∈Z

inf
z∈Z

≥

g(x⋆, z).

For each z

, we have

∈ Z

exp(g(x⋆, z)) = det

n

Xi=1

i eziviv⊤
x⋆
i

= det

!

eziviv⊤
i

=

!

Xi∈S⋆

det

XR⊆S⋆:|R|=d

Xi∈R

eziviv⊤
i

!

where we use the Cauchy-Binet formula for the last equality. For each R

S⋆ of size d, we have

⊆

det

eziviv⊤
i

=

!

Xi∈R

ezi

det

!

Yi∈R

Xi∈R

viv⊤
i

det

! ≥

viv⊤
i

!

Xi∈R

where the last inequality follows from the constraint z(S)
and R

). Therefore, we obtain

∈ Id(

M

0,

S

∀

≥

∈ Id(

M

) in the deﬁnition of

Z

exp(g(x⋆, z))

≥

det

viv⊤
i

= det

viv⊤
i

= OPT

(23)

Xi∈R
where we apply the Cauchy-Binet formula again for the ﬁrst equality. Since (23) holds for each
z
∈ Z
OPT.

, we have exp(inf z∈Z g(x⋆, z))

OPT, and therefore exp(OPTCP)

exp(inf z∈Z g(x⋆, z))

XR⊆S⋆:|R|=d

≥
✷

≥

≥

Xi∈S⋆

!

!

B Preprocessing and Solvability

In this section, we show how to transform the matroid and the input vectors so that Lemma 2.2
holds. We ﬁrst prove Lemma 2.1, which gives suﬃcient conditions for the inf z∈Z g(x, z) to attain
its inﬁmum. This motivates the modiﬁcations to the input, which we carry out next. Finally, we
argue that the appropriately modiﬁed input gives an eﬃciently solvable convex relaxation.

B.1 Attaining the Inﬁmum

In this section, we prove Lemma 2.1. We ﬁrst prove an auxiliary lemma, from which the result will
follow. For notational convenience, let us deﬁne vectors ˆvi = xivi, and let us denote by
the bases
n
ˆvi}
of the linear matroid generated by the
i=1, i.e.
{

V

=

S

(

V

⊆

[n] :

S
|

|

= d and det

ˆvi ˆv⊤
i

.

= 0

)

! 6

Xi∈S

Recall that

) is the convex hull of indicator vectors of sets in

(
V

P

the relative interior of
a convex combination of indicator vectors of
are positive. We claim the following lemma.

(
V

P

). Equivalently, relint P (

V

)
(
V
) is the set of all points that can be written as
such that all coeﬃcients in the convex combination

. We denote by relint

P

V

V

26

 
 
 
 
 
 
 
 
 
 
Lemma B.1 Suppose that
z⋆

Id(
(
g(x, z⋆) is equal to

, and

P

∈ Z

−

))

relint

)

=

(
V

P

. Then, inf z∈Z g(x, z) is achieved at some
∅

∩

M

inf
µ∈D,ν∈RId(M) 


XS∈V

e∈S ˆvi ˆv⊤
i

where cS = det

µS log

µS
cS (cid:19)

:

(cid:18)

and

=

µ
{

D

∈

Proof: The objective function h(µ) :=

(cid:0)P

(cid:1)

D
x′

is a non-empty convex set. Moreover, since
(
V

D
). Then, we can write

Id(
(

relint

∈ P

M

P

))

P

∩

νI 1I =

µS1S,

µS = 1, ν

XI∈Id(M)
RV : µS > 0

XS∈V

XS∈V

S

.
∈ V}

∀
µS
cS

S∈V µS log

is easily seen to be convex in µ, and
is open, it is equal to its relative interior. Let

(cid:16)

(cid:17)

(24)

≥

,

0





x′ =

νI 1I =

µS1S

XI∈Id(M)

XS∈V

Id(M)
≥0

∈

R

such that

for some ν
is feasible, and the assumptions of Theorem A.10 are satisﬁed. We claim that
supz∈Z −

g(x, z) is equivalent to the dual problem of (24).

I∈Id(M) νI = 1 and µ

such that

∈ D

P

P

We can write the Lagrangian of (24) as

−

S∈V µS = 1. Therefore, (24)
inf z∈Z g(x, z) =

L(µ, ν, z, γ, t) =

µS log

XS∈V

µS
cS (cid:19)

(cid:18)

+

n

Xi=1

zi 


XI∈Id(M):i∈I

νI −

XS∈V:i∈S

+ t

µS

µS −

XS∈V

1
! −

XI∈Id(M)

γI νI ,

where the Lagrange multipliers are λ = (z, γ, t), and we have γ
g(x, z) is equivalent to
unconstrained. We will show that supz∈Z −

∈

R

Id(M)
≥0

, while t and z are

sup

z∈Rn,t∈R,γ∈R

Id(M)
≥0

inf
µ∈D,ν∈RId(M)

L(µ, ν, z, γ, t).

In particular, we will show that for any z,

sup
t∈R,γ∈R

Id(M)
≥0

inf
µ∈D,ν∈RId(M)

L(µ, ν, z, γ, t) =

g(x, z) z
z

−
−∞

(

∈ Z
6∈ Z

,

.

(25)

(26)

Since, by Theorem A.10, the supremum in (25) is achieved and equals (24), the lemma will follow.
Let us ﬁx some µ, z, γ, t, and ﬁrst take the inﬁmum over ν. The terms in L(µ, ν, z, γ, t) that

depend on ν are

We see that

νI (z(I)

γI ).

−

XI∈Id(M)

inf
ν∈RId (M)

XI∈Id(M)

νI (γI −

z(I)) =

0

(

−∞

z(I) = γI ∀
otherwise

I

.

27

6
 
z : z(I)
{

. If z
)
0
}
M
≥
), and we have infν∈RId (M) L(µ, ν, z, γ, t) =
unless z(I) = γI for every I

∈
∈ Z
). So, we may restrict the domain of z to

Id(M)
R
≥0
∈
, then inf ν∈RId (M) L(µ, ν, z, γ, t) =
and simplify

= γI for some I

and γ
. If z

6∈ Z
−∞

, then z(I)

∀

I

∈ Id(
∈ Id(

M

Z

=

Recall that
Id(
−∞
L(µ, ν, z, γ, t) to

M

Z

L′(µ, z, t) :=

µS log

XS∈V

=

µS log

(cid:18)

µS
cS (cid:19)
µS
cS (cid:19)

+

n

zi

µS + t

Xi=1

XS∈V:i∈S

XS∈V

−

µS −

1
!

µS(t

z(S))

t.

−

−

XS∈V
In other words, either inf ν∈RId (M) L(µ, ν, z, γ, t) =

XS∈V

(cid:18)

, or else we have z

and

∈ Z

−∞
L(µ, ν, z, γ, t) = L′(µ, z, t).

inf
ν∈RId(M)

Let us next ﬁx t and z, and compute inf µ∈D L′(µ, z, t). By taking derivatives over µ, we see

that the inﬁmum is achieved for µS := cSez(S)−t−1 and is equal to

L′(µ, z, t) =

inf
µ∈D

e−t−1

−

cSez(S)

t.

−

XS∈V

Taking the derivative over t, we see that the right-hand side is maximized for t = log
1, and we have, for every z,

S∈V cSez(S)

−
(cid:1)

(cid:0)P

sup
t∈R

inf
µ∈D

L′(µ, z, t) =

log

−

cSez(S)

=

−

!

log det

XS∈V

n

Xi=1

ˆvi ˆv⊤
i

=

−

!

g(x, z),

where the penultimate equality follows by the Cauchy-Binet formula. This establishes (26) and
✷
proves the lemma.

We can now prove Lemma 2.1.

Proof of Lemma 2.1: By Lemma B.1, we only need to show that

Id(
(

P

M

))

relint

)

=

(
V

P

.
∅

∩

Since we assumed that the vectors are in general position, we have

and therefore

=

S

{

V

⊆

supp(x) :

S
|

|

= d
}

,

relint

) =

(
V

P

x′

(

∈

Rn : 0 < x′

i < 1

i

∀

∈

supp(x) and x′

i = 0

supp(x) and

i

∀

6∈

n

Xi=1

x′
i = d

)

.

We ﬁrst claim the following.

Claim B.2 For any x

∈ P

1. there exists S

2. there exists S′

∈ Id(
∈ Id(

M

M

(
M
) such that S

) such that S′

⊆

supp(x) and i

∈
supp(x) and i /
∈

⊆

S

S, and

) such that maxi∈[n] xi < 1, we have that for all i

supp(x),

∈

28

6
 
 
 
6
Proof: Since x
Id(
M
that i
have S, S′

). For each i
S and i /
∈
supp(x).

∈

⊆

), x is a convex combination of indicator vectors of sets

(
M
supp(x), we have 0 < xi < 1. Hence, there exist S, S′

∈ P
∈
S′. Since x is a convex combination of indicator vectors of

Sj}j∈J in
{
Sj}j∈J such
∈ {
Sj}j∈J , we must
✷

{

We now construct a point x′ in

B.2 to obtain Si, S′
indicator vectors of
supp(x′) = supp(x), and

i ∈ Id(
{

P
M
Si}i∈supp(x) ∪ {
d
i=1 x′

). For each i
Id(
(
(
∈
M
P
V
i. Let x′
S′
Id(
(
) such that i
M
∈
i}i∈supp(x). Then, we have that 0 < x′
S′
i < 1 for all i
relint

supp(x), we apply Claim
)) be the average of all
supp(x),
✷

))
relint
Si and i /
∈

i = d. Therefore, x′

∈ P

∈

∩

).

∈

(
V

P

P

B.2 Transforming the Input

In order to guarantee that the assumptions of Lemma 2.1 hold, we transform the input to our
problem Determinant Maximization. The transformation will preserve the values of integral
solutions. It consists of two steps: ﬁrst we construct a new matroid and corresponding new vectors,
and then we perturb the vectors to ensure that they are in general position.

We deﬁne the new matroid

′) to be derived from the original matroid
′,
I
by introducing two copies for each element, forming a circuit. In particular, we let
and

′ = (

M

U

= ([n],

M
′ = [n]
U

×

)
I
[2]

′ :=

S

′ :

⊆ U

i

∀

∈

[n],

(i, 1), (i, 2)
}

{

I

* S and

j
{

⊆

[n] : (j, 1)

(cid:8)

We also create new vectors, corresponding to the elements of
i

U

∈

[n] and j
The next claim shows that if

[2].

∈

M
Determinant Maximization on the new instance is preserved.

M

S or (j, 2)

S

.

(27)

∈
∈
(cid:9)
′, by setting v′
(i,j) = vi for any

} ∈ I

is a matroid, then so is

′, and that the objective value of

Claim B.3 For any

′ = (
= ([n],
′ deﬁned as in (27) is a matroid. Moreover, the vectors

), the set system

M

M

U

I
for all i and j satisfy

′) constructed by

′,
U
I
v′
(i,j) : (i, j)
{

′

∈ U

}

′ = [n]
×
deﬁned by v′

U

[2] and
i,j = vi

max
S∈B

det

Xi∈S

viv⊤
i

!

= max
S′∈B′

det

e(v′
v′

e)⊤

,

!

Xe∈S

where

B

′ are the bases of

′.
M

. Let A, B

Proof: We ﬁrst show that
property of
∈ I
I
and similarly for BM. Then, by the deﬁnition of
. Therefore, there exists b
AM, BM ∈ I
A and A + (b, 1)
B
Then, (b, 1)
∈

′ is a matroid. If B
′ and A
∈ I
⊆
∈ I
[n] : (j, 1)
j
. Deﬁne AM =
A
|
|
⊆
{
|
′, we have
,
AM|
=
A
|
|
|
AM such that AM + b
BM \
∈ I
′, ﬁnishing the proof. The other case (b, 2)

M
′ be such that

′ by the hereditary
A or (j, 2)
A
∈
}
, and
BM|
=
|
B.
. Suppose (b, 1)
B is similar.

To show that the optimal value of Determinant Maximization is preserved, observe that,

B, then A

∈
B
|

B
|

∈ I

>

∈

∈

∈

I

\

|

for any S

∈ B

, the set S′ =

(i, 1) : i
{

S

}

∈

is a basis of

′, and observe that

M

det

viv⊤
i

!

= det

Xi∈S

e(v′)⊤
v′
e

.

!

Xe∈S′

29

 
 
 
 
In the other direction, we have that, by the deﬁnition of
S′ or (i, 1)

is a basis of

S′

, and the equality above is, again, satisﬁed.

∈ B

M

′, for any S′

, the set S =

i : (i, 1)
{

∈
✷

∈

}

M

The next step is to transform the vectors so that they are in general position. We use the

following lemma.

Lemma B.4 Let V ′ =
a collection V ′′ =
complexity of V ′, log(1/δ), and log log(1/γ) such that, with probability at least 1
V ′′ are in general position, and that for all S

e}e∈U ′ be a collection of vectors in Rd. For any δ, γ > 0, there exists
v′
{
e }e∈U ′ of vectors in Rd, computable in randomized polynomial time in the bit
v′′
{
γ, the vectors
S
|

′ of size

= d,

⊆ U

−

|

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

det
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

e(v′
v′

e)⊤

det

! −

Xe∈S

Xe∈S

e (v′′
v′′

e )⊤

δ.

≤

(28)

·

Proof: Let σ > 0 be a constant, to be determined shortly. For each vector v′
e, we add a Gaussian
Id to obtain v′′
vector with mean 0 and covariance matrix σ
e and covariance
·
v′′
e }e∈U ′ are linearly independent with probability 1
σ
Id. For any σ > 0, any subset of d vectors of
{
(because the set of singular matrices has Lebesgue measure 0 on Rd×d [CT05b], and the multivariate
Gaussian distribution is absolutely continuous with respect to Lebesgue measure). Therefore, by
such subsets are simultaneously linearly independent with probability 1,
the union bound, all
proving that V ′′ are in general position.
′ with

e which has mean v′

= d. Let V′

n
d

(cid:1)

(cid:0)

S, V′′

d matrices whose columns are
S. The following inequality is an easy consequence

S be d

×

We now show (28). Let S
e for e

v′
e, v′′
of the Brunn-Minkowski inequality [B+97]: for any d

S
|
|
S, respectively. Let W = V′′
S −

⊆ U

V′

∈

d matrices A and B,

1/d
det(A + B)
|

≥ |

×
1/d +
det(A)
|

1/d.
det(B)
|

|

We set A = V′

|
S and B = W to obtain
S)1/d

det(V′′

det(V′

S)1/d +

1/d
det(W)
|

|

≥

and set A = V′′

S and B =

W to obtain

−

det(V′

S)

≥

det(V′′

S)1/d +

det(

−

|

1/d.
W)
|

Therefore, we have

S)1/d

det(V′

S)1/d

−

1/d
det(W)
|

≤ |

(29)

We now bound the determinant of W with high probability. Note that W is a random matrix
wij}i,j∈[d] are independently sampled from the Gaussian distribution with mean zero

whose entries
and variance σ. From a standard tail bound of a Gaussian, we have that for each i, j,

{

(cid:12)
(cid:12)
(cid:12)

det(V′′
(cid:12)
(cid:12)
(cid:12)

wij|
P[
|

> t]

2
π ·

σe−t2/2σ2
t

σe−t2/2σ2
t

.

≤

By union bound, all entries wij satisfy

t with probability at least 1

, we have from the Leibniz formula for determinants

d2σe−t2/2σ2
t

.

−

In this event of probability 1

that

−

≤ r
wij| ≤
|
d2σe−t2/2σ2
t

det(W)
|

|

=

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

sgn(τ )

Xτ ∈perm(S)

Ye∈S

30

wi,τ (i)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

td

d!

·

≤

≤

(dt)d.

(30)

 
 
Also, using the convexity of the function h(x) = x2d, we have that for all a, b

Setting a = det(V′

a2d
|

b2d

a

b

[h′(x)]x=max{a,b} = 2d

a

−

| ·
| ≤ |
|
S)1/d and using (29), we obtain
S)1/d, b = det(V′′

−

−

|

b

max

a, b
{

}

0,

≥
2d−1 .

e(v′
v′

e)⊤

det

! −

Xe∈S

Xe∈S

e (v′′
v′′

e )⊤

det
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

=

≤

≤

2d

S)1/d

det(V′
(cid:16)

(cid:17)
det(W)1/d

(cid:12)
(cid:12)
(cid:12)
(cid:12)
2d

−

det(V′′
(cid:16)
det(V′

2d

S )1/d

(cid:12)
(cid:12)
2d(dt)
(cid:12)

(cid:12)
(cid:16)
(cid:12)
S)1/d + dt
det(V′
(cid:12)

(cid:16)
= det

S)1/d +
2d−1

(cid:17)

(cid:12)
(cid:12)
(cid:12)
1/d
(cid:12)
det(W)
|

|

2d−1

(cid:17)

(31)

,

(cid:17)
= det (V′

S)2 and similarly

22L [Sch98]. Hence, we set

where the ﬁrst equality is by det
e )⊤
det

e)⊤
e(v′
S)2, and the last equality is by (30).
Let L be the bit complexity of V ′. Then we have det (V′
S)
(cid:0)P

= det (V′′

e∈S v′′

e∈S v′

t = d−2δ−12−8L so that (31) implies

e (v′′

(cid:0)P

V′

(cid:1)

(cid:0)

(cid:1)

SV′⊤
S

(cid:1)
≤

as desired.

e(v′
v′

e)⊤

det

! −

Xe∈S

Xe∈S

det
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

Recall that this desired bound happens with probability 1

e (v′′
v′′

e )⊤

δ

≤

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
d2σe−t2/2σ2
t

−

. Set σ =

t
d2√2 log(1/γ)

=

d4δ28L+ 1
(cid:16)

2 log1/2(1/γ)
(cid:17)

−1

so that

d2σe−t2/2σ2
t

≤

e− log(1/γ)d2

log−1/2(1/γ)

γ

≤

as required. The bit complexity of σ is poly
δ , log log 1

in time poly

log 1

γ , L

.

log 1

δ , log log 1

γ , L

. Therefore, the algorithm also runs

(cid:16)

(cid:17)

✷

(cid:16)

(cid:17)

The next lemma shows that we can, without modifying the optimal value, replace the original

instance by

′ and V ′.

M

Lemma B.5 Suppose that maxS∈B det
= 0. Then for any ǫ > 0, there exists a value
of δ > 0 such that log(1/δ) is polynomial in the bit complexity of V and in log(1/ǫ), and that
′
the vectors V ′′ constructed in Lemma B.4 from V ′ and the matroid
constructed above satisfy

′) with bases

′ = (

(cid:0)P

M

B

′,

U

I

(cid:1)

i∈S viv⊤
i

(1

ǫ) max
S∈B

det

−

viv⊤
i

max
S′∈B′

det

! ≤

Xi∈S

Moreover, if S⋆ achieves the maximum over
S⋆

, then
}

M

e (v′′
v′′

e )⊤

! ≤

Xe∈S
and we let S1 :=

(1 + ǫ) max
S∈B

det

viv⊤
i

.

!

(i, 1) : i
{

∈

S⋆

(i, 2) : i
{

∈

Xi∈S
, S2 :=
}

det

1
2





Xe∈S1

e (v′′
v′′

e )⊤ +

1
2

Xe∈S2

e (v′′
v′′

e )⊤



≥

(1

−

ǫ) det



31

viv⊤
i

.

!

Xi∈S⋆

 
 
 
 
6
 
 
 
 
Proof: Note that, since maxS∈B det

i∈S viv⊤
i

= 0, by Claim B.3 we must have

(cid:0)P

viv⊤
i

= max
S∈B′

!

(cid:1)
det

max
S∈B

det

Xi∈S

ve(v′

e)⊤

! ≥

2−poly(L).

Xe∈S

where L is the bit complexity of V . For any S
formula, we have

⊆

[n] of size k, by Lemma B.4 and the Cauchy-Binet

det

e(v′
v′

e)⊤

det

(cid:12)
(cid:12)
(cid:12)
We can then choose δ suﬃciently small so that
(cid:12)
(cid:12)

Xe∈S

! −

δ

≤

k
d

.
(cid:19)

(cid:18)

e (v′′
v′′

e )⊤

Xe∈S

!(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

k
d

δ

(cid:18)

≤

(cid:19)

ǫ max
S∈B

det

viv⊤
i

.

!

Xi∈S

This choice of δ suﬃces for the claim after “moreover” by an analogous reasoning.

✷

We are now ready to prove Lemma 2.2, except for the polynomial time solvability of OPTCP,
which is deferred to the next section. The following lemma captures the non-algorithmic statements
in Lemma 2.2, when the original instance (

, V ) is replaced by (

′, V ′′).

M

M

Lemma B.6 Let
RU ′

S

:

∈ Id(

M

∀

M
′), z(S)

′ be as constructed above, and let V ′′ be as in Lemma B.5. Let
such that

. Then there exists a x⋆
0
}

(
M

0, 1
2

∈ P

≥

′)

∩

U ′

′) =

(
M

Z

z
{

∈

inf
z∈Z(M′)

log det

xf ezf v′′

f (v′′

f )⊤

log max
S∈B

det



≥



(cid:2)

(cid:3)
viv⊤
i

Xi∈S

O(ǫ)

! −





Xf ∈U ′

Xf ∈S
Moreover, there exists z⋆ attaining the inﬁmum on the left hand side.



log max
S∈B′

det



≥

f (v′′
v′′

f )⊤

O(ǫ).



−



Proof: Let S⋆
x(i,j) = 1
S⋆
and S2 :=
}
vectors of those two sets.

∈
(i, 2) : i

2 if i

∈

{

achieve the maximum in the middle expression. Let x⋆

∈ B
S⋆ and 0 otherwise. To see that x⋆
are both bases in

RU ′
be deﬁned by
S⋆
′), observe that S1 :=
(i, 1) : i
}
{
′, and that x⋆ is a convex combination of indicator

(
M

∈ P

∈

∈

M

For each z

, we have

∈ Z

ˆxf ezf v′′

f (v′′

f )⊤

det





Xf ∈U ′

= det





1
2





Xf ∈S1

ezf v′′

f (v′′

f )⊤ +

1
2

Xf ∈S2

ezf v′′

f (v′′

f )⊤





=

ez(R)2−d

XR⊆S1∪S2:|R|=d

f (v′′
v′′

f )⊤

,









Xf ∈R

32

6
 
 
 
 
 
 
where we use the Cauchy-Binet formula for the last equality. For each R
have R

S1 ∪
1, so the right-hand side above is at least

′), and therefore ez(R)

⊆

S2 of size d, we

∈ Id(

M

≥

XR⊆S1∪S2:|R|=d

f (v′′
v′′

f )⊤

2−d





Xf ∈R

= det





1
2





Xf ∈S1

(1

−

≥

ǫ) det

ezf v′′

f (v′′

f )⊤ +

1
2

Xf ∈S2

viv⊤
i

,

!

Xi∈S⋆

ezf v′′

f (v′′

f )⊤





where we used the Cauchy-Binet formula for the equality and Lemma B.5 for the inequality. This
and Lemma B.5 prove the claim before “moreover”.

The fact that the inﬁmum is attained at some z⋆ is guaranteed by Lemma 2.1 and applied to
′. Indeed, the vectors in V ′′ can be assumed to be in general
′. Therefore, the assumptions of
✷

the vectors V ′′ and to the matroid
position by Lemma B.4, and x⋆ satisﬁes xe ≤
Lemma 2.1 hold, and the inﬁmum is achieved.

1
2 < 1 for all e

∈ U

M

Lemma B.6 allows us to replace

′ and V ′′, respectively, for the rest of the
paper. In particular, it allows us to assume that the input vectors are in general position, which
will be useful in the next section, when we address polynomial time solvability of the relaxation
CP.

and V with

M

M

B.3 Solvability of the Convex Program

The next theorem shows that CP can be solved approximately to any degree of accuracy. This will
),
complete the proof of Lemma 2.2. For conciseness we denote
and

Id =

)
(
M

Id(

0, 1
2

Id) the base polytope of the matroid ([n],
(

Id).

M

=

X

P

P

∩

(cid:3)

n

,

Theorem B.7 There is an algorithm that, given input vectors v1, . . . , vn ∈
shows that the optimal value of Determinant Maximization is 0, or returns a solution (˜x, ˜z)

(cid:2)
Rd and ǫ > 0, either

∈

such that

X × Z

1. f (˜x)

≥
2. g(˜x, ˜z)

f (x⋆)

−

ǫ, and

infz∈Z g(˜x, z) + ǫ.

≤

The algorithm runs in time polynomial in the size of the input and log

1
ǫ

.

The goal of this section is to prove Theorem B.7. First, we provide an algorithm to check

(cid:0)

(cid:1)

whether OPT = 0.

Lemma B.8 There exists a polynomial time algorithm that, given input vectors v1, . . . , vn ∈
correctly decides if OPT = 0.

Rd,

vi : i
{

Proof: Observe that, by the Cauchy-Binet formula, for any set S of size k, det
i∈R viv⊤
if and only if there exists a subset R
i
of vectors
[n] such that
and
intersection algorithm (see, for example, Chapter 41.2 of [Sch03]), since both
collections of bases of matroids.

i∈S viv⊤
> 0
i
> 0, i.e. the set
(cid:1)
(cid:0)P
be the collection of subsets R of size d of
)
Id(
M
have a non-empty intersection. This can be checked in polynomial time using a matroid
are the
✷

is linearly independent. Let
R

is linearly independent. We have that OPT > 0 if and only if

S of size d such that det

∈
vi : i
{

Id(

) and

}
∈

(cid:0)P

M

⊆

R

V

V

V

}

(cid:1)

33

 
Since it is polynomial-time to check if OPT = 0, we may now assume OPT > 0 for the rest of
this section. Let L denote the bit complexity of the input. Since the size of OPT is bounded by
[2−p(L), 2p(L)] for some polynomial p. Therefore, by binary search
poly(L) ([Sch98]), we have OPT
on OPT, the problem of maximizing f (x) reduces to the feasibility problem Det-Feasibility:

∈

Input: β

R

∈

•

(
M

Output: ‘yes’ if there exists x

) such that f (x)

β, ‘no’ otherwise.

∈ P

•
We will solve Det-Feasibility by the ellipsoid method. We will ﬁrst show that the feasible set
can be bounded with a small loss of accuracy so that the starting ellipsoid contains the feasible set.
Then, we will note some properties of the continuous function g(x, z) = log det
,
show an oracle to solve the inner problem f (x) = inf z∈Z g(x, z), and ﬁnally present an algorithm
(cid:1)
to Det-Feasibility.

n
i=1 xieziviv⊤
i

(cid:0)P

≥

Lemma B.9 For all ǫ > 0, we have

sup
x∈X :x≥ 1
M

inf
z∈Z:−M ≤z≤M

g(x, z)

ǫ

sup
x∈X

inf
z∈Z

≤

−

g(x, z)

≤

sup
x∈X :x≥ 1
M

inf
z∈Z:−M ≤z≤M

g(x, z) + ǫ

(32)

for some M with bit complexity log M = poly(L, log( 1

ǫ )).

The key to the proof of Lemma B.9 is to relate inf z∈Z g(x, z) with

sup
α∈P(Id)

inf
z

g(x, z)

α, z
i

− h

(33)

and then apply Lemma 3.4 of [AG17], which states that the supreme over the inﬁmum (33) can be
well-approximated when the feasible set is made bounded. We now state several claims, setting up
for the proof of Lemma B.9.

Claim B.10 Let

D ⊆

Rn be a compact set and

′

D

⊆

Rn be a convex set. Then,

inf
z∈D′

sup
α∈D

g(x, z)

α, z
i

− h

= sup
α∈D

inf
z∈D′

g(x, z)

α, z
i

.

− h

(34)

Proof: The function g(x, z)
is continuous and concave in z (see (43) in the proof of Lemma
B.12) and linear in α. Moreover, the domain of α is compact. The claim now follows from Sion’s
✷
minimax theorem [Sio58].

α, z
i

− h

The claim above implies, in particular, that

inf
z

sup
α∈P(Id)

g(x, z)

α, z
i

− h

= sup

α∈P(Id)

g(x, z)

inf
z

α, z
i

.

− h

(35)

We now relate infz∈Z g(x, z) with supα∈P(Id) infz g(x, z)

α, z
i

− h

by claiming that they are equal.

Claim B.11 We have

inf
z∈Z

g(x, z) = sup

α∈P(Id)

g(x, z)

inf
z

α, z
i

− h

.

(36)

34

Proof: Let z⋆ achieve the inﬁmum in infz∈Z g(x, z). Then, for all α
α =

0, and we have

S∈Id

λS1S for λS ≥

P

α, z⋆

h

i

=

where the inequality is by z⋆

∈ Z

1S, z⋆

λS h

λSz⋆(S)

0

≥

=

i

XS∈Id

XS∈Id
. Hence,

Id), we can write
(

∈ P

(37)

sup
α∈P(Id)

inf
z

g(x, z)

α, z

− h

i ≤

sup
α∈P(Id)

g(x, z⋆)

α, z⋆

− h

g(x, z⋆) = inf
z∈Z

i ≤

g(x, z).

We now show the other direction of the inequality. By (35), we switch the order of inﬁmum and
and let z⋆ achieve the
α, z⋆
− h
i
∈ Id. Let
i
= z⋆(S), and therefore

supremum in supα∈P(Id) inf z g(x, z)
outer inﬁmum. Since g(x, z⋆)
can be attained at α⋆ = 1S for some S
ˆz = z⋆
ˆz

α, z
i
S, z⋆
∈ Id such that
h
T , z⋆
h

is linear in α, the supremum supα∈P(Id) g(x, z⋆)

∈ Id, we have ˆz(T ) =

to infz supα∈P(Id) g(x, z)

. The feasible solution ˆz gives

. Then, for all T

for all T

α, z
i

− h
α, z⋆

z⋆(S)
d

i ≤ h

T , z⋆

i ≥ h

S, z⋆

− h

− h

−

i

i

∈ Z

g(x, ˆz) = log det

n

Xi=1

xiez⋆

i − z⋆(S)

d viv⊤
i

= log

det

!

n

Xi=1

xiez⋆

i viv⊤
i

e−z⋆(S)

!

! ·

= g(x, z⋆)

1S, z⋆

−h

i

which is the same as the right-hand side of (36). Therefore, we have

g(x, z)

inf
z∈Z

sup
α∈P(Id)

inf
z

≤

g(x, z)

α, z
i

− h

ﬁnishing the proof of the claim.

We are now ready to prove Lemma B.9.

✷

Proof of Lemma B.9: First, we show that the outer supremum is well-approximated after putting
a bound on the feasible set:

sup
x∈X

inf
z∈Z

g(x, z)

≥

sup
x∈X :x≥ 1
M1

inf
z∈Z

g(x, z)

ǫ

−

(38)

for M1 = poly(L, 1
the mass n
M1
g(x⋆, z)
g(˜x, z)
≥
M1 = poly(L, 1
ǫ ).

−

ǫ ). Let x⋆

∈
to some coordinates of ˜x so that
n
kM1

supx∈X inf z∈Z g(x, z). Then, we scale ˜x
1
M1 ≤
). We can bound the error term d log(1

d log(1

−
1
2 . Then for any z

n
kM1

←

(1

≤

˜x

n
kM1

)x⋆ and add
, we have
) by ǫ by setting

∈ Z

−

−

Next, we show that the inner inﬁmum problem is well-approximated after putting a bound
1
M . By Claim B.11, we have inf z∈Z g(x, z) =

on the feasible set. Let x
supα∈P(Id) inf z g(x, z)

∈ X
α, z
i

− h

such that x
≥
. Now, observe that

exp(g(x, log y)) = det

n

Xi=1

xiyiviv⊤
i

=

!

xRyR det

viv⊤
i

!

Xi∈R

XR⊆[n]:|R|=d

(39)

is a polynomial of degree d in y. By Lemma 3.4 of [AG17],

sup
α∈P(Id)

inf
z

g(x, z)

α, z

− h

i ≥

sup
α∈P(Id)

inf
z:−M2≤z≤M2

g(x, z)

35

α, z

− h

i −

ǫ

(40)

 
 
 
 
 
for some M2 = poly(L, log 1

ǫ ). We now claim that

sup
α∈P(Id)

inf
z:−M2≤z≤M2

g(x, z)

α, z

− h

i ≥

inf
z∈Z:−M3≤z≤M3

g(x, z)

(41)

for some M3 = poly(L, log 1

ǫ ).

The proof is similar to the proof of Claim B.11 as follow(s). First, we switch the supremum
and inﬁmum on the left-hand side of (41) using Claim B.10. Second, we ﬁx a solution z⋆ of the
outer inﬁmum and observe that α⋆ achieving inner supremum is in the form of 1S for some S
∈ Id.
Next, we construct ˆz = z⋆
and argue that ˆz is feasible to the right-hand side of (41) and
achieves the same objective g(x, ˆz) as the left-hand side of (41). This same argument follows here
except that the feasibility constraint on the right-hand side of (41) contains
M3. To
≤
z⋆
remedy, we ﬁrst set M3 = 2M2. By
2M2, and
therefore ˆz is now feasible for the right-hand side of (41).

M2, we have that 2M2 ≤

M2 ≤
Combining (36), (40), and (41), we obtain

M3 ≤
−
z⋆(S)
−

z⋆(S)
d

d ≤

≤

−

−

z

z

g(x, z)

inf
z∈Z

inf
z∈Z:−M3≤z≤M3

≥

g(x, z)

ǫ.

−

(42)

Therefore, the statement to be proved follows from (38) and (42) with M = max

.
M1, M3}
M , M ]n for some M of
polynomial size, which allows us to set the initial ellipsoid of the ellipsoid algorithm. Consequently,
the size of x, z during the run of the algorithm are polynomial in input size.

By Lemma B.9, we now assume that the feasible region are bounded in [

−

{

✷

Next, we show Lipschitz property of g(x, z) and that its Lipschitz constant is bounded by the

complexity of the input.

Lemma B.12 Let v1, . . . , vn ∈
is concave
in x and convex in z. Furthermore, for x0 and z0 of size polynomial in the input size L such
that infz∈Z g(x0, z) is ﬁnite, there exists a polynomial p(L) such that g(x, z0) as a function of x is
2p(L)-Lipschitz, and that g(x0, z) as a function of z is p(L)-Lipschitz.

Rd. The function g(x, z) = log det

n
i=1 xieziviv⊤
i

(cid:0)P

(cid:1)

We note that it is required for the Lipschitz constants of g(x, z) in x and in z to be at most 2poly(L)
so that the ellipsoid algorithm to be introduced has small error.
Proof: The proof of concavity of g(x, z) in x follows from the concavity of log-determinant function
[BV04]. The Lipschitz property in x follows from the calculation of gradient of log-determinant
Sn and symmetric matrix Y of Frobenius norm
function in the proof in [BV04]: for a matrix X
1, the function h(t) = log det(X + tY) (on t such that X + tY
0) has derivative

∈

log det(X) + log det

(cid:23)
I + tX− 1

2 YX− 1

2

n

log

(1 + t

λi

·

2 YX− 1

2

(cid:16)
X− 1
(cid:16)

(cid:17)(cid:17)

!!

(cid:17)

Yi=1
X− 1

2 YX− 1

2

d
dt
d
dt  

(cid:16)

d
dt

h(t) =

=

=

n

λi

1 + t

Xi=1

(cid:16)
λi
·

(cid:17)
2 YX− 1

2

X− 1
(cid:16)

(cid:17)

where λi (M) is the ith eigenvalues of matrix M. For any z = poly(L), we have that the Frobenius
is at most 2poly(L) (due to the exponent in zi). Hence, for a symmetric
norm of X =
2 YX− 1
matrix Y of Frobenius norm 1, the Frobenius norm of X− 1
2 is at most 2poly(L). Hence,

n
i=1 xieziviv⊤
i

P

36

 
2

2 YX− 1

λi
property on h(λ) implies 2poly(L)-Lipschitz property on g(x, z) by the chain rule of derivative.
(cid:12)
(cid:12)
(cid:12)

X− 1
(cid:16)
Observe that, by the Cauchy-Binet formula,

2poly(L) for each i, and therefore d

2poly(L). The 2poly(L)-Lipschitz

dt h(t)

(cid:17)(cid:12)
(cid:12)
(cid:12)

≤

≤

g(x, z) = log



XR⊆[n]:|R|=d


i∈R xiviv⊤
i

det

xieziviv⊤
i

Xi∈R

!


= log



XR⊆[n]:|R|=d



ez(R)cR


(43)

(cid:0)P

(cid:1)

where cR = det
function. The convexity in z of log-sum-exponential functions is proven in [BV04].

, showing that g(x, z) as a function of z is a log-sum-exponential

The Lipschitz property of g(x, z) in z can be shown by a direct calculation of gradient. Let x0

be such that infz∈Z g(x0, z) is ﬁnite. Then,

∂
∂zi

g(x, z) =

R⊆[n]:|R|=d,i∈R ez(R)cR
R⊆[n]:|R|=d ez(R)cR

P

which implies 0

∂
∂zi

g(x, z)

≤

1 as cR ≥

≤

P

0 for all R. Hence,

z(x, z)

1
||2 ≤

≤

√n.

||∇

✷

Since g(x, z) as a function of z is convex, we obtain an eﬃcient oracle

inﬁmum problem using an ellipsoid method:

to solving the inner

F

, error ǫ.

Input: x0 ∈ X
Output: z

•

•

such that g(x0, z)

≤

infz∈Z g(x0, z) + ǫ.

∈ Z

Because the sizes of the inﬁma and Lipschitz constants are bounded by poly(L), the oracle
in time poly(L, log( 1

runs

F

ǫ )).

We now present an oracle

for solving the feasibility problem of CP, namely Det-Feasibility,

G

using the oracle

.

F

Lemma B.13 There exists an oracle

for the following problem:

G

Input: target β

•

Output: ¯x

•

∈ X
Moreover, the oracle

R, error ǫ.

∈
such that inf z∈Z g(¯x, z)

runs in poly(L, log( 1

G

≥
−
ǫ )) time.

β

ǫ, or a proof that supx∈X infz∈Z g(x, z) < β.

Proof: We assume without loss of generality that for all x
, infz∈Z g(x, z) is ﬁnite (by Lemma
B.8 and we can check in polynomial time if infz∈Z g(x, z) is ﬁnite) and that f (x) attains its minimum
at a ﬁnite z (by Lemma 2.1).
We initiate the oracle

as an ellipsoid algorithm with an ellipsoid containing

∈ X

. At an iteration
, then we can eﬃciently ﬁnd
to get a separating hyperplane and continue.

∈ X

X

return zt such that

G

t of the algorithm, we denote xt the center of the ellipsoid. If xt /
a violating constraint in
If xt ∈ X

due to the structure of
X
with input xt and ǫ
4 . Let

, we call the oracle

F

F

X

g(xt, zt)

inf
z∈Z

≤

g(xt, z) +

ǫ
4

.

If g(xt, zt)

β

−

≥

ǫ
2 , then let

G

return xt. In this case, we have

inf
z∈Z

g(xt, z)

≥

g(xt, zt)

ǫ
4 ≥

β

−

ǫ
2 −

ǫ
4 ≥

−

β

ǫ

−

37

 
as needed.

−

ǫ
2 , then return the separating hyperplane

Otherwise, if g(xt, zt) < β
to the ellipsoid algorithm (note that

xti ≥
xg(x, z) has a closed-form expression and can be eﬃciently
0
}
calculated). We now claim that the returned hyperplane is valid, i.e. that any point x⋆ such that
β satisﬁes the constraint as given by the separating hyperplane. Let x⋆ be such
inf z∈Z g(x⋆, z)
that inf z∈Z g(x⋆, z)
0. By concavity
of g(x, z) in x,

β, and therefore g(x⋆, zt)

β. Thus g(x⋆, zt)

xg(xt, zt), x

g(xt, zt)

x :
{

h∇

∇

≥

−

≥

−

≥

≥

g(x⋆, zt)

0

≤

g(xt, zt)

−

≤ h∇

xg(xt, zt), x⋆

xti

−

as claimed.

Finally, if the ellipsoid algorithm

then we have g(xt, zt) < β
xt −
k

k2 ≤

x⋆

−

ǫ
2 . Moreover, if there exists x⋆

ends without returning any point after the tth iteration,
β, then

such that g(x⋆, zt)

G

∈ X

≥

ηt where ηt denotes the radius of the ellipsoid at iteration t. But then we have

ǫ
2

< g(x⋆, zt)

g(xt, zt)

−

2p(L)

x⋆
k

xtk2 ≤

−

≤

2p(L)

ηt

·

(44)

It remains to run the algorithm until 2p(L)

where 2p(L) is the Lipschitz constant (Lemma B.12). We run the algorithm until 2p(L)
that we get a contradiction in (44). This is the proof that supx∈X inf z∈Z g(x, z) < β, as claimed.
poly( 1
ηt < ǫ
ǫ ).
Since in an ellipsoid algorithm, the radius ηt shrinks exponentially in t, we only need T = O(log M
)
ηt
iterations, where M is the size of the initial ellipsoid. By Lemma B.9, we may choose M such that
ǫ )). Therefore, we have T = O(log M + log 1
log M = poly(L, log( 1
ǫ )). Each of
ηt
these T iterations runs in poly(L, log( 1
ǫ )) time, so the total runtime of the ellipsoid algorithm is
✷
poly(L, log( 1

2 , which is equivalent to 1
ηt

) = poly(L, log( 1

= 2poly(L)

2 , so

ηt < ǫ

·

·

·

ǫ )).

The existence of the polynomial-time algorithm for Det-Feasibility ﬁnishes the proof of

Theorem B.7.

C Optimality Conditions

In this section, we prove Lemma 2.4. We use Theorem A.10 (Strong Duality) with

h(z) = g(x, z) = log det

n

Xi=1

xieziviv⊤
i

,

!

0

{

z

Z

=

Rn : z(S)

where x will be either x⋆ or ˆx. For any ﬁxed x, g(x, z) is convex in z (Lemma B.12). We can
for a matrix A whose rows
)
∈ Id(
write
S
M
}
∀
∈
≥
). Thus, for any x, the optimization problem inf z∈Z g(x, z)
are indicator vectors of sets in
Id(
M
where h(z) = g(x, z) is a convex function with domain Rn.
Az
can be written as inf
0
}
≤
contains, for example, the
As we assumed the inﬁmum is achieved, it must also be ﬁnite. Since
non-negative orthant of Rn, it is not empty, and the assumptions of Theorems A.10 and A.11 hold.
Note that we apply the theorems with ℓ = 0.

h(z) :
{

Rn : Az

z
{

0
}

as

=

−

≥

Z

Z

∈

Let us ﬁrst show Lemma 2.4 before “moreover”. Observe, ﬁrst, that by condition 3.
of
Lemma 2.4, we have that g(x⋆, z⋆) = g(ˆx, z⋆). Therefore, to show that f (ˆx) = f (x⋆), it is enough to
show that z⋆ achieves an inﬁmum in infz∈Z g(ˆx, z). We do so by verifying that conditions a. and b.
of Theorem A.11 hold for z⋆ and λ. Indeed, condition 1. of Lemma 2.4 is exactly the condition a.

38

 
of Theorem A.11. Moreover, the function g(ˆx, z) is diﬀerentiable in z, and its partial derivatives at
z⋆ are given by

∂g
∂zi
i . We also have (M⊤λ)i =

(ˆx, z⋆) = ˆxiez⋆

i v⊤

i X−1vi,

n

i=1 ˆxiez⋆

i viv⊤

for X =
S∈Id(M):i∈S λS, and, therefore,
condition 2. of Lemma 2.4 is exactly condition b. of Theorem A.11. Lemma 2.4 before “moreover”
now follows by Theorem A.11.

P

P

−

−

(A⊤λ)i =

Next we establish Lemma 2.4 after “moreover”. The Lagrangian of infz∈Z g(x⋆, z) equals

L(z, λ) = g(x⋆, z)

λSz(S).

−

XS∈Id(M )

Theorem A.10 implies that there exists λ⋆

R

Id(M)
≥0

∈

such that

inf
z∈Rn

L(z, λ⋆) = inf
z∈Z

g(x⋆, z) = g(x⋆, z⋆).

Therefore, conditions a. and b. hold for z⋆ and λ⋆, and, as we argued above, they are equivalent
to conditions 1. and 2. of Lemma 2.4.

D Proofs from Section 3

Rn over the ground set [n] and a smaller ground set ˜

For a vector x
[n], we denote by
x| ˜U ∈
. We show that restricting the ground set to the support of a
R
solution of CP preserves the optimal value and optimal solutions (after restricting to the support
of the inner inﬁmum.

˜U the vector x restricted to ˜
U

U ⊆

∈

Lemma D.1 Let x⋆ be a feasible solution to CP. Let ˜
U
( ˜
U

) be a matroid where

, ˜
I

= supp(x) and let ˜x⋆ = x⋆
| ˜U

. Let ˜
M

=

Deﬁne ˜
Z

and ˜g : R

˜U

˜U

R

×

→

R as

.

S

n

=

: S

˜
˜
∈ I
U
I
⊆
o
and g restricted to ˜
U
∈ Id( ˜
M

˜U : ˜z(S)

by

≥

R

S

∀

0

∈

)

o

=

˜
Z

Z

˜z

n

and

Then,

˜g(˜x, ˜z) = log det

xieziviv⊤

i 

.







Xi∈ ˜U

inf
z∈Z

g(x⋆, z) = inf
˜z∈ ˜Z

˜g(˜x⋆, ˜z).

(45)

Moreover, for any z⋆

inf z∈Z g(x⋆, z), we have ˆz⋆

∈
Proof: We ﬁrst show that infz∈Z g(x⋆, z)
˜
Z
inf ˜z∈ ˜Z ˜g(˜x⋆, ˜z).

≥

inf ˜z∈ ˜Z ˜g(˜x⋆, ˜z).

| ˜U ∈

inf ˜z∈ ˜Z ˜g(˜x⋆, ˜z). For any z

and that by the deﬁnitions of g, ˜g we have g(x⋆, z) = ˜g(˜x⋆, z| ˜U ). Hence, infz∈Z g(x⋆, z)

∈ Z

, observe that z| ˜U ∈
≥

39

To prove the other direction of the inequality, let ˜z

adding zj for each j
), and hence z
Id(
Therefore, inf z∈Z g(x⋆, z) = inf ˜z∈ ˜Z ˜g(˜x⋆, ˜z), as claimed.

′ with value zj ≥

i∈U ′
\ U
. Again, g(x⋆, z) = ˜(˜x⋆, ˜z), so we have inf z∈Z g(x⋆, z)

∈
∈ Z

M

[n]

P

≤

˜
. We then construct z
Z
. This ensures that z(S)

∈
zi|
|

from ˜z by

∈ Z
0 for all S
≥
∈
inf ˜z∈ ˜Z ˜g(˜x⋆, ˜z).

Next, let z⋆

inf z∈Z g(x⋆, z). Since g(x⋆, z⋆) = ˜g(˜x⋆, z⋆
| ˜U

we have ˆz⋆

| ˜U ∈

∈
inf ˜z∈ ˜Z ˜g(˜x⋆, ˜z).

) and infz∈Z g(x⋆, z) = inf ˜z∈ ˜Z ˜g(˜x⋆, ˜z),
✷

The following statement that minimum-weight bases form a matroid is standard; we include its

proof for completeness.

= ([n],

Lemma D.2 Let
denote a weight function and let
weight bases of
B
an independence oracle, then

. Then,

M

M

I

M

) be a matroid and let
S
{

∈ B
′ are bases of another matroid

: w(S) = minT ∈B w(T )
}
′ = ([n],

′ =

B

B

denote the set of bases of

M

. Let w : [n]

R
denote the set of minimum
′). Moreover, if
admits

→

′ admits an independence oracle.

M

I

M

Proof: A set system
that S1 6
([Whi35]).

B
= S2, we have that for all v

′ are bases of a matroid if it has an exchange property:

S1, there is u

S2 such that S1 + v

S1 \

∈

∀

′ such
S1, S2 ∈ B
′
u
−

∈ B

Let S1, S2 ∈ B

′ be such that S1 6
∈
the strong basis exchange property, there exists u
We consider diﬀerent cases based on wu, wv.

S2 \
∈

S1. Since S1, S2 are bases of matroid
S1 \

S2 such that S1 + v

u, S2 + u

−

−

M
v

, by
.

∈ B

∈

S2 \
= S2. Let v

If wu < wv, then w (S2 + u

If wu > wv, then w (S1 + v
basis. Therefore, wu = wv, and so S1 + v

−

v) < w(S2), a contradiction to S2 being a minimum-weight basis.
−
u) > w(S1), again a contradiction to S1 being a minimum-weight

−

u is also a minimum weight basis, as desired.
′, we check if Q is an independent set in

To test if a set Q is an independent set in

and if
M
/Q denote
the minimum-weight independent set in
M
in polynomial
after contracting Q. Since we can check if a set is independent in
the matroid
time and optimize a linear function over a matroid constraint in polynomial time, we can check if
✷
Q is independent in

/Q is equal to minT ∈B w(T )

′ in polynomial time.

w(Q). Here,

M

M

M

M

−

M

We now complete a missing proof of Lemma 3.4 using the uncrossing technique.

Proof of Lemma 3.4: Let x⋆ be an extreme solution to LPx-OPT. A chain
C1 corresponding
to tight linearly independent constraints in (7)-(8) of a matroid base polytope can be obtained
C2 with
by an uncrossing argument (see Lemma 5.2.4 of [LRS11]). We will show that a chain
similar property can be obtained for constraints (9)-(10). Let a = (ez⋆
⊆
[n], we denote aS a vector obtained from a by setting ai = 0 for each coordinate i /
S. Let
∈
be the set of tight constraints in (9)-(10). The
is also closed under union and intersection as follow(s).

i viX−1vi)i∈[n]. For S

i viX−1vi = r⋆(S)

F
uncrossing argument applies to show that
P

i∈S xiez⋆

( S

[n] :

=

⊆

F

(cid:9)

(cid:8)

∅

Lemma D.3 If A, B

, then A

B, A

B

. Moreover, aA + aB = aA∪B + aA∩B.

∈ F
Proof: The proof follows similarly from the proof for a base polytope (see Lemma 5.2.2 of [LRS11]).

∈ F

∪

∩

40

We have

r⋆(A) + r⋆(B) =

xiai +

xiai

Xi∈A

Xi∈B
xiai +

xiai

Xi∈A∪B
r⋆(A
∪
r⋆(A) + r⋆(B)

Xi∈A∩B
B) + r⋆(A
∩

B)

=

≤

≤

The ﬁrst equality is by A, B
. The ﬁrst inequality follows from constraints (9). The last inequal-
ity follows from submodularity of rank function of a matroid. The equality aA + aB = aA∪B + aA∩B
✷
is straight-forward from the basic set property.

∈ F

C2 follows similarly from the standard uncrossing
The rest of the proof to show an existence of
argument (Lemma 5.2.4 of [LRS11]). Note that chains
C2 obtained from the uncrossing argument
are in the same linear program with some tight constraints P in (11). However, we may remove
linearly dependent constraints when we take the set of constraints in
C2, P together until we
have linearly independent constraints.
Finally, LPx-OPT, which has n variables, must have n linearly independent constraints to specify
, there are
|
✷

an extreme solution. Since the number of tight constraints in (7)-(11) is
n
|C1|

tight constraints in (12). Therefore,

+
|C2|
P
+
|

supp(x)
|
|

+
|C2|

P
|
.
|

− |C1|

|C1|
+

C1,

C1,

|C2|

P
|

=

+

+

|

E Proofs from Section 4

Proof of Lemma 4.3: We prove this lemma using the inequality proven for log-concave polyno-
mials in [AGV18] (see Lemma A.16 in Appendix) by setting up an appropriate polynomial. For a
given x⋆, let g(y1, . . . , yn) = det

. For a matroid

= ([n],

), let

n
i=1 yix⋆

i viv⊤
i

M

I

⋆ be the set of all subsets of sets in W . That is,

Let

I

(cid:0)P

W =

[n]
{

(cid:1)
S
\

|

S

.
∈ Id}

⋆ =

I

S
{

⊆

[n]

T

| ∃

∈

W such that S

T

.
}

⊆

(46)

The following claim follows from the fact that for any matroid, independent sets of a ﬁxed size
form a basis of another matroid and that complements of these independent sets form a basis of
the dual matroid (see Chapter 2, Theorem 1 in [Wel10]).

Claim E.1 For a matroid
is a matroid with basis set W =

M

= ([n],
[n]
{

\

) and
I
S
S

|

I
.
∈ Id}

⋆ deﬁned as in (46), the set system

⋆ = ([n],

M

⋆)

I

Let h(z1, . . . , zn) be the bases generating polynomial of

⋆. That is,

M

h(z1, . . . , zn) =

Πi∈[n]\Szi =

z[n]\S

XS∈Id

XS∈Id

We use several lemmas, which can be found in Appendix A. For a nonzero scalar a
vector p

R and a
i=1 be a vector obtained from element-wise division. For vectors

Rn, we let p

a := [ pi

a ]n

∈

∈

41

Rn, we let yp :=

∈

y, z, p
. By Lemmas A.15 and A.17, both g and
h are completely log-concave polynomials. By Lemma A.18, g(y)h(z) is a completely log-concave
Q
polynomial. Hence, by Lemma A.16, for any p
∈

[0, 1]n,

Q

i and z1−p :=

i=1 z1−pi

i=1 ypi

n

n

i

(Πn

i=1(∂yi + ∂zi)) g(y)h(z)
y=z=0 ≥
|

We ﬁrst simplify the left-hand side of (47).

p
e2

(cid:16)

p

(cid:17)

inf
y,z∈Rn
>0

g(y)h(z)
ypz1−p .

(47)

Claim E.2 We have (Πn

i=1(∂yi + ∂zi)) g(y)h(z)
y=z=0 =
|

Proof: By the Cauchy-Binet formula,

det

S∈Id

i∈S x⋆

i viv⊤
i

P

(cid:0)P

(cid:1)

g(y) = det

n

Xi=1

yix⋆

i viv⊤
i

=

!

det

XS∈([n]
d )

Xi∈S

yix⋆

i viv⊤
i

=

!

yS det

XS∈([n]
d )

Xi∈S

i viv⊤
x⋆
i

.

!

By deﬁnition, h(z) =

z[n]\S. Hence,

S∈Id

P

g(y)h(z) =

XS1∈([n]

d ) XS2∈Id

yS1z[n]\S2 det

i viv⊤
x⋆

i 

.







Xi∈S1

So, we have

(Πn

i=1(∂yi + ∂zi)) g(y)h(z) =





XT ⊆[n]

Πi∈T ∂yiΠj∈[n]\T ∂zj 


XS1∈([n]

d ) XS2∈Id

yS1z[n]\S2 det





Xi∈S1

i viv⊤
x⋆

i 

=

XS1∈([n]

d ) XS2∈Id





XT ⊆[n]

Πi∈T ∂yiΠj∈[n]\T ∂zj

yS1z[n]\S2

det





(cid:0)

(cid:1)


i viv⊤
x⋆

i 

.



Xi∈S1




Πi∈T ∂iΠj∈[n]\T ∂j

yS1z[n]\S2

(cid:0)

(cid:1)

It is easy to see that for T

[n] and S1, S2 such that

is equal to 1 if T = S1 = S2 and 0 otherwise. Hence,

⊆

=

S1|
|

,
S2|
|

(Πn

i=1(∂yi + ∂zi)) g(y)h(z)
y=z=0 =
|

det

XS∈Id

Xi∈S

i viv⊤
x⋆
i

!

ﬁnishing the proof of the claim.

Next, we reformulate the right-hand side of (47). For vectors y, w, p
n
i=1(yiwi)pi and wp−1 :=
.

i=1 wpi−1

n

i

✷

Rn, we let (yw)p :=

∈

Q
Claim E.3 We have

Q

inf
y,z∈Rn

>0

g(y)h(z)
ypz1−p = inf
y,w∈Rn

>0

det

(cid:0)P

42

n
i=1 x⋆

i yiviv⊤
i
(yw)p

(cid:1) (cid:16)P

wS

S∈Id

(cid:17)

 
 
 
 
Proof: By a change of variable w = 1/z coordinate-wise, we have

inf
y,z∈Rn

>0

g(y)h(z)
ypz1−p = inf
y,w∈Rn

>0

g(y)h(1/w)
ypwp−1

Substituting g and h by their deﬁnitions, we get

inf
y,z∈Rn
>0

g(y)h(z)
ypz1−p = inf
y,w∈Rn
>0

det

det

n
i=1 x⋆

i yiviv⊤
i
(cid:1) (cid:16)P
ypwp−1

(cid:0)P

wS /w[n]

S∈Id

(cid:17)

= inf

y,w∈Rn

>0

(cid:0)P

n
i=1 x⋆

i yiviv⊤
i
(yw)p

(cid:1) (cid:16)P

wS

S∈Id

(cid:17)

as claimed.

✷

Applying the two claims above to the left- and right-hand sides of (47), we get that for any
[0, 1]n,

∈

p

det

i viv⊤
x⋆
i

XS∈Id

Xi∈S

p
e2

p

(cid:17)

! ≥

(cid:16)

det

inf
y,w∈Rn

>0

(cid:0)P

n
i=1 x⋆

i yiviv⊤
i
(yw)p

(cid:1) (cid:16)P

wS

S∈Id

.

(cid:17)

In particular, if we consider all p

Id), we get
(

∈ P

det

XS∈Id

Xi∈S

i viv⊤
x⋆
i

sup
p∈P(Id)

! ≥

det

inf
y,w∈Rn

>0

(cid:0)P

p
e2

(cid:16)

p

(cid:17)

For any p

Id), we have
(

∈ P

n
i=1 pi = d. Hence,

n
i=1 x⋆

i yiviv⊤
i
(yw)p

(cid:1) (cid:16)P

wS

S∈Id

.

(cid:17)

P
i viv⊤
x⋆
i

! ≥

e−2d

sup
p∈P(Id)

inf
y,w∈Rn
>0

det

n
i=1 x⋆

i yiviv⊤
i
p
(cid:1) (cid:16)P
yw
p

(cid:0)P

wS

S∈Id

.

(cid:17)

det

XS∈Id

Xi∈S

By changing variable p to α, we get the desired result.

(cid:16)

(cid:17)

✷

Proof of Lemma 4.4: Let R = inf z∈Z det
we get

(cid:0)P

n
i=1 x⋆

i eziviv⊤
i

. By the change of variable yi = ezi,

R =

inf
y>0:∀S∈Id,yS ≥1

det

.

!

(48)

n

(cid:1)
i yiviv⊤
x⋆
i

Xi=1

We now claim a condition to check the feasibility of y of the inﬁmum (48).

Claim E.4 For any y

yS

≥

Proof: Suppose yS

Rn

≥0,

∈
1 for all S

1 for all S

≥
λS = 1 and α =

S∈Id

S∈Id

P

∈ Id if and only if yα
∈ Id. Let α

1 for all α

Id).
(
Id). Then, there exists λ
(

∈ P

∈ P

≥

∈

λS1S where 1S is an indicator vector of a set S. Then, we have

RId

≥0 such that

P

yα = yPS∈Id

λS 1S = ΠS∈Id (Πi∈Syi)λS

ΠS∈Id1λS

1,

≥

≥

43

 
 
 
 
proving one direction of the claim. Next, suppose that yα
1S ∈ P

∈ Id, so we may use yα
Applying the above claim to (48), we get

Id) for any S
(

≥

≥
1 with α = 1S . Hence, yS

1 for any α

Id). Note that
(
∈ P
∈ Id. ✷
1 for any S

≥

n

det

R =

inf
y>0:∀α∈P(Id),yα≥1

Xi=1
is a degree d polynomial in y,

i yiviv⊤
x⋆
i

.

!

Since det

n
i=1 x⋆

i yiviv⊤
i

(cid:0)P

R = inf
y>0

(cid:1)
det

i yiviv⊤
i

n
i=1 x⋆
infα∈P(Id) yα
(cid:0)P

(cid:1)

= inf
y>0

sup
α∈P(Id)

det

i yiviv⊤
i

n
i=1 x⋆
yα

.

(cid:1)

(cid:0)P

Applying Claim B.10 (Sion’s minimax theorem) on log R, we get

R = inf
y>0

sup
α∈P(Id)

det

(cid:0)P

i yiviv⊤
i

n
i=1 x⋆
yα

= sup

α∈P(Id)

inf
y>0

(cid:1)

det

i yiviv⊤
i

n
i=1 x⋆
yα

.

(cid:1)

(cid:0)P

(49)

Next, we relate the right-hand side of (49) to the left-hand side of the inequality in Lemma 4.4

by the following claim. We denote

Claim E.5 For any w

0 and α

≥

w
α

α

:=

n
i=1

wi
αi

(cid:16)
(cid:1)
(cid:0)
Q
Id), we have
(
∈ P

(cid:17)

αi

.

wS

S∈Id

≥

α

.

w
α

(cid:1)

(cid:0)

P

Setting ζ =
such that µ(S)
generating polynomial for µ is gµ(z) =

Proof: We assume that α is strictly inside the base polytope with the base set
Id. If not, we
can focus on the matroid with bases corresponding to the vertices of the smallest face in
Id)
(
containing α. By Proposition 2.3 in [FS05], every face of a matroid polytope is a matroid polytope.
R+ and λ1, . . . , λn > 0
µ(S)1S. The
λSzS, which we claim to be log-concave. By
zS is log-concave. By Lemma A.13, substituting zi by λizi in and multiplying
1
zS preserve log-concavity. Hence, gµ(z) is
PS∈Id

Id and p = α in Lemma A.19, we get a distribution µ : 2Id
∈ Id. Moreover, αi = PS∼µ[i
∝

Lemma A.15,
with a constant
P

λS to the polynomial

→
S] and α =

1
PS∈Id

λS for S

S∈Id

S∈Id

S∈Id

S∈Id

P

P

P

λS

∈

log-concave, as claimed, and so µ is a log-concave distribution.

P

By Lemma A.20,

which is equivalent to

Now, we are ready to prove the claim. We have

µ(S) log

1

µ(S) ≥

αi log

1
αi

n

Xi=1

XS∈Id

1
(µ(S))µ(S) ≥

1
ααi
i

=

1
αα .

n

Yi=1

YS∈Id

µ(S)1S (cid:17)

µ(S)

w1S
µ(S)

(cid:19)

(cid:18)

= ΠS∈Id

wS.

w
α

(cid:16)

α

=

(cid:17)

≤

≤

w(cid:16)PS∈Id
αα

wα
αα =
ΠS∈Idwµ(S)1S
ΠS∈Idµ(S)µ(S)
w1S =

XS∈Id

XS∈Id

44

 
where the last inequality follows from the weighted AM-GM inequality since

µ(S) = 1. ✷

S∈Id

We continue of the proof of the lemma. By (49) and Claim E.5, for any w

P
≥

0, we have

R

sup
α∈P(Id)

inf
y≥0

≤

det

i yiviv⊤
i

n
i=1 x⋆
yα

(cid:1)

(cid:0)P

wS
α .

S∈Id
w
α

P

Therefore,

(cid:0)

(cid:1)

R = inf
z∈Z

det

i eziviv⊤
x⋆
i

sup
α∈P(Id)

inf
y,w>0

! ≤

det

n

Xi=1

ﬁnishing the proof of the lemma.

n
i=1 x⋆

i yiviv⊤
i
α
yw
(cid:1) (cid:16)P
α

wS

S∈Id

(cid:17)

(cid:0)P

(cid:0)

(cid:1)

✷

F Oblivious Rounding Scheme

In this section, we show that none of the previous approaches for Determinant Maximization
yield an approximation factor independent of the size of the output solution k even if the dimension
of the vectors d is 2. Formally, we show that any relaxation and rounding schemes satisfying the
following properties cannot achieve an approximation factor independent of k.

•

•

Let the relaxation be supx∈P(M) g(x) for some function g. Then, for any x
write as x =

0, we have g(x)

(
M
maxT ∈B det

T ∈B λT = 1 and λ

T ∈B λT 1T for

∈ P

) which we
λT

i∈T viv⊤
i

≥

≥
with probability dependent

P

(cid:0)

.

(cid:1)

P
Given x
(
M
only on x and

∈ P

), the rounding scheme outputs a solution T

P

(and so independent of vi’s).

∈ B

M
We construct an instance as follow(s).

∈ P
b, c
}

(
M
and an edge set E =

Matroid
and x
M
V =
a1, . . . , am} ∪ {
{
of matroid
M

): Consider the graphic matroid with a graph G on n + 2 vertices
. All spanning trees of G are bases
[m]
}
2m for every edge e

. Consider a fractional spanning tree x⋆ such that x⋆
Let the rounding scheme pick a subset of edges with distribution µ : 2E

R+. Since the
rounding scheme outputs a basis of the matroid, it must be that µ(F ) = 0 if the graph (V , F ) has
a cycle. Suppose we sample a subgraph as per distribution µ. We let

aib, aic
{

e = m+1

E.

→

∈

∈

i

|

Bi := the event that both aib and aic are picked.

We now prove some properties about these events.

Claim F.1 For any i

= j, P[Bi ∩

Bj] = 0. Hence,

i

∃

∈

[m] such that P[Bi]

1
m .

≤

Proof: If both Bi and Bj occur, then our sampled subgraph contains edges aib, aic, ajb, ajc which
implies that there is a cycle in the subgraph. However, by the deﬁnition, µ(F ) = 0 if the sampled
1, so
subgraph (V , F ) has a cycle. Hence, P[Bi ∩
✷
there exists i

Bj] = 0. Therefore,

m
i=1 P[Bi] = P[

[m] such that P[Bi]

m
i=1Bi]

≤

∪

∈

1
m .

≤

P

We continue constructing the instance with the description of input vectors.

45

 
6
Vector Set: Consider the vector set as follows: vaib =

2
0

(cid:21)

(cid:20)

, vaic =

0
2

(cid:20)

, and for j

= i,

(cid:21)

. Since the rounding scheme is oblivious to the set of vectors, we can make

vaj b = vaj c =

(cid:20)
such a selection.

0
0

(cid:21)

By the assumption on the relaxation, we have that g(x⋆)

=
m2 > 1. The rounding scheme with distribution µ outputs a solution with non-zero value only

2m vaicv⊤
aic

m+1
2m vaibv⊤

aib + m+1

det

≥

(cid:1)

(cid:0)

(m+1)2

if both aiu and aiv are picked. Hence, the expected objective value of the solution returned is

and the approximation factor achieved is larger than 1
factor tends to inﬁnity even for d = 2.

4
m

(cid:16)

P[Bi] det

vaibv⊤

aib + vaicv⊤
aic

4
m

≤

(cid:17)
= m

4 . As m

, the approximation

→ ∞

To construct a similar instance for d > 2, we add vectors vi =

and include them in the bases of the matroid.

0i−1
1
0d−i 






for each i

3, . . . , d
}

∈ {

G Improved Approximation for a Partition Matroid

In this section, we show an e3d-estimation algorithm for Determinant Maximization under a
partition matroid. Algorithm 2 and the same analysis of the algorithm will imply an eﬃcient
derandomization with approximation factor exp(O(d3)) for a partition matroid.

Theorem G.1 There is an eﬃciently computable convex program whose objective value estimates
the objective of Determinant Maximization problem under a partition matroid constraint within
a multiplicative factor of eO(d).

We start by discussing the rounding scheme presented in Algorithm 3.

Algorithm 3 Rounding Scheme for a Partition Matroid
) with bases

= ([n],

1: Input: a partition matroid
2: Output: a basis T
3: Sample a set T
4: Return T

∈ B

M

I

∈ B
with probability

xT
PR∈B xR

, and x

)
(
M

∈ P

B

To see that Algorithm 3 is polynomial time, observe that if we sample a set W of bi elements
from the partition Pi with probability proportional to xW for each i, then our sample would be a set
T
PR∈B xR . Such a sampling can be done eﬃciently as proved by Singh and
Xie [SX18]. Next, we show that for every independent set S of size d, we sample a basis containing
S with a large probability.

with probability

∈ B

xT

Lemma G.2 Let T denote the random set returned by Algorithm 3. Then, for any set S
have

∈ Id, we

≥
The statement then implies a lower bound on the expected objective value of the solution

⊆

P[S

T ]

e−dxS.

returned.

46

6
Lemma G.3 Algorithm 3 returns a basis T

with expected objective value

∈ B

det

E

"

viv⊤
i

!# ≥

Xi∈T

e−d

det

XS∈Id

Xi∈S

xiviv⊤
i

!

Next, we relate this lower bound of the objective of the convex relaxation CP by using Lemma 4.3
and Lemma 4.4. Before we prove these lemmas, we prove Theorem G.1.
Proof of Theorem G.1: We start by solving the convex relaxation CP (which can be done in
polynomial time from Theorem B.7). Let x⋆ be an optimal solution to CP (same argument works
for a near optimal solution as well). Let T
be the random solution returned by Algorithm 3
with input x = x⋆. By Lemma G.3, the expected value of the solution returned is

∈ B

det

E

"

viv⊤
i

!# ≥

Xi∈T

e−d

det

XS∈Id

Xi∈S

xiviv⊤
i

.

!

By Lemmas 4.3 and 4.4, the right-hand side of the above inequality is further bounded, and we get

det

E

"

Xi∈T

viv⊤
i

!# ≥

e−d

e−2d inf
z∈Z

·

det

Since x⋆ is an optimal solution to CP, we have inf z∈Z det

n

i eziviv⊤
x⋆
i

Xi=1
n
i=1 x⋆

(cid:0)P

.

!

(cid:1)

i eziviv⊤
i

= OPTCP which is

at least OPT. Hence, we get a random solution T in polynomial time with expected value

E

det

"

Xi∈T

viv⊤
i

!# ≥

e−3d

·

OPT

which ﬁnishes the proof.

✷

To prove Lemma G.2, we make use of a similar result proved by Singh and Xie [SX18] in the

context of a uniform matroid.

Theorem G.4 (Proposition 2 in [SX18]) For a uniform matroid with rank at least d and a frac-
tional solution y in the matroid polytope, if we sample a basis Q with probability yQ, then for each
set W of size d, all elements of W are selected with probability at least e−dyW .
+ be a vector such that

m
i=1 yi = ℓ where ℓ is an integer. Then for

Rm

More formally, let y
[m] such that

any W

⊆

∈
W
|

ℓ, we have

| ≤

P

Q∈([m]

ℓ ):W ⊆Q yQ
ℓ ) yQ
Q∈([m]

P

≥

e−|W |yW .

Proof of Lemma G.2: A set T
S

∈ Id, we have

P

∈ B

is sampled with probability

xT

PR∈B xR . Hence, for any set

P[S

⊆

T ] =

XT ∈B:S⊆T

xT
R∈B xR =

T ∈B:S⊆T xT
R∈B xR .

P

(50)

Let the partition matroid be
M
let the rank of Pi be bi. For any i

= ([n],

P

P
) with partitions P1, . . . , Pt such that

t
i=1Pi = [n] and
Pi. Then, the numerator and denominator of

∪

[t], let Si = S

I

∈

∩

47

 
 
 
 
 
 
 
the right-hand side of (50) can be decomposed into products across each partition as

R∈B xR =

t
i=1

Q

(cid:18)

P

) xR

R∈(Pi
bi

(cid:19)

and

T ∈B:S⊆T xT =

t
i=1

(cid:18)

Q

t

):Si⊆T xT

T ∈(Pi
bi

P
T ∈(Pi
bi

):Si⊆T xT
) xR

.

R∈(Pi
bi

P

P[S

T ] =

⊆

P

Yi=1
xi = bj. Since S

Since x
i∈Pj
Applying Theorem G.4, we get

), we have

(
M

∈ P

P

P
∈ Id, we also have

=

Sj|
|

S
|

Pj| ≤

∩

bj =

xi.

i∈Pj

P

. Therefore,

P

(cid:19)

Since S

∈ Id, we have d =

S
|

|

=

| ∪

P[S

⊆

T ]

≥

t

e−|Si|xSi = e−

P

t

i=1 |Si|x∪t

i=1Si.

t

Yi=1
i=1 Si|
P[S

=

. Therefore,

t
i=1 |
T ]

Si|
e−dxS

≥

P
⊆

as desired.

We now prove Lemma G.3.
Proof of Lemma G.3: Let T
Cauchy-Binet formula, we have

∈ B

be the random set returned by Algorithm 3. Then, by the

✷

det

E

"

viv⊤
i

!#

= E



Xi∈T

XS⊆T :|S|=d

Xi∈S


∈ Id, we have P[S
By Lemma G.2, for each set S

T ]

⊆

≥

⊆

XS∈([n]
d )

!

e−dxS. Therefore,

det

viv⊤
i

=

P[S

T ] det

viv⊤
i

.

!

Xi∈S

E

det

"

viv⊤
i

Xi∈T

!# ≥

XS∈Id

e−dxS det

viv⊤
i

!

Xi∈S

= e−d

det

XS∈Id

Xi∈S

xiviv⊤
i

.

!

✷

48

 
 
 
 
 
 
