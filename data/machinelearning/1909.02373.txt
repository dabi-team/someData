1
2
0
2

n
u
J

7
2

]
L
M

.
t
a
t
s
[

3
v
3
7
3
2
0
.
9
0
9
1
:
v
i
X
r
a

LSMI-Sinkhorn: Semi-supervised Mutual
Information Estimation with Optimal Transport

Yanbin Liu[(cid:0)]1(cid:63), Makoto Yamada2,3(cid:63), Yao-Hung Hubert Tsai4, Tam Le3,
Ruslan Salakhutdinov4, and Yi Yang1

1AAII, University of Technology Sydney
2Kyoto University, 3RIKEN AIP, 4Carnegie Mellon University
csyanbin@gmail.com

(xi, yi)
}
{

Abstract. Estimating mutual information is an important statistics and
machine learning problem. To estimate the mutual information from
n
data, a common practice is preparing a set of paired samples
i=1
i.i.d.
p(x, y). However, in many situations, it is diﬃcult to obtain a large
∼
number of data pairs. To address this problem, we propose the semi-
supervised Squared-loss Mutual Information (SMI) estimation method
using a small number of paired samples and the available unpaired ones.
We ﬁrst represent SMI through the density ratio function, where the ex-
pectation is approximated by the samples from marginals and its assign-
ment parameters. The objective is formulated using the optimal trans-
port problem and quadratic programming. Then, we introduce the Least-
Squares Mutual Information with Sinkhorn (LSMI-Sinkhorn) algo-
rithm for eﬃcient optimization. Through experiments, we ﬁrst demon-
strate that the proposed method can estimate the SMI without a large
number of paired samples. Then, we show the eﬀectiveness of the pro-
posed LSMI-Sinkhorn algorithm on various types of machine learning
problems such as image matching and photo album summarization. Code
can be found at https://github.com/csyanbin/LSMI-Sinkhorn

Keywords: Mutual information estimation · Density ratio · Sinkhorn
algorithm · Optimal transport.

1

Introduction

Mutual information (MI) represents the statistical independence between two
random variables [4], and it is widely used in various types of machine learning
applications including feature selection [20,21], dimensionality reduction [19],
and causal inference [23]. More recently, deep neural network (DNN) models have
started using MI as a regularizer for obtaining better representations from data
such as infoVAE [26] and deep infoMax [9]. Another application is improving the
generative adversarial networks (GANs) [8]. For instance, Mutual Information
Neural Estimation (MINE) [1] was proposed to maximize or minimize the MI

(cid:63) Equal contribution, (cid:0) Corresponding Author

 
 
 
 
 
 
2

Liu Y., Yamada M., Tsai YH., Le T., Salakhutdinov R., Yang Y.

in deep networks and alleviate the mode-dropping issues in GANS. In all these
examples, MI estimation is the core of all these applications.

In various MI estimation approaches, the probability density ratio function

is considered to be one of the most important components:

r(x, y) =

p(x, y)
p(x)p(y)

.

A straightforward method to estimate this ratio is the estimation of the
probability densities (i.e., p(x, y), p(x), and p(y)), followed by calculating their
ratio. However, directly estimating the probability density is diﬃcult, thereby
making this two-step approach ineﬃcient. To address the issue, Suzuki et al. [21]
proposed to directly estimate the density ratio by avoiding the density estimation
[20,21]. Nonetheless, the abovementioned methods requires a large number of
paired data when estimating the MI.

Under practical setting, we can only obtain a small number of paired samples.
For example, it requires a massive amount of human labor to obtain one-to-one
correspondences from one language to another. Thus, it prevents us to easily
measure the MI across languages. Hence, a research question arises:

Can we perform mutual information estimation using unpaired sam-

ples and a small number of data pairs?

To answer the above question, in this paper, we propose a semi-supervised MI
estimation approach, particularly designed for the Squared-loss Mutual Informa-
tion (SMI) (a.k.a., χ2-divergence between p(x, y) and p(x)p(y)) [20]. We ﬁrst
formulate the SMI estimation as the optimal transport problem with density-
ratio estimation. Then, we propose the Least-Squares Mutual Information with
Sinkhorn (LSMI-Sinkhorn) algorithm to optimize the problem. The algo-
rithm has the computational complexity of O(nxny); hence, it is computationally
eﬃcient. Through experiments, we ﬁrst demonstrate that the proposed method
can estimate the SMI without a large number of paired samples. Then, we vi-
sualize the optimal transport matrix, which is an approximation of the joint
density p(x, y), for a better understanding of the proposed algorithm. Finally,
for image matching and photo album summarization, we show the eﬀectiveness
of the proposed method.

The contributions of this paper can be summarized as follows:

– We proposed the semi-supervised Squared-loss Mutual Information (SMI)
estimation approach that does not require a large number of paired samples.
– We formulate mutual information estimation as a joint density-ratio ﬁtting
and optimal transport problem, and propose an eﬃcient LSMI-Sinkhorn
algorithm to optimize it with a monotonical decreasing guarantee.

– We experimentally demonstrate the eﬀectiveness of the proposed LSMI-
Sinkhorn for MI estimation, and further show its broader applications to
the image matching and photo album summarization problems.

LSMI-Sinkhorn

3

2 Problem Formulation

In this section, we formulate the problem of Squared-loss Mutual Information
(SMI) estimation using a small number of paired samples and a large number of
unpaired samples.

Formally, let X ⊂ Rdx be the domain of random variable x and Y ⊂ Rdy be
the domain of another random variable y. Suppose we are given n independent
and identically distributed (i.i.d.) paired samples:

{(xi, yi)}n

i=1,

where the number of paired samples n is small. Apart from the paired samples,
we also have access to nx and ny i.i.d. samples from the marginal distributions:

{xi}n+nx
i=n+1

i.i.d.∼ p(x) and {yj}n+ny
j=n+1

i.i.d.∼ p(y),

where the number of unpaired samples nx and ny is much larger than that of
paired samples n (e.g., n = 10 and nx = ny = 1000). We also denote x(cid:48)
i =
xi−n, i ∈ {n + 1, n + 2, . . . , n + nx} and y(cid:48)
j = yj−n, j ∈ {n + 1, n + 2, . . . , n + ny},
respectively. Note that the input dimensions dx, dy and the number of samples
nx, ny may be diﬀerent.

This paper aims to estimate the SMI [20] (a.k.a., χ2-divergence between
i=1 with the help of the extra unpaired
j=n+1. Speciﬁcally, the SMI between random vari-

p(x, y) and p(x)p(y)) from {(xi, yi)}n
samples {xi}n+nx
ables X and Y is deﬁned as

i=n+1 and {yj}n+ny

SMI(X, Y ) =

(cid:90) (cid:90)

1
2

(r(x, y)−1)2p(x)p(y)dxdy,

(1)

where r(x, y) = p(x,y)
p(x)p(y) is the density-ratio function. SMI takes 0 if and only if
X and Y are independent (i.e., p(x, y) = p(x)p(y)), and takes a positive value
if they are not independent.

Naturally, if we know the estimation of the density-ratio function, then we

can approximate the SMI in Eq. 1 as

(cid:100)SMI(X, Y ) =

1
2(n+nx)(n+ny)

n+nx(cid:88)

n+ny
(cid:88)

(rα(xi, yj)−1)2 ,

i=1

j=1

where rα(x, y) is an estimation of the true density ratio function r(x, y) param-
eterized by α. More details are discussed in §3.1.

However, in many real applications, it is diﬃcult or laborious to obtain suf-
ﬁcient paired samples for density ratio estimation, which may result in high
variance and bias when computing the SMI. In this paper, the key idea is to
align the unpaired samples under this limited number of paired samples setting,
and propose an objective to incorporate both the paired samples and aligned
samples for a better SMI estimation.

4

Liu Y., Yamada M., Tsai YH., Le T., Salakhutdinov R., Yang Y.

3 Methodology

In this section, we propose the SMI estimation algorithm with limited number
of paired samples and large number of unpaired samples.

3.1 Least-Squares Mutual Information with Sinkhorn Algorithm

We employ the following density-ratio model. It ﬁrst samples two sets of basis
vectors {(cid:101)xi}b

i=1 from {xi}n+nx
i=1

j=1 , then computes

i=1 and {(cid:101)yi}b

and {yj}n+ny

rα(x, y) =

b
(cid:88)

(cid:96)=1

α(cid:96)K((cid:101)x(cid:96), x)L((cid:101)y(cid:96), y) = α(cid:62)ϕ(x, y),

(2)

where α ∈ Rb, K(· , ·) and L(· , ·) are kernel functions, ϕ(x, y) = k(x)◦l(y) with
k(x) = [K((cid:101)x1, x), . . . , K((cid:101)xb, x)](cid:62) ∈ Rb, l(y) = [L((cid:101)y1, y), . . . , L((cid:101)yb, y)](cid:62) ∈ Rb.

In this paper, we optimize α by minimizing the squared error loss between

the true density-ratio function r(x, y) and its parameterized model rα(x, y):

Loss =

=

1
2
1
2

(cid:90) (cid:90) (cid:18)

rα(x, y) −

(cid:19)2

p(x, y)
p(x)p(y)

p(x)p(y)dxdy

(cid:90) (cid:90)

rα(x, y)2p(x)p(y)dxdy −

(cid:90) (cid:90)

rα(x, y)p(x, y)dxdy + const. (3)

For the ﬁrst term of Eq. (3), we can approximate it by using a large number
of unpaired samples as it only involves p(x), p(y). However, to approximate
the second term, paired samples from the joint distribution (i.e., p(x, y)) are
required. Since we only have a limited number of paired samples in our setting,
the approximation of the second term may have high bias and variance.

To deal with this issue, we leverage the abundant unpaired samples to help
the approximation of the second term. Since we have no access to the true
pair information for these unpaired samples, we propose a practical way to es-
timate their pair information. Speciﬁcally, we introduce a matrix Π (πij ≥ 0,
(cid:80)nx
j=1 πi,j = 1) that can be regarded as a parameterized estimation of the
i=1
joint density function p(x, y). Then, we approximate the second term of Eq. (3)

(cid:80)ny

(cid:90) (cid:90)

rα(x, y)p(x, y)dxdy ≈

β
n

n
(cid:88)

i=1

rα(xi, yi) + (1 − β)

nx(cid:88)

ny
(cid:88)

i=1

j=1

πijrα(x(cid:48)

i, y(cid:48)

j), (4)

where 0 ≤ β ≤ 1 is a parameter to balance the terms of paired and unpaired
i, y(cid:48)
samples. Ideally, if we can set πij = δ(x(cid:48)
j) is 1 for all paired
(x(cid:48)
j) and 0 otherwise, and n(cid:48) is the total number of pairs, then we can recover
the original empirical estimation (i.e., πij = p(x(cid:48)

j)/n(cid:48) where δ(x(cid:48)

i, y(cid:48)

i, y(cid:48)

j) ideally).

i, y(cid:48)

Now, we can substitute Eq. (2) and Eq. (4) back into the squared error loss

function Eq. (3) to obtain the ﬁnal loss function as

J(Π, α) =

1
2

α(cid:62)Hα − α(cid:62)hΠ,β,

Algorithm 1: LSMI-Sinkhorn Algorithm.

LSMI-Sinkhorn

5

Π(0)

Π(1)
(cid:107)

Initialize Π(0) and Π(1) such that
parameter), and α(0), set the regularization parameters (cid:15) and λ, the number
of maximum iterations T , and the iteration index t = 1.
Π(t−1)
(cid:107)
(cid:107)
α(t+1) = argmin α J(Π(t), α).
Π(t+1) = argmin Π J(Π, α(t+1)).
t = t + 1.

F > η (η is the stopping
(cid:107)

F > η do

while t

T and

Π(t)

−

≤

−

return Π(t−1) and α(t−1).

where

H =

1
(n + nx)(n + ny)

n+nx(cid:88)

n+ny
(cid:88)

i=1

j=1

ϕ(xi, yj)ϕ(xi, yj)(cid:62),

hΠ,β =

β
n

n
(cid:88)

i=1

ϕ(xi, yi) + (1 − β)

nx(cid:88)

ny
(cid:88)

i=1

j=1

πijϕ(x(cid:48)

i, y(cid:48)

j).

Since we want to estimate the density-ratio function by minimizing Eq. (3), the
optimization problem is then given as

J(Π, α) =

1
min
2
Π,α
x 1nx and Π(cid:62)1nx = n−1
s.t. Π1ny = n−1

α(cid:62)Hα − α(cid:62)hΠ,β +(cid:15)H(Π)+

y 1ny .

λ
2

(cid:107)α(cid:107)2
2

(5)

Here, we add several regularization terms. H(Π) = (cid:80)nx
j=1 πij(log πij −1) is
i=1
the negative entropic regularization to ensure Π non-negative, and (cid:15) > 0 is the
corresponding regularization parameter. (cid:107)α(cid:107)2
2 is the regularization on α, and
λ ≥ 0 is the corresponding regularization parameter.

(cid:80)ny

3.2 Optimization

The objective function J(Π, α) is not jointly convex. However, if we ﬁx one vari-
able, it becomes a convex function for the other. Thus, we employ the alternating
optimization approach (see Algorithm 1) on Π and α, respectively.

1) Optimizing Π using the Sinkhorn algorithm. When ﬁxing α, the

term in our objective relating to Π is

nx(cid:88)

ny
(cid:88)

i=1

j=1

πijα(cid:62)ϕ(x(cid:48)

i, y(cid:48)

j) =

nx(cid:88)

ny
(cid:88)

i=1

j=1

πij[Cα]ij,

nx )) ∈ Rb×nx ,
where Cα = K(cid:62)diag(α)L ∈ Rnx×ny , K = (k(x(cid:48)
ny )) ∈ Rb×ny . This formulation can be considered
2), . . . , l(y(cid:48)
and L = (l(y(cid:48)
as an optimal transport problem if we maximize it with respect to Π [5]. It is

2), . . . , k(x(cid:48)

1), k(x(cid:48)

1), l(y(cid:48)

6

Liu Y., Yamada M., Tsai YH., Le T., Salakhutdinov R., Yang Y.

worth noting that the rank of Cα is at most b (cid:28) min(nx, ny) with b being a
constant (e.g., b = 100), and the computational complexity of the cost matrix
Cα is O(nxny). The optimization problem with ﬁxed α becomes

nx(cid:88)

ny
(cid:88)

−

πij(1 − β)[Cα]ij + (cid:15)H(Π)

min
Π

i=1
s.t. Π1ny = n−1

j=1

x 1nx and Π(cid:62)1nx = n−1

y 1ny ,

(6)

which can be eﬃciently solved using the Sinkhorn algorithm [5,17] 1. When α is
ﬁxed, problem (6) is convex with respect to Π.

2) Optimizing α. Next, when we ﬁx Π, the optimization problem becomes

min
α

1
2

α(cid:62)Hα − α(cid:62)hΠ,β +

λ
2

(cid:107)α(cid:107)2
2 .

(7)

Problem (7) is a quadratic programming and convex. It has an analytical solution

(cid:98)α = (H + λIb)−1hΠ,β,
where Ib ∈ Rb×b is an identity matrix. Note that the H matrix does not depend
on either Π or α, and it is a positive deﬁnite matrix.

(8)

Convergence Analysis. To optimize J(Π, α), we alternatively solve two

convex optimization problems. Thus, the following property holds true.

Proposition 1 Algorithm 1 will monotonically decrease the objective function
J(Π, α) in each iteration.

Proof. We show that J(Π(t+1), α(t+1)) ≤ J(Π(t), α(t)). First, because α(t+1) =
argmin α J(Π(t), α) and α(t+1) is the globally optimum solution, we have

J(Π(t), α(t+1)) ≤ J(Π(t), α(t)).

Moreover, because Π(t+1) = argmin Π J(Π, α(t+1)) and Π(t+1) is the globally
optimum solution, we have

J(Π(t+1), α(t+1)) ≤ J(Π(t), α(t+1)).

Therefore,

J(Π(t+1), α(t+1)) ≤ J(Π(t), α(t)).

(cid:3)

Model Selection. Algorithm 1 is dubbed as LSMI-Sinkhorn algorithm since
it utilizes Sinkhorn algorithm for LSMI estimation. It includes several tuning
parameters (i.e., λ and β) and determining the model parameters is critical to

1 In this paper, we use the log-stabilized Sinkhorn algorithm [16].

obtain a good estimation of SMI. Accordingly, we use the cross-validation with
the hold-out set to select the model parameters.

LSMI-Sinkhorn

7

First, the paired samples {(xi, yi)}n

i=1 are divided into two subsets Dtr and
Dte. Then, we train the density-ratio rα(x, y) using Dtr and the unpaired sam-
ples: {xi}n+nx
j=n+1. The hold-out error can be calculated by approx-
imating Eq. (3) using the hold-out samples Dte as

i=n+1 and {yj}n+ny

(cid:98)Jte =

1
2|Dte|2

(cid:88)

r (cid:98)α(x, y)2 −

1
|Dte|

(cid:88)

r (cid:98)α(x, y),

x,y∈Dte
(x,y)∈Dte
where |D| denotes the number of samples in the set D, (cid:80)
x,y∈Dte denotes the
summation over all possible combinations of x and y in Dte, and (cid:80)
(x,y)∈Dte
denotes the summation over all pairs of (x, y) in Dte. We select the parameters
that lead to the smallest (cid:98)Jte.

3.3 Discussion

Relation to Least-Squares Object Matching (LSOM). In this section, we
show that the LSOM algorithm [22,24] can be considered as a special case of the
proposed framework. If Π is a permutation matrix and n(cid:48) = nx = ny,

Π = {0, 1}n(cid:48)×n(cid:48)

, Π1n(cid:48) = 1n(cid:48), and Π(cid:62)1n(cid:48) = 1n(cid:48),

where Π(cid:62)Π = ΠΠ(cid:62) = In(cid:48). Then, the estimation of SMI using the permutation
matrix can be written as

(cid:100)SMI(X, Y ) =

β
2n

n
(cid:88)

i=1

rα(xi, yi) +

1
2n(cid:48)

n(cid:48)
(cid:88)

(1 − β)rα(x(cid:48)

i, y(cid:48)

π(i)) −

i=1

1
2

,

where π(i) is the permutation function. In order to calculate (cid:100)SMI(X, Y ), the
optimization problem is written as

1
2

α(cid:62)Hα − α(cid:62)hΠ,β +

min
Π,α
s.t. Π1n(cid:48) = 1n(cid:48), Π(cid:62)1n(cid:48) = 1n(cid:48), Π ∈ {0, 1}n(cid:48)×n(cid:48)

(cid:107)α(cid:107)2
2

.

λ
2

To solve this problem, LSOM uses the Hungarian algorithm [10] instead
of the Sinkhorn algorithm [5] for optimizing Π. It is noteworthy that in the
original LSOM algorithm, the permutation matrix is introduced to permute the
Gram matrix (i.e., ΠLΠ(cid:62)) and Π is also included within the H computation.
However, in our formulation, the permutation matrix depends only on hΠ,β.
This diﬀerence enables us to show a monotonic decrease for the loss function of
the proposed algorithm.

Since LSOM aims to seek the alignment, it is more suitable to ﬁnd the exact
matching among samples when the exact matching exists. In contrast, the pro-
posed LSMI-Sinkhorn is reliable even when there is no exact matching. More-
over, LSOM assumes the same number of samples (i.e., nx = ny), while our

8

Liu Y., Yamada M., Tsai YH., Le T., Salakhutdinov R., Yang Y.

LSMI-Sinkhorn does not have this constraint. For computational complexity,
the Hungarian algorithm requires O(n(cid:48)3) while the Sinkhorn requires O(n(cid:48)2).

Computational Complexity. First, the computational complexity of esti-
mating Π is based on the computation of the cost matrix Cα and the Sinkhorn
iterations. The computational complexity of Cα is O(nxny) and that of Sinkhorn
algorithm is O(nxny). Therefore, the computational complexity of the Sinkhorn
iteration is O(nxny). Second, for the α computation, the complexity to compute
H is O((n + nx)2 + (n + ny)2) and that for hΠ,β is O(nxny). In addition, es-
timating α has the complexity O(b3), which is negligible with a small constant
b. To conclude, the total computational complexity of the initialization needs
O((n + nx)2 + (n + ny)2) and the iterations requires O(nxny). In particular, for
small n and large nx = ny, the computational complexity is O(n2

x).

As a comparison, for another related algorithm, Gromove-Wasserstein [11,14],
x) for general
x) for some speciﬁc losses (e.g. L2 loss, Kullback-Leibler loss) [14].

the time complexity of computing the objective function is O(n4
cases and O(n3

4 Related Work

i=1, respectively.

i=1, p(x) from {xi}n

In this paper, we focus on the mutual information estimation problem. Moreover,
the proposed LSMI-Sinkhorn algorithm is related to Gromov-Wasserstein [14,11]
and kernelized sorting [15,6].
Mutual information estimation. To estimate the MI, a straightforward ap-
proach is to estimate the probability density p(x, y) from the paired samples
{(xi, yi)}n

i=1, and p(y) from {yi}n
Because the estimation of the probability density is itself a diﬃcult problem,
this straightforward approach does not work well. To handle this, a density-ratio
based approach can be promising [21,20]. More recently, deep learning based
mutual information estimation algorithms have been proposed [1,12]. However,
these approaches still require a large number of paired samples to estimate the
MI. Thus, in real world situations when we only have a limited number of paired
samples, existing approaches are not eﬀective to obtain a reliable estimation.
Gromov-Wasserstein and Kernelized Sorting. Given two set of vectors in
diﬀerent spaces, the Gromov-Wasserstein distance [11] can be used to ﬁnd the
optimal alignment between them. This method considers the pairwise distance
between samples in the same set to build the distance matrix, then it ﬁnds a
matching by minimizing the diﬀerence between the pairwise distance matrices:

nx(cid:88)

ny
(cid:88)

nx(cid:88)

ny
(cid:88)

πijπi(cid:48)j(cid:48)(D(xi, xi(cid:48)) − D(yj, yj(cid:48)))2,

min
Π

i=1

j=1

i(cid:48)=1
s.t. Π1ny = a, Π(cid:62)1nx = b, πij ≥ 0,

i(cid:48)=1

where a ∈ Σnx , b ∈ Σny , and Σn = {p ∈ R+
simplex.

n ; (cid:80)

i pi = 1} is the probability

Computing Gromov-Wasserstein distance requires solving the quadratic as-
signment problem (QAP), and it is generally NP-hard for arbitrary inputs [14,13].

LSMI-Sinkhorn

9

In this work, we estimate the SMI by simultaneously solving the alignment and
ﬁtting the distribution ratio by eﬃciently leveraging the Sinkhorn algorithm and
properties of the squared-loss. Recently, semi-supervised Gromov-Wasserstein-
based Optimal transport has been proposed and applied to the heterogeneous
domain adaptation problems [25]. However, their method cannot be directly used
to measure the independence between two sets of random variables. In contrast,
we can achieve this by the estimation of the density-ratio function.

Kernelized sorting methods [15,6] are highly related to Gromov-Wasserstein.
Speciﬁcally, the kernelized sorting determines a set of paired samples by max-
imizing the Hilbert-Schmidt independence criterion (HSIC) between samples.
Similar to LSOM [24], the kernelized sorting also has the assumption of the
same number of samples (i.e., {x(cid:48)
i=1 and {y(cid:48)
j=1). This assumption prohibits
both LSOM and kernelized sorting from being applied to a broader range of ap-
plications, such as photo album summarization in Section 5.5. To the contrary,
since the proposed LSMI-Sinkhorn does not rely on this assumption, it can be
applied to more general scenarios when nx (cid:54)= ny.

i}n(cid:48)

i}n(cid:48)

5 Experiments

In this section, we ﬁrst estimate the SMI on both the synthetic data and bench-
mark datasets. Then, we apply our algorithm to real world applications, i.e.,
deep image matching and photo album summarization.

5.1 Setup

For the density-ratio model, we utilize the Gaussian kernels:

K(x, x(cid:48)) = exp

(cid:19)

(cid:18)
−

(cid:107)x−x(cid:48)(cid:107)2
2
2σ2
x

, L(y, y(cid:48)) = exp

(cid:18)
−

(cid:107)y−y(cid:48)(cid:107)2
2
2σ2
y

(cid:19)

,

where σx and σy denote the widths of the kernel that are set using the median
heuristic [18] as σx = 2−1/2median({(cid:107)xi −xj(cid:107)2}nx
i,j=1), σy = 2−1/2median({(cid:107)yi −
yj(cid:107)2}ny
i,j=1). We set the number of basis b = 200, (cid:15) = 0.3, the maximum number
of iterations T = 20, and the stopping parameter η = 10−9. β and λ are chosen
by cross-validation.

5.2 Convergence and Runtime

We ﬁrst demonstrate the convergence of the loss function and the estimated
SMI value. Here, we generate synthetic data from y = 0.5x + N (0, 0.01) and
randomly choose n = 50 paired samples and nx = ny = 500 unpaired samples.
The convergence curve is shown in Figure 1. The loss value and SMI value
converge quickly (<5 iterations), which is consistent with Proposition 1.

Then, we perform a comparison between the runtimes of the proposed LSMI-
Sinkhorn and Gromov-Wasserstein for CPU and GPU implementations. The

10

Liu Y., Yamada M., Tsai YH., Le T., Salakhutdinov R., Yang Y.

Fig. 1. Convergence curves of the loss and SMI values.

Fig. 2. Runtime comparison of LSMI-Sinkhorn and Gromov-Wasserstein. A base-10
log scale is used for the Y axis.

data are sampled from two 2D random measures, where nx = ny ∈ {100, 200, . . . ,
9000, 10000} is the number of unpaired data and n = 100 is the number of paired
data (only for LSMI-Sinkhorn). For Gromov-Wasserstein, we use the CPU im-
plementation from Python Optimal Transport toolbox [7] and the Pytorch GPU
implementation from [2]. We use the squared loss function and set the entropic
regularization (cid:15) to 0.005 according to the original code. For LSMI-Sinkhorn,
we implement the CPU and GPU versions using numpy and Pytorch, respec-
tively. For fair comparison, we use the log-stabilized Sinkhorn algorithm and the
same early stopping criteria and the same maximum iterations as in Gromov-
Wasserstein. As shown in Figure 2, in comparison to the Gromov-Wasserstein,
LSMI-Sinkhorn is more than one order of magnitude faster for the CPU version
and several times faster for the GPU version. This is consistent with our compu-
tational complexity analysis. Moreover, the GPU version of our algorithm costs
only 3.47s to compute 10, 000 unpaired samples, indicating that it is suitable for
large-scale applications.

5.3 SMI Estimation

For SMI estimation, we set up four baselines:

05101520Iteration3.53.02.5Loss value05101520Iteration0.51.01.52.0SMI value0200040006000800010000Number of Unpaired Samples101100101102103SecondsRunning TimeLSMI-Sinkhorn (CPU)LSMI-Sinkhorn (GPU)Gromov Wasserstein (CPU)Gromov Wasserstein (GPU)LSMI-Sinkhorn

11

Fig. 3. SMI estimation on synthetic data (nx = ny = 500).

Fig. 4. Visualization of the matrix Π.

– LSMI (full): 10, 000 paired samples are used for cross-validation and SMI

estimation. It is considered as the ground truth value.

– LSMI: Only n (usually small) paired samples are used for cross-validation

and SMI estimation.

– LSMI (opt): n paired samples are used for SMI estimation. However, we
use the optimal parameters from LSMI (full) here. This can be seen as the

20406080100n (# Paired Samples)0.00.51.01.5SMI valueRandomLSMILSMI (Opt)LSMI-SinkhornGromov-SMILSMI (full)20406080100n (# Paired Samples)0.00.51.01.52.02.5SMI valueLinearLSMILSMI (Opt)LSMI-SinkhornGromov-SMILSMI (full)20406080100n (# Paired Samples)0.00.51.01.52.0SMI valueNonlinearLSMILSMI (Opt)LSMI-SinkhornGromov-SMILSMI (full)20406080100n (# Paired Samples)0.00.51.01.52.0SMI valuePCALSMILSMI (Opt)LSMI-SinkhornGromov-SMILSMI (full)RandomLinearNonlinearPCAlowhigh12

Liu Y., Yamada M., Tsai YH., Le T., Salakhutdinov R., Yang Y.

Fig. 5. SMI estimation on synthetic data (nx = 1000, ny = 500).

upper bound of SMI estimation with limited number of paired data because
the optimal parameters are usually unavailable.

– Gromov-SMI: The Gromov-Wasserstein distance is applied on unpaired
samples to ﬁnd potential matching (ˆn = min(nx, ny)). Then, the ˆn matched
pairs and existing n paired samples are combined to perform cross-validation
and SMI estimation.

Synthetic Data. In this experiment, we manually generate four types of paired
samples: random normal, y = 0.5x+N (0, 0.01) (Linear), y = sin(x) (Nonlinear),
and y = PCA(x). We change the number of paired samples n ∈ {10, 20, . . . , 100}
while ﬁxing nx = 500 and ny = 500 for Gromov-SMI and the proposed LSMI-
Sinkhorn, respectively. The model parameters λ and β are selected by cross-
validation using the paired examples with λ ∈ {0.1, 0.01, 0.001, 0.0001} and β ∈
{0.2, 0.4, 0.6, 0.8, 1.0}. The results are shown in Figure 3. In the random case,
the data are nearly independent and our algorithm achieves a small SMI value.
In other cases, LSMI-Sinkhorn yields a better estimation of the SMI value and
it lies near the ground truth when n increases. In contrast, Gromov-SMI has a
small estimation value, which may be due to the incorrect potential matching.
We further show the heatmaps of the matrix Π in Figure 4. For the random
case, Π distributes uniformly as expected. For all other cases, Π concentrate on
the diagonal, indicating good estimation for the unpaired samples.

To show the ﬂexibility of the proposed LSMI-Sinkhorn algorithm, we set
nx = 1000, ny = 500 and ﬁx all other settings. The results are shown in Figure 5.
Similarly, LSMI-Sinkhorn achieves the best performance among all methods. We
also notice that Gromov-SMI achieves even worse estimation than nx = ny case,

20406080100n (# Paired Samples)0.00.10.20.30.40.5SMI valueRandomLSMILSMI (Opt)LSMI-SinkhornGromov-SMILSMI (full)20406080100n (# Paired Samples)0.00.51.01.52.02.5SMI valueLinearLSMILSMI (Opt)LSMI-SinkhornGromov-SMILSMI (full)20406080100n (# Paired Samples)0.00.51.01.52.0SMI valueNonlinearLSMILSMI (Opt)LSMI-SinkhornGromov-SMILSMI (full)20406080100n (# Paired Samples)0.00.51.01.52.0SMI valuePCALSMILSMI (Opt)LSMI-SinkhornGromov-SMILSMI (full)LSMI-Sinkhorn

13

Fig. 6. SMI estimation on UCI datasets.

Fig. 7. Deep image matching.

which means it is not as stable as our algorithm to handle sophisticated situations
(nx (cid:54)= ny).
UCI Datasets. We selected four benchmark datasets from the UCI machine
learning repository. For each dataset, we split the features into two sets as paired
samples. To ensure high dependence between these two subsets of features, we
utilized the same splitting strategy as [15] according to the correlation matrix.
The experimental setting is the same as the synthetic data experiment. We show
the SMI estimation results in Figure 6. Similarly, LSMI-Sinkhorn obtains better
estimation values in all four datasets. Gromov-SMI tends to overestimate the
value by a large margin, while other baselines underestimate the value.

5.4 Deep Image Matching

Next, we consider an image matching task with deep convolution features. We use
two commonly-used image classiﬁcation benchmarks: CIFAR10 and STL10 [3].

20406080100n (# Paired Samples)0.51.01.5SMI valuesatimageLSMILSMI (Opt)LSMI-SinkhornGromov-SMILSMI (full)20406080100n (# Paired Samples)0.20.40.60.81.01.2SMI valuesegmentLSMILSMI (Opt)LSMI-SinkhornGromov-SMILSMI (full)20406080100n (# Paired Samples)0.000.250.500.751.001.25SMI valueoptdigitsLSMILSMI (Opt)LSMI-SinkhornGromov-SMILSMI (full)20406080100n (# Paired Samples)0.250.500.751.001.25SMI valueabaloneLSMILSMI (Opt)LSMI-SinkhornGromov-SMILSMI (full)20406080100n (# Paired Samples)0.00.20.40.60.81.0Matching AccuracyCIFAR10Top-1 AccTop-2 AccClass Acc20406080100n (# Paired Samples)0.00.20.40.60.81.0Matching AccuracySTL10Top-1 AccTop-2 AccClass Acc14

Liu Y., Yamada M., Tsai YH., Le T., Salakhutdinov R., Yang Y.

Fig. 8. Photo album summarization on Flickr dataset. In (a), we ﬁxed the corners with
blue, orange, green, and black images. In (b), we ﬁxed the center of each character with
a diﬀerent image.

Fig. 9. Photo album summarization on CIFAR10 dataset. In (a), we ﬁxed the corners
with automobile, airplane, dog, and horse images. In (b), we ﬁxed the center of each
character with a diﬀerent image.

We extracted 64-dim features from the last layer (after pooling) of ResNet20
pretrained on the training set of CIFAR10. The features are divided into two
32-dim parts denoted by {xi}N
i=1. We shuﬄe the samples of y and
attempt to match x and y with limited pair samples (n ∈ {10, 20, . . . , 100}) and
unpaired samples (nx = ny = 500). Other settings are the same as the above
experiments.

i=1 and {yi}N

To evaluate the matching performance, we used top-1 accuracy, top-2 accu-
racy (correct matching is achieved in the top-2 highest scores), and class accuracy
(matched samples are in the same class). As shown in Figure 7, LSMI-Sinkhorn
obtains high accuracy with only a few tens of supervised pairs. Additionally, the
high class matching performance implies that our algorithm can be applied to
further applications such as semi-supervised image classiﬁcation.

5.5 Photo Album Summarization

Finally, we apply the proposed LSMI-Sinkhorn to the photo album summariza-
tion problem, where images are matched to a predeﬁned structure according to
the Cartesian coordinate system.

14LiuY,YamadaM,TsaiYH,LeT,SalakhutdinovR,YangY.(a)16⇥20(b)“ECMLPKDD”Fig.8:PhotoalbumsummarizationonFlickrdataset.In(a),weﬁxedthecornerswithblue,orange,green,andblackimages.In(b),weﬁxedthecenterofeachcharacterwithadi↵erentimage.(a)16⇥20(b)“ECMLPKDD”Fig.9:PhotoalbumsummarizationonCIFAR10dataset.In(a),weﬁxedthecornerswithautomobile,airplane,dog,andhorseimages.In(b),weﬁxedthecenterofeachcharacterwithadi↵erentimage.unpairedsamples(nx=ny=500).Othersettingsarethesameastheaboveexperiments.Toevaluatethematchingperformance,weusedtop-1accuracy,top-2accu-racy(correctmatchingisachievedinthetop-2highestscores),andclassaccuracy(matchedsamplesareinthesameclass).AsshowninFigure7,LSMI-Sinkhornobtainshighaccuracywithonlyafewtensofsupervisedpairs.Additionally,thehighclassmatchingperformanceimpliesthatouralgorithmcanbeappliedtofurtherapplicationssuchassemi-supervisedimageclassiﬁcation.5.5PhotoAlbumSummarizationFinally,weapplytheproposedLSMI-Sinkhorntothephotoalbumsummariza-tionproblem,whereimagesarematchedtoapredeﬁnedstructureaccordingtotheCartesiancoordinatesystem.14LiuY,YamadaM,TsaiYH,LeT,SalakhutdinovR,YangY.(a)16⇥20(b)“ECMLPKDD”Fig.8:PhotoalbumsummarizationonFlickrdataset.In(a),weﬁxedthecornerswithblue,orange,green,andblackimages.In(b),weﬁxedthecenterofeachcharacterwithadi↵erentimage.(a)16⇥20(b)“ECMLPKDD”Fig.9:PhotoalbumsummarizationonCIFAR10dataset.In(a),weﬁxedthecornerswithautomobile,airplane,dog,andhorseimages.In(b),weﬁxedthecenterofeachcharacterwithadi↵erentimage.unpairedsamples(nx=ny=500).Othersettingsarethesameastheaboveexperiments.Toevaluatethematchingperformance,weusedtop-1accuracy,top-2accu-racy(correctmatchingisachievedinthetop-2highestscores),andclassaccuracy(matchedsamplesareinthesameclass).AsshowninFigure7,LSMI-Sinkhornobtainshighaccuracywithonlyafewtensofsupervisedpairs.Additionally,thehighclassmatchingperformanceimpliesthatouralgorithmcanbeappliedtofurtherapplicationssuchassemi-supervisedimageclassiﬁcation.5.5PhotoAlbumSummarizationFinally,weapplytheproposedLSMI-Sinkhorntothephotoalbumsummariza-tionproblem,whereimagesarematchedtoapredeﬁnedstructureaccordingtotheCartesiancoordinatesystem.LSMI-Sinkhorn

15

Color Feature. We ﬁrst used 320 images from Flickr [15] and extracted the
RGB pixels as color feature. Figure 8 depicts the semi-supervised summarization
to the 16 × 20 grid with the corners of the grid ﬁxed to blue, orange, green,
and black images. Similarly, we show the summarization results on an “ECML
PKDD” grid with the center of each character ﬁxed. It can be seen that these
layouts show good color topology according to the ﬁxed color images.
Semantic Feature. We then used CIFAR10 with the ResNet20 feature to il-
lustrate the semantic album summarization. Figure 9 shows the layout of 1000
images into the same 16 × 20, and “ECML PKDD” grids. In Figure 9a, we ﬁxed
corners of the grid to automobile, airplane, dog, and horse images. In Figure
9b, we ﬁxed the eight character centers. It can be seen that objects are aligned
together by their semantics rather than colors according to the ﬁxed images.

Compared with previous summarization algorithms, LSMI-Sinkhorn has two
advantages. (1) The semi-supervised property enables interactive album sum-
marization, while kernelized sorting [15,6] and object matching [22] can not. (2)
We obtained a solution for general rectangular matching (both nx = ny and
nx (cid:54)= ny), e.g., 320 images to a 16 × 20 grid, 1000 images to a 16 × 20 grid, while
most previous methods [15,22] relied on the Hungarian algorithm [10] to obtain
square matching (nx = ny) only.

6 Conclusion

In this paper, we proposed the Least-Square Mutual Information with Sinkhorn
(LSMI-Sinkhorn) algorithm to estimate the SMI from a limited number of paired
samples. To the best of our knowledge, this is the ﬁrst semi-supervised SMI
estimation algorithm. Experiments on synthetic and real data show that the
proposed algorithm can successfully estimate SMI with a small number of paired
samples. Moreover, we demonstrated that the proposed algorithm can be used
for image matching and photo album summarization.

Acknowledgements

Yanbin Liu and Yi Yang are supported by ARC DP200100938. Makoto Yamada
was supported by MEXT KAKENHI 20H04243 and partly supported by MEXT
KAKENHI 21H04874. Tam Le acknowledges the support of JSPS KAKENHI
Grant number 20K19873. Yao-Hung Hubert Tsai and Ruslan Salakhutdinov were
supported in part by the NSF IIS1763562, IARPA D17PC00340, ONR Grant
N000141812861, and Facebook PhD Fellowship.

References

1. Belghazi, M.I., Baratin, A., Rajeshwar, S., Ozair, S., Bengio, Y., Hjelm, D.,

Courville, A.: Mutual information neural estimation. In: ICML (2018)

2. Bunne, C., Alvarez-Melis, D., Krause, A., Jegelka, S.: Learning generative models

across incomparable spaces. In: ICML (2019)

16

Liu Y., Yamada M., Tsai YH., Le T., Salakhutdinov R., Yang Y.

3. Coates, A., Ng, A., Lee, H.: An analysis of single-layer networks in unsupervised

feature learning. In: AISTATS (2011)

4. Cover, T.M., Thomas, J.A.: Elements of Information Theory. John Wiley & Sons,

Inc., Hoboken, NJ, USA, 2nd edn. (2006)

5. Cuturi, M.: Sinkhorn distances: Lightspeed computation of optimal transport. In:

NIPS (2013)

6. Djuric, N., Grbovic, M., Vucetic, S.: Convex kernelized sorting. In: AAAI (2012)
7. Flamary, R., Courty, N.: Pot python optimal transport library (2017), https:

//github.com/rflamary/POT

8. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,

Courville, A., Bengio, Y.: Generative adversarial nets. In: NIPS (2014)

9. Hjelm, R.D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P.,
Trischler, A., Bengio, Y.: Learning deep representations by mutual information
estimation and maximization. In: ICLR (2019)

10. Kuhn, H.: The Hungarian method for the assignment problem. Naval Research

Logistics Quarterly 2(1-2), 83–97 (1955)

11. M´emoli, F.: Gromov–wasserstein distances and the metric approach to object
matching. Foundations of Computational Mathematics 11(4), 417–487 (2011)
12. Ozair, S., Lynch, C., Bengio, Y., Oord, A.v.d., Levine, S., Sermanet, P.: Wasserstein

dependency measure for representation learning. NeurIPS (2019)

13. Peyr´e, G., Cuturi, M.: Computational optimal transport. Foundations and

Trends® in Machine Learning 11(5-6), 355–607 (2019)

14. Peyr´e, G., Cuturi, M., Solomon, J.: Gromov-wasserstein averaging of kernel and

distance matrices. In: ICML (2016)

15. Quadrianto, N., Smola, A., Song, L., Tuytelaars, T.: Kernelized sorting. IEEE
transactions on pattern analysis and machine intelligence 32, 1809–1821 (2010)
16. Schmitzer, B.: Stabilized sparse scaling algorithms for entropy regularized trans-
port problems. SIAM Journal on Scientiﬁc Computing 41(3), A1443–A1481 (2019)
17. Sinkhorn, R.: Diagonal equivalence to matrices with prescribed row and column
sums. Proceedings of the American Mathematical Society 45(2), 195–198 (1974)
18. Sriperumbudur, B.K., Fukumizu, K., Gretton, A., Lanckriet, G.R., Sch¨olkopf, B.:
Kernel choice and classiﬁability for rkhs embeddings of probability distributions.
In: NIPS (2009)

19. Suzuki, T., Sugiyama, M.: Suﬃcient dimension reduction via squared-loss mutual

information estimation. In: AISTATS (2010)

20. Suzuki, T., Sugiyama, M., Kanamori, T., Sese, J.: Mutual information estimation
reveals global associations between stimuli and biological processes. BMC Bioin-
formatics 10(S52) (2009)

21. Suzuki, T., Sugiyama, M., Tanaka, T.: Mutual information approximation via max-

imum likelihood estimation of density ratio. In: ISIT (2009)

22. Yamada, M., Sigal, L., Raptis, M., Toyoda, M., Chang, Y., Sugiyama, M.: Cross-
domain matching with squared-loss mutual information. IEEE transactions on Pat-
tern Analysis and Machine Intelligence 37(9), 1764–1776 (2015)

23. Yamada, M., Sugiyama, M.: Dependence minimizing regression with model selec-
tion for non-linear causal inference under non-gaussian noise. In: AAAI (2010)
24. Yamada, M., Sugiyama, M.: Cross-domain object matching with model selection.

In: AISTATS (2011)

25. Yan, Y., Li, W., Wu, H., Min, H., Tan, M., Wu, Q.: Semi-supervised optimal

transport for heterogeneous domain adaptation. In: IJCAI (2018)

26. Zhao, S., Song, J., Ermon, S.: Infovae: Balancing learning and inference in varia-

tional autoencoders. In: AAAI (2019)

