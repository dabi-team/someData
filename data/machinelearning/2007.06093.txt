1
2
0
2

l
u
J

4
1

]

G
L
.
s
c
[

5
v
3
9
0
6
0
.
7
0
0
2
:
v
i
X
r
a

Interval Universal Approximation for Neural Networks

Zi Wang
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI 53706
zw@cs.wisc.edu

Aws Albarghouthi
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI 53706
aws@cs.wisc.edu

Gautam Prakriya
Institute of Theoretical Computer Science and Communications
The Chinese University of Hong Kong
Shatin, NT, Hong Kong SAR
gautamprakriya@gmail.com

Somesh Jha
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI 53706
jha@cs.wisc.edu

Abstract

To verify safety and robustness of neural networks, researchers have successfully applied
abstract interpretation, primarily using the interval abstract domain. In this paper, we study the
theoretical power and limits of the interval domain for neural-network veriﬁcation.

First, we introduce the interval universal approximation (IUA) theorem. IUA shows that
neural networks not only can approximate any continuous function f (universal approximation) as
we have known for decades, but we can ﬁnd a neural network, using any well-behaved activation
function, whose interval bounds are an arbitrarily close approximation of the set semantics of
f (the result of applying f to a set of inputs). We call this notion of approximation interval
approximation. Our theorem generalizes the recent result of Baader et al. (2020) from ReLUs
to a rich class of activation functions that we call squashable functions. Additionally, the IUA
theorem implies that we can always construct provably robust neural networks under (cid:96)∞-norm
using almost any practical activation function.

Second, we study the computational complexity of constructing neural networks that are
amenable to precise interval analysis. This is a crucial question, as our constructive proof of
IUA is exponential in the size of the approximation domain. We boil this question down to the
problem of approximating the range of a neural network with squashable activation functions.
We show that the range approximation problem (RA) is a ∆2-intermediate problem, which
is strictly harder than NP-complete problems, assuming coNP (cid:54)⊂ NP. As a result, IUA is an
inherently hard problem: No matter what abstract domain or computational tools we consider to
achieve interval approximation, there is no eﬃcient construction of such a universal approximator.
This implies that it is hard to construct a provably robust network, even if we have a robust
network to start with.

1

 
 
 
 
 
 
IUA theorem (semi-formally): For a function f : Rm →
R that we wish to approximate and error δ > 0, there is a
neural network N that has the following behavior:
Let B ⊂ Rm be a hyperrectangle (box) in Euclidean space.
The red interval (top) is the tightest interval that contains
all outputs of f when applied to elements of the set B.

If we abstractly interpret N on the box B, we may get the
black interval (bottom) N #(B), whose lower/upper bounds
are up to δ away from those of the red interval. Note that
N #(B) may not necessarily subsume the top interval, since
N is an approximation of f .

minx∈B f (x)

maxx∈B f (x)

(cid:54) δ

(cid:54) δ

N #(B)

Figure 1: Illustration and semi-formal statement of the interval universal approximation (IUA)
theorem (Right is adapted from Baader et al. (2020))

1

Introduction

Over the past decade, machine learning with neural
Neural networks and approximation.
networks has revolutionized a vast array of tasks—from computer vision (Krizhevsky et al., 2012),
to natural-language processing (Mikolov et al., 2013), to program-analysis tasks (Raychev et al.,
2015), and beyond. While these advances are recent, it has been well-known that neural networks
are a powerful class of models: The universal approximation theorem (Hornik et al., 1989; Cybenko,
1989) states that neural networks can approximate any continuous function with arbitrary precision.
Moreover, we only need a single hidden layer of neurons to realize this theorem. By adding more
neurons, one gets a more and more precise approximation. The intuition is that each neuron can
encode a step function. So, by adding more neurons, one gets a ﬁner-grained, step-like approximation
of a continuous function (see Nielsen (2015, Ch.4) for an interactive visualization).
Abstract interpretation of neural networks. With the wide adoption of neural networks, new
safety and security concerns arose. The most prominent property of study has been robustness (Good-
fellow et al., 2015): small perturbations to the input of a network should not change the prediction.
For example, a small change to an image of a stop sign should not cause a classiﬁer to think it is a
speed-limit sign. A number of researchers have proposed the use of abstract interpretation (Cousot
and Cousot, 1977) techniques to prove robustness of neural networks to small perturbations (Gehr
et al., 2018; Wang et al., 2018; Anderson et al., 2019) and to train robust models (Mirman et al.,
2018; Gowal et al., 2018; Huang et al., 2019).

Suppose we want to verify robustness of a neural network to small changes in the brightness of
an image. We can represent a large set of images, with varying brightness, as an element of some
abstract domain, and propagate it through the network, eﬀectively “executing” the network on an
intractably large number of images. If all images lead to the same prediction, then we have a proof
that the network is robust on the original image.

The simplest abstract domain that leads to practical veriﬁcation results in this setting is the
interval domain. In our example above, if each pixel in a monochrome image is a real number r,
then the pixel can be represented as an interval [r − (cid:15), r + (cid:15)], where (cid:15) denotes the range of brightness
we wish to be robust on. Then, the box representing the interval of each pixel is propagated through
the network using interval arithmetic operations and other custom abstract transformers.

The interval abstract domain has been successfully used
The power of the interval domain.
for verifying properties of neural networks for image classiﬁcation (Gehr et al., 2018; Gowal et al.,

2

2018), natural-language processing (Huang et al., 2019), as well as cyber-physical systems (Wang
et al., 2018). Why does the interval domain work for verifying neural networks?

In investigating this question, Baader et al. (2020) demonstrated a surprising connection between
the universal approximation theorem and interval-based veriﬁcation. Their theorem states that
not only can neural networks approximate any function f , but we can ﬁnd a neural network, using
rectiﬁed linear unit (ReLU) activation functions (Nair and Hinton, 2010), whose interval abstract
interpretation is arbitrarily close to the collecting (or set) semantics of f .
In this paper, our ﬁrst goal is to deepen our
Interval universal approximation theorem.
understanding of the power of interval analysis of neural networks, broadly construed. Speciﬁcally,
we set out to answer the following question:

Can we always construct a neural network, with any activation function, whose interval
abstract interpretation is arbitrarily close to the collecting (or set) semantics of f ?

The theorem of Baader et al. (2020) is restricted to networks that use ReLU activations. In this
work, we generalize the result of Baader et al. (2020) to neural networks that use arbitrary well-
behaved activation functions. Speciﬁcally, we prove what we call the interval universal approximation
theorem, or IUA theorem for short: Let f be the function we wish to approximate, and let δ > 0
be the tolerated error. Then, there exists a neural network N , built using any activation function,
such that for any box of inputs B, the abstract interpretation of N on B is δ close to the collecting
semantics of f over B. If the box of inputs is a single point in Euclidean space, then the IUA theorem
reduces to the universal approximation theorem; thus, IUA generalizes universal approximation. The
IUA theorem is illustrated in more detail in Fig. 1.

We deﬁne a rich class of activation functions, which we call squashable functions, for which our
IUA theorem holds. This class includes popular activation functions, like ReLU, sigmoid, tanh,
ELU, and other activation functions that have been shown to be useful for training robust neural
networks (Xie et al., 2020). The key idea behind squashable activation functions is that they have
left and right limits (or we can use them to construct functions with limits); we exploit limits to
approximate step functions, and therefore construct step-like approximations of f , while controlling
approximation error δ.
Existence of provably robust networks. While our results are theoretical in nature, they shed
light on the existence of provably correct neural networks. Suppose there is some ideal robust image
classiﬁer f using the (cid:96)∞-norm, which is typically used to deﬁne a set of images in the neighborhood of
a given image. The classical universal approximation theorem tells us that, for any desired precision,
there is a neural network that can approximate f . The IUA theorem further tells us that there exists
a neural network for which we can automatically construct proofs of robustness using the interval
domain. In addition, this neural network can be built using almost any activation function in the
literature, and more.

Our proof of IUA, like that of Baader et al. (2020), is
Hardness of range approximation.
constructive. Given f and δ, one can construct a neural network that δ-interval approximates f .
However, the constructions are exponential in the size of the function’s domain. A key open problem
is whether there is an eﬃcient construction of such neural networks; therefore, the second question
we set out to answer in this paper is

Can we eﬃciently build an interval universal approximator for any continuous function f ?

We answer this question by boiling it down to studying the hardness of what we call the range

3

approximation (RA) problem: Given a function f , how hard it is to approximate the range of f .
Speciﬁcally, we consider the case where f is given as a neural network N with domain [0, 1]m and
codomain [0, 1], and our goal is to approximate the range of N with tolerance δ. We show a surprising
dichotomy result: if δ (cid:62) 1/2, then this is a trivial task; if δ < 1/2, then this is a ∆2-intermediate
(Theorem 5.6) problem, where ∆2 is the smallest class in the polynomial hierarchy that contains
both the NP and coNP classes. As a consequence, there is no eﬃcient construction of the interval
universal approximating neural network, and the veriﬁcation of robustness using the interval domain
is hard. If one can approximate the collective semantics of a neural network using the interval
domain as required for verifying robustness, then one can immediately approximate the range of the
network.

Contributions.

Our contributions can be summarized as follows:

1. We characterize a rich class of activation functions, which we call squashable functions, that
includes most activation functions used in neural networks (ReLU, sigmoid, tanh, ELU, etc.).
We show that squashable functions can arbitrarily approximate step functions. Since neural
networks using step functions can encode Boolean formulas, interpreting any activation function
as a squashable function provides a uniﬁed view of neural networks. We believe that it will
beneﬁt future researchers in understanding the theoretical and formal properties of neural
networks. (Section 2)

2. We prove the interval universal approximation (IUA) theorem: Given a continuous f over a
compact domain, one can always construct a neural network N with any squashable function
whose interval semantics is arbitrarily close to the set semantics of f . Our result generalizes the
work of Baader et al. (2020), which is specialized for ReLU activations. Our proof follows the
general framework put forth by Baader et al. (2020), which can be viewed as a careful design
of summation of indicator functions. Baader et al. (2020) use a construction from He et al.
(2018) to construct the min function with ReLU units. We present a smaller construction that
is simpler to analyze and applies to any squashable activation function. (Sections 3 and 4)

3. We demonstrate that the IUA theorem implies the existence of provably robust neural networks
for any classiﬁcation task at hand that has a robust solution f . Speciﬁcally, there exists a
neural network for which we can automatically construct proofs of robustness using the interval
domain and whose classiﬁcations match those of f . In addition, this neural network can be
built using any squashable activation. (Section 7)

4. We study the hardness of building neural networks for IUA. We show a dichotomy result, that
it is either trivial or ∆2-intermediate to approximate the range of a neural network that is
polynomial-time executable. As a consequence, there is no eﬃcient construction of the interval
universal approximator. To our best knowledge, this is the ﬁrst work to classify the complexity
of a veriﬁcation task of neural networks.1 (Sections 5 and 6)

2 Squashable Activation Functions

In this section, we deﬁne squashable functions, and how they can be used to build other functions
that are essential in our analysis of neural networks.

1Katz et al. (2017) and Weng et al. (2018) study the complexity of bug ﬁnding (falsiﬁcation) instead of veriﬁcation.

4

Activation functions that satisfy Eq. (1)

1

0.5

−4 −2

2

4

σ(x) =

1
1 + e−x

1

−4 −2

2

4

tanh(x) =

2

1 + e−2x − 1

1

−8 −4

4

8

−1

softsign(x) =

x
1 + |x|

Activation functions that do not directly satisfy Eq. (1)

1

0.5

−1 −0.5

0.5

1

ReLU(x) =

(cid:26) x, x (cid:62) 0
x < 0

0,

2

−2−1

1 2

2

−1

1 2 3

ELU(x) =

(cid:26) x,

x (cid:62) 0
ex − 1, x < 0

2

−1

2

4

softplus(x) = log(1 + ex)

smoothReLUa(x) =

(cid:26) x − 1
0,

a log(ax + 1), x (cid:62) 0, a > 0

x < 0

Figure 2: Example activation functions. Smooth ReLU (smoothReLUa) is parameterized by a > 0
(a = 1 is plotted).

2.1 Neural Networks and Squashable Activation Functions

A neural network in our setting is a function in Rm → R, where m is the
Neural networks.
number of inputs to the network. We will take a general view and deﬁne a network N following
a simple grammar, a composition of primitive arithmetic operations and activation functions.
Throughout, we will use x ∈ Rm to denote a vector, and use x1, . . . , xm to denote the m elements of
x.

Deﬁnition 2.1 (Neural network grammar). Let x be the input to the neural network. A neural
network N is deﬁned as follows

N :− c
| xi
| N1 + N2
c ∗ N1
|
t(N1)
|

where c ∈ R, xi is the ith input to the network, and t : R → R is an activation function. Whenever
we discuss neural networks, we will ﬁx a single activation function t to be used in the grammar. (cid:4)

This grammar is rich enough to encode standard feed-forward neural networks, convolutional

neural networks, and other non-recurrent architectures.

5

In Fig. 2, we deﬁne and plot a number of popular activation functions,
Activation Functions.
and other more recent ones: sigmoid, tanh, rectiﬁed linear units (ReLU) (Nair and Hinton, 2010),
exponential linear unit (ELU) (Clevert et al., 2016), softplus (Glorot et al., 2011), softsign (Bergstra
et al., 2009), and smooth ReLU (Xie et al., 2020).

Example 2.2. Consider the following simple neural network with 2-dimensional input x = (x1, x2)
and a sigmoid activation function: N (x) = σ(x1 + 0.5x2). This is typically depicted as:

x1

x2

0.5

+

σ

Observe that the coeﬃcient of x2 is shown on the arrow.

(cid:4)

Squashable activation functions. We provide the deﬁnitions of activation functions above
to ground our discussion. Our results, however, are more general: they apply to a general class of
activation functions that we will call squashable activation functions:

Deﬁnition 2.3 (Squashable activation functions). t : R → R is squashable iﬀ

1. there is a < b ∈ R such that

lim
x→−∞

t(x) = a,

lim
x→∞

t(x) = b,

and ∀x < y. t(x) (cid:54) t(y)

(1)

2. or a function t(cid:48) : R → R that satisﬁes Eq. (1) and can be expressed using the grammar in

Theorem 2.1 with copies of t. For example, t(cid:48)(x) = t(2 ∗ t(x) − t(x + 10)).

In other words, squashable functions are the smallest set of functions that can use the grammar in
(cid:4)
Theorem 2.1 to build a function that satisﬁes Eq. (1).

Informally, an activation function is in this class if we can use it to construct a monotonically
increasing function that has limits in the left and right directions, −∞ and ∞.2 Squashable activation
functions extend the squashing functions used by Hornik et al. (1989). All of the activation functions
we have deﬁned in Fig. 2 are squashable.

Fig. 2 (top, blue) shows all activation functions that satisfy Eq. (1), and are therefore squashable.
For example, sigmoid and tanh easily satisfy Eq. (1): both have limits and are monotonically
increasing. What about activation functions like ReLU, ELU, etc., shown in Fig. 2 (bottom, red)?
It is easy to see that they do not satisfy Eq. (1): none of them have a right limit. However, by point
(2) of Theorem 2.3, given an activation function t, if we can construct a new activation function t(cid:48)
that is squashable, using the operations in the grammar in Theorem 2.1, then t is squashable. In
the following proposition, we give a general and simple construction that works for all activation
functions in Fig. 2 (bottom, red).

Proposition 2.4 (Squashable activations). Let

t ∈ {ReLU, softplus, smoothReLUa, ELU}

The function t(cid:48)(x) = t(1 − t(−x)) satisﬁes Eq. (1). Therefore, ReLU, softplus, Smooth ReLU, and
ELU, are squashable.

2In our construction and proof, we do not need the function to be monotonic; however, in practice, most activation

functions are monotonic and abstractly interpreting arbitrary functions is impractical.

6

1

1

−2−1

1 2

−8 −4

4

8

(a) ReLU(1 − ReLU(−x))

(b) softplus(1 − softplus(−x))

Figure 3: Two activation functions after applying construction in Theorem 2.4. Observe that the
resulting function satisﬁes Eq. (1), and therefore ReLU and softplus are squashable.

Proof. It is easy to see that all the activation functions t are monotonically increasing with

lim
x→−∞

t(x) = l

and

lim
x→∞

t(x) = ∞.

for some l ∈ R.

Because t is increasing, t(−x) and t(1−x) are both decreasing; thus, their composition t(1−t(−x))

is increasing.

lim
x→−∞

t(1 − t(−x)) = t( lim

x→−∞

(1 − t(−x))) = l

lim
x→∞

t(1 − t(−x)) = t(1 − lim
x→∞

t(−x)) = t(1 − l)

ReLU.

: l = 0, and t(1 − l) = ReLU(1 − 0) = 1.

ELU.

: l = −1, and t(1 − l) = ELU(2) = 2.

softplus.

: l = 0, and t(1 − l) = softplus(1) = log(1 + e).

smoothReLU.
a log(a + 1) < 1 for a (cid:54)= 0).
1

:

l = 0, and t(1 − l) = smoothReLUa(1) = 1 − 1

a log(a + 1).

(Note that

Throughout this paper, we will work with neural networks with squashable activation functions.
Theorem 2.4 guarantees that our results are general enough to account for many diﬀerent neural
networks, including ReLU networks.

Example 2.5. Fig. 3 shows t(1 − t(−x)), for t = ReLU and t = softplus. Both have left/right limits
and are monotonic. Thus, they satisfy Eq. (1) and therefore ReLU and softplus are squashable. (cid:4)

2.2 Squashable-Function Constructions

In this section, we will show some constructions using squashable and step functions. This is a key
idea of the whole paper, and essential for proving the IUA and the hardness of range approximation
theorems. As we will demonstrate in the subsequent sections, we will use squashable functions to
approximate some gadgets that are fundamental in mathematics and complexity theory. We believe
that these constructions are important in understanding the computational and formal properties of
neural networks in the future.

Step Function.

A step function is

sign(x) =

(cid:26) 1, x > 0
0, x (cid:54) 0

One can view the step function as the indication of whether x is a positive number. The step
function can be used to build indicator functions, which is a fundamental tool in mathematical

7

1

0.5

1

0.5

1

0.5

1

0.5

−4 −2

2

4

−4 −2

2

4

−4 −2

2

4

−4 −2

2

4

(a) sign(x)

(b) sign(x) − sign(x − 1)

(c) dilated sigmoid σ(10x)

(d) σ(10x) − σ(10(x − 1))

Figure 4: Approximations of step function and indicator function

analysis. For example, the standard way of deﬁning integration with respect to the probability
measure is using the summation of indicator functions (Durrett, 2010). Also, neural networks with
step functions as the activation function can encode Boolean formula as we will show later.

Squashable functions do not have an explicit formula, as it is a very
Squashable Function.
expressive class of functions. All we know is that it is monotonic and has both left and right limits
(see Theorem 2.3). Proving the properties of neural network with squashable functions might appear
a challenging task.

However, the key observation is that if we dilate t properly, i.e., multiply the input with a large
number µ to get t(µx), we will obtain an approximation of the step function. See Figs. 4a and 4c on
how one can use the sigmoid function to approximate the step function.

Indicator Function.

An indicator function associated with a set S ⊂ Rm is deﬁned as

indS =

(cid:26) 1, x ∈ S
0, x (cid:54)∈ S

Note how the value is 1 if x is in the set S, and 0 otherwise.

We can use step functions to build indicator functions. For example,

sign(x) − sign(x − 1)

returns 1 for x ∈ (0, 1], and 0 otherwise. See Figs. 4b and 4d for illustrations of the (approximating)
indicator function of (0, 1].
A Boolean formula is a composition of operators ¬, ∧ and ∨
Encoding Boolean Formula.
and variables that take values {0, 1}. This is one of the most fundamental objects in logic and
computer science, and has been extensively studied. If the Boolean formula is expressible using
neural networks, then we can understand the properties of the neural networks from their Boolean
formula counterparts. To simulate a Boolean formula, for each variable, we can build an input node
corresponding to the variable. We only need to encode the logical operators.

1. For ¬φx, we only need to use 1 − X, where X is the neural network node corresponds to the

expression φx.

2. For φx ∧ φy, we can use sign(X + Y − 1.5). For X and Y that takes values in {0, 1},

sign(X + Y − 1.5) evaluates to 1 only when both X and Y are 1.

3. For φx ∨ φy, we can use sign(X + Y − 0.5). For X and Y that takes values in {0, 1},

sign(X + Y − 0.5) evaluates to 0 only when both X and Y are 0.

One can then build a neural network that encodes a Boolean formula recursively according to
the syntactic composition of the formula. In Section 5, we will consider Boolean formulas of special
forms, i.e., 3CNF (conjunction normal form) and 3DNF (disjunction normal form). We will present

8

encodings of the 3CNF and 3DNF formulas using neural networks with squashable functions, and
the construction essentially captures the computation of corresponding logical operators. (Section 6)

As we shall see later, the step-function formulation serves as the intuition for
Remark.
understanding the neural network with squashable activations. However, because we do not have the
perfect step function as the activation, and the values are continuous rather than {0, 1}, to rigorously
prove the results, we need to carefully control the imprecision introduced by the approximation
of squashable functions, and the construction that works for discrete values might not work for
continuous values directly. Nevertheless, one can expect the formal property of neural networks with
squashable activations will not be fundamentally diﬀerent from its Boolean formula or step-function
neural network counterparts. Moreover, the step function gadget can still guide the design for the
network with squashable functions. In fact, we will show that the complexity result in Katz et al.
(2017)—that it is NP-hard to falsify correctness3 of a neural network—can be easily proved using
the squashable function idea, as a corollary of the result we will present (Theorem 6.8).

3 The Interval Universal Approximation Theorem

In this section, we present the interval universal approximation (IUA) theorem. We begin with
background on abstract interpretation for neural networks.

3.1

Interval Abstraction

We will now deﬁne the interval abstract domain and use it to abstractly interpret the semantics of
neural networks.

Set semantics.
collecting (or set) semantics. Formally, given a set S ⊆ Rm,

Given a function f : Rm → R, we will use f s : P(Rm) → P(R) to deﬁne its

f s(S) = {f (x) | x ∈ S}

Henceforth, we will simply use f (S) to denote the collecting semantics version, f s(S), as it will be
clear from context that we are applying the function f to a set.
Evaluating the set semantics on elements of P(Rm), the
The interval abstract domain.
concrete domain, is generally infeasible. The abstract interpretation framework (Cousot and Cousot,
1977) enables constructing sound approximations of collecting semantics by restricting operations to
sets of a certain shape—abstract domains. In this work, we consider the well-known interval abstract
domain, where the kinds of sets are limited to boxes in Rm. An m-dimensional box B is a tuple of
intervals, deﬁning the lower and upper bounds of each dimension:

(cid:104)[l1, u1], . . . , [lm, um](cid:105)

where li, ui ∈ R (we do not need to consider unbounded boxes because we only consider bounded
input space, e.g., ui = ∞).

The abstraction function α transforms an element of the concrete domain to a box. Let

S ∈ P(Rm).

α(S) = (cid:104)[inf Si, sup Si](cid:105)m
i=1
where Si = {xi | x ∈ S} and xi refers to the ith element of x. In other words, Si is a projection of
vectors in S onto their ith element.

3Katz et al. (2017) prove complexity of falsifying, rather than verifying, correctness properties presented as

conjunctions of linear inequalities over inputs and outputs of a network.

9

The concretization function γ transforms boxes into their concrete domain counterparts.

γ((cid:104)[l1, u1], . . . , [lm, um](cid:105)) = {x ∈ Rm | li (cid:54) xi (cid:54) ui}

For clarity of presentation, we will often drop the use of the concretization operator, and treat a

box B as a subset of Rm.
We can now deﬁne abstract versions of
Abstract transformers for neural operations.
the operations of a neural network. We start with primitive arithmetic operations, where we use
superscript # to denote the abstract transformer. Since all of our operations are over scalars, we
deﬁne arithmetic abstract transformers over 1-dimensional boxes.

Deﬁnition 3.1 (Arithmetic abstract transformers). Let B be an m-dimensional box input to the
neural network. We follow the grammar in Theorem 2.1 to deﬁne the abstract transformers.

c# = [c, c]
x#
i = [li, ui],

where li, ui are the ith lower and upper bounds of B

[l1, u1] +# [l2, u2] = [l1 + l2, u1 + u2]

[c, c] ∗# [l, u] = [min(c ∗ l, c ∗ u), max(c ∗ l, c ∗ u)]

We also need to deﬁne abstract transformers for activation functions. We give a general deﬁnition

that works for any function satisfying Eq. (1).

(cid:4)

Deﬁnition 3.2 (Abstract transformer for activations (Gehr et al., 2018)). Let B = (cid:104)[l, u](cid:105) be a
1-dimensional box.

(cid:28)(cid:20)

t#(B) =

min
l(cid:54)x(cid:54)u

t(x), max
l(cid:54)x(cid:54)u

t(x)

(cid:21)(cid:29)

Intuitively, we simply take the minimum and maximum values of t over the interval deﬁned by the
box B. This may not generally be easy to compute, as it involves solving a constrained optimization
problem; however, for monotonically increasing activation functions (all activation functions in
Fig. 2), we can simplify the deﬁnition as follows:

t#(B) = (cid:104)[t(l), t(u)](cid:105)

where we only apply t to the lower and upper bounds of B, since by monotonicity we know that
(cid:4)
t(γ(B)) ⊆ [t(l), t(u)].

Example 3.3. Recall the neural network N (x) = σ(x1 + 0.5x2), deﬁned in Theorem 2.2. Suppose
we want to abstractly interpret it on the 2-dimensional box B = (cid:104)[0, 1], [0.6, 1](cid:105), i.e., the set of all
values where x1 ∈ [0, 1] and x2 ∈ [0.6, 1].

N #(B) = σ#([0, 1] +# [0.5, 0.5] ∗# [0.6, 1])
= σ#([0, 1] +# [0.3, 0.5])
= σ#([0.3, 1.5])
= [σ(0.3), σ(1.5)]

(evaluate ∗#)
(evaluate +#)
(evaluate σ#; σ is monotonic)

(cid:4)

Soundness.
following theorem establishes soundness of our abstract transformers.

Finally, we shall use N # to denote the abstract version of a neural network N . The

10

Theorem 3.4 (Soundness of abstract transformers). Let N : S → R be a neural network with
domain S ⊆ Rm. Let B be an m-dimensional box such that γ(B) ⊆ S. Then, N (γ(B)) ⊆ γ(N #(B)).

The soundness of abstract interpretation enables the veriﬁcation of robustness and other correct-
ness properties. However, because abstract interpretation is not necessarily complete, and therefore
for some correctness properties we may fail to construct proofs. As we shall see, the interval universal
approximation theorem that we will present in Section 3.2 implies that in fact it is possible to verify
certain robustness deﬁnitions ((cid:96)∞) using interval abstract interpretation.

3.2 The Interval Universal Approximation Theorem

In this section, we state the interval universal approximation (IUA) theorem.
Interval approximation. We begin by deﬁning what it means to approximate a function using
a neural network. We assume some ﬁxed continuous function f : C → R, with a compact domain
C ⊂ Rm, that we wish to approximate.

Deﬁnition 3.5 (δ-approximation). Let δ > 0. A neural network N δ-approximates f iﬀ for all
(cid:4)
x ∈ C, we have f (x) − δ (cid:54) N (x) (cid:54) f (x) + δ.

We now generalize this point-wise approximation deﬁnition to elements of our abstract domain.

Deﬁnition 3.6 (δ-interval approximation). Let δ > 0. A neural network N δ-interval approximates
f iﬀ for every box B ⊆ C, we have

[l + δ, u − δ] ⊆ N #(B) ⊆ [l − δ, u + δ]

where l = min f (B) and u = max f (B).

(cid:4)

Informally, δ-interval approximation says that the box output of abstract interpretation N #(B)
is up to δ away from the tightest bounding box around the collecting semantics f (B). Revisit Fig. 1
from Section 1 for an illustration of δ-interval approximation. Observe that δ-approximation is a
special case of δ-interval approximation, when the box B is a point in C, i.e., γ(B) is a singleton set.
Interval universal approximation (IUA). We now state the IUA theorem:

Theorem 3.7 (Interval universal approximation). Let f : C → R be a continuous function
on a compact domain C ⊂ Rm. Let t be a squashable activation function. For all δ > 0, there
exists a neural network N , using only activations t, that δ-interval approximates f .

Informally, the theorem says that we can always ﬁnd a neural network whose abstract interpre-
tation is arbitrarily close to the collecting semantics of the approximated function. Note also that
there exists such a neural network for any ﬁxed squashable activation function t.

As we discuss in Section 7, the IUA theorem has very exciting implications: We can show that
one can always construct provably robust neural networks using any squashable activation function
(Theorem 7.4). The robustness property, which states that small perturbations in the input result
in the same classiﬁcation by a neural network, has been heavily studied recently, and the interval
domain has been used to prove robustness in a range of domains (Gehr et al., 2018; Wang et al.,
2018; Anderson et al., 2019). Our result hints at a very close theoretical connection between robust
neural networks and proofs using interval-based abstract interpretation.

In the supplementary materials, we give a generalization of the IUA theorem to functions and

networks with multiple outputs.

11

3

2

1

3

2

1

3

2

1

G

−2 −1

1

2

3

−2 −1

1

2

3

−2 −1

1

2

3

(a) 1-grid over R2

(b) Three boxes in G

(c) Box G & neighborhood ν(G)

Figure 5: A grid illustration

4 Proof of IUA Theorem

We will show the IUA theorem, as stated in Theorem 3.7. Our proof uses the framework of Baader et al.
(2020), which is a delicate design of a summation of indicator functions. Though constructing indicator
functions is a classical idea in approximation theory, we are working in interval approximation, which is
harder than pointwise approximation because interval approximation implies pointwise approximation.
The interval approximation construction diﬀers from the usual pointwise approximation one in the
following two ways:

1. In the pointwise case, we only need to grid the input domain. As long as one can approximate
the target function within each grid using an indicator function, the pointwise approximation
is achieved. However, this does not work in the interval case because the input can be a box
which might span over several grids. Baader et al. (2020) discovered an ingenious observation
that if one slices the domain of a function, and approximate each slice, then the usual indicator
approximation works because one can control the loss of precision of each slice.

2. The interval semantics and the pointwise semantics can be vastly diﬀerent, therefore, the
pointwise indicator function might not remain an indicator in the interval semantics. As we
demonstrate in Theorem 5.8, it is in general a hard task to build a network whose interval
semantics approximate another network’s set semantics. Baader et al. (2020) use a construction
from He et al. (2018) to build the indicator function from the ReLU units, and carefully analyze
that this construction is indeed an indicator under the interval semantics, which is in fact
among the most technical and involved parts. We instead use ideas of squashable functions
introduced in Section 2 to approximate the indicator function. This results in a technique that
is simpler to analyze and also works for a larger set of functions, including ReLU.

To summarize, we extend the IUA restricted to ReLU-network shown in Baader et al. (2020) to
a more general class of neural networks, and provide a simple-to-analyze indicator construction from
squashable functions. If we only consider ReLU network, our construction will achieve a linear factor
reduction in the usage of activation units to build the approximation network compared to Baader
et al. (2020).

4.1 Approximating Indicator Functions

In this section, we will give the precise construction of the approximating indicator function and the
rigorous proof of correctness. Because we will grid the input space, we need an indicator function
for each grid cell, i.e., high-dimensional box. We start from building the one-dimensional indicator
function and then use that to build the high-dimensional one.

Fix (cid:15) > 0. Consider a standard grid of vertices over a compact set C, where any two neighboring
vertices are axis-aligned and of distance (cid:15); we will call this an (cid:15)-grid. Let [a1, b1] × . . . × [am, bm]
be a box G on the grid, where [ai, bi] is the range of G at dimension i. In other words, bi − ai is a

12

multiple of (cid:15). Let G be the set of boxes whose vertices are in the grid. The neighborhood ν(G) of
G is [a1 − (cid:15), b1 + (cid:15)] × . . . × [am − (cid:15), bm + (cid:15)]. Our goal is to construct an indicator function whose
value is close to 1 within G, and close to 0 outside G’s neighborhood ν(G). The idea of using grid is
similar to the nodal basis in He et al. (2018). See Fig. 5 for an example of grid and boxes in the grid.

4.1.1 One-dimensional indicator function

We will ﬁrst show how to construct an indicator function for a 1-dimensional box, using a squashable
activation function as we have seen in Section 2. The main challenge is choosing the dilation factor
that results in small precision loss when abstractly interpreting the neural network.

By the IUA theorem statement, we are given some squashable activation function t. Without

loss of generality, we make the following two assumptions about t:

1. We assume that t already satisﬁes Eq. (1) (Theorem 2.3):

lim
x→−∞

t(x) = a

and

lim
x→∞

t(x) = b

and

∀x ∈ R. t(x) ∈ [a, b]

Otherwise, by Theorem 2.3, we can use t to build a t(cid:48) that satisﬁes Eq. (1).

2. We assume that the left and right limits of t are 0 and 1, respectively. (If not, we can apply an

aﬃne transformation to the results of t to make the left and right limits 0 and 1.)

The activation function t has limits at both sides, but the
Loss of precision from limits.
function might never reach the limit. For example, the right limit of the sigmoid function, σ, is
1, but ∀x. σ(x) (cid:54)= 1. This will lead to a loss of precision when we use t to model a step function.
However, we can carefully apply mathematical analysis to rigorously bound this imprecision.
Dilation to approximate step function. We now discuss how to dilate t to get a step-function-
like behavior. By deﬁnition of limit, we know the following lemma, which states that by suﬃciently
increasing the input of t, we can get θ close to the right limit of 1, and analogously for the left limit.

Lemma 4.1. ∃D > 0 such that:

1. If x (cid:62) D, then t(x) ∈ (1 − θ, 1].

2. If x (cid:54) −D, then t(x) ∈ [0, θ).

Because the grid size is (cid:15), we want the step-function ap-
proximation to achieve a transition from ≈ 0 to ≈ 1 within
(cid:15). Let µ be the dilation factor. Following Theorem 4.1, we
would like the following:

1. if x (cid:62) 0.5(cid:15), then t(µx) ∈ (1 − θ, 1];

2. if x (cid:54) −0.5(cid:15), then t(µx) ∈ [0, θ).

From Theorem 4.1, we only need µx > D when x >

0.5(cid:15); therefore, µ = 2D/(cid:15) suﬃces as the dilation factor.

Lemma 4.2. Let µ = 2D/(cid:15). The following is true:

1. if x (cid:62) 0.5(cid:15), then t(µx) ∈ (1 − θ, 1];

2. if x (cid:54) −0.5(cid:15), then t(µx) ∈ [0, θ).

13

1

(cid:54) θ

−0.5(cid:15)

0.5(cid:15)

Figure 6: Illustrating the loss of precision
θ incurred through using a squashable
activation to approximate a step func-
tion. The length of the red arrows is
(cid:54) θ.

Example 4.3. Fig. 6 illustrates the loss of precision θ incurred by our construction.

(cid:4)

Now that we have discussed how to approximate a step
Indicator function on dimension i.
function, we are ready to show how to approximate an indicator function for one dimension of a box
G in the grid.

Suppose the projection of a box G on dimension i is [ai, bi]. Because G is in the (cid:15)-grid, bi − ai (cid:62) (cid:15);
and the projection of neighborhood ν(G) on dimension i is [ai − (cid:15), bi + (cid:15)]. We want to build an
indicator function that has value close to 1 on [ai, bi], and value close to 0 on R \ [ai − (cid:15), bi + (cid:15)].
Notice how we may lose precision within the neighborhood of G; this is expected, because our
approximation may not be able to exactly tell if we are in G or its neighborhood.

Inspired by how to construct an indicator function from a step function, we will take the diﬀerence

between two shifted step functions. Let

ˆt(x) = t (µ (x + 0.5(cid:15) − ai)) − t (µ (x − 0.5(cid:15) − bi))

(2)

The following lemmas show that ˆt roughly behaves like an indicator function:
Properties of ˆt.
its value within a box’s ith dimension [ai, bi] is ≈ 1; its value outside of the neighborhood is ≈ 0; its
value globally is bounded by 1 We will analyze the values of the two terms in ˆt.

The following lemma states that if x is within the box’s ith dimension, then the ﬁrst term is

close to 1 and the second term is close to 0, resulting in ˆt(x) ≈ 1.

Lemma 4.4. If x ∈ [ai, bi], then the following is true:

1. t(µ(x + 0.5(cid:15) − ai)) ∈ (1 − θ, 1].

2. t(µ(x − 0.5(cid:15) − bi)) ∈ [0, θ).
The next two lemmas state that if x is outside the neighborhood, then the two terms are similar,

resulting in a ˆt(x) ≈ 0.

Lemma 4.5. If x (cid:54) ai − (cid:15), then the following is true:

1. t(µ(x + 0.5(cid:15) − ai)) ∈ [0, θ).

2. t(µ(x − 0.5(cid:15) − bi)) ∈ [0, θ).

Lemma 4.6. If x (cid:62) bi + (cid:15), then the following is true:

1. t(µ(x + 0.5(cid:15) − ai)) ∈ (1 − θ, 1].

2. t(µ(x − 0.5(cid:15) − bi)) ∈ (1 − θ, 1].

Abstract precision of ˆt. We are now ready to prove properties about the abstract interpretation
of our 1-dimensional indicator approximation, ˆt. The following lemma states that the abstract inter-
pretation of ˆt, ˆt#(B), is quite precise: if the 1-dimensional input box B is outside the neighborhood
of G, on G’s ith dimension, then the output box is within θ from 0; if the input box B is within the
ith dimension of G, then the output box is within 2θ from 1.

Lemma 4.7. For a 1-dimensional box B, the following is true:

1. ˆt#(B) ⊂ (−∞, 1].

2. If B ⊆ (−∞, ai − (cid:15)] or B ⊆ [bi + (cid:15), ∞), then ˆt#(B) ⊆ (−θ, θ).

3. If B ⊆ [ai, bi], then ˆt#(B) ⊆ (1 − 2θ, 1] .

14

x1
...

ˆt1
...

xm

ˆtm

µ

(cid:80)

t

0.5

0
−5

5

0

0

5 −5

(a) Illustration of NG (added constants elided)

(b) Plot of NG on G = [0, 1] × [0, 1] using the sigmoid activation,
with µ = 10, 2θ = 0.05, and (cid:15) = 1. Observe how NG(x) is ≈ 1
for values of x ∈ G, and ≈ 0 elsewhere.

Figure 7: Step 2 Illustration of neural network NG

4.1.2 Approximating an m-dimensional indicator

We saw how to construct an indicator approximation for a 1-dimensional box. We will now show
how to construct an indicator function approximation NG for an m-dimensional box.

Throughout, we assume a box G = [a1, b1] × · · · × [am, bm]. So, if x ∈ G, then xi ∈ [ai, bi] for all
i ∈ {1, . . . , m}; if x (cid:54)∈ ν(G), i.e., not in the neighborhood, then ∃i such that xi (cid:54) ai − (cid:15) or xi (cid:62) bi + (cid:15).
Constructing NG. We want to construct an indicator function whose value within a box G is
close to 1 and outside the neighborhood ν(G) is close to 0. In the multi-dimensional case, m (cid:62) 2,
we do not know at which, if any, dimension j of an input is outside the neighborhood of G. The
1-dimensional indicator approximation, ˆt, which we constructed earlier, can be used to tell us, for
each dimension j, whether xj is within the bounds of the neighborhood of G. Therefore we can
construct a logical OR approximation that applies ˆt to each dimension and takes the OR of the
results. Speciﬁcally,

1. We will construct a function that applies ˆt to each dimension, and sums the results such that

the answer is > 0 if x ∈ G, and < 0 if x (cid:54)∈ ν(G).

2. Then, we can use the step-function approximation to indicate the step of the answer.

Formally, we deﬁne the neural network NG as follows:

(cid:32)

NG(x) = t

µ

(cid:32) m
(cid:88)

(cid:33)(cid:33)

Hi(xi) + 0.5(cid:15)

(3)

i=1
where Hi(x) = ˆti(x) − (1 − 2θ), and ˆti is ˆt using the range [ai, bi] of the ith dimension of G. The
neural network NG is graphically depicted in Fig. 7a.

The function term (cid:80)m

i=1 Hi(xi) evaluates to a positive value if x ∈ G and to a negative value if
x (cid:54)∈ ν(G). Observe that we need to shift the result of ˆt by (1 − 2θ) to ensure a negative answer if
one of the dimensions is outside the neighborhood. Then, we use t to approximate the step function,
as we did in the 1-dimensional case, giving ≈ 1 if x ∈ G, and ≈ 0 if x (cid:54)∈ ν(G).

Example 4.8. Fig. 7b shows a plot of NG for x ∈ R2.

(cid:4)

Abstract precision of NG. We are now ready to analyze the abstract precision of NG. We ﬁrst
consider Hi in the following lemma. For any box B ⊆ C, let Bi be its projection on dimension i,
which is an interval.

The following lemma states that if B is in the box G, then (cid:80)

i H #

i

is positive; otherwise, if B is

outside the neighborhood of G, then (cid:80)

i H #

i

is negative.

15

Lemma 4.9 (Abstract interpretation of Hi). For any box B ⊆ C, the following is true:

1. If B ⊆ G, then (cid:80)m

i=1 H #
2. If B ⊆ C \ ν(G), then (cid:80)m

i (Bi) ⊆ (0, ∞).

i=1 H #

i (Bi) ⊆ (−∞, −(cid:15)).

The following theorem states the precision of the abstract interpretation of NG: if the input box
is in G, then the output box is within θ from 1; if B is outside the neighborhood of G, then the
output box is within θ from 0.

Theorem 4.10 (Abstract interpretation of NG). For any box B ⊆ C, the following is true:

1. N #

G (B) ⊆ [0, 1].

2. If B ⊆ G, then N #

G (B) ⊆ (1 − θ, 1].

3. If B ⊆ C \ ν(G), then N #

G (B) ⊆ [0, θ).

Proof.
Statement (1): See deﬁnition of NG in Eq. (3). The outer function of NG is t, whose range is
[0, 1] by the deﬁnition of squashable functions and our assumption that the left and right limits are
0 and 1. Therefore, N #
Statement (2): If B ⊆ G, from Theorem 4.9, we know that (cid:80)m

G (B) ⊆ [0, 1].

i (Bi) ⊆ (0, ∞). Then,

i=1 H #

m
(cid:88)

i

H #

i (Bi) +# (0.5(cid:15))# ⊆ (0, ∞) +# (0.5(cid:15))# ⊆ (0.5(cid:15), ∞)

From Theorem 4.2, we know that if x (cid:62) 0.5(cid:15), then 1 − θ < t(µx) (cid:54) 1. Therefore,

N #

G (B) = t#(µ# ∗# (0.5(cid:15), ∞)) ⊆ (1 − θ, 1]

Statement (3): If B ⊆ C \ ν(G), from Theorem 4.9, we know that (cid:80)m
Then,

i=1 H #

i (Bi) ⊆ (−∞, −(cid:15)).

(cid:80)m

i=1 H #

i (Bi) +# (0.5(cid:15))# ⊆ (−∞, −(cid:15)) +# (0.5(cid:15))# ⊆ (−∞, −0.5(cid:15))

From Theorem 4.2, we know that if x (cid:54) −0.5(cid:15), then 0 (cid:54) t(µx) < θ. Therefore,

N #

G (B) = t#(µ# ∗# (−∞, −0.5(cid:15))) ⊆ [0, θ)

To construct a single indicator function, we use 2m + 1 activation
Complexity of construction.
functions, with depth 2 and width 2m. If we restrict ourselves to ReLU activations, we use 4m + 2
neurons, with depth 4 and width 2m; in contrast, Baader et al. (2020) use 10m − 3 ReLu functions,
with depth 3 + log2(m), and width 4m.

16

2
1.5
1
0.5

2
1.6
1.2
0.8
0.4

1

2

3

4

5

1

2

3

f3

f0
4

2
1.6
1.2
0.8
0.4

2
1.6
1.2
0.8
0.4

5

1

2

3

4

5

1

2

3

4

5

(a) f (x) = sin(2x) + 1

(b) Sliced f (x)

(c) Example slice f0

(d) Example slice f3

Figure 8: Slicing f (x) = sin(2x) + 1 with approximation tolerance δ = 1.2.

4.2 Overview of Complete Proof of IUA

We have shown how to approximate an indicator function and how to control the precision of its
abstract interpretation (Theorem 4.10). We now complete the construction of the neural network N
following the technique of Baader et al. (2020) for ReLU networks. Because we use an arbitrary
squashable function to approximate the step function, this introduces extra imprecision in comparison
with ReLUs. We thus need a ﬁner function slicing to accommodate it, i.e., we use a slicing size of
δ/3 instead of δ/2 in Baader et al. (2020). We provide the detailed analysis in the supplementary
materials. In what follows, we outline on how to build the network N that satisﬁes the IUA theorem.
Let f : C → R be the continuous function we need to approximate, and δ be the
Slicing f .
approximation tolerance, as per IUA theorem statement (Theorem 3.7). Assume min f (C) = 0.4
Let u = max f (C). In other words, the range of f is [0, u].

Let τ = δ
3

. We will decompose f into a sequence of function slices fi, whose values are restricted
to [0, τ ]. Let K = (cid:98)u/τ (cid:99). The sum of the sequence of function slices is f . The sequence of functions
fi : C → [0, τ ], for i ∈ {0, . . . , K}, is:



fi(x) =

f (x) − iτ,
0,
τ,



iτ < f (x) (cid:54) (i + 1)τ
f (x) (cid:54) iτ
(i + 1)τ < f (x)

Example 4.11. See Fig. 8 for slicing f (x) = sin(2x) + 1 with δ = 1.2.

(cid:4)

Approximating fi. We will use the indicator approximation NG (Eq. (3)) to construct a neural
network Ni that approximates fi. Because C is compact, |G| is ﬁnite. Consider 1
τ fi(x); it is roughly
similar to an indicator function for the set S = {x ∈ C | f (x) > (i + 1)τ }, i.e., indicating when f (x)
τ fi(x), we will consider all boxes
is greater than the upper bound of the ith slice. To approximate 1
in G that are subsets of S, and construct an indicator function to tell us whether an input x is in
τ fi(x) as
those boxes. Let Gi = {G ∈ G | f (G) > (i + 1)τ }. Now construct Ni(x) that approximates 1








Ni(x) = t

µ



(cid:88)

NG(x) − 0.5



 .

G∈Gi

Sum all Ni.

Because (cid:80)K

i=0 fi(x) = f (x), and Ni(x) approximates 1

τ fi(x), we deﬁne N as

N (x) = τ

K
(cid:88)

i=0

Ni(x)

N δ-interval approximates f ; therefore, the IUA theorem holds.

4Otherwise, we can shift f such that min f (C) = 0.

17

5 Hardness of Range Approximation

In this section, we will present the range approximation (RA) problem and some basics of computa-
tional complexity theory. By studying the complexity of RA, one can understand the hardness of
IUA.

5.1 The Polynomial Hierarchy

The polynomial hierarchy generalizes the deﬁnitions of P, NP, coNP. Let L be a language.

Deﬁnition 5.1 (The NP and coNP classes). L is an NP language if there exists a polynomial-time
Turing machine M , and a polynomial q such that

x ∈ L if and only if ∃u ∈ {0, 1}q(|x|). M (x, u) = 1.

coNP languages are similarly deﬁned, but with a universal (∀) quantiﬁer instead.

(cid:4)

Example 5.2. Deciding whether a Boolean formula is satisﬁable is an NP problem. Deciding whether
(cid:4)
a Boolean formula is a tautology is a coNP problem.

We write Σ1 = NP, and Π1 = coNP. Notice that the diﬀerence between Σ1 and Π1 is the leading
quantiﬁer. Indeed, the deﬁnitions of Σn and Πn have similar structure as NP and coNP, with n
alternating quantiﬁers rather than a single quantiﬁer. In this paper, we only need to consider Σ2
and Π2, which we deﬁne below. (Fig. 9 illustrates the polynomial hierarchy).

Deﬁnition 5.3 (The Σ2 class). L is a Σ2 language if there exists a polynomial-time Turing machine
M , and a polynomial q such that

x ∈ L if and only if ∃u1 ∈ {0, 1}q(|x|)∀u2 ∈ {0, 1}q(|x|). M (x, u1, u2) = 1.

(cid:4)

Deﬁnition 5.4 (The Π2 class). L is a Π2 language if there exists a polynomial-time Turing machine
M , and a polynomial q such that

x ∈ L if and only if ∀u1 ∈ {0, 1}q(|x|)∃u2 ∈ {0, 1}q(|x|). M (x, u1, u2) = 1.

Deﬁnition 5.5 (The ∆2 class). ∆2 = Σ2 ∩ Π2.

(cid:4)

(cid:4)

Note that NP, coNP ⊆ ∆2, because one can substitute an empty string to u1 or u2 in Theorems 5.3

and 5.4. The polynomial hierarchy is the union of all Σn languages.

Deﬁnition 5.6 (∆2-intermediate language). A set of languages L is ∆2-intermediate if NP∪coNP ⊆
(cid:4)
L and L ⊆ ∆2.

By deﬁnition, ∆2 = Π2 ∩ Σ2, and NP ∪ coNP ⊆ ∆2 because both NP and coNP
Remark.
are subsets of ∆2. It is unknown whether NP ∪ coNP = ∆2 or not. However, if coNP (cid:54)⊂ NP as is
commonly believed, NP (cid:40) L when L is ∆2-intermediate.

18

Σn

Σ2

...

∆n

∆2

Πn

Π2

Σ1 = NP

coNP = Π1

Σ0 = P = Π0

Figure 9: A diagram of the polynomial hierarchy, where arrows denote the inclusion relationship

5.2 The Range Approximation Problem

In this section, we present the RA problem. This will reveal one of the fundamental diﬀerences
between the classical UA and IUA because the interval approximation can be studied via decision
problems (Section 6.3) and therefore one can attempt to understand its computational complexity.
We restrict our attention to neural networks that map [0, 1]m to codomain [0, 1], i.e., f : [0, 1]m →
[0, 1] is a neural network. Throughout the paper, when we use polynomial-time executable, we mean
polynomial in terms of m.

Deﬁnition 5.7 (δ-range approximation). Let δ > 0 and f : [0, 1]m → [0, 1] be a neural network. We
can δ-range approximate f if we ﬁnd a (cid:54) b ∈ [0, 1] such that

[l + δ, u − δ] ⊆ [a, b] ⊆ [l − δ, u + δ]

where l = min f ([0, 1]m) and u = max f ([0, 1]m).

(cid:4)

Note that δ-range approximation (Theorem 5.7) is weaker than δ-interval approximation (IA)

(Theorem 3.6) in the following ways:

1. In RA, we only need f to be a neural network, while in IA, we aim to approximate any

continuous function;

2. In IA, we require the approximation holds for any B in the domain, while in RA, we only need

it holds for the domain [0, 1]m;

3. In IA, we need to ﬁnd a neural network that approximates f , but in RA, we do not require
any speciﬁc ways to ﬁnd a, b. If one can ﬁnd the δ-interval approximation neural network,
abstractly executing the neural network will return a, b automatically.

As a result, if we show that the δ-range approximation problem is hard, then building a δ-interval
approximation neural network that can be polynomial-time executable has to be hard.

We now state a dichotomy theorem on the δ-range approximation problem:

19

Theorem 5.8 (Dichotomy of δ-range approximation). Let f : [0, 1]m → [0, 1] be a neural
network with any squashable functions. Then

1. If δ (cid:62) 1/2, it is trivial to δ-range approximate f .

2. If δ < 1/2, it is NP-hard and coNP-hard to δ-range approximate f . Moreover, if we
assume that the neural network takes polynomial time to execute and the input has ﬁnite
precision, then it is ∆2-intermediate to δ-range approximate f .

The ﬁrst statement is trivial, because if δ (cid:62) 1/2, we can choose a = b = 1/2. Because we have

0 (cid:54) l (cid:54) u (cid:54) 1, then it is always true that when δ (cid:62) 1/2,

and

u − δ (cid:54) 1/2 (cid:54) u + δ

l − δ (cid:54) 1/2 (cid:54) l + δ.

We will show the second statement in Section 5. The idea is to reduce the problem of determining
the range of a Boolean formula to the δ-range approximation problem, by encoding the Boolean
formula as a neural network with step functions as the activation functions. Since squashable units
can arbitrarily approximate the step function, the δ-range approximation problem for the neural
network is also hard.

Even though we knew that exactly ﬁnding the range is hard, δ-
Implications of RA hardness.
range approximation might be much easier. As an analogy, many NP-complete optimization problems
have polynomial-time approximation algorithms (Vazirani, 2003). Theorem 5.8 is surprising because
it shows a dichotomy that it is either trivial or very hard to achieve the approximation of the set
semantics depending on how close/tolerant one demands the approximation to be. As we will reveal
in Section 6, exactly deciding or approximating the range of a neural network are not very diﬀerent
from the complexity-theoretical view, even though the former appears a harder task because it
implies the latter (Theorem 6.4).

Theorem 5.8 also implies that even if we have a neural network N0 that approximates some
function in the pointwise sense, it does not help build the interval approximator because one cannot
simply build another network N whose abstract interpretation approximates the set semantics of N0.
This shows the non-triviality of the IUA, even though UA is a classical topic and has been studied
extensively.

6 Proof of Hardness of RA

In this section, we will show the second statement of Theorem 5.8, that it is ∆2-intermediate to
δ-range approximate f for δ < 1/2 and f : [0, 1]m → [0, 1], where f is a neural network with any
squashable functions. Before showing this, we will consider the Boolean formula counterpart of
this problem. This will provide us with an intuition for the original neural network version of the
range-approximation problem.

Because a Boolean formula is only valued in {0, 1}, δ-range approximating a Boolean formula
eﬀectively decides the exact range of the Boolean formula. Therefore, let’s consider the following
problem: deciding exactly the range of a Boolean formula. We show that this problem is ∆2-
intermediate.

20

6.1 Deciding the Range of a Boolean Formula

Let φ be an arbitrary Boolean formula. Let Rφ be the range of φ. To decide the range of φ amounts
to deciding whether Rφ = {0}, Rφ = {1}, or Rφ = {0, 1}. To show a problem is ∆2-intermediate, we
need to show that it is in ∆2 = Σ2 ∩ Π2; and it is both Σ1-hard and Π1-hard. Recall that Σ1 = NP
and Π1 = coNP.

Rφ = {0} can be expressed as

Similarly, Rφ = {1} can be expressed as

Rφ = {0, 1} can be expressed as

∀x. φ(x) = 0.

∀x. φ(x) = 1.

∃x, y. φ(x) = 1 ∧ φ(y) = 0.

All of them can be expressed within both Σ2 and Π2 languages. Therefore, deciding the range of a
Boolean formula is in Σ2 ∩ Π2 = ∆2.

The canonical NP-hard problem is deciding whether a Boolean formula is satisﬁable, and the
canonical coNP-hard problem is deciding whether a Boolean formula is a tautology. Indeed, if one
can decide the range of a Boolean formula, then one can easily tell whether the Boolean formula is
satisﬁable or not, and whether the Boolean formula is a tautology or not.
Therefore, deciding the range of a Boolean formula is ∆2-intermediate.

6.2 Range Approximating a Neural Network

Showing that deciding the range of a formula is a ∆2-intermediate problem provides an intuition for
why the range approximation problem is ∆2-intermediate. Our proof of the hardness of RA consists
of 3 parts:

1. The RA problem is in Σ2 ∩Π2. We show that we can express deciding the range of a polynomial-
time executable neural network using Π2 and Σ2 languages. Because deciding the range of a
function is harder than approximating the range, this shows that approximating the range is
also in ∆2.

2. The RA problem is NP-hard.

3. The RA problem is coNP-hard.

We will build reductions from the NP-hard and coNP-hard problems to the RA problem, and this
shows that both exactly deciding and approximating the range of a neural network are NP- and
coNP-hard.

The NP-hard problem is whether a Boolean formula in 3CNF is satisﬁable. The coNP-hard
problem is whether a Boolean formula in 3DNF is a tautology. In particular, we will encode the
3CNF and 3DNF formulas computation using neural networks with squashable functions, which
comes from on how to encode a Boolean formula using neural networks with perfect step functions.
RA is in ∆2. We ﬁrst show that the RA problem is ∆2. We will need the assumptions that
the neural network is polynomial-time executable in terms of m and the input precision is ﬁnite,
otherwise, we cannot use a polynomial-time Turing machine to simulate the execution of the neural
network. We require ﬁnite input-precision because we want to ensure there are only exponentially
many inputs. Note that the NP-hardness and coNP-hardness of RA do not need these assumptions,
so if coNP (cid:54)⊂ NP as commonly believed, RA is always harder than any NP-complete problem.

21

Lemma 6.1. The δ-range approximation problem as deﬁned in Theorem 5.8 is in ∆2 for δ < 1/2,
if the neural network is polynomial-time executable and the input has ﬁnite precision.

Proof. We will show that exactly deciding the range of f is in ∆2. Because exactly deciding a range
is harder than the approximating it, this also shows that approximating the range a polynomial-time
executable neural network is in ∆2.

Because f is a continuous function, deciding the range of f is [a, b] can be written as

∃x, y. ∀z. f (x) = a ∧ f (y) = b ∧ f (z) (cid:54) b ∧ f (z) (cid:62) a.

(4)

In Eq. (4), because z is not dependent on x, y, we can also switch the order of the quantiﬁer.

Therefore, deciding the range of f is in both Σ2 and Π2, and thus in ∆2.

Hardness of RA. We need to show that the RA problem is both NP-hard and coNP-hard, which
is formally stated in the following lemmas:

Lemma 6.2. The δ-range approximation problem as deﬁned in Theorem 5.8 is NP-hard for δ < 1/2.

Lemma 6.3. The δ-range approximation problem as deﬁned in Theorem 5.8 is coNP-hard for
δ < 1/2.

We will present a decision problem formulation of approximating the maximum value of the
neural network, and show a reduction from the SAT problem to the decision problem in Section 6.3.
This shows that the δ-range approximation problem is NP-hard.

The idea of the reduction is to use neural networks with squashable functions to encode a 3CNF
formula as discussed in Section 2. Approximating the range of the neural network also approximates
the range of the Boolean formula. As discussed in Section 6.1, given the range of the Boolean
formula, it is easy to know its satisﬁability. Then we know the δ-range approximation problem is
NP-hard.

The proof of Theorem 6.3 very much resembles that of Theorem 6.2. We encode a 3DNF formula

instead of a 3CNF formula, and delegate the proof to the supplementary materials.
Theorems 6.2 and 6.3 also imply that deciding the range
Range decision of neural networks.
of a neural network is both NP-hard and coNP-hard. Together with the result that deciding the
range of the neural network is in ∆2, we have the following corollary:

Corollary 6.4. Let f : [0, 1]m → [0, 1] be a neural network that takes polynomial time to execute
and the input has ﬁnite precision, then it is ∆2-intermediate to decide the range of f .

6.3 The NP-Hardness of Range Approximation

In this section, we will prove Theorem 6.2. f : [0, 1]m → [0, 1] is a neural network with any squashable
units. We will show that for δ < 1/2, it is NP-hard to approximate the maximum value of f over
[0, 1]m up to an additive factor of δ (Theorem 6.5). This is accomplished by a reduction from the
3SAT problem, and we show that there is an gap between the maximums of neural networks that
encode either satisﬁable formulas or unsatisﬁable formulas (Theorem 6.7). This gap enables us to
show that approximating of maximum of neural network is NP-hard because approximating the
maximum can tell whether the 3CNF formula is satisﬁable or not. Because if one can approximate the
range of a neural network, one can also approximate its maximum. This implies that approximating
the range of a neural network is NP-hard (Theorem 6.2).

Our reduction maps 3CNF formulas φ over m variables to a neural network f on m variables,
such that if φ is satisﬁable then the maximum value attained by f over [0, 1]m lies in (1/2 + δ, 1],

22

and if φ is unsatisﬁable, the maximum value of f over [0, 1]m lies in [0, 1/2 − δ]. The neural network
returned by the reduction can be built using any squashable activation.

Decision Problem Formulation.
map [0, 1]m to codomain [0, 1], and let F = (cid:83)

Let Fm be the set of neural networks over m variables that

m(cid:62)1 Fm. For δ < 1/2, let
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(f (x)) > 1/2 + δ

max
x∈[0,1]m

f ∈ Fm

f ∈ Fm

(cid:12)
(cid:12)
(cid:12)
(cid:12)

max
x∈[0,1]m

(f (x)) (cid:54) 1/2 − δ

F +

δ =

F −

δ =

(cid:26)

(cid:91)

m(cid:62)1

(cid:26)

(cid:91)

m(cid:62)1

(cid:27)

(cid:27)

(5)

(6)

Lemma 6.5. Given f ∈ F +

δ ∪ F −

δ , it is NP-hard to determine whether f ∈ F +

δ or f ∈ F −
δ .

Since an eﬃcient algorithm for δ-range approximating a neural network also approximates its

maximum value, Theorem 6.2 is an immediate consequence of Theorem 6.5.

Let X1, . . . , Xm be Boolean variables, and Li = (¬)Xi is called a literal (with
SAT reduction.
the negation operator, it is called a negative literal). A 3CNF instance φ is a conjunction of clauses
of the form C1 ∧ . . . ∧ Ck, where each clause Cj is a disjunction of 3 literals. To distinguish the
3CNF instance and its simulation using the neural network, we will use uppercase letters to denote
components in the 3CNF instance, and lowercase letters to denote the corresponding construction in
the neural network.
Simulation of 3CNF. We will need to simulate the logical operations using the neural network
operations. If we have perfect step functions as the activations and the input values are discrete,
then the 3CNF instance can be easily simulated. Using the idea presented in Section 2, we will
scale and shift the activations to simulate the step function. Deﬁne the following three activation
functions that will be used in the reduction:

t1(z) =

t2(z) =

t3(z) =

(cid:40)(cid:62) −0.2 and (cid:54) −0.1,
(cid:62) 0.5 and (cid:54) 0.6,

z (cid:54) 0.6
z (cid:62) 0.7

(cid:40)(cid:62) 1
2k
(cid:54) −1,

and (cid:54) 1
k ,

z (cid:62) 0.1
z (cid:54) 0

(cid:40)

> 1/2 + δ and (cid:54) 1,
< 1/2 − δ and (cid:62) 0,

z (cid:62) 0.5
z (cid:54) 0

(7)

(8)

(9)

Recall that we use upper case letters for Boolean variables and lower case letters for neural
network variables. The goal is to show Theorem 6.5, and it can be proved via Theorem 6.7, i.e., for
satisﬁable or unsatisﬁable instances, the neural networks have diﬀerent upper bounds. On one hand,
if the instance φ is satisﬁable, we can take the satisﬁable assignment X of values 0s and 1s as an
input to the neural network and show that f (x) > 1/2 + δ. On the other hand, if f (x) > 1/2 − δ,
we can use x to construct a satisﬁable assignment for φ. The output values in Eq. (9) are chosen to
generate the gap as in Theorem 6.5. The choice for other values in Eqs. (7) to (9) are not unique.
We only need to ensure that for satisﬁable or unsatisﬁable instances, we can produce the gap.

We will simulate the 3CNF instance using a neural network in the following way.

• For each variable Xi, construct an input node xi.

23

x1

x2

x3

xn

l21 = 1 − x1

l11 = x1

l12 = 1 − x2

l22 = x2

l13 = x3

l23 = x4

t1

t2

t3

c1 = t2(t1(l11) + t1(l12) + t1(l13))

y

y = t3(c1 + c2)

c2 = t2(t1(l21) + t1(l22) + t1(l23))

Figure 10: The neural network encoding for (X1 ∨ ¬X2 ∨ X3) ∧ (¬X1 ∨ X2 ∨ X4)

• Simulate the negation operator using li = 1 − xi. If there is no negation operator for li, we use

li = xi directly. Then transform each literal using t1.

• For each disjunction operator, we will use t2 to control the output value. For example, if

Cj = Lj1 ∨ Lj2 ∨ Lj3, build the gadget cj = t2(t1(lj1) + t1(lj2) + t1(lj3)).

• For the conjunction operator, we will use t3. For example, if φ = (cid:86)k

i=1 Ci, then let y =

t3((cid:80)k

i=1 ci).

Example 6.6. Fig. 10 shows an example of the neural network corresponds to the 3SAT instance
(cid:4)
(X1 ∨ ¬X2 ∨ X3) ∧ (¬X1 ∨ X2 ∨ X4).

Gap in upper bounds. We need to ensure that there is a gap between the upper bound of
neural networks obtained from satisﬁable or unsatisﬁable 3CNF instances. This shows that even the
approximation of the upper bound can diﬀerentiate satisﬁable and unsatisﬁable 3CNF instances.

Proposition 6.7. For a 3CNF instance φ, let Nφ be the encoding neural network. Let yu =
max Nφ([0, 1]m). The following two statements are true:

1. If the 3CNF instance φ is satisﬁable, then yu > 1/2 + δ.

2. If φ is unsatisﬁable, then yu (cid:54) 1/2 − δ.

Proof. Statement (1): If φ is satisﬁable, let vi be a satisfying assignment of Xi and use them
as the input to Nφ. For each clause Cj, at least one literal is valued 1. WLOG, assume Lj1 = 1.
Therefore, t1(lj1) (cid:62) 0.5, the remaining two literals are valued either 0 or 1, then t1(ljk) (cid:62) −0.2 for the
gadgets corresponding to the two literals. Thus, t1(lj1) + t1(lj2) + t1(lj3) (cid:62) 0.5 − 0.2 − 0.2 (cid:62) 0.1, and
cj = t2(t1(lj1) + t1(lj2) + t1(lj3)) (cid:62) 1
i=1 ci) > 1/2 + δ,
2k
and so yu > 1/2 + δ.

i=1 ci (cid:62) 1/2, then y = t3((cid:80)k

. Therefore, (cid:80)k

24

Statement (2): We will prove that if yu > 1/2 − δ, then φ is satisﬁable. Let z be such that
Nφ(z) > 1/2 − δ. For each i ∈ {1, . . . , m}, if zi (cid:62) 0.6, let xi = 1; otherwise, let xi = 0. We will show
that x is a satisfying assignment for φ.

We need to show that for each clause Ci, the assignment makes Ci true. Equivalently, at least
one of the literals in Ci is true. Let us consider the corresponding gadget ci in the network. Because
Nφ(z) > 1/2 − δ, from Eq. (9) and the construction y = t3((cid:80)k

i=1 ci), then (cid:80)k

i=1 ci(z) > 0.

k − 1 clauses are valued at most 1
k

This implies that ci(z) > −1 for every i. Otherwise, the gadgets corresponding to the remaining
(see Eq. (8)), if ci(z) (cid:54) −1, then (cid:80)k
k < 0.
Because ci(z) > −1 and ci = t2(t1(li1) + t1(li2) + t1(li3)), [t1(li1) + t1(li2) + t1(li3)](z) > 0

i=1 ci(z) (cid:54) −1 + (k − 1) 1

(see Eq. (8)).

For all the three literals Li1, Li2, Li3 in Ci, consider the three gadgets corresponding to them
in the neural network. Because [t1(li1) + t1(li2) + t1(li3)](z) > 0, at least one of the literals
li1, li2, li3 is evaluated to > 0.6, otherwise t1(lij)(z) (cid:54) −0.1 for j = 1, 2, 3, and it is impossible that
[t1(li1) + t1(li2) + t1(li3)](z) > 0.

WLOG, let’s assume li1(z) > 0.6. We can consider the corresponding literal Li1 in Ci. Let Li1
come from variable Xj. Either Li1 = Xj or Li1 = ¬Xj. In the former case, because li1(z) > 0.6,
then li1(z) = li1(zj) = zj > 0.6. According to our assignment rule, xj = 1 and Xj is evaluated
true, and so is Ci. In the latter case, li1(z) = li1(zj) = 1 − zj > 0.6, so zj < 0.4. According to the
assignment rule, xj = 0, and so Xj is evaluated to false. Ci is still evaluated to true.

We have shown that the assignment x satisﬁes all clauses, and so the 3-SAT instance φ.

Consequences of NP-hardness. We have shown that the decision problem in Theorem 6.5 is
NP-hard. Because RA implies the decision problem, RA is NP-hard. Also, as described in Section 5.2,
IUA implies RA, so IUA is also NP-hard. Additionally, we also have the following result:

Corollary 6.8. It is NP-hard to falsify correctness5 of neural networks with any squashable activation
function.

Proof. To show that this is NP-hard, given a 3CNF instance φ, let’s build the neural network Nφ
in Section 6.3. From Theorem 6.7 we know that Nφ ∈ F +
. Let the correctness constraint
i=1(xi (cid:54) 1) ∧ (cid:86)m
be (cid:86)m
i=1(xi (cid:62) 0) ∧ (y > 0.5). If one can decide the satisﬁability of this constraint,
then one can decide Nφ ∈ F +
or Nφ ∈ F −
. Therefore, the falsiﬁcation of neural networks with any
δ
δ
squashable activation functions is NP-hard.

δ ∪ F −

δ

Because ReLU is also squashable, the result of Katz et al. (2017), showing that falsifying ReLU

networks is NP-hard, is a special case of Theorem 6.8.

7 Provably Robust Neural Networks

In this section, we discuss the connection between the IUA theorem and robust classiﬁers. Because
of the soundness of abstract interpretation, we can use it to verify the robustness of neural networks.
However, abstract interpretation is not complete, so some robust points might not be veriﬁed. One
consequence of the IUA is that not only a neural network can arbitrarily approximate any continuous
function on a compact domain, as we knew from classical universal approximation, but one could
also construct a neural network as shown in the proof of IUA, where all the robust inputs can be
veriﬁed using interval abstract interpretation. We begin with some deﬁnitions on robustness and

5Falsiﬁcation is deﬁned as deciding whether the conjunction of linear constraints on the input and output of the

network is satisﬁable, as used by Katz et al. (2017) for veriﬁcation.

25

provably robust neural networks, and then show how IUA implies the existence of provably robust
neural networks.
Robust classiﬁers. We begin by deﬁning a robust classiﬁer using (cid:96)∞-norm. Throughout this
section, we assume that f : C → R is a continuous function over compact domain C. We treat f as
a binary classiﬁer, where an output < 0.5 represents one class and (cid:62) 0.5 represents another.

We start by deﬁning the notion of an (cid:15)-ball, which can represent, for example, a set of copies of
the same image but with varying brightness. Recall that (cid:96)∞-norm is deﬁned as (cid:107)z(cid:107)∞ = maxi |zi|.
(cid:54) (cid:15)}. (cid:4)
Deﬁnition 7.1 ((cid:15)-Ball). Let x ∈ Rm and (cid:15) > 0. The (cid:15)-ball of x is R(cid:15)(x) = {z | (cid:107)z − x(cid:107)∞
Next, we deﬁne an (cid:15)-robust classiﬁer. Informally, given a set of points M , for each x ∈ M , an

(cid:15)-robust classiﬁer returns the same classiﬁcation for all points in the (cid:15)-ball of x.
Deﬁnition 7.2 ((cid:15)-Robustness). Let M ⊆ C and (cid:15) > 0. We say that f is (cid:15)-robust on set M iﬀ, for
(cid:4)
all x ∈ M and z ∈ R(cid:15)(x), we have f (x) < 0.5 iﬀ f (z) < 0.5.

Next, we deﬁne provably robust neural networks. These
Provably robust neural networks.
are neural networks for which we can automatically prove (cid:15)-robustness. Note that an (cid:15)-ball is a box
in Rm, and so there is no loss of precision while using the interval domain, i.e., γ(α(R(cid:15)(x))) = R(cid:15)(x).
Deﬁnition 7.3 (Provably robust networks). A neural network N is (cid:15)-provably robust on M iﬀ, for
(cid:4)
all x ∈ M , we have N #(B) ⊆ (−∞, 0.5) or N #(B) ⊆ [0.5, ∞), where B = α(R(cid:15)(x)).

From an automation perspective, the set M is typically a ﬁnite set of points, e.g., images. For
every x ∈ M , the veriﬁer abstract interprets N on the (cid:15)-ball of x, deriving a lower bound and upper
bound of the set of predictions N (R(cid:15)(x)). If the lower bound is (cid:62) 0.5 or the upper bound is < 0.5,
then we have proven that all images in the (cid:15)-ball have the same classiﬁcation using N .
The following theorem states the existence of
Existence of provably robust networks.
provably robust neural networks. Speciﬁcally, assuming there is some ideal robust classiﬁer, then,
following the IUA theorem, we can construct a neural network, using any squashable activation
function, that matches the classiﬁer’s predictions and is provably robust.

Theorem 7.4 (Existence of robust networks). Let f : C → R be (cid:15)-robust on set M ⊆ C. Assume
that ∀x ∈ M, z ∈ R(cid:15)(x). f (z) (cid:54)= 0.5.6 Let t be a squashable activation function. Then, there exists a
neural network N , using activation functions t, that

1. agrees with f on M , i.e., ∀x ∈ M. N (x) < 0.5 iﬀ f (x) < 0.5, and

2. is (cid:15)-provably robust on M .

Proof. Let set Z = (cid:83)
x∈M R(cid:15)(x). Let δ(cid:48) = minz∈Z |f (z) − 0.5|. That is, δ(cid:48) > 0 is the smallest
distance from the classiﬁcation boundary. Following the IUA theorem, we know that there is a
neural network N that δ-interval approximates f , for any δ < δ(cid:48). Fix such network N .
Statement (1): Pick any x ∈ M . Suppose that f (x) < 0.5. Then, we know that 0.5 − f (x) (cid:62) δ(cid:48).
By the IUA theorem, we know that |N (x) − f (x)| (cid:54) δ < δ(cid:48). It follows that N (x) < 0.5. The case
where f (x) > 0.5 is symmetric.
Statement (2): Let x ∈ M . Suppose that f (x) < 0.5. Because f is robust, ∀z ∈ R(cid:15)(x). f (z) < 0.5.
Then, we know that 0.5−max f (R(cid:15)(x)) (cid:62) δ(cid:48). By the IUA theorem, we know that N #(R(cid:15)(x)) = (cid:104)[l, u](cid:105),
It follows that N #(R(cid:15)(x)) ⊆ (−∞, 0.5). The case where
where |u − max f (R(cid:15)(x))| (cid:54) δ < δ(cid:48).
f (x) > 0.5 is symmetric. So, N is (cid:15)-provably robust on M .

6Informally, this assumption eliminates the corner case where a point sits exactly on the classiﬁcation boundary,

0.5.

26

n-ary classiﬁers.
analogous fashion. Please refer to the supplementary materials for the formalization and proof.

The above theorem can be extended to n-ary classiﬁers, for n > 2, in an

8 Related Work

The classical universal approximation (UA) theorem has been
Universal approximation.
established for decades. In contrast to IUA, UA states that a neural network with one single hidden
layer can approximate any continuous function on a compact domain. One of the ﬁrst versions
goes back to Cybenko (1989); Hornik et al. (1989), who showed that the standard feed-forward
neural network with sigmoidal or squashing activations is a universal approximator. The most
general version of UA was discovered by Leshno et al. (1993), who showed that the feed-forward
neural network is a universal approximator if and only if the activation function is non-polynomial.
Because IUA implies UA, this means IUA cannot hold beyond non-polynomial activation functions.
There are also other variants of UA. Some of them study the expressiveness of neural networks with
structural constraints, such as restricted width per layer (Lu et al., 2017; Kidger and Lyons, 2019),
or speciﬁc neural network architectures (Lin and Jegelka, 2018). Another line of work focuses on
speciﬁc functions that one wants to approximate rather than arbitrary continuous functions, such as
Anil et al. (2019); Cohen et al. (2019), who study approximation of Lipschitz functions.

Baader et al. (2020) showed the ﬁrst UA theorem adapted to interval analysis, and our high-level
construction resembles theirs. However, we proved that the neural networks with any squashable
activation functions can be an interval universal approximator. In contrast, they only showed the
IUA theorem restricted to ReLU activation functions.

Neural-network veriﬁcation has received a lot of attention
Neural-network veriﬁcation.
in recent years. Consult Albarghouthi (2021) for an introduction. Most techniques are either
based on decision procedures, like SMT solvers (Ehlers, 2017; Katz et al., 2017) and integer linear
programming (ILP) solvers (Tjeng et al., 2019), or abstract interpretation. The former class can
often provide sound and complete veriﬁcation on neural networks with piecewise-linear operations,
like ReLU, but is not scalable due to the complexity of the problem and the size of the networks.
Abstract-interpretation-based techniques sacriﬁce completeness for eﬃcient veriﬁcation. We have
considered the simplest non-trivial numerical domain, intervals, that has been shown to produce
strong results, both for robustness veriﬁcation and adversarial training (Gehr et al., 2018; Zhang
et al., 2021, 2020; Anderson et al., 2019; Huang et al., 2019; Mirman et al., 2018; Wang et al., 2018).
Researchers have considered richer domains (Singh et al., 2018, 2019), like zonotopes (Ghorbal et al.,
2009) and forms of polyhedra (Cousot and Halbwachs, 1978). Since such domains are strictly more
precise than intervals, the IUA theorem holds for them.
Complexity of Neural Network Veriﬁcation. Katz et al. (2017) proved that the falsiﬁcation
of ReLU neural networks is NP-complete. It introduced a reduction from 3SAT to the ReLU neural
network falsiﬁcation problem. Our result implies theirs as we have shown. Weng et al. (2018) proved
the inapproximability of ﬁnding the optimal (cid:96)1-distortion of ReLU networks, using a reduction from
the set cover problem, a well-known hard-to-approximate problem. However, they are working on (cid:96)1
ReLU robustness falsiﬁcation problem and their reduction does not imply our result.

9 Conclusion

We identify a set of activation functions, squashable functions, which includes most commonly used
activation functions. We prove that neural networks with any squashable functions are interval

27

universal approximators. We further study the computational complexity to range-approximate a
neural network, which implies that building the interval universal approximator is in general a hard
task. Our proof uses the idea that squashable functions can arbitrarily approximate step functions
and neural networks with step functions are formally well-behaved objects. We believe that this
perspective can be important to understand the formal aspect of neural network in the future.

Acknowledgments

This work is partially supported by Air Force Grant FA9550-18-1-0166, the National Science
Foundation (NSF) Grants CCF-FMitF-1836978, SaTC-Frontiers-1804648 and CCF-1652140 and
ARO grant number W911NF-17-1-0405.

References

Aws Albarghouthi. 2021. Introduction to Neural Network Veriﬁcation. veriﬁeddeeplearning.com.

http://verifieddeeplearning.com.

Greg Anderson, Shankara Pailoor, Isil Dillig, and Swarat Chaudhuri. 2019. Optimization and
abstraction: A synergistic approach for analyzing neural network robustness. In Proceedings of
the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation.
731–744.

Cem Anil, James Lucas, and Roger Grosse. 2019. Sorting Out Lipschitz Function Approximation. In
Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine
Learning Research, Vol. 97), Kamalika Chaudhuri and Ruslan Salakhutdinov (Eds.). PMLR, Long
Beach, California, USA, 291–301. http://proceedings.mlr.press/v97/anil19a.html

Maximilian Baader, Matthew Mirman, and Martin Vechev. 2020. Universal Approximation
https:

with Certiﬁed Networks. In International Conference on Learning Representations.
//openreview.net/forum?id=B1gX8kBtPr

James Bergstra, Guillaume Desjardins, Pascal Lamblin, and Yoshua Bengio. 2009. Quadratic

polynomials learn better image features. Technical report, 1337 (2009).

Djork-Arné Clevert, Thomas Unterthiner, and Sepp Hochreiter. 2016. Fast and Accurate Deep
Network Learning by Exponential Linear Units (ELUs). In 4th International Conference on
Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1511.07289

Jeremy E. J. Cohen, Todd Huster, and Ra Cohen. 2019. Universal Lipschitz Approximation in

Bounded Depth Neural Networks. arXiv:1904.04861 [cs.LG]

Patrick Cousot and Radhia Cousot. 1977. Abstract Interpretation: A Uniﬁed Lattice Model for
Static Analysis of Programs by Construction or Approximation of Fixpoints. In Proceedings of the
4th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages (Los Angeles,
California) (POPL ’77). Association for Computing Machinery, New York, NY, USA, 238–252.
https://doi.org/10.1145/512950.512973

Patrick Cousot and Nicolas Halbwachs. 1978. Automatic discovery of linear restraints among variables
of a program. In Proceedings of the 5th ACM SIGACT-SIGPLAN symposium on Principles of
programming languages. 84–96.

28

George Cybenko. 1989. Approximation by superpositions of a sigmoidal function. Mathematics of

Control, Signals and Systems 2 (1989), 303–314.

Rick Durrett. 2010. Probability: Theory and Examples (4 ed.). Cambridge University Press.

https://doi.org/10.1017/CBO9780511779398

Ruediger Ehlers. 2017. Formal veriﬁcation of piece-wise linear feed-forward neural networks. In
International Symposium on Automated Technology for Veriﬁcation and Analysis. Springer, 269–
286.

T. Gehr, M. Mirman, D. Drachsler-Cohen, P. Tsankov, S. Chaudhuri, and M. Vechev. 2018. AI2:
Safety and Robustness Certiﬁcation of Neural Networks with Abstract Interpretation. In 2018
IEEE Symposium on Security and Privacy (SP). 3–18.

Khalil Ghorbal, Eric Goubault, and Sylvie Putot. 2009. The zonotope abstract domain taylor1+. In

International Conference on Computer Aided Veriﬁcation. Springer, 627–633.

Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectiﬁer neural networks.
In Proceedings of the fourteenth international conference on artiﬁcial intelligence and statistics.
315–323.

Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining and Harnessing Adversarial
Examples. In International Conference on Learning Representations. http://arxiv.org/abs/
1412.6572

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. 2018. On the eﬀectiveness of
interval bound propagation for training veriﬁably robust models. arXiv preprint arXiv:1810.12715
(2018).

Juncai He, Lin Li, Jinchao Xu, and Chunyue Zheng. 2018. ReLU deep neural networks and linear

ﬁnite elements. arXiv preprint arXiv:1807.03973 (2018).

Kurt Hornik, Maxwell Stinchcombe, Halbert White, et al. 1989. Multilayer feedforward networks

are universal approximators. Neural networks 2, 5 (1989), 359–366.

Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal,
Krishnamurthy Dvijotham, and Pushmeet Kohli. 2019. Achieving Veriﬁed Robustness to Symbol
Substitutions via Interval Bound Propagation. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019. 4081–4091.
https://doi.org/10.18653/v1/D19-1419

Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. 2017. Reluplex: An
eﬃcient SMT solver for verifying deep neural networks. In International Conference on Computer
Aided Veriﬁcation. Springer, 97–117.

Patrick Kidger and Terry Lyons. 2019. Universal approximation with deep narrow networks. arXiv

preprint arXiv:1905.08539 (2019).

Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. 2012. Imagenet classiﬁcation with deep
convolutional neural networks. In Advances in neural information processing systems. 1097–1105.

29

Moshe Leshno, Vladimir Ya. Lin, Allan Pinkus, and Shimon Schocken. 1993. Multilayer feedforward
networks with a nonpolynomial activation function can approximate any function. Neural Networks
6, 6 (1993), 861 – 867. https://doi.org/10.1016/S0893-6080(05)80131-5

Hongzhou Lin and Stefanie Jegelka. 2018.

ResNet with one-neuron hidden layers is a
Universal Approximator.
In Advances in Neural Information Processing Systems 31,
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Gar-
nett
http://papers.nips.cc/paper/
7855-resnet-with-one-neuron-hidden-layers-is-a-universal-approximator.pdf

(Eds.). Curran Associates,

6169–6178.

Inc.,

Zhou Lu, Hongming Pu, Feicheng Wang, Zhiqiang Hu, and Liwei Wang. 2017. The Expressive Power
of Neural Networks: A View from the Width. In Proceedings of the 31st International Conference
on Neural Information Processing Systems (Long Beach, California, USA) (NIPS’17). Curran
Associates Inc., Red Hook, NY, USA, 6232–6240.

Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeﬀ Dean. 2013. Distributed
representations of words and phrases and their compositionality. In Advances in neural information
processing systems. 3111–3119.

Matthew Mirman, Timon Gehr, and Martin Vechev. 2018. Diﬀerentiable Abstract Interpretation for
Provably Robust Neural Networks. In International Conference on Machine Learning (ICML).
https://www.icml.cc/Conferences/2018/Schedule?showEvent=2477

Vinod Nair and Geoﬀrey E Hinton. 2010. Rectiﬁed linear units improve restricted boltzmann

machines. In ICML.

Michael A Nielsen. 2015. Neural networks and deep learning. Vol. 2018. Determination press San

Francisco, CA.

Veselin Raychev, Martin Vechev, and Andreas Krause. 2015. Predicting program properties from"

big code". ACM SIGPLAN Notices 50, 1 (2015), 111–124.

W. Rudin. 1986. Principles of Mathematical Analysis. McGraw - Hill Book C. https://books.

google.com/books?id=frdNAQAACAAJ

Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Martin Vechev. 2018. Fast
and eﬀective robustness certiﬁcation. In Advances in Neural Information Processing Systems.
10802–10813.

Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. 2019. An abstract domain for
certifying neural networks. Proceedings of the ACM on Programming Languages 3, POPL (2019),
1–30.

Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. 2019. Evaluating Robustness of Neural Networks
with Mixed Integer Programming. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.
net/forum?id=HyGIdiRqtm

V.V. Vazirani. 2003. Approximation Algorithms. Springer Berlin Heidelberg. https://doi.org/

10.1007/978-3-662-04565-7

30

Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. 2018. Formal security
analysis of neural networks using symbolic intervals. In 27th USENIX Security Symposium
(USENIX Security 18). 1599–1614.

Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Duane Boning, Inderjit S.
Dhillon, and Luca Daniel. 2018. Towards Fast Computation of Certiﬁed Robustness for ReLU
Networks. In International Conference on Machine Learning (ICML).

Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, and Quoc V Le. 2020. Smooth Adversarial

Training. arXiv preprint arXiv:2006.14536 (2020).

Yuhao Zhang, Aws Albarghouthi, and Loris D’Antoni. 2020. Robustness to Programmable String
Transformations via Augmented Abstract Training. In Proceedings of the 37th International
Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event (Proceedings of
Machine Learning Research, Vol. 119). PMLR, 11023–11032. http://proceedings.mlr.press/
v119/zhang20b.html

Yuhao Zhang, Aws Albarghouthi, and Loris D’Antoni. 2021. Certiﬁed Robustness to Programmable
Transformations in LSTMs. CoRR abs/2102.07818 (2021). arXiv:2102.07818 https://arxiv.
org/abs/2102.07818

A Vector-valued networks and Robustness

In this section, we extend the IUA theorem to vector-valued functions. We also extend our robustness
results to n-ary classiﬁers.

A.1 Higher-Dimensional Functions

So far we have considered scalar-valued neural networks. We
Vector-valued neural networks.
can generalize the neural-network grammar (Theorem 2.1) to enable vector-valued neural networks.
Simply, we can compose a sequence of n scalar-valued neural networks to construct a neural network
whose range is Rn. Formally, we extend the grammar as follows, where Ei are the scalar-valued
sub-neural networks.

Deﬁnition A.1 (Vector-valued neural network grammar). A neural network N : Rm → Rn is
deﬁned as follows

N :− (E1, . . . , En)
E :− c
| xi
| E1 + E2
c ∗ E2
|
t(E1, . . . , Ek)
|

where c ∈ R, xi is one of the m inputs to the network, and t is an activation function.

(cid:4)

Example A.2. Consider the following neural network N : R2 → R2:

N (x) = (σ(x1 + 0.5x2), σ(0.1x1 + 0.3x2))

which we can pictorially depict as the following graph:

31

x1

x2

0.5

+

+

0.1

0.3

σ

σ

(cid:4)

Generalized IUA theorem. We now generalize the IUA theorem to show that we can δ-interval
approximate vector-valued functions.

Theorem A.3. Let f : C → Rn be a continuous function with compact domain C ⊂ Rm. Let
δ > 0. Then, there exists a neural network N : Rn → Rm such that for every box B ⊆ C, and for all
i ∈ [1, m],

[li + δ, ui − δ] ⊆ N #(B)i ⊆ [li − δ, ui + δ]

(10)

where

1. N #(B)i is the ith interval in the box N #(B), and

2. li = min Si and ui = max Si, where S = f (B) (recall that Si is the set of ith element of every

vector in S).

Proof. From the IUA theorem, we know that there exists a neural network Ni that δ-interval
approximates fi : C → R, which is like f but only returns the ith output. We can then construct
the network N = (N1, . . . , Nn). Since each Ni satisﬁes Eq. (10) separately, then N δ-interval
approximates f .

A.2 Robustness in n-ary classiﬁcation

We now extend the deﬁnition of (cid:15)-robustness to n-ary classiﬁers. We use a function f : C → Rn to
denote an n-class classiﬁer. f returns a value for each of the n classes; the class with the largest
value is the result of classiﬁcation. We assume there are no ties. Formally, for a given x ∈ C, we
denote classiﬁcation by f as class(f (x)), where

class(y) = arg max
i∈{1,...,m}

yi

Deﬁnition A.4 (n-ary robustness). Let M ⊂ C. We say that f is (cid:15)-robust on M , where (cid:15) > 0, iﬀ
(cid:4)
for all x ∈ M and x(cid:48) ∈ R(cid:15)(x), we have class(f (x)) = class(f (x(cid:48))).

We now extend the provably robust neural networks deﬁnition to the n-class case. Recall that

R(cid:15)(x) = {x(cid:48) | ||x − x(cid:48)|| (cid:54) (cid:15)}.

Deﬁnition A.5 (Provably robust networks). A neural network N is (cid:15)-provably robust on M iﬀ,
(cid:4)
for all x ∈ M , for all y, y(cid:48) ∈ γ(N #(α(R(cid:15)(x)))), we have class(y) = class(y(cid:48)).

Existence of robust networks. We now show existence of robust networks that approximate
some robust n-ary classiﬁer f .

Theorem A.6 (Existence of robust networks). Let f : C → Rn be a continuous function that is
(cid:15)-robust on set M . Then, there exists a neural network that

1. agrees with f on M , i.e., ∀x ∈ M. class(N (x)) = class(f (x)), and

32

2. is (cid:15)-provably robust on M .

Proof. First, we need to post-process the results of f as follows: For all x ∈ C,

ˆf (x) = (0, . . . , |yi|, . . . , 0)
where y = f (x) and class(f (x)) = i. In other words, ˆf is just like f , but it zeroes out the values of
all but the output class i. This is needed since the interval domain is non-relational, and therefore
it cannot capture relations between values of diﬀerent classes, namely, keeping track which one is
larger. Note that if f is continuous, then ˆf is continuous.

Let δ(cid:48) be the smallest non-zero element of any vector in the set { ˆf (x) | x ∈ C}. Following the

IUA theorem, let N be a neural network that δ-interval approximates ˆf , where δ < 0.5δ(cid:48).
Statement (1): Pick any x ∈ M . Let the ith element of ˆf (x) (cid:54)= 0; call it c. By construction
i = class(f (x)). Let N (x) = (y1, . . . , yn). By IUA theorem, we know that 0 (cid:54) yj < 0.5δ(cid:48), for j (cid:54)= i,
and yi (cid:62) c − 0.5δ(cid:48). Since c (cid:62) δ(cid:48), class(N (x)) = class(f (x)) = i.
Statement (2): Let x ∈ M . Let S = ˆf (R(cid:15)(x)). Let Si be the projection of all vectors in S on
their ith element, where i = class( ˆf (x)). We know that min Si (cid:62) δ(cid:48). By construction of ˆf and the
fact that f is robust, all other elements of vectors of S are zero, i.e., Sj = {0}, for j (cid:54)= i.

Let N #(α(R(cid:15)(x))) = (cid:104)[lj, uj](cid:105)j. By IUA theorem and its proof, for j (cid:54)= i, we have [lj, uj] ⊂
It follows that for all y, y(cid:48) ∈
[0, 0.5δ(cid:48)). Similarly, [li, ui] ⊆ [min Si − 0.5δ(cid:48), ui] ⊆ [0.5δ(cid:48), ui].
γ(N #(α(R(cid:15)(x)))), we have class(y) = class(y(cid:48)) = i. This is because any value in [δ(cid:48) − 0.5δ(cid:48), ui] is
larger than any value in [0, 0.5δ(cid:48)).

B Appendix: Elided Deﬁnitions and Proofs

B.1 Proof of Theorem 3.4

All of the interval arithmetic operations we have deﬁned are standard and are sound.

The only non-standard abstract transformers are t#. We start with the general deﬁnition and

prove its soundness:

t#(B) =

(cid:28)(cid:20)

(cid:21)(cid:29)

min
l(cid:54)x(cid:54)u

t(x), max
l(cid:54)x(cid:54)u

t(x)

Let B = (cid:104)[l, u](cid:105) be a 1-dimensional box. Since t satisﬁes Eq. (1), the lower bound and upper bound
above exist. The collecting semantics t(γ(B)) = {t(x) | l (cid:54) x (cid:54) u}. It follows that t(γ(B)) ⊆ t#(B).

If t is monotonically increasing, we deﬁned the transformer

t#(B) = (cid:104)[t(l), t(u)](cid:105)

By monotonicity of t, we have ∀x ∈ [l, u]. t(l) (cid:54) t(x) (cid:54) t(u). Therefore, t(γ(B)) ⊆ [t(l), t(u)]. It
follows that t(γ(B)) ⊆ t#(B).

Therefore all abstract transformers are sound. Soundness of N # follows compositionally from

soundness of all operators.

B.2 Choice of Parameters θ and (cid:15)

4|G| ),
Because the our construction works for any ﬁxed θ and (cid:15), we will choose θ = min( 1
4m+2 , 1
K+1 ,
where τ , K and G are deﬁned in Section 4.2; and (cid:15) < 0.5 be such that if (cid:107)x − y(cid:107)∞
(cid:54) (cid:15), then
|f (x) − f (y)| < τ . The latter is achievable from the Heine–Cantor Theorem (see Rudin (1986)), so
f is uniformly continuous on C.

1

33

B.3 Proof of Theorem 4.4

Proof. Statement (1): Because x (cid:62) ai, x+0.5(cid:15)−ai (cid:62) 0.5(cid:15). From Theorem 4.2, t(µ(x+0.5(cid:15)−ai)) ∈
(1 − θ, 1].
Statement (2): Because x (cid:54) bi, x−0.5(cid:15)−bi (cid:54) −0.5(cid:15). From Theorem 4.2, t(µ(x−0.5(cid:15)−bi)) ∈ [0, θ).

B.4 Proof of Theorem 4.5

Proof. Statement (1): Because x (cid:54) ai − (cid:15), x + 0.5(cid:15) − ai (cid:54) −0.5(cid:15). From Theorem 4.2, t(µ(x +
0.5(cid:15) − ai)) ∈ [0, θ).
Statement (2): Because x (cid:54) ai − (cid:15) and ai < bi, x (cid:54) bi − (cid:15). Then x − 0.5(cid:15) − bi (cid:54) −0.5(cid:15).
From Theorem 4.2, t(µ(x − 0.5(cid:15) − bi)) ∈ [0, θ).

B.5 Proof of Theorem 4.6

Proof. Statement (1): Because x (cid:62) bi + (cid:15) and bi (cid:62) ai, x (cid:62) ai + (cid:15). Then x + 0.5(cid:15) − ai (cid:62) 0.5(cid:15).
From Theorem 4.2, t(µ(x + 0.5(cid:15) − ai)) ∈ (1 − θ, 1].
Statement (2): Because x (cid:62) bi + (cid:15), x − 0.5(cid:15) − bi (cid:62) 0.5(cid:15). From Theorem 4.2, t(µ(x − 0.5(cid:15) − bi)) ∈
(1 − θ, 1].

B.6 Proof of Theorem 4.7

Proof. We begin the proof by simplifying the expression ˆt#(B). Recall that ˆt(x) = t(µ(x + 0.5(cid:15) −
ai)) − t(µ(x − 0.5(cid:15) − bi)). Let B = (cid:104)[a, b](cid:105). By applying abstract transformer t# (Theorem 3.2) and
subtracting the two terms, we get ˆt#(B) = [T1 − T4, T2 − T3], where

T1 = minx∈[a,b] t(µ(x + 0.5(cid:15) − ai))
T3 = minx∈[a,b] t(µ(x − 0.5(cid:15) − bi))

T2 = maxx∈[a,b] t(µ(x + 0.5(cid:15) − ai))
T4 = maxx∈[a,b] t(µ(x − 0.5(cid:15) − bi))

We are now ready to prove the three statements.

Statement (1): By the limits of t, ∀x. t(x) ∈ [0, 1], so T1, T2, T3, T4 ∈ [0, 1]. Therefore, the upper
bound of ˆt#(B) is T2 − T3 (cid:54) 1.
Statement (2):
Case 1: B ⊆ (−∞, ai − (cid:15)]. From Theorem 4.5, T1, T2, T3, T4 ∈ [0, θ), then T2 − T3 < θ, and
T1 − T4 > −θ.
Case 2: B ⊆ [bi + (cid:15), ∞). From Theorem 4.6, T1, T2, T3, T4 ∈ (1 − θ, 1], then T2 − T3 < θ, and
T1 − T4 > −θ.

In either case, ˆt#(B) ⊆ (−θ, θ).

Statement (3): If B ⊆ [ai, bi], a, b ∈ [ai, bi]. From Theorem 4.4(1), T1, T2 ∈ (1 − θ, 1]. From Theo-
rem 4.4(2), T3, T4 ∈ [0, θ). Then T1 − T4 > 1 − 2θ and T2 − T3 (cid:54) 1.

Therefore, ˆt#(B) ⊆ (1 − 2θ, 1].

34

B.7 Proof of Theorem 4.9

Proof. Statement (1): If B ⊆ G, then ∀i. Bi ⊆ [ai, bi]. From Theorem 4.7 (3), ˆt#
thus,

i (Bi) ⊆ (1 − 2θ, 1];

H #

i (Bi) = ˆt#

i (Bi) +# −(1 − 2θ)#

⊆ (0, 2θ]
⊂ (0, ∞)

Sum over all m dimensions, (cid:80)m
Statement (2): If B ⊆ C \ ν(G), then there is a dimension j such that either Bj ⊆ (−∞, aj − (cid:15)]
or Bj ⊆ [bj + (cid:15), ∞). From Theorem 4.7 (2), we know that ˆt#(Bj) ⊆ (−θ, θ). Therefore,

i=1(0, ∞) = (0, ∞).

i (Bi) ⊆ (cid:80)m

i=1 H #

H #

j (Bj) = ˆt#(Bj) +# −(1 − 2θ)#
⊆ (θ − 1, 3θ − 1)

(11)

For the remaining m − 1 dimensions, from Theorem 4.7 (1), we know that ˆt#(Bi) ⊂ (−∞, 1]

when i (cid:54)= j. Therefore,

H #

i (Bi) = ˆt#(Bi) +# −(1 − 2θ)#

⊆ (−∞, 2θ]

Take the sum of all the m − 1 dimensions,

(cid:80)

i∈{1,...,m}\{j} H #

i (Bi) ⊆ (cid:80)

i∈{1,...,m}\{j}(−∞, 2θ]
= [m − 1, m − 1] ∗# (−∞, 2θ]
= (−∞, 2(m − 1)θ]

(12)

(13)

(substitute Eq. (12))
(turn sum into ∗#)
(apply ∗#)

Now, take sum over all the m dimensions,

(cid:80)m

i=1 H #

i (Bi) = (cid:80)

i∈{1,...,m}\{j} H #

j (Bj)
⊆ (−∞, 2(m − 1)θ] +# (θ − 1, 3θ − 1)
= (−∞, (2m + 1)θ − 1)

i (Bi) +# H #

(decompose sum)
(substitute Eqs. (11) and (13))
(apply ∗#)

Because of our choice of θ, θ (cid:54) 1
therefore

4m+2

(see Appendix B.2). Then (2m + 1)θ (cid:54) 2m+1

4m+2 = 0.5, and

m
(cid:88)

H #

i (Bi) ⊆ (−∞, −0.5)

Also we have assumed that (cid:15) < 0.5 (see Appendix B.2); therefore

i=1

m
(cid:88)

i=1

H #

i (Bi) ⊆ (−∞, −(cid:15))

B.8 Abstract Interpretation of Ni
Observe how for any box B ⊆ C from the abstract domain, it is overapproximated by a larger box
G ⊇ B from the ﬁnitely many boxes in the (cid:15)-grid. Intuitively, our abstract approximation of Ni
incurs an error when the input B is not in the grid. We formalize this idea by extending the notion
of neighborhood (Section 4.1) to boxes from the abstract domain. For a box B ⊆ C, if B ∈ G,

35

then B’s neighborhood GB = ν(B); otherwise, let GB be the smallest G ∈ G, by volume, such that
B ⊆ G. Note that GB is uniquely deﬁned.

The following lemma says that considering the neighborhood of B only adds up to τ of imprecision

to the collecting semantics of f .

Lemma B.1 (Properties of GB). The following is true:

1. If f (B) (cid:62) β, then f (GB) (cid:62) β − τ .

2. If f (B) (cid:54) β, then f (GB) (cid:54) β + τ .

Proof. Both of the statements follow from our choice of (cid:15) in constructing the grid (see Appendix B.2).
(cid:54) (cid:15), then |f (x) − f (y)| < τ . Consider the B and its neighborhood GB. By deﬁnition
If (cid:107)x − x(cid:107)∞
of neighborhood, ∀x ∈ GB, ∃y ∈ B, such that (cid:107)x − y(cid:107)∞
Statement (1) Because f (B) (cid:62) β, then f (y) (cid:62) β, so f (x) (cid:62) f (y) − τ (cid:62) β − τ . Then ∀x ∈ GB,
f (x) (cid:62) β − τ .
Statement (2) Because f (B) (cid:54) β, then f (y) (cid:54) β, so f (x) (cid:54) f (y) + τ (cid:54) β + τ . Then ∀x ∈ GB,
f (x) (cid:54) β + τ .

(cid:54) (cid:15).

Theorem B.2 (Abstract interpretation of Ni). For any box B ⊆ C, let u = max f (B), and
l = min f (B). The following is true:

1. N #

i (B) ⊆ [0, 1].

2. If l (cid:62) (i + 2)τ , then ∃ui ∈ (1 − θ, 1] such that [ui, ui] ⊆ N #

i (B) ⊆ (1 − θ, 1].

3. If u (cid:54) (i − 1)τ , then ∃li ∈ [0, θ) such that [li, li] ⊆ N #

i (B) ⊆ [0, θ).

i (B) ⊆ (1 − θ, 1], and N #

i (B) ⊆ (1 − θ, 1]. Because if N #

Proof. We begin by noting that in Statement (2), [ui, ui] ⊆ N #
direct corollary of N #
contains at least one point in (1 − θ, 1]. Similarly, in Statement (3), [li, li] ⊆ N #
li ∈ [0, θ) is a direct corollary of N #
i (B) ⊆ [0, θ).
In Appendix B.2, we have chosen that θ (cid:54) 1
4|G|
Statement (1): The outer function of Ni is t, whose range is [0, 1], by the deﬁnition of squashable
function and our construction, so N #
Statement (2): Because f (B) (cid:62) (i + 2)τ , by Theorem B.1, f (GB) (cid:62) (i + 1)τ , so GB ∈ Gi. Thus,
we can break up the sum as follows:

i (B) for some ui ∈ (1 − θ, 1] is a
i (B)
i (B) for some

, a fact we will use later in the proof.

i (B) (cid:54)= ∅, then N #

i (B) ⊆ [0, 1].

(cid:88)

G∈Gi

NG(x) =





(cid:88)

G∈(Gi\{GB})



NG(x)

 + NGB (x)

From Theorem 4.10, N #
GB

(B) ⊆ (1 − θ, 1], and N #

G (B) ⊆ [0, 1] for G ∈ Gi \ {GB}. Therefore, we

can conclude the following two facts:

N #

G (B) ⊆ (1 − θ, ∞)

(cid:88)

G∈Gi

and (cid:88)
G∈Gi

N #

G (B) +# [−0.5, −0.5] ⊆ (0.5 − θ, ∞) ⊂ (0.5(cid:15), ∞)

The second inequality follows from the fact that we assumed θ (cid:54) 1
4|G|
(see Appendix B.2). Therefore, 0.5 − θ > 0.25 > 0.5(cid:15).

(cid:54) 0.25 (above) and (cid:15) < 0.5

36

It follows from Theorem 4.2 that

N #

i (B) = t#


µ# ∗#





(cid:88)

G∈Gi

N #

G (B) +# [−0.5, −0.5]



 ⊆ (1 − θ, 1]





Statement (3): If u (cid:54) (i − 1)τ , we will show that ∀G ∈ Gi. B ⊂ C \ ν(G).

Pick any G ∈ Gi, then we have f (G) (cid:62) (i + 1)τ . Thus, from Theorem B.1, f (GB) (cid:62) iτ .
Recall that if B ∈ G, then GB = ν(B). Hence, f (ν(G)) (cid:62) iτ . However, f (B) (cid:54) u (cid:54) (i − 1)τ , so
B ∩ ν(G) = ∅. Equivalently, B ⊂ C \ ν(G).
From Theorem 4.10, ∀G ∈ Gi. N #

G (B) ⊆ [0, θ), so

N #

G (B) ⊆ [0, |Gi|θ) ⊆ [0, |G|θ)

(cid:88)

G∈Gi

We assumed that θ (cid:54) 1
4|G|

−0.25 (cid:54) −0.5(cid:15). Hence,

and (cid:15) < 0.5 (see Appendix B.2), so |G|θ (cid:54) 0.25, and |G|θ − 0.5 (cid:54)

N #

G (B) ⊆ [0, 0.25)

(cid:88)

G∈Gi

and (cid:88)
G∈Gi

It follows from Theorem 4.2 that

N #

G (B) +# [−0.5, −0.5] ⊆ [−0.5, −0.25) ⊆ (−∞, −0.5(cid:15))

N #

i (B) = t#


µ# ∗#





(cid:88)

G∈Gi

N #

G (B) +# [−0.5, −0.5]



 ⊆ [0, θ)





B.9 Abstract Interpretation of N
Because (cid:80)K
N as N (x) = τ (cid:80)K

i=0 fi(x) = f (x), and Ni(x) approximates 1

i=0 Ni(x).

τ fi(x), we will construct the neural network

Before proceeding with the proof, we give a general lemma that will be useful in our analysis.

The lemma follows from the fact that, by construction, θ (cid:54) 1

K+1

.

Lemma B.3. If η0, . . . , ηK ∈ [−θ, θ], then (cid:80)K

i=0 ηi ∈ [−1, 1].

Proof. This simply follow from the choice of θ (cid:54) 1

K+1

.

Proof outline of the existence of δ-interval approximating neural networks. Our proof
involves three pieces, outlined below:

(A) Because N #(B) = τ # ∗# (cid:80)K

i=0 N #

i (B), we need only analyze (cid:80)K

i=0 N #

i (B). We will decompose

the sum into ﬁve sums and analyze each separately, arriving at ﬁve results of the form:

(cid:104) ˜L1j, ˜U1j

(cid:105)

⊆

(cid:88)

i∈Sj

N #

i (B) ⊆

(cid:104) ˜L2j, ˜U2j

(cid:105)

for j ∈ {1, . . . , 5}, where (cid:83)

j Sj = {0, . . . , K} and Sj are mutually disjoint sets.

37

(B) Then, we sum over all ﬁve cases, getting





5
(cid:88)

j=1

˜L1j,

5
(cid:88)

j=1



˜U1j

 ⊆

K
(cid:88)

i=0

N #

i (B) ⊆





5
(cid:88)

j=1

˜L2j,





˜U2j

5
(cid:88)

j=1

j=1

˜Lij and Ui = τ (cid:80)5

(C) Let Li = τ (cid:80)5
[L2, U2].
Finally, we show that [L2, U2] ⊆ [l − δ, u + δ] and [l + δ, u − δ] ⊆ [L1, U1].
Equivalently, we will show that

j=1

˜Uij. Then, we get the bound [L1, U1] ⊆ N #(B) ⊆

l − δ (cid:54) L2 (cid:54) L1 (cid:54) l + δ

and

u − δ (cid:54) U1 (cid:54) U2 (cid:54) u + δ

Proof assumptions. We will assume that l ∈ [pτ, (p + 1)τ ) and u ∈ [qτ, (q + 1)τ ), for some
p (cid:54) q (cid:54) K. Additionally, let c, d ∈ B be such that f (c) = l and f (d) = u.
Step A: Decompose sum and analyze separately. We begin by decomposing the sum into
ﬁve terms.

This is the most important step of the proof. We want to show that most Ni’s in (cid:80)K

i (B)
are (almost) precise. By almost we mean that their values are ≈ 1 and ≈ 0. The motivation is then
to extract as many precise terms as possible. The only tool used in the analysis is Theorem B.2.

i=0 N #

• Consider the function slices represented by Term 1 and 5; for example, Term 1 represents
abstractions N #
of function slices fi, for i ∈ [0, p − 2]. The function slices of Term 1 and 5
i
are referred to in Theorem B.2 (Statements 2 and 3): they have an (almost) precise abstract
interpretation. That is, the abstract semantics of N #
i (B) and the collecting semantics of fi(B)
agree. For Term 1, the abstract interpretation of all N #
i (B) ≈ [1, 1] and fi(B) = [τ, τ ]. For
Term 5, the abstract interpretation of all N #

i (B) ≈ [0, 0] and fi(B) = [0, 0].

• Now consider function slices fi, where i ∈ [p + 2, q − 2]. The abstraction of these function
slices is also (almost) precise. We can see f (c) = l is below the lower bound of the slices
and f (d) = u is above the upper bound of the slices. Hence, fi(d) = τ and N #
i ({d}) ≈ [1, 1].
Similarly, fi(c) = 0 and N #
i ({c}) ≈ [0, 0]. Because c, d ∈ B, and due to continuity of f , we
have fi(B) = [0, 1], and N #
i (B) ≈ [0, 1].

• The remaining function slices are those in Term 2 and Term 4, and they are at the neighborhood

of the boundary of [l, u]. Most precision loss of N #

i (B) comes from those two terms.

This drives us to decompose the sum as follows:

K
(cid:88)

i=0

N #

i (B) =

p−2
(cid:88)

N #

i (B)

+#

p+1
(cid:88)

N #

i (B)

+#

q−2
(cid:88)

N #

i (B)

+#

q+1
(cid:88)

N #

i (B)

+#

K
(cid:88)

N #

i (B)

i=0
(cid:124)

(cid:123)(cid:122)
Term 1

(cid:125)

(cid:125)
(14)
We will analyze the ﬁve terms in Eq. (14) separately, and then take their sum to get the ﬁnal result.
For now, assume that q (cid:62) p + 3; the q (cid:54) p + 2 case will follow easily.

(cid:123)(cid:122)
Term 4

(cid:123)(cid:122)
Term 5

(cid:123)(cid:122)
Term 2

(cid:123)(cid:122)
Term 3

(cid:125)

(cid:125)

(cid:125)

i=p+2
(cid:124)

i=q−1
(cid:124)

i=q+2
(cid:124)

i=p−1
(cid:124)

38

(i) Term 1: ∀i (cid:54) p − 2, we have pτ (cid:62) (i + 2)τ . Because l = min f (B) and l ∈ [pτ, (p + 1)τ ), then

f (B) (cid:62) pτ (cid:62) (i + 2)τ .
From Theorem B.2, ∃ui ∈ (1−θ, 1] such that [ui, ui] ⊆ N #
(cid:80)p−2

i (B) ⊆ (cid:80)p−2

i=0 (1 − θ, 1].

i=0 N #

i (B) ⊆ (1−θ, 1]. Then (cid:80)p−2

i=0 [ui, ui] ⊆

p−2
(cid:88)

[ui, ui] ⊆

i=0

p−2
(cid:88)

i=0

N #

i (B) ⊆ (p − 1)# ∗# (1 − θ, 1]

(ii) Term 5: ∀i (cid:62) q + 2, we have (q + 1)τ (cid:54) (i − 1)τ . Because u = max f (B) and u ∈ [qτ, (q + 1)τ ),

then f (B) < (q + 1)τ (cid:54) (i − 1)τ .
From Theorem B.2, ∃li ∈ [0, θ) such that [li, li] ⊆ N #
(cid:80)K

i=q+2 N #

i (B) ⊆ (cid:80)K

i=q+2[0, θ).

i (B) ⊆ [0, θ). Then (cid:80)K

i=q+2[li, li] ⊆

K
(cid:88)

[li, li] ⊆

i=q+2

K
(cid:88)

i=q+2

N #

i (B)

⊆ (K − q − 1)# ∗# [0, θ)

(iii) Term 3: ∀i ∈ [p + 2, q − 2], we have (p + 1)τ (cid:54) (i − 1)τ and qτ (cid:62) (i + 2)τ .

f (c) = l < (p + 1)τ (cid:54) (i − 1)τ , and f (d) = u (cid:62) qτ (cid:62) (i + 2)τ .
From Theorem B.2, N #
N #
Also by Theorem B.2, N #
(cid:80)q−2

i (B) ⊆ [0, 1]. Hence, (cid:80)q−2

i ({c}) ⊆ [0, θ) and N #

i (B).

i=p+2[0, 1].

i ({d}) ⊆ (1 − θ, 1]. Because c, d ∈ B, [θ, 1 − θ] ⊆

i=p+2[θ, 1 − θ] ⊆ (cid:80)q−2

i=p+2 N #

i (B) ⊆

q−2
(cid:88)

[θ, 1 − θ] ⊆

i=p+2

q−2
(cid:88)

i=p+2

N #

i (B) ⊆ (q − p − 3)# ∗# [0, 1]

(iv) Term 2: ∀i ∈ [p − 1, p + 1], since we have assumed that q (cid:62) p + 3, then q (cid:62) p + 3 (cid:62) i + 2.

i ({d}) ⊆ (1 − θ, 1].

Because f (d) (cid:62) qτ (cid:62) (i + 2)τ , from Theorem B.2, ∃ui ∈ (1 − θ, 1] such that [ui, ui] ⊆
N #
Because d ∈ B, [ui, ui] ⊆ N #
i (B) ⊆ (cid:80)p+1
(cid:80)p+1
i=p−1[0, 1].

i (B). Hence, [ui, ui] ⊆ N #

i (B) ⊆ [0, 1] and (cid:80)p+1

i=p−1[ui, ui] ⊆

i=p−1 N #

p+1
(cid:88)

i=p−1

[ui, ui] ⊆

p+1
(cid:88)

i=p−1

N #

i (B) ⊆ 3# ∗# [0, 1]

(v) Term 4: For ∀q − 1 (cid:54) i (cid:54) q + 1, because q (cid:62) p + 3, we have p + 1 (cid:54) q − 2 (cid:54) i − 1. Then f (c) =
i ({c}) ⊆ [0, θ).

l < (p + 1)τ (cid:54) (i − 1)τ . From Theorem B.2, ∃li ∈ [0, θ) such that [li, li] ⊆ N #
i (B). Thus, [li, li] ⊆ N #
Because c ∈ B, [li, li] ⊆ N #

i (B) ⊆ [0, 1].

q+1
(cid:88)

[li, li] ⊆

q+1
(cid:88)

i=q−1

i=q−1

N #

i (B) ⊆ 3# ∗# [0, 1]

39

Step B: Sum all ﬁve cases. We now sum up all ﬁve inequalities we derived above to derive an
1] ⊆ (cid:80)K
overall bound of the sum in the form [L(cid:48)

2]. For example,

i (B) ⊆ [L(cid:48)

i=0 N #

1, U (cid:48)

2, U (cid:48)

L(cid:48)

1 =

p−2
(cid:88)

i=0

K
(cid:88)

q−2
(cid:88)

p+1
(cid:88)

q+1
(cid:88)

li

ui +

θ +

li +

ui +

i=q+2

i=p+2

i=p−1

i=q−1

Recall that, by Theorem B.2, ∀i ∈ {0, . . . , K}, ui ∈ (1 − θ, 1] and li ∈ [0, θ). Let ˜li = 1 − ui, so

˜li ∈ [0, θ).

We simplify L(cid:48)
1

, L(cid:48)
2

, U (cid:48)
1

and U (cid:48)
2

as follows:

1 = (cid:80)p−2
L(cid:48)

i=0 ui + (cid:80)K

i=q+2 li + (cid:80)q−2

i=p+2 θ + (cid:80)p+1

i=p−1 ui + (cid:80)q+1

i=q−1 li

(sum of the left bound)

= (cid:80)p−2

i=0 (1 − ˜li) + (cid:80)K

i=q+2 li + (cid:80)q−2

i=p+2 θ + (cid:80)p+1

i=p−1(1 − ˜li) + (cid:80)q+1

i=q−1 li

= ((cid:80)p−2

i=0 + (cid:80)p+1

i=p−1)(1) + (cid:80)p−2

i=0 (−˜li) + (cid:80)K

i=q+2 li + (cid:80)q−2

i=p+2 θ + (cid:80)p+1

(substitute ui with ˜li)
i=p−1(−˜li) + (cid:80)q+1
i=q−1 li
(Rearrange the terms)

= (p + 2) + (cid:80)p+1

i=0 (−˜li) + (cid:80)K

i=q−1 li + (cid:80)q−2

i=p+2 θ

(Sum all the 1’s)

From Theorem B.3, (cid:80)p+1

i=0 (−˜li) + (cid:80)q−2

i=p+2 θ + (cid:80)K

i=q−1 li ∈ [1, 1] by plugging in −˜li, li, θ to ηi. So,

L(cid:48)

1 ∈ [p + 1, p + 3]

1 = (cid:80)p−2
U (cid:48)
= (cid:80)p−2
= (q − 1) + (cid:80)p+1

i=0 ui + (cid:80)K
i=0 (1 − ˜li) + (cid:80)K

i=q+2 li + (cid:80)q−2

i=p+2(1 − θ) + (cid:80)p+1

i=p−1 ui + (cid:80)q+1

i=q+2 li + (cid:80)q−2

i=p+2(1 − θ) + (cid:80)p+1

i=0 (−˜li) + (cid:80)K

i=q−1 li + (cid:80)q−2

i=p+2(−θ)

i=q−1 li
i=p−1(1 − ˜li) + (cid:80)q+1

i=q−1 li

(sum of right bound)
(substitute ui with ˜li)
(sum all the 1’s)

From Theorem B.3, (cid:80)p+2

i=0 (−˜li) + (cid:80)q−2

i=p+2(−θ) + (cid:80)K

i=q−1 li ∈ [−1, 1]. Thus,

U (cid:48)

1 ∈ [q − 2, q]

L(cid:48)

2 = (p − 1)(1 − θ)

= (p − 1) + (p − 1)(−θ)

(sum of left bound)
(rearrange terms)

Because θ (cid:54) 1

K+1

, and −K (cid:54) p − 1 (cid:54) K, we have (p − 1)(−θ) ∈ [−1, 1]. Hence,

L(cid:48)

2 ∈ [p − 2, p]

2 = (p − 1) + (K − q − 1)θ + (q − p − 3) + 3 + 3 (sum of right bound)
U (cid:48)
(rearrange terms)
(sum all the 1’s)

= (p − 1 + q − p − 3 + 3 + 3) + (K − q − 1)(θ)
= q + 2 + (K − q − 1)(θ)

Because θ (cid:54) 1

K+1

, and −K (cid:54) (K − q − 1) (cid:54) K, we have (K − q + 1)(−θ) ∈ [−1, 1]. Then,

U (cid:48)

2 ∈ [q + 1, q + 3]

40

Step C: Analyze the bound.
u − δ (cid:54) U1 (cid:54) U2 (cid:54) u + δ.

Recall that we have set that δ = 3τ . Also l ∈ [pτ, (p + 1)τ ), then

It remains to show that l − δ (cid:54) L2 (cid:54) L1 (cid:54) l + δ and

l − δ < (p − 2)τ

and

l + δ (cid:62) (p + 3)τ

Since u ∈ [qτ, (q + 1)τ ), then

u − δ < (q − 2)τ

and

u + δ (cid:62) (q + 3)τ

We have just analyzed L(cid:48)
, L(cid:48)
1
2
L1 = τ L(cid:48)
1
U1 = τ U (cid:48)
1

(cid:62) (q − 2)τ
It follows from the above inequalities that

, U (cid:48)
1

and U (cid:48)
2
(cid:54) (p + 3)τ

above. Now we have:

L2 = τ L(cid:48)
2
U2 = τ U (cid:48)
2

(cid:62) (p − 2)τ

(cid:54) (q + 3)τ

l − δ < (p − 2)τ (cid:54) L2 (cid:54) L1 (cid:54) (p + 3)τ (cid:54) l + δ

and

u − δ < (q − 2)τ (cid:54) U1 (cid:54) U2 (cid:54) (q + 3)τ (cid:54) u + δ

This concludes the proof for the case where q (cid:62) p + 3.
Previously, we have shown that Terms 1, 3, and 5 are almost precise. The
Excluded case.
imprecise terms can only come from Terms 2 and 4. If q (cid:54) p + 2, the only analyses that will be
aﬀected are those of Terms 2 and 4. Since q (cid:54) p + 2, we have p + 1 (cid:62) q − 1, which means Terms 2
and 4 have potentially less sub-terms in this case. Thus imprecise terms are less than the q (cid:62) p + 3
case and we can apply the same analysis as above and derive the same bound.

We have thus shown that the neural network N that we construct δ-interval approximates f ,

and therefore the IUA theorem is true.

B.10 coNP-hardness of Range Approximation

To show the coNP-hardness of the range approximation problem, we will show that approximating
the minimum value of a neural network can decide whether a 3DNF formula is a tautology. For
δ < 1/2, let

(cid:26)

G+

δ =

(cid:91)

m(cid:62)1

f ∈ Fm

min
x∈[0,1]m

(f (x)) (cid:62) 1/2 + δ

(cid:27)

G−

δ =

(cid:26)

(cid:91)

f ∈ Fm

min
x∈[0,1]m

(f (x)) < 1/2 − δ

(cid:27)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:12)
(cid:12)
(cid:12)
(cid:12)

(15)

(16)

Lemma B.4. Given f ∈ G+

m(cid:62)1
δ , it is coNP-hard to determine whether f ∈ G+
δ ∪ G−

δ or f ∈ G−
δ .

A 3DNF instance φ is a disjunction of clauses of the form C1 ∨ . . . ∨ Ck, where each clause Cj is

a conjunction of 3 literals.

t4(z) =

t5(z) =

(cid:40)(cid:54) −0.5 and (cid:62) −0.6,
(cid:62) 0.1 and (cid:54) 0.2,

z (cid:54) 0.3
z (cid:62) 0.4

(cid:40)(cid:62) − 1
(cid:62) 1,

2k

and (cid:54) − 1
4k ,

z (cid:54) 0
z (cid:62) 0.1

41

(17)

(18)

We simulate the 3DNF instance using a neural network in the following way. For each variable
Xi, construct an input node xi. Simulate the negation operator using li = 1 − xi. If there is no
negation operator for li, we use li = xi directly. Then transform each literal using t4.

For each conjunction operator, we will use t5 to control the output value. For example, if
Cj = Lj1 ∧ Lj2 ∧ Lj3, build the gadget cj = t5(t4(lj1) + t4(lj2) + t4(lj3)). For the disjunction operator,
i=1 Ci, then let y = t3((cid:80)k
we will use t3. For example, if φ = (cid:87)k

i=1 ci).

Proposition B.5. For a 3DNF instance φ, let Nφ be the encoding neural network. Let yl =
min Nφ([0, 1]m). The following two statements are true:

1. If the 3DNF instance φ is not a tautology, then yl < 1/2 − δ.

2. If φ is a tautology, then yl (cid:62) 1/2 + δ.

Proof. Statement (1): If φ is not a tautology, let vi be an unsatisfying assignment of Xi and
use them as the input to Nφ. All clauses are evaluated 0, thus at least one literal from each
clause are valued 0. WLOG, assume Lj1 = 0 for j = 1, . . . , k. Therefore, t4(lj1) (cid:54) −0.5, and
t4(lj1) + t4(lj2) + t4(lj3) (cid:54) −0.5 + 0.2 + 0.2 = −0.1. cj = t5(t4(lj1) + t4(lj2) + t4(lj3)) (cid:54) − 1
.
4k
Therefore, (cid:80)k

i=1 ci) < 1/2 − δ, and so yl < 1/2 − δ.

i(cid:54)=j ci (cid:54) −1/4, then y = t3((cid:80)k

i=1 ci (cid:54) k (cid:80)

Statement (2): We will prove that if yl < 1/2 + δ, then φ is not a tautology. Let z be such that
Nφ(z) < 1/2 + δ. For each i ∈ {1, . . . , m}, if zi (cid:62) 0.4, let xi = 1; otherwise, let xi = 0. We will show
that x is an unsatisfying assignment for φ.

We need to show that for all clause Cj, the assignment makes Cj false. Equivalently, there exists
at least one literal in Cj that is false. Let us consider the corresponding gadget cj in the network.
Because Nφ(z) < 1/2 + δ, from Eq. (9) and the construction y = t3((cid:80)k
i=1 ci(z) < 0.5.
This implies that for all j ∈ {1, . . . , k} such that cj(z) < 1. Otherwise the remaining k − 1

i=1 ci), then (cid:80)k

gadgets are at least − 1
2k

, (cid:80)k

i=1 ci(z) (cid:62) 1 − (k − 1) 1
2k

(cid:62) 0.5.

Because cj(z) < 1 and cj = t5(t4(lj1) + t4(lj2) + t4(lj3)), [t4(lj1) + t4(lj2) + t4(lj3)](z) < 0.1.
For all the three literals Lj1, Lj2, Lj3 in Cj, consider the three gadgets corresponding to them in
the neural network. Because [t4(lj1) + t4(lj2) + t4(lj3)](z) < 0.1, at least one literal is valued < 0.4.
Otherwise, the [t4(lj1) + t4(lj2) + t4(lj3)](z) (cid:62) 0.1 + 0.1 + 0.1 = 0.3. WLOG, let’s assume lj1 < 0.4.
Let the corresponding literal Lj1 come from variable Xe. Either Lj1 = Xe or Lj1 = ¬Xe. In
the former case, because lj1(z) < 0.4, then lij(z) = lij(ze) = ze < 0.4. According to our assignment
rule, xe = 0 and Xe is evaluated false. In the latter case, li1(z) = li1(zj) = 1 − zj < 0.4, so zj > 0.6.
According to the assignment rule, xj = 1, and so Xj is evaluated to true, and ¬Xj is evaluated to
false. In either case, Cj is evaluated to false, because Cj is a conjunction of literals.

We have shown that the assignment x unsatisﬁes Ci, and so the 3DNF Boolean instance φ is not

a tautology.

42

