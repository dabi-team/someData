DomiKnowS: A Library for Integration of Symbolic Domain Knowledge
in Deep Learning

EMNLP Demo Track’2021

Hossein Rajaby Faghihi1, Quan Guo2, Andrzej Uszok3, Aliakbar Nafar1, Elaheh Raisi1, and Parisa Kordjamshidi1
1 Michigan State University, 2 Sichuan University
3 Florida Institute for Human and Machine Cognition
rajabyfa@msu.edu, guoquan@scu.edu.cn, auszok@ihmc.org,

{nafarali, raisiela, kordjams}@msu.edu

1
2
0
2

g
u
A
7
2

]

G
L
.
s
c
[

1
v
0
7
3
2
1
.
8
0
1
2
:
v
i
X
r
a

Abstract

We demonstrate a library for the integration of
domain knowledge in deep learning architec-
tures. Using this library, the structure of the
data is expressed symbolically via graph decla-
rations and the logical constraints over outputs
or latent variables can be seamlessly added
to the deep models. The domain knowledge
can be deﬁned explicitly, which improves the
models’ explainability in addition to the per-
formance and generalizability in the low-data
regime. Several approaches for such an inte-
gration of symbolic and sub-symbolic models
have been introduced; however, there is no li-
brary to facilitate the programming for such an
integration in a generic way while various un-
derlying algorithms can be used. Our library
aims to simplify programming for such an in-
tegration in both training and inference phases
while separating the knowledge representation
from learning algorithms. We showcase vari-
ous NLP benchmark tasks and beyond. The
framework is publicly available at Github1.

1

Introduction

Current deep learning architectures are known to
be data-hungry with issues mainly in generalizabil-
ity and explainability (Nguyen et al., 2015). While
these issues are hot research topics, one approach
to address them is to inject external knowledge di-
rectly into the models when possible. While learn-
ing from examples revolutionized the way that in-
telligent systems are designed to gain knowledge,
many tasks lack adequate data resources. Gen-
erating examples to capture knowledge is an ex-
pensive and lengthy process and especially not
efﬁcient when such a knowledge is available ex-
plicitly. Therefore, one main motivation of our
DomiKnowS is to facilitate the integration of do-
main knowledge in deep learning architectures, in
particular when this knowledge is represented sym-
bolically.

1https://github.com/HLR/DomiKnowS

In this demonstration paper, we highlight the
components of this framework that help to com-
bine learning from data and exploiting knowledge
in learning, including: 1) Learning problem speciﬁ-
cation 2) Knowledge representation 3) Algorithms
for integration of knowledge and learning. Cur-
rently, DomiKnowS implementation relies on py-
Torch and off-the-shelf optimization solvers such as
Gurobi. However, it can be extended by developing
hooks to other solvers and deep learning libraries
since the interface is generic and independent from
the underlying computational modules.

In general, the integration of domain knowledge
can be done 1) using pretrained models and trans-
ferring knowledge (Devlin et al., 2018; Mirzaee
et al., 2021), 2) designing architectures that in-
tegrate knowledge expressed in knowledge bases
(KB) and knowledge graphs (KG) in a way that the
KB/KG context inﬂuences the learned representa-
tions (Yang and Mitchell, 2019; Sun et al., 2018),
or 3) using the knowledge explicitly and logically
as a set of constrains or preferences over the in-
puts or outputs (Li and Srikumar, 2019a; Nand-
wani et al., 2019b; Muralidhar et al., 2018; Stew-
art and Ermon, 2017). Our current library aims
at facilitating the third approach. While applying
the constraints on input is technically trivial and
could be done in a data pre-processing step, apply-
ing constraints over outputs and considering those
structural constraints during training is a research
challenge (Nandwani et al., 2019b; Li and Sriku-
mar, 2019a; Guo et al., 2020). This requires encod-
ing the knowledge at the algorithmic level. How-
ever, given that the constraints can be expressed
logically and symbolically, having a language to
express such a knowledge in a principled way is
lacking in the current machine learning libraries.
Using our developed DomiKnowS library, the do-
main knowledge will be provided symbolically and
by the user utilizing a logical language that we have
deﬁned. This knowledge is used in various ways:

 
 
 
 
 
 
a) As soft constraints by considering the violations
as a part of loss function, this is done using a prim-
dual formulation (Nandwani et al., 2019b) and can
be expanded to probabilistic and sampling-based
approaches (Xu et al., 2018) or by mapping the con-
straint to differentiable operations (Li and Sriku-
mar, 2019b) b) mapping the constrains to an integer
linear program and perform inference-based train-
ing by masking the loss (Guo et al., 2020). Indepen-
dent form the training paradigm the constraint can
be always used as hard constraints during inference
or not used at all.

An interactive online demo of the framework is
available at Google Colab2 and the framework is
accessible on GitHub3.

2 Related Research

Integration of domain knowledge in learning relates
to tools that try to express the prior or posterior in-
formation about variables beyond what is in the
data. This relates to probabilistic programming
languages such as (Pfeffer, 2016), Venture (Mans-
inghka et al., 2014), Stan (Carpenter et al., 2017),
and InferNet (Minka et al., 2012). The logical
expression of domain knowledge is used in prob-
abilistic logical programming languages such as
ProbLog (De Raedt et al., 2007), PRISM (Sato and
Kameya, 1997), the recent version of Problog, that
is, Deep Problog (Manhaeve et al., 2018), Statis-
tical Relational Learning tools, such as Markov
logic networks (Domingos and Richardson, 2004),
Probabilistic soft logic (Broecheler et al., 2010),
Bayesian Logic (BLOG) (Milch et al., 2005),
and slightly related to learning over graph struc-
tures (Zheng et al., 2020). Considering the struc-
ture of the output without its explicit declara-
tion is considered in structured output prediction
tools (Rush, 2020). This library is mostly related to
the previous efforts for learning based program-
ming and the integration of logical constraints
in learning with classical machine learning ap-
proaches (Rizzolo and Roth, 2010; Kordjamshidi
et al., 2015, 2016). Our framework makes this
connection to deep neural network libraries and ar-
bitrarily designed architectures. The unique feature
of our library is that, the graph structure is deﬁned
symbolically based on the concepts in the domain.
Despite Torch-struct (Rush, 2020), our library is in-
dependent from the underlying algorithms, and ar-

2https://hlr.github.io/domiknows-nlp/
3https://github.com/HLR/DomiKnowS

bitrary structures can be expressed and used based
on various underlying algorithms. In contrast to
DeepProbLog, we are not limited to probabilis-
tic inference and any solver can be used for infer-
ence depending on the training paradigm that is
used for exploiting the logical constraints. Prob-
abilistic soft logic is another framework that con-
siders logical constraints in learning by mapping
the constraint declarations to a Hing loss Markov
random ﬁeld (Bach et al., 2017). DRaiL is an-
other declarative framework that is using logical
constraints on top of deep learning and converts
them to an integer linear program at at inference
time (Zhang et al., 2016). None of the above men-
tioned frameworks accommodate working with raw
sensory data nor help in putting that in an opera-
tional structure that can form the domain predicates
and be used by learning modules while our frame-
work tries to address that challenge. We support
training paradigms that make use of the inference
as a black box and in those cases any constraint op-
timization, logical inference engine or probabilistic
inference tool can be integrated and used based on
our abstraction and the provided modularity.

3 Declarative Learning-based

Programming

We use the Entity-Mention-Relation (EMR) extrac-
tion task to describe the framework. We discuss
more showcases in Section 5.

Given an input text such as "Washington is em-
ployed by Associated Press.", the task is to extract
the entities and classify their types (e.g., people,
organizations, and locations) as well as relations
between them (e.g., works for, lives in). For ex-
ample, for the above sentence [Washington] is a
person [Associated Press] is an organization
and the relationship between these two entities is
work-f or. We choose this task as it includes the
prediction of multiple outputs at the sentence level,
while there are global constraints over the outputs.4
In DomiKnowS, ﬁrst, using our python-based
speciﬁcation language the user describes the prob-
lem and its logical constraints declarativly inde-
pendent from the solutions. Second, it deﬁnes the
necessary computational units (here PyTorch-based
architectures) and connect the solution to the prob-
lem speciﬁcation. Third, a program instance is

4Please note this is just an example of a learning problem
and does not have anything to do with the main functionality
of the framework.

created to execute the model using a background
knowledge integration method with respect to the
problem description.

The entity, people, organization and location are
the derived concepts from the phrase concept and
the rest are derived from the pair.

3.1 Problem Speciﬁcation

To model a problem in DomiKnowS, the user
should specify the problem domain as a conceptual
graph G(V, E). The nodes in V represent concepts
and the edges in E are relationships. Each node
can take a set of properties P = P1, P2, ..., Pn.
Later, the logical constraints are expressed using
the concepts in the graph. In EMR task, the graph
contains some initial NLP concepts such as sen-
tence, phrase, pair and additional domain concepts
such as people, organization, and work-for.

3.1.1 Concepts

Each problem deﬁnition can contain three main
types of concepts (nodes).
Basic Concepts deﬁne the structure of the input
of the learning problem. For instance sentence,
phrase, and word are all base concepts that can be
deﬁned in the EMR task.
Compositional Concepts are used to deﬁne the
many-to-many relationships between the basic con-
cepts. Here, the pair concept in the EMR task is
a compositional concept. This is used as a basic
concept for the relation extraction. We will further
discuss this when describing edges in Section 3.1.2.
Decision Concepts are derived concepts which are
usually the outputs of the problem and subject to
prediction. They are derived from the basic or
compositional concepts. The people, organization,
and work-for are examples of derived concepts in
the EMR conceptual graph. Following is a partial
snippet showing the deﬁnition of basic and compo-
sitional concepts for EMR task.

1

2

3

4

word = Concept(name='word')
phrase = Concept(name='phrase')
sentence = Concept(name='sentence')
pair = Concept(name='pair')

The following snippet also shows the deﬁnition of
some derived concepts in EMR example.

1

2

3

4

5

6

entity = phrase(name='entity')
people = entity(name='people')
org = entity(name='organization')
location = entity(name='location')
work_for = pair(name='work_for')
located_in = pair(name='located_in')

3.1.2 Edges
After deﬁning the concepts, the user should spec-
ify existing relationships between them as edges
in the conceptual graph. Edges are used to either
map instances from one concept to another, or gen-
erate instances of a concept from another concept.
DomiKnowS only supports a set of predeﬁned edge
types, namely is_a, has_a, contains,. is_a is auto-
matically deﬁned between a derived concept and its
parent. In the EMR example, there is an is_a edge
between people and entity.
is_a is mostly used
to introduce hierarchical constraints and relate the
basic and derived concepts.
Has_a connects a compositional concept to its com-
ponents (also referred to as arguments).
In the
EMR example, pair concept has two has_a edges
to the phrase concept to specify the arg1 and arg2
of the composition. We allow an arbitrary number
of arguments in a has_a relationship, see below.

1

pair.has_a(arg1=phrase, arg2=phrase)

Contains edge deﬁnes a one-to-many relationship
between two concepts to represent (parent, child)
relationship. Here, the parents of a concept is not
necessarily limited to be only one. Following is a
sample snippet to deﬁne a contains edge between
sentence and phrase:

1

sentence.contains(phrase)

3.1.3 Global Constraints
The constraint deﬁnition is the part where the prior
knowledge of the problem is deﬁned to enable
domain integration. The constraints of each task
should be deﬁned on top of the problem using the
speciﬁed concepts and relationships there.

The constraints can be 1) automatically in-
ferred from the conceptual graph structure, 2) ex-
tracted from the standard ontology formalism (here
OWL5), 3) explicitly deﬁned in the DomiKnowS’s
logical constraint language. The framework inter-
nally uses deﬁned constraints in the training-time
or inference-time optimization depending on the in-
tegration method selected for the task. We discuss
the inference phase in more details in section 4.1.

5Ontology Web Language

Here is an example of constraint in our constraint
language for the EMR task:

and straightforward linear neural model to classify
phrases and pairs into different classes such as
people, organization, etc.

1

ifL(work_for('x'), andL(people(path=
(cid:44)→

('x',arg1)),
organization(path='x',arg2)))

(cid:44)→

The above constraint indicates that a work_for re-
lationship only holds between people and organi-
zation. Other syntactic variations of this constraint
are shown in the Appendix.

To process constraints, DomiKnowS maps those
to a set of equivalent algebraic inequalities or their
soft logic interpretation depending on the integra-
tion method. We discuss this more in Section 4.1.

3.2 Model Declaration

Model declaration phase is about deﬁning the com-
putational units of the task. The basic building
blocks of the model in DomiKnowS are sensors
and learners, which are used to deﬁne either de-
terministic or probabilistic functionalities of the
model. Sensor/Learners interact with the concep-
tual graph by deﬁning properties on the concepts
(nodes). Each sensor/learner receives a set of in-
puts either from the raw data or property values
on the graph and introduces new property values.
Sensors are computational units with no trainable
parameters; and learners are the ones including the
neural models. As stated before, the model dec-
laration phase only deﬁnes the connection of the
graph properties to the computational units and the
execution is done later by the program instances.
The user can use any deep learning architecture
compatible with pyTorch modules alongside the
set of pre-designed and commonly-used neural ar-
chitectures currently existing in the framework. To
facilitate modeling different architectures and com-
putational algorithms in DomiKnowS, we provide
a set of predeﬁned sensors to do basic mathemat-
ical operations and linguistic feature extraction.
Following is a short snippet of deﬁning some sen-
sors/learners for the EMR task.

4 Learning and Evaluation in

DomiKnowS

To execute the deﬁned model with respect to the
problem graph, DomiKnowS uses program in-
stances. A program instance is responsible to run
the model, apply loss functions, optimize the pa-
rameters, connect the output decisions to the in-
ference algorithms, and generate the ﬁnal results
and metrics. Executing the program instance relies
on the problem graph, model declaration, dataload-
ers and a backbone data structure called DataNode.
DataLoader provides an iterable object to loop
over the data. DataNode is an instance of the con-
ceptual graph to keep track of the data instances
and store the computational results of the sensors
and learners. For the EMR task the program deﬁni-
tion is as follows:

1

program = Program(graph,
(cid:44)→

poi=(sentence, phrase, pair),
loss=NBCrossEntropyLoss(),
metric=PRF1())

(cid:44)→

(cid:44)→

Here, the concepts passed to the poi ﬁeld speciﬁes
the training points of the program. This enables the
user to only train parts of the deﬁned model.

For each program instance, the user should spec-
ify the domain knowledge integration method. The
available methods for integration is discussed in
Section 4.1. After initializing the program, the user
can call train, test, and prediction functionalities to
train and evaluate the designed model. Below snip-
pet is to run training and evaluation on the EMR
task:

1

2

program.train(train_reader,
test_reader, epochs=10,
(cid:44)→
Optim=torch.optim.SGD(param,
lr=.001))

(cid:44)→
program.test(new_test_reader)

(cid:44)→

1

2

3

forward=word2vec)

phrase['w2v'] = FunctionalSensor('text',
(cid:44)→
phrase[people] = ModuleLearner('w2v',
(cid:44)→
pair[work_for] = ModuleLearner('emb',
module=Classifier(FEATURE_DIM*2))
(cid:44)→

module=Classifier(FEATURE_DIM))

In this example, the sensor Word2Vec is used to
obtain token representations from the “text” prop-
erty of each phrase. There is also a very simple

Here, the user will specify the dataloaders for dif-
ferent sets of the data and the hyper-parameters
required to train the model.

Programs can be composed to address different
training paradigms such as end-to-end or pipeline
training by deﬁning different training points for
each program. More details available in the Ap-
pendix. Following is an alternative program deﬁni-
tion for the pre-training phrases and training pairs:

1

2

3

poi=(phrase, sentence))

program_1 = Program(graph,
(cid:44)→
program_2 = Program(graph,
(cid:44)→
program_1.train(); program_2.train()

poi=(pair))

4.1

Inference and Optimization

DomiKnowS provides access to a set of approaches
to integrate background knowledge in the form of
constraints on the output decisions or latent vari-
ables/concepts. Currently, DomiKnowS addresses
three different paradigms for integration: 1) Learn-
ing + prediction time inference (L+I) 2) Training-
time integration with hard constraints 3) Training-
time integration with soft constraints. The ﬁrst
method, which we refer to as enforcing global con-
straints can also be combined and applied on top of
the second and third approaches at inference-time.
Prediction-time Inference: In the back-end of
DomiKnowS, ILP 6 (Roth and Yih, 2005) solvers
are used to make inference under global linear con-
straints. The constraints are denoted by C (·) ≤ 0.
Without loss of generality, we can denote the struc-
tured output as a binary vector y ∈ Rn. Given
local predictions F (θ) from the neural network,
the global inference can be modeled to maximize
the combination of log probability scores subject
to the constraints (Roth and Yih, 2005; Guo et al.,
2020) as follows,

F ∗(θ) = argmax

log F (θ)(cid:62)y

y

subject to C (y) ≤ 0.

(1)

To handle constraints in ILP, we create variables
for each local decision of instances and trans-
form the logical constraints to algebraic inequal-
ities (Rizzolo and Roth, 2010) in terms of those
variables. Auxiliary variables are added to repre-
sent the nested constraints. The inference method
can be extended to support other approaches such
as probabilistic inference and dynamic program-
ming in future.
Integration of hard constraint in training: Here,
we use our proposed inference-masked loss ap-
proach (IML) (Guo et al., 2020) which constructs
a mask over local predictions based on the global
inference results. The main intuition is to avoid
updating the model based on local violations when
the global inference can recover true labels from
the current predictions. Given structured predic-
tion F (θ) from a neural network and its global

6Integer Linear Programming

inference F ∗(θ) subject to the constraints, IML is
extended from negative log likelihood as follows

LIML (F (θ), Y ) =
− ((1 − F ∗(θ)) (cid:12) Y )(cid:62) log F (θ),

(2)

where Y is the structured ground-truth labels and (cid:12)
indicates element-wise product. We implemented
LIML(λ) which balances between negative log like-
lihood and IML with a factor λ as in (Guo et al.,
2020). IML works best for very low-resource tasks
where label disambiguation cannot be learned from
the data but easier to be done given available re-
lational constraints between instances of training
samples. The required constraint mapping for the
IML is the same as the global constraint optimiza-
tion tool (here ILP).
Integration of soft constraints in training: We
use the primal-dual formulation of constraints pro-
posed in (Nandwani et al., 2019a) to integrate soft
constraints in training the models. Primal-Dual con-
siders the constraints in the neural network training
by augmenting the loss function using Lagrangian
multipliers Λ for the violations of the constraints by
the set of predictions. The constraints are regular-
ized by a hinge function [C (F (θ))]+. The problem
is formulated as a min-max optimization where it
maximizes the Lagrangian function with the mul-
tipliers to enforce the constraints and minimize it
with the parameters in the neural network. Here,
instead of solving the min-max primal, we solve
the max-min dual of the original problem.

max
Λ

min
θ

L (F (θ) , Y ) + Λ(cid:62) [C (F (θ))]+ .

(3)

During training, we optimize by minimization and
maximization alternatively. With Primal-Dual strat-
egy, the model learns to obey the constraints with-
out the help from additional inference. Primal-Dual
is less time-consuming at prediction-time than the
previous methods as it does not need an additional
ILP optimization phase. This can also be used for
semi-supervised setting using domain knowledge.
Handling constraints in Primal-Dual is done by
mapping them to their respective soft logic inter-
pretation (Nandwani et al., 2019b).

It is an open research topic to identify which of
the integration methods performs best for different
tasks. However, DomiKnowS makes it effortless
to use one problem speciﬁcation and run all the
aforementioned methods.

5 Showcases

5.3

Image Classiﬁcation

1

The effectiveness of ILP (Roth and Yih, 2005),
IML (Guo et al., 2020), and Primal-Dual (Nand-
wani et al., 2019a) methods have been already
shown in their respective papers. Here, we pro-
vide different tasks and settings to showcase our
framework’s abilities and ﬂexibility to model vari-
ous problems. The results, models’ implementation
and details of experiments are (partially) available
in the Supplementary part of this paper and (com-
pletely) in DomiKnowS’ GitHub Repository7.

5.1 EMR

Our implementation of the EMR task is based on
the CoNLL (Sang and De Meulder, 2003) bench-
mark and follows the same setting as in (Guo et al.,
2020). The model uses BERT (Devlin et al., 2019)
for token representation and a linear boolean clas-
siﬁer for each derived concept. The constraints
used in this experiment are the domain and range
constraints of pairs and the mutual exclusiveness of
different derived concepts, which were seen during
the previous sections. IML and Primal-Dual meth-
ods perform the same as the baseline using both
100% and 25% of the data, while ILP inference
achieves 1.3% improvement on the 100% of data
and 0.6% improvement on 25% of the data. More
details are available in the Appendix.

5.2 Question Answering

We use WIQA (Tandon et al., 2019) benchmark as
a sample question answering task in DomiKnowS.
The problem graph contains paragraph, question,
symmetric, and transitive concepts. Each para-
graph comes with a set of questions. As explained
by Asai and Hajishirzi, enforcing constraints be-
tween different question answers is beneﬁcial to
the models’ performance. By modeling those con-
straints in DomiKnowS, ILP improves the model’s
accuracy from 74.22% to 79.05%, IML reaches
75.49%, Primal-Dual achieves 76.59%, and the
combination of Primal-Dual and ILP performs best
with 80.35% accuracy. More details are available
in the Appendix. Following is a sample constraint
deﬁned for the task.

1

2

arg2=question)

symmetric.has_a(arg1=question,
(cid:44)→
ifL(is_more('x'), is_less(path=('x',
(cid:44)→

arg2))

7https://github.com/HLR/DomiKnowS

We use CIFAR-10 benchmark (Krizhevsky et al.)
to show image classiﬁcation task in DomiKnowS.
CIFAR-10 consists of 60,000 colourful images of
10 classes with 6,000 image for each class. To con-
struct the graph, we deﬁned the derived concepts,
airplane, dog, truck, automobile, bird, cat, deer,
frog, horse and ship, and the base concept image.
We introduce the disjoint constraint between the
labels of an image. For this problem, hierarchical
constraints between class labels can also be used
easily.

disjoint(truck, dog, airplane,
(cid:44)→

automobile, bird, cat, deer, frog,
horse, ship)

(cid:44)→

Both the disjoint and hierarchical constraints do
not affect the accuracy of the task by a large margin
and ILP can only achieve near 0.5% improvement
over the classiﬁcation task.

5.4

Inference-Only Example

This example is to show that DomiKnowS can
solve pure optimization problems as well. The task
is similar to the classic graph-coloring problem. A
set of cities are given each of which can have a ﬁre
station or not. We want to allocate the ﬁre stations
to cities in a way that the following constraint is
met.
Constraint: For each city x, it either has a ﬁre
station or there exists a city y which is a neighbor
of city x and has a ﬁre station.

To implement this we deﬁne the basic concept
city, the neighbor relationship between two cities
and the derived concept FirestationCity.

1

2

neighbor.has_a(arg1=city, arg2=city)
orL(firestationCity('x'),
(cid:44)→

existsL(firestationCity(path=('x',
neighbor.arg2))

(cid:44)→

We have also included more showcases to solve
sentiment analysis (Go et al., 2009) and email spam
detection in our GitHub repository 7. We will add
models for procedural reasoning (Faghihi and Kord-
jamshidi, 2021) and spatial role labeling (Mirzaee
et al., 2021) in future.

6 Conclusions and Future Work

DomiKnowS makes it effortless to integrate do-
main knowledge into deep neural network models

using a uniﬁed framework. It allows users to switch
between different algorithms and beneﬁt from a
rich source of abstracted functionalities and com-
putational modules developed for multiple tasks.
It allows naming concepts, deﬁning their relation-
ships symbolically and combining symbolic and
sub-symbolic reasoning over the named concepts.
DomiKnowS helps in interpretability of neural ar-
chitectures by providing named layers and access
to the neural models’ computations at each stage
of the training and evaluation process. As a future
direction, we are looking to enrich our library with
predeﬁned functionalities and neural models and
further extend the framework’s ability to support
more methods on integration of domain knowledge
with deep neural models as well as seamless model
composition. DomiKnowS is publicly available at
our website8 and on GitHub.9

Acknowledgements

This project is partially funded by the Ofﬁce of
Naval Research (ONR) grant #N00014-20-1-2005.

References

Akari Asai and Hannaneh Hajishirzi. 2020. Logic-
guided data augmentation and regularization for con-
sistent question answering.

Stephen H. Bach, Matthias Broecheler, Bert Huang,
and Lise Getoor. 2017. Hinge-loss markov random
ﬁelds and probabilistic soft logic. Journal of Ma-
chine Learning Research (JMLR), 18:1–67.

Matthias Broecheler, Lilyana Mihalkova, and Lise
Getoor. 2010. Probabilistic similarity logic. In Con-
ference on Uncertainty in Artiﬁcial Intelligence.

Bob Carpenter, Andrew Gelman, Matthew Hoffman,
Daniel Lee, Ben Goodrich, Michael Betancourt,
Marcus Brubaker, Jiqiang Guo, Peter Li, and Allen
Riddell. 2017. Stan: A probabilistic programming
language. Journal of Statistical Software, Articles,
76(1):1–32.

Luc De Raedt, Angelika Kimmig, and Hannu Toivonen.
2007. Problog: a probabilistic Prolog and its appli-
cation in link discovery. In Proceedings of the 20th
International Joint Conference on Artiﬁcial Intelli-
gence, pages 2468–2473. AAAI Press.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.

8https://hlr.github.io/domiknows-nlp/
9https://github.com/HLR/DomiKnowS

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
In Proceedings of the 2019 Conference
standing.
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers),
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.

Perdo Domingos and Matthew Richardson. 2004.
Markov logic: A unifying framework for statistical
In ICML’04 Workshop on Sta-
relational learning.
tistical Relational Learning and its Connections to
Other Fields, pages 49–54.

Hossein Rajaby Faghihi and Parisa Kordjamshidi. 2021.
Time-stamped language model: Teaching language
models to understand the ﬂow of events.

Alec Go, Lei Huang, and Richa Bhayani. 2009. Twitter

sentiment analysis.

Quan Guo, Hossein Rajaby Faghihi, Yue Zhang,
Andrzej Uszok, and Parisa Kordjamshidi. 2020.
Inference-masked loss for deep structured output
In Proceedings of the Twenty-Ninth In-
learning.
ternational Joint Conference on Artiﬁcial Intelli-
gence, IJCAI-20, pages 2754–2761. International
Joint Conferences on Artiﬁcial Intelligence Organi-
zation. Main track.

P. Kordjamshidi, D. Roth, and H. Wu. 2015. Saul: To-
wards declarative learning based programming. In
Proc. of the International Joint Conference on Artiﬁ-
cial Intelligence (IJCAI).

Parisa Kordjamshidi, Daniel Khashabi, Christos
Christodoulopoulos, Bhargav Mangipudi, Sameer
Singh, and Dan Roth. 2016. Better call saul: Flex-
ible programming for learning and inference in nlp.
In Proc. of the International Conference on Compu-
tational Linguistics (COLING).

Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
Cifar-10 (canadian institute for advanced research).

Tao Li and Vivek Srikumar. 2019a. Augmenting neu-
ral networks with ﬁrst-order logic. arXiv preprint
arXiv:1906.06298.

Tao Li and Vivek Srikumar. 2019b. Augmenting neu-
ral networks with ﬁrst-order logic. In Proceedings
of the 57th Conference of the Association for Com-
putational Linguistics, ACL 2019, Florence, Italy,
July 28- August 2, 2019, Volume 1: Long Papers,
pages 292–302. Association for Computational Lin-
guistics.

Robin Manhaeve, Sebastijan Dumancic, Angelika Kim-
mig, Thomas Demeester, and Luc De Raedt. 2018.
Deepproblog: Neural probabilistic logic program-
ming. In Advances in Neural Information Process-
ing Systems, volume 31. Curran Associates, Inc.

Erik Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the conll-2003 shared task: Language-
independent named entity recognition. In Proceed-
ings of the Seventh Conference on Natural Language
Learning at HLT-NAACL 2003, pages 142–147.

Taisuke Sato and Yoshitaka Kameya. 1997. Prism: A
language for symbolic-statistical modeling. In Pro-
ceedings of the International Joint Conference on Ar-
tiﬁcial Intelligence (IJCAI), pages 1330–1339.

Russell Stewart and Stefano Ermon. 2017. Label-free
supervision of neural networks with physics and do-
main knowledge. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence, volume 31.

Haitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn
Mazaitis, Ruslan Salakhutdinov, and William W Co-
hen. 2018. Open domain question answering using
early fusion of knowledge bases and text. arXiv
preprint arXiv:1809.00782.

Niket Tandon, Bhavana Dalvi, Keisuke Sakaguchi, Pe-
ter Clark, and Antoine Bosselut. 2019. WIQA: A
dataset for “what if...” reasoning over procedural
In Proceedings of the 2019 Conference on
text.
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
6076–6085, Hong Kong, China. Association for
Computational Linguistics.

Jingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and
Guy Van den Broeck. 2018. A semantic loss func-
tion for deep learning with symbolic knowledge. In
Proceedings of the 35th International Conference on
Machine Learning, volume 80 of Proceedings of Ma-
chine Learning Research, pages 5502–5511. PMLR.

Bishan Yang and Tom Mitchell. 2019. Leveraging
knowledge bases in lstms for improving machine
reading. arXiv preprint arXiv:1902.09091.

Xiao Zhang, Maria Leonor Pacheco, Chang Li, and
Dan Goldwasser. 2016. Introducing DRAIL – a step
towards declarative deep relational learning. In Pro-
ceedings of the Workshop on Structured Prediction
for NLP, pages 54–62, Austin, TX. Association for
Computational Linguistics.

Da Zheng, Minjie Wang, Quan Gan, Zheng Zhang, and
George Karypis. 2020. Learning graph neural net-
works with deep graph library. WWW ’20, New
York, NY, USA. Association for Computing Machin-
ery.

Vikash K. Mansinghka, Daniel Selsam, and Yura N.
Perov. 2014. Venture: a higher-order probabilis-
tic programming platform with programmable infer-
ence. CoRR, abs/1404.0099.

Brian Milch, Bhaskara Marthi, Stuart Russell, David
Sontag, Daniel L. Ong, and Andrey Kolobov. 2005.
BLOG: Probabilistic models with unknown objects.
In Proceedings of the International Joint Conference
on Artiﬁcial Intelligence (IJCAI).

Tom Minka,

John M. Winn,

and David A. Knowles. 2012.
2.5.
Research
Microsoft
http://research.microsoft.com/infernet.

John P. Guiver,
Infer.NET
Cambridge.

Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang
Ning, and Parisa Kordjamshidi. 2021. Spartqa: A
textual question answering benchmark for spatial
reasoning. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 4582–4598.

Nikhil Muralidhar, Mohammad Raihanul Islam, Man-
ish Marwah, Anuj Karpatne, and Naren Ramakrish-
Incorporating prior domain knowledge
nan. 2018.
In 2018 IEEE Interna-
into deep neural networks.
tional Conference on Big Data (Big Data), pages
36–45. IEEE.

Yatin Nandwani, Abhishek Pathak, Mausam, and Parag
Singla. 2019a. A primal dual formulation for deep
learning with constraints. In NeurIPS.

Yatin Nandwani, Abhishek Pathak, Parag Singla, et al.
2019b. A primal dual formulation for deep learning
with constraints.

Anh Nguyen, Jason Yosinski, and Jeff Clune. 2015.
Deep neural networks are easily fooled: High con-
In
ﬁdence predictions for unrecognizable images.
Proceedings of the IEEE conference on computer vi-
sion and pattern recognition, pages 427–436.

Avi Pfeffer. 2016. Practical Probabilistic Program-

ming. Manning Publications.

N. Rizzolo and D. Roth. 2010. Learning based Java
for rapid development of NLP systems. In Proceed-
ings of the Seventh Conference on International Lan-
guage Resources and Evaluation.

D. Roth and W. Yih. 2005. Integer linear programming
inference for conditional random ﬁelds. In Proc. of
the International Conference on Machine Learning
(ICML), pages 737–744.

Alexander Rush. 2020. Torch-struct: Deep structured
In Proceedings of the 58th An-
prediction library.
nual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 335–
342, Online. Association for Computational Linguis-
tics.

1

2

3

4

1

1

2

3

A Global Constraints and Mapping

Following is an example of the mapping between
OWL constraint, graph structure and the logic
python constraint. The ontology deﬁnition in
OWL:

just by deﬁning different program instances and
calling them one after another. For instance, we
can seamlessly switch between the following varia-
tions of learning paradigms on the EMR task.
End-To-End training:

<owl:ObjectProperty rdf:ID="work_for">

rdf:resource="#people"/>

<rdfs:domain
(cid:44)→
<rdfs:range
(cid:44)→

rdf:resource="#organization"/>

</owl:ObjectProperty>

1

2

program = Program(graph, poi=(phrase,
(cid:44)→
program.train()

sentence, pair))

Pre-train phrase then just train pair:

or equivalent graph structure deﬁnition:

work_for.has_a(arg1=people,
arg2=organization)
(cid:44)→

1

2

3

poi=(phrase, sentence))

program_1 = Program(graph,
(cid:44)→
program_2 = Program(graph,
(cid:44)→
program_1.train(); program_2.train()

poi=(pair))

DomiKnowS’s constrain language representation:

Pre-train phrase and use the result in the end-to-

end training:

1

ifL(work_for('x'), andL(people(path=
(cid:44)→

('x',arg1)),
organization(path='x',arg2)))

(cid:44)→

All three above constraints represent the same
knowledge that a work_for relationship only holds
between people and organization.

In order to map this logical constrain to ILP, the
solver collects sets of candidates for each used in
the constrain concepts.

ILP inequalities are created for each of the com-
binations of candidates sets. The internal nested
andL logical expression is translated to a set of
three algebraic inequalities. The new variable
varAND) is created to transfer the result of the
internal expression into the external one.

varAND <= varPhraseIsPeople
varAND <= varPhraseIsOrganization
varPhraseIsPeople +
(cid:44)→

varPhraseIsOrganization <= varAND
+ 1

(cid:44)→

External ifL expression is translated to a single al-
gebraic inequality (refers to the variable varAND):

1

varPhraseIsWorkFor <= varAND

B Program Composition

The Program instances allow the user to deﬁne dif-
ferent training tasks without extra effort to change
the underlying models. One can deﬁne end-to-end,
pipelines, and pre-training and ﬁne-tuning steps

1

2

3

4

poi=(phrase, sentence), ...)

program_1 = POIProgram(graph,
(cid:44)→
program_2 = POIProgram(graph,
(cid:44)→

poi=(phrase, sentence, pair),
...)

(cid:44)→
program_1.train(...)
program_2.train(...)

C Experiments

C.1 EMR

Figure 1 shows the prior structural domain knowl-
edge expressed as a graph and used in this example.
It contains the basic concepts such as ‘sentence‘,
‘phrase‘, ‘word‘, and ‘pair‘ and the existing relation-
ships between them alongside the possible output
concepts such as ‘people‘ and ‘work_for‘. Figure
2 also represents a sampe DataNode graph popu-
lated for a single phrase alongside its properties
and decisions.

Tables 1 and 2 summarize the results of the
model applied to 25% of the training set and the
whole training set of the CONLL dataset respec-
tively.

C.2 Question Answering

WIQA dataset contains 39,705 multiple choice
questions regarding cause and effects in the context
of a procedural paragraph. The answer is always ei-
ther is less, is more, or no effect. To model this task
in DomiKnowS, we deﬁne paragraph, question,
symmetric, and transitive concepts. Each para-
graph comes with a set of questions. As explained

Figure 1: The domain knowledge used for the named entity and relation extraction task expressed as a graph in
DomiKnowS

Figure 2: Sample DataNode graph populated for one single phrase from the Named Entity and Relation Extraction
task.

Precision
Entity
0.898

Baseline
Baselin +
ILP
Baseline + IML 0.8851
Baseline + PD

0.896

0.9

Relation All
0.960

0.933

Recall
Entity
0.7619

Relation All
0.807

0.787

F1
Entity Relation All
0.872
0.818

0.848

0.986

0.979
0.952

0.946

0.816

0.77

0.791

0.847

0.86

0.937
0.929

0.746
0.765

0.712
0.789

0.727
0.779

0.8
0.821

0.82
0.859

0.854

0.811
0.842

Table 1: The results on the 25% of the data on Conll benchmark.

Precision
Entity Relation All
0.954
0.909

0.934

Recall
Entity Relation All
0.914
0.824

0.874

F1
Entity Relation All
0.934
0.86

0.901

0.989

0.989
0.934

0.954

0.855

0.903

0.882

0.877

0.944

0.951
0.923

0.831
0.827

0.884
0.915

0.861
0.876

0.86
0.862

0.933
0.924

0.914

0.901
0.897

Baseline
Baselin +
ILP
Baseline + IML 0.904
0.910
Baseline + PD

0.911

Table 2: The results on 100% of data on Conll benchmark.

Model
Baseline
Baseline + IML
Baseline + PD
Baseline + ILP
Baseline + PD + ILP

Test Accuracy
74.22%
75.49%
76.59%
79.05%
80.35%

Table 3: Results of accuracy on WIQA dataset

by Asai and Hajishirzi enforcing constraints be-
tween different question answers can be beneﬁcial
to the models’ performance. Here, two questions
can be the opposite of each other with a symmetric
relationship between their answers, or three ques-
tions may introduce a chain of reasoning of cause
and effects leading to a transitivity property among
their answers. Following is a sample constraint
deﬁned in DomiKnowS to represent the symmetric
property between questions.

1

2

arg2=question)

symmetric.has_a(arg1=question,
(cid:44)→
ifL(is_more('x'), is_less(path=('x',
(cid:44)→

arg2))

The results for the WIQA dataset are shown in
table 3. As it is shown, IML incorporates the con-
straints in itself and improves the baseline a little.
Primal Dual does a better job compared to IML in
improving the accuracy with the help of the con-
straints. However, it is with the help of ILP that
Baseline and Primal Dual achieve the best results.

