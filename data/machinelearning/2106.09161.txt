Mungojerrie: Reinforcement Learning of
Linear-Time Objectives(cid:63)

Ernst Moritz Hahn1

, Mateo Perez2

Ashutosh Trivedi2

, Sven Schewe3
, and Dominik Wojtczak3

, Fabio Somenzi2

,

1
2
0
2

n
u
J

8
1

]

G
L
.
s
c
[

2
v
1
6
1
9
0
.
6
0
1
2
:
v
i
X
r
a

1 University of Twente, The Netherlands
2 University of Colorado Boulder, USA
3 University of Liverpool, UK

Abstract. Reinforcement learning synthesizes controllers without prior
knowledge of the system. At each timestep, a reward is given. The con-
trollers optimize the discounted sum of these rewards.
Applying this class of algorithms requires designing a reward scheme,
which is typically done manually. The designer must ensure that their
intent is accurately captured. This may not be trivial, and is prone to
error. An alternative to this manual programming, akin to programming
directly in assembly, is to specify the objective in a formal language and
have it “compiled” to a reward scheme.
Mungojerrie (plv.colorado.edu/mungojerrie) is a tool for testing reward
schemes for ω-regular objectives on ﬁnite models. The tool contains rein-
forcement learning algorithms and a probabilistic model checker. Mungo-
jerrie supports models speciﬁed in PRISM and ω-automata speciﬁed in
HOA.

Keywords: ω-regular speciﬁcations · LTL · Reinforcement Learning ·
logic · stochastic games · probabilistic model checking

1

Introduction

Reinforcement learning (RL [33]) has seen a surge of impressive results in recent
years [31,25,24]. RL agents explore a potentially unknown environment, receiv-
ing rewards which provide feedback on performance. The agent then seeks to
maximize performance. This process is known as “learning.” For environments
which are unknown, RL is a particularly attractive technique due to its ability
to optimize without needing to explicitly construct a model internally. Apply-
ing RL requires converting the objective of the problem into one that can be
optimized via RL; a reward function must be designed.

Currently, the most common way to design the reward function is by hand.
This is prone to error [19,35]. Instead, one may consider writing the objective in a
formal language and have it converted into a reward function. A natural choice

(cid:63) This work was supported in part by the Engineering and Physical Sciences Re-
search Council through grant EP/P020909/1 and by the National Science Founda-
tion through grant 2009022.

 
 
 
 
 
 
2

E. M. Hahn, M. Perez, S. Schewe, F. Somenzi, A. Trivedi, and D. Wojtczak

g ∧ ¬b

Objective

safe

b

trap

¬g ∧ ¬b

(cid:62)

3

Model

c

0

d

1

1 − p

a

b

2
f

e

p

State

n
o
i
t
c
A

Interpreter

Observation, Reward

Agent

Fig. 1. The reinforcement learning loop implemented within Mungojerrie. The inter-
preter assigns reward to the agent based on the state of the model and automaton.

for this language is Linear Temporal Logic (LTL) [22,27], or more generally,
ω-regular languages [26].

ω-regular languages describe inﬁnite sequences. If an inﬁnite run of the sys-
tem is a word in the ω-regular language, then the property is said to have
been satisﬁed. A valid reward scheme for ω-regular objectives must be such that
the optimal strategies for reinforcement learning are guaranteed to be optimal
strategies for the ω-regular objective.

Mungojerrie provides the capability to test this by providing the tools to
learn reinforcement learning strategies with performance statistics, and test their
optimality with respect to the ω-regular objective with a model checker. The tool
supports ﬁnite state and action models speciﬁed in PRISM [21], with ω-automata
in HOA [2].

Figure 2 shows an example Markov Decision Process in which a gambler
places bets with the aim of accumulating a wealth of 7 units. In addition the
gambler will quit if her wealth wanes to just one unit more than once. This
objective is captured by the (deterministic) B¨uchi automaton of Fig. 3.

Mungojerrie computes a strategy for the gambler that maximizes the prob-
ability of satisfaction of the objective. Figure 4 shows the Markov chain that
results from following this strategy. This ﬁgure was minimally modiﬁed from
GraphViz [7] output from Mungojerrie. Note that the strategy altogether avoids
the state in which x = 1; hence it achieves the same probability of success
(5/7) as an optimal strategy for the simpler objective of eventually reaching

Mungojerrie: Reinforcement Learning of Linear-Time Objectives

3

x = 7 (without going broke). Mungojerrie computes the strategy of Fig. 4 by
reinforcement learning; it can also verify it by probabilistic model checking.

2 Overview of Mungojerrie

2.1 Models

The model for the systems used in Mungojerrie consist of ﬁnite sets of states and
actions, where states are labelled with atomic propositions. There are at most
two strategic players: Max player and Min player. Each state is controlled by
one player. We call models where all states are controlled by Max player Markov
Decision Processes (MDPs) [28]. Else, we refer to them as stochastic games [5].
Mungojerrie supports parsing models speciﬁed in the PRISM language. The
allowed model types are “mdp” (Markov Decision Process) and “smg” (Stochas-
tic Multiplayer Game) with two players. There should be one initial state. The
interface for building the model is exposed, allowing extensions of Mungojerrie
to connect with parsers for other languages.

2.2 Properties

Our properties are ω-regular languages. Starting from the initial state, the play-
ers produce an inﬁnite sequence of states with a corresponding inﬁnite sequence
of atomic propositions, an ω-word. The inclusion of this ω-word in our ω-regular
language determines whether this particular run satisﬁes the property or not.
Max player is trying to maximize the probability that a run is satisfying while
Min player is minimizing.

We specify our ω-regular language as an ω-automaton, which may be nonde-
terministic. For model checking and reinforcement learning, this nondeterminism
must be resolved on the ﬂy. Automata where this can be done in any MDP with-
out changing acceptance are said to be Good-for-MDPs (GFM) [12]. Automata
where this can be done in any stochastic game without changing acceptance are
said to be Good-for-games (GFG) [18].

In general, nondeterministic B¨uchi automata are not GFM, but two classes of
GFM B¨uchi automata with limited nondeterminism have been studied: suitable
limit-deterministic B¨uchi automata [9,30] and slim B¨uchi automata [12].

The user of Mungojerrie can either provide the ω-automaton directly or
use one of the supported external translators to generate the automaton from
LTL with a single call to Mungojerrie. Mungojerrie reads automata speciﬁed in
the HOA format. The supported LTL translators are ePMC plugin (see Sec-
tion 3.1), Spot [6], and Owl [20] for generating slim B¨uchi, deterministic parity,
and suitable limit-deterministic B¨uchi automata. The user is responsible for the
ω-automata provided directly having the appropriate property, GFM or GFG.
For use in Mungojerrie, the automata must have labels and acceptance condi-
tions on the transitions (not on the states). The acceptance conditions supported
by Mungojerrie should be reducible to parity acceptance conditions without al-
tering the transition structure of the automaton. This includes parity, B¨uchi,

4

E. M. Hahn, M. Perez, S. Schewe, F. Somenzi, A. Trivedi, and D. Wojtczak

0 mdp

1

2

3

4

5

6

c o n s t
c o n s t double p

i n t Wealth = 5 ;

// i n i t i a l ga m b l e r ’ s w e a l t h
= 1 / 2 ; // p r o b a b i l i t y o f w i n n i n g one b e t

l a b e l ” r i c h ” = x = 7 ;
l a b e l ” p o o r ” = x = 1 ;

7
8 module g a m b l e r

x :

[ 0 . . 7 ]

i n i t Wealth ;

9

10

11

12

13

14
15 endmodule

[ b0 ] x=0 ∨ x=7 → t r u e ; // a b s o r b i n g s t a t e s
[ b1 ] x>0 ∧ x<7 → p :
[ b2 ] x>1 ∧ x<6 → p :
[ b3 ] x>2 ∧ x<5 → p :

( x ’= x +1) + (1−p ) :
( x ’= x +2) + (1−p ) :
( x ’= x +3) + (1−p ) :

( x ’=x −1) ;
( x ’=x −2) ;
( x ’=x −3) ;

Fig. 2. A Gambler’s Ruin model in the PRISM language. Line 13, for example, says
that when 1 < x < 6, the gambler may bet two units because action b2 is enabled.
The ‘+’ sign does double duty: as addition symbol in arithmetic expressions and as
separator of probabilistic transitions.

¬(rich ∨ poor)

¬rich ∧ poor

¬(rich ∨ poor)

0

2

rich

(cid:62)

1

3

(cid:62)

rich

¬rich ∧ poor

Fig. 3. Deterministic B¨uchi automaton equivalent to the LTL formula ¬poor U(cid:0)rich ∨
(poor ∧ X(¬poor U rich))(cid:1). The transitions marked with the green dots are accepting.

Fig. 4. Optimal gambler strategy for the objective of Fig. 3. Boxes are decision states
and circles are probabilistic choice states. For a decision state, the label gives the value
of x and the state of the automaton. Transitions are labelled with either an action or
a probability, and with the priority (1 for accepting and 0 for non-accepting).

x5, 0 b2 (0) x3, 0 b3 (0) x6, 0 b1 (0) x7, 0richx7, 1rich b0 (1)  0.5 (0)  0.5 (0)  0.5 (0) x0, 0 0.5 (0)  0.5 (0)  0.5 (0)  b0 (1)  b0 (0) Mungojerrie: Reinforcement Learning of Linear-Time Objectives

5

co-B¨uchi, Streett 1 (one pair), and Rabin 1 (one pair) conditions. Nondetermin-
istic automata must have B¨uchi acceptance conditions. Generalized acceptance
conditions are not supported in version 1.0.

2.3 Reinforcement Learning

In reinforcement learning, we have a model as described before with the addition
of a reward function. The reward function probabilistically assigns a reward
Rt+1 ∈ R dependent on the state and action at timestep t. As our players
move through the model, we produce a sequence of states, actions, and rewards
which we index for each timestep (S0, A0, R1, S1, A1, R2, . . .). The objective in
reinforcement learning is to solve

max
π

min
ν

Eπ,ν

(cid:35)

γtRt+1

,

(cid:34) ∞
(cid:88)

t=0

where π is the strategy for Max player, ν is the strategy for Min player, γ ∈ [0, 1)
is the discount factor, and Rt is the reward at timestep t. We can set γ = 1 when
with probability 1 we enter an absorbing sink (termination) where we receive no
reward. This is called the episodic setting.

Version 1.0 of Mungojerrie includes the stochastic game extensions of Q-
learning [34], Double Q-learning [17], and Sarsa(λ) [32] for reinforcement learning
in ﬁnite state and action models. We collectively refer to parameters which are
set by hand prior to running a reinforcement learning algorithm as hyperparam-
eters. Mungojerrie supports changing all hyperparameters from the command
line. As the design of Mungojerrie separates the learning agent(s) from the re-
ward scheme, extending Mungojerrie to include another reinforcement learning
algorithm is easy.

2.4 Reward Schemes

The user of Mungojerrie can either select one of the reward schemes included
with the tool or extend the tool to include a new reward scheme. The following
reward schemes are included in version 1.0 of Mungojerrie:

– Reward from the PRISM speciﬁcation.
– The reward scheme from [10].
– The reward scheme from [3].
– The reward schemes from [11].
– The reward schemes from [13].
– The reward scheme from [29].

Note that the reward scheme of [29] may produce sub-optimal strategies [10].
All other schemes guarantee that a strategy that maximizes the expected return
of the reward also maximizes the probability that the ω-regular objective be
satisﬁed. The separation of the learning agent(s) from the reward scheme makes
the inclusion of new schemes easy. The primary eﬀort here will be to modify the
construction of the model passed to the model checker if there are additional
states beyond those due to the original model and automaton.

6

E. M. Hahn, M. Perez, S. Schewe, F. Somenzi, A. Trivedi, and D. Wojtczak

LTL

ePMC Plugin

SPOT

Owl

. . .

HOA

PRISM

HOA Parser

Automaton

PRISM Parser

Product
Construction

Model

Product

Model Checking
+
Game Solver

Interpreter
(Gym)

Agent(s)
(Learner)

Strategy + Values

Q-table(s)

Fig. 5. Mungojerrie’s block diagram

3 Tool Design

Mungojerrie begins its execution by parsing the input PRISM and HOA. (See
upper part of Fig. 5.) The HOA is either read in from a ﬁle or piped from a call
to one of the supported LTL translators. In particular an LTL translator capable
of producing slim B¨uchi automata [12] comes with the tool. See Section 3.1 for
details. Requested automaton modiﬁcations, such as determinization, are run
after this step.

If speciﬁed, Mungojerrie creates the synchronous product between the au-
tomaton and the model, and runs model checking or game solving. The requested
strategy and values are returned. Algorithms used at this step can be found in
[1,14,15].

If learning has been speciﬁed, the interpreter takes the automaton and model,
without explicitly forming the product, and provides an interface akin to [4] for
the reinforcement learning agent to interact with the environment and receive
rewards. When learning is complete, the Q-table(s) can be saved to a ﬁle for

Mungojerrie: Reinforcement Learning of Linear-Time Objectives

7

later use with Mungojerrie, and the interpreter forms the Markov chain induced
by the learned strategy and sends it the model checker for veriﬁcation.

Mungojerrie is written in C++. It has been tested on Ubuntu 20.04, Ubuntu

18.04, and MacOS 11.2.

3.1 Slim B¨uchi Automata Generation

LTL formula (1)

HOA ﬁle (2)

translate (Spot) (3)

parse (4)

ePMC plugin

NTLBA (5)

construct SBA (6)

construct LDBA (7)

LDBA (9)

minimize LDBA (10)

SBA (8)

minimized LDBA (11)

construct simulation game (12)

simulation game (13)

won

game solver (14)

lost

HOA ﬁle (15)

Fig. 6. Automata generation block diagram

We have implemented slim B¨uchi automata generation as a plugin of the
probabilistic model checker ePMC [16]. The process is described in Fig. 6. The
starting point is a transition-labelled B¨uchi automaton in HOA format [2] (2)
or an LTL formula (1). In case we are given an automaton in HOA format, we

8

E. M. Hahn, M. Perez, S. Schewe, F. Somenzi, A. Trivedi, and D. Wojtczak

parse this automaton (4) and if we are given an LTL formula, we use the tool
Spot [6] to transform the formula into an automaton (3). In both cases, we end
up with a transition-labelled B¨uchi automaton (5).

Afterwards, we have two possibilities. The ﬁrst option is to transform (6) this
automaton into a slim B¨uchi automaton (8) [12]. These automata can then be
directly composed with MDPs for model checking or used to produce rewards
for learning.

The other option is to construct (7) a suitable limit-deterministic B¨uchi au-
tomaton (SLDBA) (9). Automata of this type consist of an initial part and a
ﬁnal part. A nondeterministic choice only occurs when moving from the initial
to the ﬁnal part by an ε transition (a transition without reading a character).
SLDBA can be directly composed with MDPs. However, SLDBA directly con-
structed from general B¨uchi automata are often quite large, which in turn also
means that the product with MDPs would be quite large as well. Therefore, we
have implemented further optimization steps. We can apply a number of algo-
rithms to minimize (10) this automaton so as to achieve a smaller SLDBA (11).
To do so, we implemented several methods:

– Subsuming the states in the ﬁnal part with an empty language
– Signature-based strong bisimulation minimization in the ﬁnal part
– Signature-based strong bisimulation minimization in the initial part
– Language-equivalence of states in the ﬁnal part
– If we have a state s in the initial part for which we ﬁnd a state s(cid:48) in the
ﬁnal part where the language of s and s(cid:48) are the same, we can remove all
transitions of s and add an ε transition from s to s(cid:48) instead. Automaton
states which cannot be reached anymore afterwards can be removed.

Each of these methods has a diﬀerent potential for minimization as well as
runtime: We allow to specify which optimizations are to be used and in which
order they are applied.

Once we have optimized the SLDBA, we could directly use it for later compo-
sition with an MDP. Another possibility is to prove that the original automaton
is already suitable for MDPs. If this is the case, then it is often preferable to use
the original automaton: being constructed by specialized tools such as Spot, it
is often smaller than the minimized SLDBA.

The original automaton is suitable if it simulates the SLDBA [12]. If it does,
then it is also composable with MDPs. Otherwise, it is unknown whether it is
suitable for MDPs. In this case, sometimes more complex notions of simulation
can be used.

To show simulation, we construct (12) a simulation game, which in our case
is a transition-labelled parity game (13) with 3 colors. We solve these games
using (a slight variation of) the McNaughton algorithm [23]. (We are aware
that specialized algorithms for parity games with 3 colors exist [8]. However, so
far the construction of the arena, not solving the game, turned out to be the
bottleneck here). If the even player is winning, the simulation holds. Otherwise,
more complex notions of simulation can be used, which however lead to larger
parity games being constructed. In case the even player is winning for any of

Mungojerrie: Reinforcement Learning of Linear-Time Objectives

9

them, we can use the original automaton, otherwise we have to use the SLDBA.
In any case, we export the result to an HOA ﬁle (15). For illustration and
debugging purposes, automata and simulation games can also be exported to
the GraphViz format [7].

4 Two Use Cases

Comparing Automata. An ω-regular objective may be described by diﬀerent
automata, many of which may be good-for-MDPs. Mungojerrie can be used to
compare the eﬀectiveness of such automata when used in reinforcement learning.
Consider the two nondeterministic B¨uchi automata shown in Fig. 7. Both are
equivalent to the LTL formula (F G x) ∨ (G F y), but the one on the right should
be better for learning: long transient sequences of observations that satisfy x∧¬y
may convince the agent to commit to State 1 of the left automaton too soon.

y

0

¬y

x ∧ ¬y

1

x

x ∧ ¬y

y

1

x ∧ ¬y

y

0

¬y

Fig. 7. Equivalent, but not equally eﬀective, B¨uchi automata.

To test this conjecture, we run Mungojerrie on a model with 108 decision
nodes organized in two long chains. In one of them the agent sees many xs for a
while, but eventually only sees ys. In the other chain the situation is reversed.
Which chain is followed is up to chance. With 2000 episodes, the right automaton
allows Q-learning to reliably ﬁnd an optimal strategy (the objective is achieved
with probability 1) while the left automaton most of the time learns strategies
that achieve the objective with probability 0.5.

A Game of Pursuit. Figure 8 describes a stochastic parity game of pursuit in
which the Max player (M ) tries to escape from the Min player (m). At each
round, each player in turn chooses a direction to move. If movement in that
direction is not obstructed by a wall, then the player moves either two squares
or one square with equal probabilities. One square of the grid is a trap, which m
must avoid at all times, but M may visit ﬁnitely many times. Player M should be
at least 5 squares away from player m inﬁnitely often. This objective is described
by the LTL property (F ¬trapmn)∨((F G ¬trapmx)∧(G F ¬close)), where trapmn
and trapmx are true when m and M visit the trap square, respectively, and
close is true when the Manhattan distance between the two players is less than

10

E. M. Hahn, M. Perez, S. Schewe, F. Somenzi, A. Trivedi, and D. Wojtczak

5 squares. This objective translates to the deterministic parity automaton in
Fig. 8, which accepts a word if the maximum recurring priority of its run is odd.

5

4

3

2

1

0

M

m

0

1

2

3

4

5

¬trapmn ∧ trapmx, (2)
¬trapmn ∧ ¬trapmx ∧ ¬close, (1)
¬trapmn ∧ ¬trapmx ∧ close, (0)

trapmn, (1)

0

1

(cid:62), (1)

Fig. 8. A grid-world stochastic game arena (left) and a deterministic parity automaton
for the objective (right).

Unlike the example of Fig. 2, inspection of the Markov chain induced by an
optimal strategy is impractical. Instead, Mungojerrie can save the strategy in
CSV format. Postprocessing can then produce a graphical representation like
the one of Fig. 9. The color gradient shows that, in the main, M ’s strategy is to
move away from m.

5

4

3

2

1

0

5

4

3

2

1

0

5

4

3

2

1

0

5

4

3

2

1

0

5

4

3

2

1

0

5

4

3

2

1

0

0

1

2

3

4

5

0

1

2

3

4

5

0

1

2

3

4

5

0

1

2

3

4

5

0

1

2

3

4

5

0

1

2

3

4

5

Fig. 9. Max player learned strategy for the game of Fig. 8 when the automaton is in
State 0. (Any strategy will do when the automaton is in State 1.) In each 6 × 6 box
the rose-colored square is the position of the minimizing player, while the light-blue
square marks the trap.

Mungojerrie: Reinforcement Learning of Linear-Time Objectives

11

References

1. de Alfaro, L.: Formal Veriﬁcation of Probabilistic Systems. Ph.D. thesis, Stanford

University (1998)

2. Babiak, T., Blahoudek, F., Duret-Lutz, A., Klein, J., Kˇret´ınsk´y, J., M¨uller, D.,
Parker, D., Strejˇcek, J.: The Hanoi omega-automata format. In: Computer Aided
Veriﬁcation (CAV). pp. 479–486 (2015), LNCS 9206

3. Bozkurt, A.K., Wang, Y., Zavlanos, M.M., Pajic, M.: Control synthesis from lin-
ear temporal logic speciﬁcations using model-free reinforcement learning. CoRR
abs/1909.07299 (2019), http://arxiv.org/abs/1909.07299

4. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,

Zaremba, W.: OpenAI Gym. CoRR abs/1606.01540 (2016)

5. Condon, A.: The complexity of stochastic games. Inf. Comput. 96(2), 203–224

(1992)

6. Duret-Lutz, A., Lewkowicz, A., Fauchille, A., Michaud, T., Renault, E., Xu, L.:
Spot 2.0 — a framework for LTL and ω-automata manipulation. In: Proceedings of
the 14th International Symposium on Automated Technology for Veriﬁcation and
Analysis (ATVA’16). Lecture Notes in Computer Science, vol. 9938, pp. 122–129.
Springer (Oct 2016)

7. Ellson, J., Gansner, E.R., Koutsoﬁos, E., North, S.C., Woodhull, G.: Graphviz and
dynagraph - static and dynamic graph drawing tools. In: J¨unger, M., Mutzel, P.
(eds.) Graph Drawing Software, pp. 127–148. Springer (2004)

8. Etessami, K., Wilke, T., Schuller, A.: Fair simulation relations, parity games,
and state space reduction for B¨uchi automata. In: Orejas, F., Spirakis, P.G., van
Leeuwen, J. (eds.) Automata, Languages and Programming: 28th International
Colloquium. pp. 694–707. Springer, Crete, Greece (Jul 2001), lNCS 2076

9. Hahn, E.M., Li, G., Schewe, S., Turrini, A., Zhang, L.: Lazy probabilistic model
checking without determinisation. In: Concurrency Theory, (CONCUR). pp. 354–
367 (2015)

10. Hahn, E.M., Perez, M., Schewe, S., Somenzi, F., Trivedi, A., Wojtczak, D.: Omega-
regular objectives in model-free reinforcement learning. In: Tools and Algorithms
for the Construction and Analysis of Systems. pp. 395–412 (2019), LNCS 11427

11. Hahn, E.M., Perez, M., Schewe, S., Somenzi, F., Trivedi, A., Wojtczak, D.: Faith-
ful and eﬀective reward schemes for model-free reinforcement learning of omega-
regular objectives. In: ATVA: Automated Technology for Veriﬁcation and Analysis.
pp. 108–124 (2020), LNCS 12302

12. Hahn, E.M., Perez, M., Schewe, S., Somenzi, F., Trivedi, A., Wojtczak, D.: Good-
for-MDPs automata for probabilistic analysis and reinforcement learning. In: Tools
and Algorithms for the Construction and Analysis of Systems. pp. 306–323 (2020),
LNCS 12078

13. Hahn, E.M., Perez, M., Schewe, S., Somenzi, F., Trivedi, A., Wojtczak, D.: Model-
free reinforcement learning for stochastic parity games. In: CONCUR: International
Conference on Concurrency Theory. pp. 21:1–21:16 (Sep 2020), LIPIcs 171

14. Hahn, E.M., Schewe, S., Turrini, A., Zhang, L.: A simple algorithm for solving
qualitative probabilistic parity games. In: Computer Aided Veriﬁcation. pp. 291–
311. Part II (2016), LNCS 9780

15. Hahn, E.M., Schewe, S., Turrini, A., Zhang, L.: Synthesising strategy improvement
and recursive algorithms for solving 2.5 player parity games. In: Veriﬁcation, Model
Checking, and Abstract Interpretation. pp. 266–287 (2017)

12

E. M. Hahn, M. Perez, S. Schewe, F. Somenzi, A. Trivedi, and D. Wojtczak

16. Hahn, E., Li, Y., Schewe, S., Turrini, A., Zhang, L.: iscasMc: A web-based proba-
bilistic model checker. In: International Symposium on Formal Methods. pp. 312–
317 (May 2014)

17. van Hasselt, H.: Double Q-learning. In: Advances in Neural Information Processing

Systems. pp. 2613–2621 (2010)

18. Henzinger, T.A., Piterman, N.: Solving games without determinization. In: 15th
Conference on Computer Science Logic. pp. 394–409. Szeged, Hungary (Sep 2006),
LNCS 4207

19. Irpan, A.: Deep reinforcement learning doesn’t work yet. https://www.alexirpan.

com/2018/02/14/rl-hard.html (2018)

20. Kˇret´ınsk´y, J., Meggendorfer, T., Sickert, S.: Owl: A library for ω-words, automata,
and LTL. In: Automated Technology for Veriﬁcation and Analysis, ATVA. pp. 543–
550 (2018), LNCS 11138

21. Kwiatkowska, M., Norman, G., Parker, D.: PRISM 4.0: Veriﬁcation of probabilistic
real-time systems. In: Computer Aided Veriﬁcation (CAV). pp. 585–591 (Jul 2011),
LNCS 6806

22. Manna, Z., Pnueli, A.: The Temporal Logic of Reactive and Concurrent Systems

*Speciﬁcation*. Springer (1991)

23. McNaughton, R.: Testing and generating inﬁnite sequences by a ﬁnite automaton.

Inf. Control. 9(5), 521–530 (1966)

24. Mnih, V., Kavukcuoglu, K., Silver, D., et al.: Human-level control through deep

reinforcement learning. Nature 518 (2015)

25. OpenAI, Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B.,
Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R., Schneider, J., Tezak,
N., Tworek, J., Welinder, P., Weng, L., Yuan, Q., Zaremba, W., Zhang, L.: Solving
rubik’s cube with a robot hand. arXiv preprint (2019)

26. Perrin, D., Pin, J.´E.: Inﬁnite Words: Automata, Semigroups, Logic and Games.

Elsevier (2004)

27. Pnueli, A.: The temporal semantics of concurrent programs. Theoret. Comput.

Science 13, 45–60 (1981)

28. Puterman, M.L.: Markov Decision Processes: Discrete Stochastic Dynamic Pro-

gramming. John Wiley & Sons, Inc., New York, NY, USA (1994)

29. Sadigh, D., Kim, E., Coogan, S., Sastry, S.S., Seshia, S.A.: A learning based ap-
proach to control synthesis of Markov decision processes for linear temporal logic
speciﬁcations. In: IEEE Conference on Decision and Control (CDC). pp. 1091–1096
(Dec 2014)

30. Sickert, S., Esparza, J., Jaax, S., Kˇret´ınsk´y, J.: Limit-deterministic B¨uchi automata
for linear temporal logic. In: Computer Aided Veriﬁcation (CAV). pp. 312–332
(2016), LNCS 9780

31. Silver, D., et al.: Mastering the game of Go with deep neural networks and tree

search. Nature 529, 484–489 (Jan 2016)

32. Sutton, R.S.: Learning to predict by the method of temporal diﬀerences. Machine

Learning 3, 9–44 (1998)

33. Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction. MIT Press,

second edn. (2018)

34. Watkins, C.J.C.H., Dayan, P.: Q-learning. In: Machine Learning. pp. 279–292

(1992)

35. Wiewiora, E.: Reward shaping. In: Encyclopedia of Machine Learning, pp. 863–865.

Springer (2010)

