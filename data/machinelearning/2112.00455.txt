1
2
0
2

c
e
D
1

]
h
p
-
t
n
a
u
q
[

1
v
5
5
4
0
0
.
2
1
1
2
:
v
i
X
r
a

Einstein-Podolsky-Rosen steering based on semi-supervised machine learning

Lifeng Zhang, Zhihua Chen∗
School of Science, Jimei University, Xiamen 361021,China

Shao-Ming Fei†
School of Mathematical Sciences, Capital Normal University, Beijing 100048, China,
Max Planck Institute for Mathematics in the Sciences, 04103 Leipzig, Germany

Einstein-Podolsky-Rosen(EPR)steering is a kind of powerful nonlocal quantum resource in quan-
tum information processing such as quantum cryptography and quantum communication. Many
criteria have been proposed in the past few years to detect the steerability both analytically and
numerically. Supervised machine learning such as support vector machines and neural networks
have also been trained to detect the EPR steerability. To implement supervised machine learning,
one needs a lot of labeled quantum states by using the semideﬁnite programming, which is very
time consuming. We present a semi-supervised support vector machine method which only uses a
small portion of labeled quantum states in detecting quantum steering. We show that our approach
can signiﬁcantly improve the accuracies by detailed examples.

I.

INTRODUCTION

Einstein-Podolsky-Rosen (EPR) steering is an inter-
mediate quantum correlation between quantum entan-
glement and quantum nonlocality [1], wherein one party
can steer the state of another distant party by making
measurements on one party of a shared bipartite state.
EPR-steering was ﬁrst observed by Schr¨odinger in the
famous EPR paradox in 1935 [2, 3]. However, the rig-
orous deﬁnition of EPR-steering was not proposed till
2007. EPR-steering is proven to be asymmetric in gen-
eral, that is, one party can steer the another but not vice
versa [4–6]. It has been shown that steering has many
applications in quantum information processing such as
one-sided device independent quantum key distribution
[7], randomness certiﬁcation [8, 9], subchannel discrim-
ination [10, 11], secret sharing [12], quantum teleporta-
tion [13], coupled qubits and magnetoreception [14], no-
cloning of quantum steering [15] and quantum networks
[16].

Many methods have been proposed to detect EPR-
steering, such as linear and nonlinear steering inequalities
[17–22], uncertainty relations [23–25], moment matrix
approach [26] and all-versus-nothing method [5]. Many
steering measures were also proposed, which can be es-
timated based on semideﬁnite programming [27]. Nev-
ertheless, it is generally very time consuming to detect
steerability of a state numerically, as a huge amount of
measurement directions has to be run over. Eﬃcient cri-
teria of steerability are still far from being satisﬁed.

Machine learning is a useful tool

for classiﬁcation
problems, which has already successful applications in
quantum information processing tasks such as entangle-
ment classiﬁcation, nonlocality discriminant, Hamilto-

∗Electronic address: chenzhihua77@sina.com
†Electronic address: feishm@cnu.edu.cn

nian learning and phase transition identiﬁcation [28–33].
Machine learning methods such as support vector ma-
chine (SVM), artiﬁcial neural networks and decision trees
have been also applied to the EPR-steering detection and
quantiﬁcation [34, 35]. However, these methods are su-
pervised machine learning ones, in which a lot of labeled
results are needed. While quantum state labeling is also
a very time consuming task.

Semi-supervised machine learning was proposed to
combine labeled and unlabeled data to improve the
learning behavior [36].
Semi-supervised support vec-
tor machine combines support vector machine and semi-
supervised learning, which can make use of the scarce la-
beled data and suﬃcient unlabeled data to improve the
classiﬁcation performance [37]. The safe semi-supervised
SVM (S4VM) was proposed to exploit multiple candi-
date low-density separators and to select the representa-
tive separators, which is never signiﬁcantly inferior to the
inductive SVM [38]. Semi-supervised SVMs have been
successfully applied to text and image classiﬁcation, face
recognition and many other areas [39–42].

In this paper, we apply semi-supervised SVM-S4VM
to deal with quantum steering problems. We need only a
small portion of labeled states generated randomly by the
semideﬁnite programming, together with a large portion
of unlabeled quantum states used in S4VM. Compared
with the inductive SVM, the accuracies can be signiﬁ-
cantly improved. Moreover, less time is needed to label
the quantum states, while high accuracy is still attained.

II. SUPERVISED MACHINE LEARNING OF
EPR-STEERING

A two-qubit quantum state, ρAB = 1

4 (I4 +

3
P
i=1

riσi ⊗

I2 +

3
P
i=1

siI2 ⊗ σi +

3
P
i,j=1

tijσi ⊗ σj), where Id is the d × d

identity matrix and σi (i = 1, 2, 3) are the standard Pauli

 
 
 
 
 
 
matrices, is said to admit a local hidden state (LHS)
model if the probability P (a, b|A, B) that Alice and Bob
performs the measurements A and B with measurement
outcomes a and b, respectively, satisfy

P (a, b|A, B) =

tr[(Ma

A ⊗ Mb
= P p(λ)p(a|A, λ)pQ(b|B, λ)

B).ρAB]

(1)

where PQ(b|B, λ) = tr[ρλMb
B], ρλ are qubit states spec-
iﬁed by the parameter λ. State ρAB is said to be not
steerable if it satisﬁes relation (1). Otherwise we say
that ρAB is steerable from Alice to Bob. Equivalently,
one may say that Alice can not steer Bob if there exist
probability pλ and a set of quantum states ρλ such that

ρa
A = trA((Ma

A ⊗ I2).ρAB) = X pλp(a|A, λ)ρλ,

(2)

where p(a|A, λ) is a probability distribution given by a
and λ.

For any ρAB if Alice performs m measurements A =
{0, 1, 2, · · · , m − 1} with q outcomes a ∈ {1, 2, · · · , q}, an
assemblage {ρa
A}A,a of m ensembles is obtained. Denote
D(a|A, λ) = δa,λ(A), that is, D(a|A, λ) = 1 if λ(A) = a
and D(a|A, λ) = 0 if λ(A) 6= a. The following SDP
algorithm can be used to detect if Alice can steer Bob
for any given ρa

A and D(a|A, λ) [27],

Fa|Aρa
A

tr P
a,A
Fa|AD(a|A, λ) ≥ 0,

min
Fa|A
such that. P
a,A
Fa|AD(a|A, λ)) = 1.

tr( P
a,A,λ

(3)

If the objective value of (3) is negative for some measure-
ments A, ρAB is steerable from Alice to Bob. Otherwise,
there exists an LHS model.

A state ρAB is labeled −1 if the objective value is
negative after using the SDP program 100 times with
diﬀerent values of measurements. Otherwise, the state
is labeled +1. In [34] a SVM is used for the classiﬁer.
For m = 2, 3, · · · , 8, 5000 samples with label +1 and
5000 samples with −1 were obtained. The last 1000 pos-
itive samples and 1000 negative samples were reserved
for tests, the remaining 4000 positive and 4000 negative
samples were kept as training set to learn a classiﬁer.

SVM is a supervised learning model used for clas-
siﬁcation and regression analysis. Given (xi, yi), i =
1, 2, · · · , n, where n is the number of samples, xi (i =
1, 2, · · · , n) are the feature vectors of the samples and yi
(i = 1, 2, · · · , n) are the labels of the samples, yi = 1 or
yi = −1. For linearly separable problems, SVM aims to
ﬁnd a hyperplane with the largest distance to the near-
est feature vectors of each class. The hyperplanes H are
deﬁned as ωxi + b ≥ 1 when yi = 1, and ωxi + b ≤ −1
when yi = −1. Two marginal hyperplanes are deﬁned by
H1 : ωxi + b = 1 and H2 : ωxi + b = −1. The vectors
on H1 and H2 are the support vectors. The distance 2
kωk
between H1 and H2 needs to be maximized. The ﬁnal

2

separating hyperlane H is selected to be the middle one
between the two marginal hyperplanes.

Hence, the problems to be solved are given as follows,

min
ω

1

2 ωT ω

(4)

such that yi(ωxi + b) ≥ 1.

When the errors {ξi}n
soft marginal hyperplane problem is attained,

i=1 in classiﬁcation are allowed, the

min
ω,b,ξ

1

2 ωT ω + C

n
P
i=1

ξi

such that yi(ωxi + b) ≥ 1 − ξi,

ξi ≥ 0,

(5)

where ξi are slack variables, ξi = 0 if there is no error for
xi, C is a tradeoﬀ parameter between the error and the
margin. For non-linearly separable problems, by using
the kernel trick the problem needing to be solved becomes

min
ω,b,ξ

1

2 ωT ω + C

n
P
i=1

ξi

(6)

such that yi(ωφ(xi) + b) ≥ 1 − ξi,

ξi ≥ 0,

where φ(x) is the kernel function [43, 44].

Concerning the steering detection problem, the train-
ing set {xi, yi} (i = 1, 2, · · · , 8000) and the radial basis
function K(φ(xi), φ(xj)) = e−γkxi−xj k2
are used with
the parameters C and γ determined by a grid search ap-
proach. The classiﬁer is attained by solving (6) [34].

III. SEMI-SUPERVISED MACHINE LEARNING
OF EPR-STEERING

A well-trained SVM needs a lot of labeled samples.
For steering detection it takes much time to generate the
labeled samples and to obtain the data set with the in-
crease of measurements [34]. Semi-supervised SVM can
be used in classifying problems with a small portion of
labeled samples and a large number of unlabeled samples.
The S3VM was ﬁrst proposed to simultaneously learn
the optimal hyperplane and the labels for unlabeled in-
stances [45, 46]. Given a set of l labeled data {xi, yi}l
and a set of u unlabeled data {ˆxj}l+u
one needs to solve the following optimization problem:

i=1
j=l+1, yi ∈ {1, −1},

min
ω,b,ˆy∈B

( 1
2 kωk2 + C1

l
P
i=1

ξi + C2

l+u
P
j=l+1

ˆξj)

(7)

such that yi(ω′φ(xi) + b) ≥ 1 − ξi, ξi ≥ 0
ˆyj(ω′φ(ˆxj) + b) ≥ 1 − ˆξj, ˆξj ≥ 0
∀i = 1, · · · , l, ∀j = l + 1, · · · , l + u,

where B = {ˆy ∈ {±1}u| − β ≤
the balanced constraint to avoid the trivial solution.

−

ˆyj

l+u
P
j=l+1
u

l
P
i=1

yi
l ≤ β} is

Unlike S3VM, the S4VM was proposed to construct di-
verse large-margin separators since multiple large-margin
low density separators may coincide with the limited la-
beled data, and then the label assignment for unlabeled
instances is optimized such that the worst-case perfor-
mance improvement over inductive SVM is maximized
[38]. Recalling the S4VM in detail, one ﬁrst has the fol-
lowing minimization problem:

min
{ωt,bt,ˆyt∈B}T

t=1

T
P
t=1

( 1
2 kωtk2 + C1

l
P
i=1

ξi + C2

l+u
P
j=l+1

ˆξj) (8)

+G P

1≤t6=˜t≤T

δ( ˆy′

t ˆy˜t
u ≥ 1 − ς),

such that yi(ω′
ˆyt,j(ω′

tφ(xi) + bt) ≥ 1 − ξi, ξi ≥ 0

tφ(ˆxj) + bt) ≥ 1 − ˆξj , ˆξj ≥ 0

∀i = 1, · · · , l, ∀j = l + 1, · · · , l + u, ∀t = 1, · · · , T.

l+u
P
j=l+1
u

l
P
i=1

ˆyt,j

−

yi
where B = {ˆyt ∈ {±1}u| − β ≤
l ≤ β},
ˆyt,j is the jth entry of ˆyt, δ is the indicator function
which represents a quantity of penalty about the diver-
sity of separators and ς ∈ [0, 1] is a constant, G is a large
constant enforcing large diversity. C1 and C2 are regu-
larization parameters trading oﬀ the complexity and the
empirical error on label and unlabeled data. T is the
number of the separators.

Second, from the global simulated annealing search
with a deterministic local search scheme method or the
sampling strategy used in solving minimization problems
Eq.(8), we have the following problem,

¯y = arg max

y∈{±1}u

min
ˆy∈M0

J(y, ˆy, ySVM),

(9)

where ySVM is the predictive labels of inductive SVM on
unlabeled instances, M0 = {ˆyt}T
t=1 is obtained from (8),

J(y, ˆy, ySVM) = gain(y, ˆy, ySVM) − λloss(y, ˆy, ySVM)

=

c′
ty + dt

(10)

with

gain(y, ˆy, ySVM) =

l+u
P
j=l+1

1+yjˆyj
2

1−ySVM
j
2

ˆyj

,

(11)

loss(y, ˆy, ySVM) =

1−yjˆyj
2

1+ySVM
j
2

l+u
P
j=l+1
4 [(1 + λ)ˆyt + (λ − 1)ySVM],
4 [−(1 + λ)ˆy′

tySVM + (1 − λ)u].

ˆyj

,

ct = 1
dt = 1

Then the minimization problem in (9) can be reformu-
lated as the following maximization problem,

max
y,τ
ty + dt, ∀t = 1, 2, · · · , T ; y ∈ {±1}u.
such that τ ≤ c′

τ

(12)

Convex linear programming problems can be solved by
relaxing y ∈ {±1}u to y ∈ [−1, 1]u, and then project it

3

back to integer solutions with minimum distance. If the
function value of the resulting integer solution is smaller
than that of ySVM, ySVM is output as the ﬁnal solution
instead.

1
2

1
2

For any two-qubit quantum state ρAB, set ρ0 = (I2 ⊗
B), where ρB = trAρAB. The coeﬃ-
B)ρAB(I2 ⊗ ρ
ρ
cients τkl = tr(ρ0(σk ⊗ σl)) can be reformulate as a
nine-dimensional feature vector x = [τ11, τ12, · · · , τ33]T ,
where T denotes transpose. Generating randomly quan-
tum states by SDP [27, 34], we get a set of l feature vec-
tors of labeled quantum states, {xi}, i = 1, 2, · · · , l, with
yi = −1 for steerable states and yi = 1 for unsteerable
states. Denote {ˆxj}l+u
j=l+1 the set of feature vectors of un-
labeled quantum states whose labels can be determined
by the above S4VM [38] method. Radial basis function
kernels are used here and K(φ(xi), φ(xj)) = e−γkxi−xj k2
.
We have three hyperparameters C1, C2, and γ which are
also determined by the grid research method.

IV. NUMERICAL RESULTS

We generate randomly quantum states and get the
class of steerable states from such random states by SDP.
For balance, we choose half positive quantum states and
half negative random states.

We implement SVMs by using l

labeled quantum
states, the radial basis function kernel, ten-folded cross
validation, and the grid search approach. u unlabeled
quantum states are considered as the test sets and the
test errors are obtained. For S4VM, sample strategy is
implemented here, three hyperparameters C1, C2, and
γ are determined by the grid search approach. We set
β = 0.1, λ = 3, the number of clusters T is 10 and the
number of samples is 100 in the sample strategy for the
examples. u unlabeled quantum states are divided into
M diﬀerent sets. Taking M = 2 as an example, we have
the following steps:

(1) The ﬁrst set of unlabeled quantum states can be
predicted by using l labeled quantum states. Then,
we implement ﬁve-fold cross validation for these l +
u/M quantum states by using SVMs. The best cross-
validation accuracy and the best hyperparameters are
then obtained. The labels of the ﬁrst set of unlabeled
quantum states from the best hyperparameters can be
regarded as the real labels and the classiﬁcation accura-
cies of these unlabeled quantum states are obtained.

(2) After labeling the ﬁrst set of unlabeled quantum
states, l+u/M labeled quantum states and the second set
of unlabeled quantum states can be utilized to implement
S4VM the second time. Then the second set of unla-
beled quantum states can also be labeled. Implementing
ﬁve-fold cross validation for these l + 2 ∗ u/M, quantum
states by using SVMs, the best cross-validation accuracy
and the best hyperparameters are obtained. The labels
of the second unlabeled quantum states from the best hy-
perparameters are considered as the real labels and the

0.15

0.1

0.05

1

2

M=1
M=2
M=4
M=8

8

9

10

4

3
7
5
Different Labeled Sets

6

FIG. 1. Classiﬁcation errors of the unlabeled quantum
states by using ten diﬀerent sets of l labeled quantum
states for m = 2, l = 30, and u = 4000. The errors of
the unlabeled states by S4VM for M = 1, 2, 4, 8 are
represented by solid line with star, thick dashed line
with x, dashed line with square and solid line with +,
respectively.

0.08

0.07

r
o
r
r

E

0.06

0.05

0.04

M=1
M=2

2

4

6

8

10

Different Labeled Sets

FIG. 2. Classiﬁcation errors of the unlabeled quantum
states by using ten diﬀerent sets of 30 labeled quantum
states for m = 8 and u = 4000. The errors of the
unlabeled states by S4VM for M = 1 and 2 are
represented by solid line with star and thick dashed line
with x, respectively.

classiﬁcation accuracies of the second set of the unlabeled
quantum states are obtained.

(3) The average accuracy of the two sets of unlabeled
quantum states is considered as the classiﬁcation accu-
racy of the unlabeled set.

We compute the errors of 4000 unlabeled quantum
states when 30 labeled quantum states are used to im-
plement S4VM for m = 2 and M = 1, 2, 4, 8. Except for
the third labeled set, the errors are smaller when M = 1
and M = 2 compared to the errors when M = 4 and
M = 8 for the most cases but the errors when M = 2 are
smaller than that when M = 1 for m = 8, see Figs. 1
and 2. In addition, the computational speed for M = 2
is three times faster than that for M = 1.

Based on the above errors and computational speed,
in the following numerical experiments we set M = 2
and use 10, 30, or 50 labeled states and 4000 unlabeled
quantum states to implement S4VM for m = 2, 4, 6, 8.

4

svm
s4vm

1 2 3 4 5 6 7 8 9 10

svm
s4vm

1 2 3 4 5 6 7 8 9 10

r
o
r
r

E

0.4

0.3

0.2

0.1

0

Different Labeled sets for m=2

Different Labeled sets for m=4

svm
s4vm

1 2 3 4 5 6 7 8 9 10

0.4

r
o
r
r

E

0.3

0.2

0.1

0

svm
s4vm

1 2 3 4 5 6 7 8 9 10

Different Labeled sets for m=6

Different Labeled sets for m=8

0.4

r
o
r
r

E

0.3

0.2

0.1

0

0.4

r
o
r
r

E

0.3

0.2

0.1

0

FIG. 3. Classiﬁcation errors of SVM and S4VM by
using ten diﬀerent sets of l labeled quantum states for
m = 2, 4, 6, and 8, l = 10, u = 4000 and M = 2. The
accuracies of the unlabeled states by SVM and S4VM
are represented by the lines with squares and ⋆,
respectively.

0.4

r
o
r
r

E

0.3

0.2

0.1

0

r
o
r
r

E

0.4

0.3

0.2

0.1

0

svm
s4vm

1 2 3 4 5 6 7 8 9 10

r
o
r
r

E

0.4

0.3

0.2

0.1

0

svm
s4vm

1 2 3 4 5 6 7 8 9 10

Different Labeled sets for m=2

Different Labeled sets for m=4

svm
s4vm

1 2 3 4 5 6 7 8 9 10

0.4

r
o
r
r

E

0.3

0.2

0.1

0

svm
s4vm

1 2 3 4 5 6 7 8 9 10

Different Labeled sets for m=6

Different Labeled sets for m=8

FIG. 4. Classiﬁcation errors of SVM and S4VM by
using ten diﬀerent sets of l labeled quantum states for
m = 2, 4, 6, and 8, l = 30, u = 4000 and M = 2. The
accuracies of the unlabeled states by SVM and S4VM
are represented by the lines with squares and ⋆,
respectively,

When Alice performs m (m = 2, 4, 6, 8) measurements,
S4VM is implemented ten times by using ten diﬀerent
sets of l labeled quantum states. The errors for these u
unlabeled quantum states are shown in the Figs. 3-5.

When two measurements are performed and ten la-
beled quantum states are used, the errors by SVM are
larger than 0.2 in many cases, which can be reduced to
about 0.05 by S4VM in most cases. When l = 30, the
errors by S4VM are less than 0.06, while the errors by
SVM are larger than 0.09 in most cases. When l = 50,
the errors by S4VM are less than 0.05, while the errors by

5

r
o
r
r

E

r
o
r
r

E

0.4

0.3

0.2

0.1

0

0.4

0.3

0.2

0.1

0

svm
s4vm

1 2 3 4 5 6 7 8 9 10

r
o
r
r

E

0.4

0.3

0.2

0.1

0

svm
s4vm

1 2 3 4 5 6 7 8 9 10

r
o
r
r

E

0.4

0.3

0.2

0.1

0

r
o
r
r

E

0.4

0.3

0.2

0.1

0

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

Different Labeled sets for m=2

Different Labeled sets for m=4

Different Labeled Sets for l=10 and m=2

Different Labeled Sets for l=10 and m=4

m=6

svm
s4vm

1 2 3 4 5 6 7 8 9 10

r
o
r
r

E

0.4

0.3

0.2

0.1

0

svm
s4vm

1 2 3 4 5 6 7 8 9 10

0.4

r
o
r
r

E

0.3

0.2

0.1

0

0.4

r
o
r
r

E

0.3

0.2

0.1

0

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

Different Labeled sets for m=6

Different Labeled sets for m=8

Different Labeled Sets for l=10 and m=6

Different Labeled Sets for l=10 and m=8

FIG. 5. Classiﬁcation errors of SVM and S4VM by
using ten diﬀerent sets of l labeled quantum states for
m = 2, 4, 6, and 8, l = 50, u = 4000 and M = 2. The
accuracies of the unlabeled states by SVM and S4VM
are represented by lines with squares and ⋆, respectively.

TABLE I. The maximum diﬀerences between the errors
from SVM and S4VM by using ten diﬀerent l labeled
quantum states and 4000 unlabeled quantum states for
l = 10, 30, and 50 and m = 2, 4, 6, and 8. ∆Emax,l is the
maximum diﬀerence, the errors from SVM minus the
errors from S4VM, by using l labeled quantum states.

m ∆Emax,10 ∆Emax,30 ∆Emax,50
0.129
2 0.216
0.088
4 0.22
0.12
6 0.36
0.028
8 0.104

0.055
0.159
0.13
0.01

SVM are larger than 0.07 in most cases. In Table I, we list
the maximum diﬀerences between errors from SVM and
S4VM by using 10, 30, and 50 labeled quantum states
and 4000 unlabeled quantum states for m = 2, 4, 6, and
8. It can be seen that the accuracies are signiﬁcantly im-
proved. To investigate the performance on positive and
negative quantum states by S4VM, we show the errors of
positive class and negative class for l = 10, 30 and 50 in
FIGs. 6, 7 and 8.

The errors for positive and negative classes are also
reduced in most cases, i.e. the errors for either the pos-
itive or negative class by SVM are also larger than the
corresponding ones by S4VM for l = 10, 30, and 50, re-
spectively. In general the errors for the negative class are
smaller than those for the positive class, maybe due to
the errors of SDP. When m = 2 and m = 8 the errors
by S4VM for the negative class are approximately zero
in most cases.

In Table II (Table III), we also list the maximum dif-
ferences between errors of positive (negative) states from
S4VM and SVM by using 10, 30, or 50 labeled quantum
states and 4000 unlabeled quantum states for m = 2, 4, 6,

FIG. 6. Classiﬁcation errors of 2000 positive quantum
states and 2000 negative quantum states by SVM and
S4VM with ten diﬀerent labeled sets for l = 10,
m = 2, 4, 6, and 8. The errors for positive and negative
quantum states by S4VM are represented by lines with
squares and ⋆, respectively, while the errors for positive
and negative quantum states by SVM are represented
by lines with diamond and +, respectively.

r
o
r
r

E

0.3

0.2

0.1

0

r
o
r
r

E

0.3

0.2

0.1

0

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

Different Labeled Sets for l=30 and m=2

Different Labeled Sets for l=30 and m=4

0.3

r
o
r
r

E

0.2

0.1

0

0.3

r
o
r
r

E

0.2

0.1

0

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

Different Labeled Sets for l=30 and m=6

Different Labeled Sets for l=30 and m=8

FIG. 7. Classiﬁcation errors of 2000 positive quantum
states and 2000 negative quantum states by SVM and
S4VM with ten diﬀerent labeled sets for l = 30,
m = 2, 4, 6, and 8. The errors for positive and negative
quantum states are represented by lines with squares
and ⋆, respectively, while the errors for positive and
negative quantum states by SVM are represented by
lines with diamond and +, respectively.

and 8.

results

from SVMs,

Compared with the

semi-
supervised SVMs can improve the accuracies in most
cases. The diﬀerences between average errors from S4VM
and SVM by using ten diﬀerent sets of labeled quantum
states are shown in Fig. 9. All the average errors can
be improved. The smaller the l (the number of labeled

r
o
r
r

E

0.3

0.2

0.1

0

r
o
r
r

E

0.3

0.2

0.1

0

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

Different Labeled Sets for l=50 and m=2

Different Labeled Sets for l=50 and m=4

0.3

r
o
r
r

E

0.2

0.1

0

0.3

r
o
r
r

E

0.2

0.1

0

1 2 3 4 5 6 7 8 9 10

1 2 3 4 5 6 7 8 9 10

Different Labeled Sets for l=50 and m=6

Different Labeled Sets for l=50 and m=8

FIG. 8. Classiﬁcation errors of 2000 positive quantum
states and 2000 negative quantum states by SVM and
S4VM with ten diﬀerent labeled sets for l = 50,
m = 2, 4, 6, and 8. The errors for positive and negative
quantum states are represented by lines with squares
and ⋆, respectively, while the errors for positive and
negative quantum states by SVM are represented by
lines with diamond and +, respectively.

TABLE II. The maximum diﬀerences between the
errors of positive states from SVM and S4VM by using
ten diﬀerent l labeled quantum states and 4000
unlabeled quantum states for l = 10, 30, and 50 and

m = 2, 4, 6, and 8. ∆E+

max,l is the maximum diﬀerence,

errors of positive states from SVM minus the errors
from S4VM, by using l labeled quantum states.

max,50

m ∆E+
2 0.337
4 0.24
6 0.43
8 0.04

max,10 ∆E+
0.14
0.13
0.19
0.007

max,30 ∆E+
0.11
0.11
0.11
0.004

TABLE III. The maximum diﬀerences between the
errors of negative states from SVM and S4VM by using
ten diﬀerent l labeled quantum states and 4000
unlabeled quantum states for l = 10, 30, and 50 and

m = 2, 4, 6, and 8. ∆E−
max,l is the maximum diﬀerence,
the errors of negative states from SVM minus the errors
from S4VM, by using l labeled quantum states.

max,50

m ∆E−
2 0.34
4 0.47
6 0.15
8 0.029

max,10 ∆E−
0.10
0.12
0.10
0.032

max,30 ∆E−
0.03
0.07
0.07
0.031

M
V
S
d
n
a

M
V
4
S
y
b

s
e
i
c
a
r
u
c
c
a

e
g
a
r
e
v
a

n
e
e
w
t
e
b

s
e
c
n
e
r
e
f
f
i
d

e
h
T

0.14

0.12

0.1

0.08

0.06

0.04

0.02

0

-0.02

2

6

l=50
l=30
l=10

4

6

8

m

FIG. 9. The diﬀerences between the average errors from
S4VM and SVM by using ten diﬀerent l labeled
quantum states and u unlabeled quantum states for
m = 2, 4, 6, and 8. The diﬀerences between the average
errors for l = 50, 30, and 10 are represented by the lines
with ⋄, ⋆ and circles, respectively.

1

y
c
a
r
u
c
c
A

0.9

0.8

0.7

0.9

1

y
c
a
r
u
c
c
A

0.9

0.8

0.7

0.9

1

y
c
a
r
u
c
c
A

0.9

0.8

0.95
Crossrate for m=2

1

0.7

0.9

0.95
Crossrate for m=4

1

1

y
c
a
r
u
c
c
A

0.9

0.8

0.95
Crossrate for m=6

1

0.7

0.9

0.95
Crossrate for m=8

1

FIG. 10. The relationship between the cross validation
accuracies and the accuracies of unlabeled quantum
states by using ten labeled quantum states and u
unlabeled quantum states for m = 2, 4, 6, and 8.

samples), the better the improved accuracies.

To show the validity of cross validation, the relation-
ship between the cross rate and the accuracies are shown
in FIG. 10 by taking 10 labeled quantum states. The
vertical coordinates represent the accuracies of the unla-
beled states, while the horizontal coordinates represent
the the accuracies of cross validation.

To investigate the validity of the S4VM, we also study

the classiﬁcation of the generalized Werner states,

ρw = p|ψihψ| + (1 − p)ρA ⊗

I2
2

,

(13)

where |ψi = cos ξ|00i + sin ξ|11i, ρA = trB(|ψihψ|). The

 
 
 
 
 
 
 
 
0.2

0.15

r
o
r
r

E

0.1

0.05

0

1

2

3

4

5

6

7

8

9

10

Different Labeled Sets

0.2

0.15

r
o
r
r

E

0.1

0.05

0

1

2

3

4

5

6

7

8

9

10

Different Labeled Sets

7

S4VM may still cost much time since the global simu-
lated annealing search, the sample strategy, and the grid
search approach in S4VM cost time. Besides, the accu-
racies by S4VM cannot be improved signiﬁcantly when
the number of the labeled samples becomes larger, while
the accuracies by SVM are greater than 0.95.

(a)m=8

(b)m=4

V. CONCLUSION

FIG. 11. Classiﬁcation errors of SVM and S4VM with
ten diﬀerent sets of l labeled quantum states for m = 4,
and m = 8, l = 30, N2 = 2500, and M = 1. The errors
of SVM for l = 30 are represented by line with ⋆, the
errors of S4VM for l = 30 are represented by line with
square.

state is unsteerable from Alice to Bob if

cos2 2ξ ≥

2p − 1
(2 − p)p3 .

(14)

Let the unlabeled set be of 2500 generalized Werner
states from p = 0 to p = 1. By using 30 random labeled
quantum states, the classiﬁcation errors of the unlabeled
generalized Werner states for ψ = π
8 by SVM and S4VM
are shown in Fig. 11. Diﬀerent from the above examples,
we take M = 1 here to show the performance of S4VM. It
can be seen that the accuracies of the unlabeled quantum
states can be improved to 0.95 or higher. The accuracies
by S4VM are 0.15 higher than those by SVM in the best
case when m = 4 and m = 8. The average accuracies by
S4VM are 0.969 and 0.979 for m = 4 and m = 8, respec-
tively, which are again 0.076 and 0.036 higher than those
by SVM.

We only need a small portion of the labeled data to im-
plement the semi-supervised learning algorithm, which
can indeed save a lot of time. However, the algorithm

Semi-supervised SVMs can be used in the situation
that the labeled samples are scarce or very diﬃcult to
obtain. We have implemented a special semi-supervised
SVM-S4VM to detect the EPR steering with a small
portion of labeled quantum states and a large portion
of unlabeled quantum states. Compared with inductive
SVM, the detection errors can be signiﬁcantly decreased
in most cases. Our approach makes a useful step to
solve the quantum correlation detection problems based
on the semi-supervised machine learning method, as it
costs time to get enough labeled quantum states. This
approach may be applied similarly to detect other quan-
tum correlations such as quantum entanglement and bell
non-locality. Our results may also highlight the appli-
cations of other more eﬃcient semi-supervised machine-
learning methods such as artiﬁcial neural networks to
quantum correlation detections.

ACKNOWLEDGEMENTS This work is supported
by the National Natural Science Foundation of China
(NSFC) under Grant Nos. 11571313, 12071179, 12075159
and 12171044; Beijing Natural Science Foundation
(Grant No. Z190005); Academy for Multidisciplinary
Studies, Capital Normal University; the Academician
Innovation Platform of Hainan Province; and Shen-
zhen Institute for Quantum Science and Engineering,
Southern University of Science and Technology (No.
SIQSE202001).

[1] H. M. Wiseman, S.J. Jones , A. C. Doherty, Steering,
entanglement, nonlocality, and the Einstein-Podolsky-
Rosen paradox. Phys. Rev. Lett., 98:140402(2007).

[2] E. Schrodinger, Discussion of probability relations be-
tween separated systems. Math. Proc. Camb. Philos. Soc.
31:555(1935).

[3] A. Einstein, B. Podolsky, N. Rosen, Can quantum-
mechanical description of physical reality be considered
complete? Phys. Rev. 47:777(1935).

[4] J. Bowles, T. Vertesi, M. T. Quintino, N. Brunner: One-
way Einstein-Podolsky-Rosen steering. Phys. Rev. Lett.
112, 200402 (2014).

[5] J. L. Chen, X. J. Ye, C. F. Wu, H. Y. Su, A. Cabello, L.
C. Kwek, C. H. Oh, All-versus-nothing proof of Einstein-
Podolsky-Rosen steering, Sci. Rep.3,2143(2013).

[6] J. Bowles, F. Hirsch, M. T. Quintino, N. Brunner: Suﬃ-
cient criterion for guaranteeing that a two-qubit state is
unsteerable. Phys. Rev. A 93, 022121 (2016).

[7] C. Branciard, E. G. Cavalcanti, S. P. Walborn,
V. Scarani, and H. M. Wiseman, One-sided device-
independent quantum key distribution: Security, feasi-
bility, and the connection with steering, Phys. Rev. A
85, 010301(R)(2012).

[8] E. Passaro, D. Cavalcanti, P. Skrzypczyk, A. Ac´in, Op-
timal randomness certiﬁcation in the quantum steer-
ing and prepare-and-measure scenarios, New J. Phys.
17,113010(2015).

[9] B. Coyle, M. J. Hoban, E. Kasheﬁ, One-sided device-
independent certiﬁcation of unbounded random numbers,
EPTCS 273, 14-26(2018)

[10] M. Piani, J. Watrous, Necessary and suﬃcient quantum
information characterization of Einstein-Podolsky-Rosen
steering, Phys. Rev. Lett. 114, 060404(2015).

[11] K. Sun, X.-J. Ye, Y. Xiao, X.-Y. Xu, Y.-C. Wu, J.-S.
Xu, J.-L. Chen, C.-F. Li, and G.-C. Guo, Demonstration
of Einstein-Podolsky-Rosen steering with enhanced sub-

8

channel discrimination, npj Quantum Inf. 4, 12(2018).

A 98, 012315 (2018).

[12] Y. Xiang, I. Kogias, G. Adesso, and Q. He, Multi-
partite gaussian steering: monogamy constraints and
quantum cryptography applications,Phys. Rev. A 95,
010101(R)(2017).

[13] Q. He, L. Rosales-Z´arate, G. Adesso, M. D. Reid, Secure
continuous variable teleportation and Einstein-Podolsky-
Rosen steering,Phys. Rev. Lett. 115, 180502(2015).
[14] H. Y. Ku, S. L. Chen, H.B Chen, N. Lambert, Y. N.
Chen, F. Nori, Temporal steering in four dimensions
with applications to coupled qubits and magnetorecep-
tion. Phys. Rev. A 94, 062126(2016).

[15] C. Y. Chiu, N. Lambert, T.L. Liao, F. Nori, C. M. Li,
No-cloning of quantum steering. NPJ Quantum Inf., 2,
16020(2016).

[16] S. L. Chen, N. Lambert, C. M. Li, G. Y. Chen, Y. N.
Chen, A. Miranowicz, F. Nori, Spatio-temporal steering
for testing nonclassical correlations in quantum networks.
Sci. Rep., 7, 3728(2017).

[17] E. G. Cavalcanti, S. J. Jones, H. M. Wiseman, M. D.
Reid, Experimental criteria for steering and the Einstein-
Podolsky-Rosen paradox. Phys. Rev. A 2009, 80, 032112.
[18] M. F. Pusey, Negativity and steering: A stronger Peres

conjecture. Phys. Rev. A 2013, 88, 032313.

[19] C .L. Ren, H. Y. Su, H. F. Shi, J. L. Chen, Maximally
steerable mixed state based on the linear steering inequal-
ity and the Clauser-Horne-Shimony-Holt-like steering in-
equality. Phys. Rev. A 2018, 97, 032119.

[20] Y. N. Chen, C. M. Li, N. Lambert, S. L. Chen, Y. Ota,
G. Y. Chen, F. Nori, Temporal steering inequality. Phys.
Rev. A 2014, 89, 032112.

[21] M. Zukowski, A. Dutta, Z. Yin, Geometric Bell-like in-
equalities for steering. Phys. Rev. A 2015, 91, 032107.
[22] H. Zhu, M. Hayashi, L. Chen, Universal steering inequal-

ities. Phys. Rev. Lett. 2016, 116, 070403.

[23] S. P. Walborn, A. Salles, R. M. Gomes, F. Toscano, P.
H. Souto Ribeiro, Revealing hidden Einstein-Podolsky-
Rosen nonlocality. Phys. Rev. Lett. 2011, 106, 130402.

[24] J. Schneeloch, C. J. Broadbent, S. P. Walborn, E. G.
Cavalcanti, J.C. Howell, Einstein-Podolsky-Rosen steer-
ing inequalities from entropic uncertainty relations. Phys.
Rev. A 2013, 87, 062103.

[25] T. Pramanik, M. Kaplan, A. S. Majumdar, Fine-
grained Einstein-Podolsky-Rosen steering inequalities.
Phys. Rev. A 2014, 90, 050305.

[26] I. Kogias, P. Skrzypczyk, D. Cavalcanti, A. Acin, and G.
Adesso, Hierarchy of Steering Criteria Based on Moments
for All Bipartite Quantum Systems 2015, Phys. Rev. Lett.
115, 210401.

[27] D. Cavalcanti, P. Skrzypczyk, Quantum steering: a re-
view with focus on semideﬁnite programming, Rep. Prog.
Phys. 80 024001 (2017).

[28] Y. C. Ma, M. H. Yung, Transforming bell’s inequalities
into state classiﬁers with machine learning, npj, Quan-
tum. Info., 4,34(2018).

[29] S. R. Liu, S. L. Huang, K. R. Li, et al, A separability-
entanglement classiﬁer via machine learning, Phys. Rev.

[30] D. L. Deng, X. Li and S. D. Sarma, Quantum entangle-
ment in neural network states, Phys. Rev. X 7, 021021
(2017).

[31] M. Yang, C. L. Ren, Y. C. Ma, Y. Xiao, X. J. Ye, L. L.
Song, J. S. Xu, M. H. Yung, C. F. Li, G. C. Guo, Experi-
mental Simultaneous Learning of Multiple Non-Classical
Correlations,Phys. Rev. Lett. 123, 190401 (2019).

[32] N. Wiebe, C. Granade, C. Ferrie, and D. G. Cory,
Hamiltonian learning and certiﬁcation using quantum re-
sources, Phys. Rev. Lett. 112, 190501 (2014).

[33] S. S. Schoenholz, E. D. Cubuk, D. M. Sussman, E. Kaxi-
ras, and A. J. Liu, A structural approach to relaxation
in glassy liquids, Nat. Phys. 12, 469 (2016).

[34] C. L. Ren,C. B. Chen, Steerability detection of arbitrary
2-qubit state via machine learning, Phys. Rev. A 100,
022314 (2019).

[35] Y.Q.Zhang, L.J. Yang, Q.L. He, L. Chen, Machine learn-
ing on quantifying quantum steerability, Quan. inf. Pro-
cess., 19(8):263(2020)

[36] X. Zhu, AB. Goldberg, Introduction to semi-supervised
learning. Synth. Lect. Artif. Intell. Mach. Learn., 3(1):1-
130(2009).

[37] S.F. Ding, Z.B. Zhu, X. K. Zhang, An overview on semi-
supervised support vector machine, Neural. Comput. Ap-
plic, 28(5):969-978(2017).

[38] Y.F. Li, Z.H. Zhou, Towards making unlabeled data
never hurt, IEEE Transactions on Pattern Analysis and
Machine Intelligence,37(1):175-188(2015).

[39] T. Joachims, Transductive inference for text classiﬁcation
using support vector machines. In Proc. 16th Int. Conf.
On Mach. Learn., Bled, Slovenia, 200-209(1999).

[40] L.M. Yang, Q.Y. Su, B.Y. Yang, D. Tong, and X. Xiao,
A new semi-supervised support vector machine classiﬁer
based on wavelet transform and its application in the
iris image recognition., Int J Appl Math Stat 52(5):86-
93(2014).

[41] K. Lu, X. He, J. Zhao, Semi-supervised support vec-
tor learning for face recognition. Advances in neural
networks-ISNN, Springer, Berlin, 104-109(2006).

[42] J. C. Ang, H. Haron, H. N. A. Hamed , Semi-supervised
SVM-based feature selection for cancer classiﬁcation us-
ing microarray gene expression data. In M. Ali., Y.
Kwon, CH. Lee , J. Kim, K. Kim (eds) Current
Approaches in Applied Artiﬁcial Intelligence IEA/AIE
2015, Seoul, South Korea,Lecture Notes in Artiﬁcial
Intelligence-ISNN, Springer, Cham., 9101,468-477(2015).
[43] C. Cortes, V. Vapnik, Support-vector networks, Mach.

Learn., 20(3), 273-297(1995).

[44] S. Abe, Support Vector Machines for Pattern Classiﬁca-

tion. Berlin, Germany: Springer-Verlag(2005).

[45] O. Chapelle, A. Zien, Semi-supervised learning by low
density separation. In Proc. of the 10th Int. Wksh. on
Arti. Intell. and Stat., Savannah,U.S.A., 57-64(2005).
[46] X. Zhu, Semi-Supervised Learning Literature Survey,
technical report, Univ. of Wisconsin-Madison, 2007

