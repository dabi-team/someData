Introduction to Neural Network Veriﬁcation

Aws Albarghouthi
University of Wisconsin–Madison

1
2
0
2

t
c
O
4

]

G
L
.
s
c
[

2
v
7
1
3
0
1
.
9
0
1
2
:
v
i
X
r
a

Author’s name in native alphabet: (cid:250)(cid:10)

(cid:17)(cid:71)(cid:241) (cid:9)(cid:171)(cid:81)(cid:30)(cid:46)(cid:203)(cid:64) (cid:128)(cid:240)

(cid:13)
(cid:64)

 
 
 
 
 
 
i

About This Book

Why This Book?

Over the past decade, a number of hardware and software advances have con-
spired to thrust deep learning and neural networks to the forefront of computing.
Deep learning has created a qualitative shift in our conception of what software
is and what it can do: Every day we’re seeing new applications of deep learn-
ing, from healthcare to art, and it feels like we’re only scratching the surface of a
universe of new possibilities.

It is thus safe to say that deep learning is here to stay, in one form or another.
The line between software 1.0 (that is, manually written code) and software 2.0
(learned neural networks) is getting fuzzier and fuzzier, and neural networks are
participating in safety-critical, security-critical, and socially critical tasks. Think,
for example, healthcare, self-driving cars, malware detection, etc. But neural net-
works are fragile and so we need to prove that they are well-behaved when ap-
plied in critical settings.

Over the past few decades, the formal methods community has developed
a plethora of techniques for automatically proving properties of programs, and,
well, neural networks are programs. So there is a great opportunity to port ver-
iﬁcation ideas to the software 2.0 setting. This book offers the ﬁrst introduction
of foundational ideas from automated veriﬁcation as applied to deep neural net-
works and deep learning.
I hope that it will inspire veriﬁcation researchers to
explore correctness in deep learning and deep learning researchers to adopt veri-
ﬁcation technologies.

ii

Who Is This Book For?

Given that the book’s subject matter sits at the intersection of two pretty much
disparate areas of computer science, one of my main design goals was to make it
as self-contained as possible. This way the book can serve as an introduction to the
ﬁeld for ﬁrst-year graduate students or senior undergraduates, even if they have
not been exposed to deep learning or veriﬁcation. For a comprehensive survey of
veriﬁcation algorithms for neural networks, along with implementations, I direct
the reader to Liu et al. (2021).

What Does This Book Cover?

The book is divided into three parts:

Part 1 deﬁnes neural networks as data-ﬂow graphs of operators over real-
valued inputs. This formulation will serve as our basis for the rest of the book.
Additionally, we will survey a number of correctness properties that are desir-
able of neural networks and place them in a formal framework.

Part 2 discusses constraint-based techniques for veriﬁcation. As the name sug-
gests, we construct a system of constraints and solve it to prove (or disprove)
that a neural network satisﬁes some properties of interest. Constraint-based
veriﬁcation techniques are also referred to as complete veriﬁcation in the litera-
ture.

Part 3 discusses abstraction-based techniques for veriﬁcation. Instead of execut-
ing a neural network on a single input, we can actually execute it on an inﬁnite
set and show that all of those inputs satisfy desirable correctness properties.
Abstraction-based techniques are also referred to as approximate veriﬁcation in
the literature.

Parts 2 and 3 are disjoint; the reader may go directly from Part 1 to Part 3 without
losing context.

iii

Acknowledgements

Thanks to the best focus group ever: the CS 839 students and TA, Swati Anand,
at the University of Wisconsin–Madison. A number of insightful people sent me
comments that radically improved the presentation: Frantisek Plasil, Georg Weis-
senbacher, Sayan Mitra, Benedikt Böing, Vivek Garg, Guy Van den Broeck, Matt
Fredrikson, in addition to some anonymous folks.

iv

Table of Contents

I Neural Networks & Correctness

1 A New Beginning 2

1.1

1.2

It Starts With Turing 2

The Rise of Deep Learning 3

1.3 What do We Expect of Neural Networks? 4

2 Neural Networks as Graphs 7

2.1

2.2

2.3

The Neural Building Blocks 7

Layers and Layers and Layers 9

Convolutional Layers 11

2.4 Where are the Loops? 12

2.5

Structure and Semantics of Neural Networks 14

3 Correctness Properties 19

3.1

3.2

Properties, Informally 19

A Speciﬁcation Language 23

3.3 More Examples of Properties 25

II Constraint-Based Veriﬁcation

4 Logics and Satisﬁability 32

4.1

4.2

Propositional Logic 32

Arithmetic Theories 36

5 Encodings of Neural Networks 40

CONTENTS

v

5.1

5.2

Encoding Nodes 40

Encoding a Neural Network 43

5.3 Handling Non-linear Activations 46

5.4

Encoding Correctness Properties 49

6 DPLL Modulo Theories 54

6.1

6.2

Conjunctive Normal Form (CNF) 54

The DPLL Algorithm 55

6.3 DPLL Modulo Theories 59

6.4

Tseitin’s Transformation 62

7 Neural Theory Solvers 67

7.1

7.2

7.3

Theory Solving and Normal Forms 67

The Simplex Algorithm 69

The Reluplex Algorithm 77

III Abstraction-Based Veriﬁcation

8 Neural Interval Abstraction 83

8.1

8.2

8.3

Set Semantics and Veriﬁcation 84

The Interval Domain 85

Basic Abstract Transformers 88

8.4 General Abstract Transformers 89

8.5

Abstractly Interpreting Neural Networks 92

9 Neural Zonotope Abstraction 96

9.1 What the Heck is a Zonotope? 97

9.2

9.3

9.4

Basic Abstract Transformers 101

Abstract Transformers of Activation Functions 103

Abstractly Interpreting Neural Networks with Zonotopes 106

10 Neural Polyhedron Abstraction 109

10.1 Convex Polyhedra 110

10.2 Computing Upper and Lower Bounds 112

CONTENTS

vi

10.3 Abstract Transformers for Polyhedra 112

10.4 Abstractly Interpreting Neural Networks with Polyhedra 116

11 Verifying with Abstract Interpretation 118

11.1 Robustness in Image Recognition 119

11.2 Robustness in Natural-Language Processing 125

12 Abstract Training of Neural Networks 129

12.1 Training Neural Networks 129

12.2 Adversarial Training with Abstraction 134

13 The Challenges Ahead 138

References 141

Part I

Neural Networks & Correctness

2

Chapter 1

A New Beginning

He had become so caught up in building sentences
that he had almost forgotten the barbaric days when
thinking was like a splash of color landing on a page.

—Edward St. Aubyn, Mother’s Milk

1.1 It Starts With Turing

This book is about verifying that a neural network behaves according to some set of
desirable properties. These ﬁelds of study, veriﬁcation and neural networks, have
been two distinct areas of computing research with almost no bridges connecting
them, until very recently. Intriguingly, however, both ﬁelds trace their genesis to
a two-year period of Alan Turing’s tragically short life.

In 1949, Turing wrote a little-known paper titled Checking a Large Routine (Alan,
1949). It was a truly forward-looking piece of work. In it, Turing asks how can
we prove that the programs we write do what they are supposed to do? Then, he
proceeds to provide a proof of correctness of a program implementing the factorial
function. Speciﬁcally, Turing proved that his little piece of code always terminates
and always produces the factorial of its input. The proof is elegant; it breaks
down the program into single instructions, proves a lemma for every instruction,

Quote found in William Finnegan’s Barbarian Days.

CHAPTER 1. A NEW BEGINNING

3

and ﬁnally stitches the lemmas together to prove correctness of the full program.
Until this day, proofs of programs very much follow Turing’s proof style from
1949. And, as we shall see in this book, proofs of neural networks will, too.

Just a year before Turing’s proof of correctness of factorial, in 1948, Turing
wrote a perhaps even more farsighted paper, Intelligent Machinery, in which he
proposed unorganized machines.1 These machines, Turing argued, mimic the infant
human cortex, and he showed how they can learn using what we now call a ge-
netic algorithm. Unorganized machines are a very simple form of what we now
know as neural networks.

1.2 The Rise of Deep Learning

The topic of training neural networks continued to be studied since Turing’s 1948
paper. But it has only exploded in popularity over the past decade, thanks to a
combination algorithmic insights, hardware developments, and a ﬂood of data
for training.

Modern neural networks are called deep neural networks, and the approach to
training these neural networks is deep learning. Deep learning has enabled incred-
ible improvements in complex computing tasks, most notably in computer vision
and natural-language processing, for example, in recognizing objects and people
in an image and translating between languages. Everyday, a growing research
community is exploring ways to extend and apply deep learning to more chal-
lenging problems, from music generation to proving mathematical theorems and
beyond.

The advances in deep learning have changed the way we think of what soft-
ware is, what it can do, and how we build it. Modern software is increasingly
becoming a menagerie of traditional, manually written code and automatically
trained—sometimes constantly learning—neural networks. But deep neural net-
works can be fragile and produce unexpected results. As deep learning becomes
used more and more in sensitive settings, like autonomous cars, it is imperative
that we verify these systems and provide formal guarantees on their behavior.

1Intelligent Machinery is reprinted in Turing (1969).

CHAPTER 1. A NEW BEGINNING

4

Luckily, we have decades of research on program veriﬁcation that we can build
upon, but what exactly do we verify?

1.3 What do We Expect of Neural Networks?

In Turing’s proof of correctness of his factorial program, Turing was concerned
that we will be programming computers to perform mathematical operations, but
we could be getting them wrong. So in his proof he showed that his implementa-
tion of factorial is indeed equivalent to the mathematical deﬁnition. This notion
of program correctness is known as functional correctness, meaning that a program
is a faithful implementation of some mathematical function. Functional correct-
ness is incredibly important in many settings—think of the disastrous effects of a
buggy implementation of a cryptographic primitive or an aircraft controller.

In the land of deep learning, proving functional correctness is an unrealistic
task. What does it mean to correctly recognize cats in an image or correctly trans-
late English to Hindi? We cannot mathematically deﬁne such tasks. The whole
point of using deep learning to do tasks like translation or image recognition is
because we cannot mathematically capture what exactly they entail.

So what now? Is veriﬁcation out of the question for deep neural networks?
No! While we cannot precisely capture what a deep neural network should do,
we can often characterize some of its desirable or undesirable properties. Let’s
look at some examples of such properties.

Robustness

The most-studied correctness property of neural networks is robustness, because
it is generic in nature and deep learning models are infamous for their fragility
(Szegedy et al., 2014). Robustness means that small perturbations to inputs should
not result in changes to the output of the neural network. For example, changing
a small number of pixels in my photo should not make the network think that I
am a cupboard instead of a person, or adding inaudible noise to a recording of my
lecture should not make the network think it is a lecture about the Ming dynasty
in the 15th century. Funny examples aside, lack of robustness can be a safety and

CHAPTER 1. A NEW BEGINNING

5

security risk. Take, for instance, an autonomous vehicle following trafﬁc signs
using cameras. It has been shown that a light touch of vandalism to a stop sign
can cause the vehicle to miss it, potentially causing an accident (Eykholt et al.,
2018). Or consider the case of a neural network for detecting malware. We do
not want a minor tweak to the malware’s binary to cause the detector to suddenly
deem it safe to install.

Safety

Safety is a broad class of correctness properties stipulating that a program should
not get to a bad state. The deﬁnition of bad depends on the task at hand. Consider
a neural-network-operated robot working in some kind of plant. We might be
interested in ensuring that the robot does not exceed certain speed limits, to avoid
endangering human workers, or that it does not go to a dangerous part of the
plant. Another well-studied example is a neural network implementing a collision
avoidance system for aircrafts (Katz et al., 2017). One property of interest is that
if an intruding aircraft is approaching from the left, the neural network should
decide to turn the aircraft right.

Consistency

Neural networks learn about our world via examples, like images. As such, they
may sometimes miss basic axioms, like physical laws, and assumptions about re-
alistic scenarios. For instance, a neural network recognizing objects in an image
and their relationships might say that object A is on top of object B, B is on top of
C, and C is on top of A. But this cannot be! (At least not in the world as we know
it.)

For another example, consider a neural network tracking players on the soccer
ﬁeld using a camera. It should not in one frame of video say that Ronaldo is on
the right side of the pitch and then in the next frame say that Ronaldo is on the left
side of the pitch—Ronaldo is fast, yes, but he has slowed down in the last couple
of seasons.

CHAPTER 1. A NEW BEGINNING

6

Looking Ahead

I hope that I have convinced you of the importance of verifying properties of
neural networks. In the next two chapters, we will formally deﬁne what neural
networks look like (spoiler: they are ugly programs) and then build a language
for formally specifying correctness properties of neural networks, paving the way
for veriﬁcation algorithms to prove these properties.

7

Chapter 2

Neural Networks as Graphs

There is no rigorous deﬁnition of what deep learning is and what it is not. In fact,
at the time of writing this, there is a raging debate in the artiﬁcial intelligence com-
munity about a clear deﬁnition. In this chapter, we will deﬁne neural networks
generically as graphs of operations over real numbers. In practice, the shape of
those graphs, called the architecture, is not arbitrary: Researchers and practition-
ers carefully construct new architectures to suit various tasks. For example, at
the time of writing, neural networks for image recognition typically look different
from those for natural language tasks.

First, we will informally introduce graphs and look at some popular architec-

tures. Then, we will formally deﬁne graphs and their semantics.

2.1 The Neural Building Blocks

A neural network is a graph where each node performs an operation. Overall,
the graph represents a function from vectors of real numbers to vectors of real
numbers, that is, a function in Rn → Rm. Consider the following very simple
graph.

x

v

y

Figure 2.1 A very simple neural network

CHAPTER 2. NEURAL NETWORKS AS GRAPHS

8

The red node is an input node; it just passes input x, a real number, to node v.
Node v performs some operation on x and spits out a value that goes to the output
node y. For example, v might simply return 2x + 1, which we will denote as the
function fv : R → R:

fv(x) = 2x + 1

In our model, the output node may also perform some operation, for example,

fy(x) = max(0, x)

Taken together, this simple graph encodes the following function f : R → R:

f (x) = fy( fv(x)) = max(0, 2x + 1)

Transformations and Activations

The function fv in our example above is afﬁne: simply, it multiplies inputs by
constant values (in this case, 2x) and adds constant values (in this case, 1). The
function fy is an activation function, because it turns on or off depending on its
input. When its input is negative, fy outputs 0 (off), otherwise it outputs its input
(on). Speciﬁcally, fy, illustrated in Figure 2.2, is called a rectiﬁed linear unit (ReLU),
and it is a very popular activation function in modern deep neural networks (Nair
and Hinton, 2010). Activation functions are used to add non-linearity into a neural
network.

relu(x)

x

Figure 2.2 Rectiﬁed linear unit

There are other popular activation functions, for example, sigmoid,

σ(x) =

1
1 + exp(−x)

CHAPTER 2. NEURAL NETWORKS AS GRAPHS

9

0.5

−4 −2

2

4

Figure 2.3 Sigmoid function

whose output is bounded between 0 and 1, as shown in Figure 2.3.

Often, in the literature and practice, the afﬁne functions and the activation
function are composed into a single operation. Our graph model of neural net-
works can capture that, but we usually prefer to separate the two operations on to
two different nodes of the graph, as it will simplify our life in later chapters when
we start analyzing those graphs.

Universal Approximation

What is so special about these activation functions? The short answer is they work
in practice, in that they result in neural networks that are able to learn complex
tasks. It is also very interesting to point out that you can construct a neural net-
work comprised of ReLUs or sigmoids and afﬁne functions to approximate any
continuous function. This is known as the universal approximation theorem (Hornik
et al., 1989), and in fact the result is way more general than ReLUs and sigmoids—
nearly any activation function you can think of works, as long as it is not polyno-
mial (Leshno et al., 1993)! For an interactive illustration of universal approxima-
tion, I highly recommend Nielsen (2018, Ch.4).

2.2 Layers and Layers and Layers

In general, a neural network can be a crazy graph, with nodes and arrows pointing
all over the place. In practice, networks are usually layered. Take the graph in
Figure 2.4. Here we have 3 inputs and 3 outputs, denoting a function in R3 → R3.

CHAPTER 2. NEURAL NETWORKS AS GRAPHS

10

x1

x2

x3

a1

a2

a3

y1

y2

y3

Figure 2.4 A multilayer perceptron

x1

x2

x3

v1

v2

v3

v4

v5

v6

y1

y2

y3

Figure 2.5 A multilayer perceptron with two hidden layers

Notice that the nodes of the graph form layers, the input layer, the output layer,
and the layer in the middle which is called the hidden layer. This form of graph—
or architecture—has the grandiose name of multilayer perceptron (MLP). Usually,
we have a bunch of hidden layers in an MLP; Figure 2.5 shows a MLP with two
hidden layers. Layers in an MLP are called fully connected layers, since each node
receives all outputs from the preceding layer.

Neural networks are typically used as classiﬁers: they take an input, e.g., pixels
of an image, and predict what the image is about (the image’s class). When we
are doing classiﬁcation, the output layer of the MLP represents the probability
of each class, for example, y1 is the probability of the input being a chair, y2 is
the probability of a TV, and y3 of a couch. To ensure that the probabilities are
normalized, that is, between 0 and 1 and sum up to 1, the ﬁnal layer employs a
softmax function. Softmax, generically, looks like this for an output node yi, where

CHAPTER 2. NEURAL NETWORKS AS GRAPHS

11

n is the number of classes:

fyi (x1, . . . , xn) =

exp(xi)
k=1 exp(xk)

∑n

Why does this work? Imagine that we have two classes, i.e., n = 2. First, we

can verify that

fy1, fy2 ∈ [0, 1]

This is because the numerators and denominators are both positive, and the nu-
merator is (cid:54) than the denominator. Second, we can see that fy1(x1, x2) + fy2(x1, x2) =
1, because

fy1(x1, x2) + fy2(x1, x2) =

ex1
ex1 + ex2

+

ex2
ex1 + ex2

= 1

Together, these two facts mean that we have a probability distribution. For an in-
teractive visualization of softmax, please see the excellent online book by Nielsen
(2018, Chapter 3).

Given some outputs (y1, . . . , yn) of the neural network, we will use

class(y1, . . . , yn)

to denote the index of the largest element (we assume no ties), i.e., the class with
the largest probability. For example, class(0.8, 0.2) = 1, while class(0.3, 0.7) = 2.

2.3 Convolutional Layers

Another kind of layer that you will ﬁnd in a neural network is a convolutional
layer. This kind of layer is widely used in computer-vision tasks, but also has uses
in natural-language processing. The rough intuition is that if you are looking at an
image, you want to scan it looking for patterns. The convolutional layer gives you
that: it deﬁnes an operation, a kernel, that is applied to every region of pixels in
an image or every sequence of words in a sentence. For illustration, let’s consider
an input layer of size 4, perhaps each input deﬁnes a word in a 4-word sentence,
as shown in Figure 2.6. Here we have a kernel, nodes {v1, v2, v3}, that is applied
to every pair of consecutive words, (x1, x2), (x2, x3), and (x3, x4). We say that this

CHAPTER 2. NEURAL NETWORKS AS GRAPHS

12

kernel has size 2, since it takes an input in R2. This kernel is 1-dimensional, since
its input is a vector of real numbers. In practice, we work with 2-dimensional
kernels or more; for instance, to scan blocks of pixels of a gray-scale image where
every pixel is a real number, we can use kernels that are functions in R10×10 → R,
meaning that the kernel is applied to every 10 × 10 sub-image in the input.

x1

x2

x3

x4

v1

v2

v3

y1

y2

y3

Figure 2.6 1-dimensional convolution

Typically, a convolutional neural network (CNN) will apply a bunch of kernels to
an input—and many layers of them—and aggregate (pool) the information from
each kernel. We will meet these operations in later chapters when we verify prop-
erties of such networks.1

2.4 Where are the Loops?

All of the neural networks we have seen so far seem to be a composition of a
number mathematical functions, one after the other. So what about loops? Can we
have loops in neural networks? In practice, neural network graphs are really just
directed acyclic graphs (DAG). This makes training the neural network possible
using the backpropagation algorithm.

That said, there are popular classes of neural networks that appear to have
loops, but they are very simple, in the sense that the number of iterations of the

1Note that there are many parameters that are used to construct a CNN, e.g., how many kernels
are applied, how many inputs a kernel applies to, the stride or step size of a kernel, etc. These are
not of interest to us in this book. We’re primarily concerned with the core building blocks of the
neural network, which will dictate the veriﬁcation challenges.

CHAPTER 2. NEURAL NETWORKS AS GRAPHS

13

loop is just the size of the input. Recurrent neural networks (RNN) is the canonical
class of such networks, which are usually used for sequence data, like text. You
will often see the graph of an RNN rendered as follows, with the self loop on node
v.

y

v

x

Figure 2.7 Recurrent neural network

Effectively, this graph represents an inﬁnite family of acyclic graphs that unroll
this loop a ﬁnite number of times. For example, Figure 2.8 is an unrolling of length
3. Notice that this is an acyclic graph that takes 3 inputs and produces 3 outputs.
The idea is that if you receive a sentence, say, with n words, you unroll the RNN
to length n and apply it to the sentence.

y1

v1

x1

y2

v2

x2

y3

v3

x3

Figure 2.8 Unrolled recurrent neural network

Thinking of it through a programming lens, given an input, we can easily stat-
ically determine—i.e., without executing the network—how many loop iterations
it will require. This is in contrast to, say, a program where the number of loop iter-
ations is a complex function of its input, and therefore we do not know how many

CHAPTER 2. NEURAL NETWORKS AS GRAPHS

14

loop iterations it will take until we actually run it. That said, in what follows, we
will formalize neural networks as acyclic graphs.

2.5 Structure and Semantics of Neural Networks

We’re done with looking at pretty graphs. Let’s now look at pretty symbols. We
will now formally deﬁne neural networks as directed acyclic graphs and discuss
some of their properties.

Neural Networks as DAGs

A neural network is a directed acyclic graph G = (V, E), where

• V is a ﬁnite set of nodes,

• E ⊆ V × V is a set of edges,

• Vin ⊂ V is a non-empty set of input nodes,

• Vo ⊂ V is a non-empty set of output nodes, and

• each non-input node v is associated with a function fv : Rnv → R, where nv
is the number of edges whose target is node v. The vector of real values Rnv
that v takes as input is all of the outputs of nodes v(cid:48) such that (v(cid:48), v) ∈ E.
Notice that we assume, for simplicity but without loss of generality, that a
node v only outputs a single real value.

To make sure that a graph G does not have any dangling nodes and that se-

mantics are clearly deﬁned, we will assume the following structural properties:

• All nodes are reachable, via directed edges, from some input node.

• Every node can reach an output node.

• There is ﬁxed total ordering on edges E and another one on nodes V.

We will use x ∈ Rn to denote an n-ary (row) vector, which we represent as a

tuple of scalars (x1, . . . , xn), where xi is the ith element of x.

CHAPTER 2. NEURAL NETWORKS AS GRAPHS

15

Semantics of DAGs

A neural network G = (V, E) deﬁnes a function in Rn → Rm where

n = |Vin| and m = |Vo|

That is, G maps the values of the input nodes to those of the output nodes.

Speciﬁcally, for every non-input node v ∈ V, we recursively deﬁne the value
in R that it produces as follows. Let (v1, v), . . . , (vnv, v) be an ordered sequence
of all edges whose target is node v (remember that we’ve assumed an order on
edges). Then, we deﬁne the output of node v as

out(v) = fv(x1, . . . , xnv)

where xi = out(vi), for i ∈ {1, . . . , nv}.

The base case of the above deﬁnition (of out) is input nodes, since they have
no edges incident on them. Suppose that we’re given an input vector x ∈ Rn. Let
v1, . . . , vn be an ordered sequence of all input nodes. Then,

A Simple Example

Let’s look at an example graph G:

out(vi) = xi

v1

v2

v3

We have Vin = {v1, v2} and Vo = {v3}. Now assume that

fv3(x1, x2) = x1 + x2

and that we’re given the input vector (11, 79) to the network, where node v1 gets
the value 11 and v2 the value 79. Then, we have

out(v1) = 11
out(v2) = 79
out(v3) = fv3(out(v1), out(v2)) = 11 + 79 = 90

CHAPTER 2. NEURAL NETWORKS AS GRAPHS

16

Data Flow and Control Flow

The graphs we have deﬁned are known in the ﬁeld of compilers and program
analysis as data-ﬂow graphs; this is in contrast to control-ﬂow graphs.2 Control-
ﬂow graphs dictate the order in which operations need be performed—the ﬂow
of who has control of the CPU. Data-ﬂow graphs, on the other hand, only tell us
what node needs what data to perform its computation, but not how to order the
computation. This is best seen through a small example.

Consider the following graph

v1

v2

v3

v4

v5

Viewing this graph as an imperative program, one way to represent it is as follows,
where ← is the assignment symbol.

out(v3) ← fv3(out(v1))
out(v4) ← fv4(out(v2))
out(v5) ← fv5(out(v3), out(v4))

This program dictates that the output value of node v3 is computed before that
of node v4. But this need not be, as the output of v3 does not depend on that of
v4. Therefore, an equivalent implementation of the same graph can swap the ﬁrst
two operations:

out(v4) ← fv4(out(v2))
out(v3) ← fv3(out(v1))
out(v5) ← fv5(out(v3), out(v4))

Formally, we can compute the values out(·) in any topological ordering of graph
nodes. This ensures that all inputs of a node are computed before its own opera-
tion is performed.

2In deep learning frameworks like TensorFlow, they call data-ﬂow graphs computation graphs.

CHAPTER 2. NEURAL NETWORKS AS GRAPHS

17

Properties of Functions

So far, we have assumed that a node v can implement any function fv it wants over
real numbers. In practice, to enable efﬁcient training of neural networks, these
functions need be differentiable or differentiable almost everywhere. The sigmoid
activation function, which we met earlier in Figure 2.3, is differentiable. How-
ever, the ReLU activation function, Figure 2.2, is differentiable almost everywhere,
since at x = 0, there is a sharp turn in the function and the gradient is undeﬁned.
Many of the functions we will be concerned with are linear or piecewise linear.

Formally, a function f : Rn → R is linear if it can be deﬁned as follows:

f (x) =

n
∑
i=1

cixi + b

where ci, b ∈ R. A function is piecewise linear if it can be written in the form

f (x) =





∑i c1
...
∑i cm

i xi + b1,

i xi + bm,

x ∈ S1

x ∈ Sm

where Si are mutually disjoint subsets of Rn and ∪iSi = Rn. ReLU, for instance,
is a piecewise linear function, as it is of the form:

relu(x) =

(cid:40)

0,

x,

x < 0
x (cid:62) 0

Another important property that we will later exploit is monotonicity. A func-
tion f : R → R is monotonically increasing if for any x (cid:62) y, we have f (x) (cid:62) f (y).
Both activation functions we saw earlier in the chapter, ReLUs and sigmoids, are
monotonically increasing. You can verify this in Figures 2.2 and 2.3: the functions
never decrease with increasing values of x.

Looking Ahead

Now that we have formally deﬁned neural networks, we’re ready to pose ques-
tions about their behavior. In the next chapter, we will formally deﬁne a language

CHAPTER 2. NEURAL NETWORKS AS GRAPHS

18

for posing those questions. Then, in the chapters that follow, we will look at algo-
rithms for answering those questions.

Most discussions of neural networks in the literature use the language of linear
algebra—see, for instance, the comprehensive book of Goodfellow et al. (2016).
Linear algebra is helpful because we can succinctly represent the operation of
many nodes in a single layer as a matrix A that applies to the output of the previ-
ous layer. Also, in practice, we use fast, parallel implementations of matrix multi-
plication to evaluate neural networks. Here, we choose a lower-level presentation,
where each node is a function in Rn → R. While this view is non-standard, it will
help make our presentation of different veriﬁcation techniques much cleaner, as
we can decompose the problem into smaller ones that have to do with individual
nodes.

The graphs of neural networks we presented are lower-level versions of the
computation graphs of deep-learning frameworks like Tensorﬂow (Abadi et al.,
2016) and PyTorch (Paszke et al., 2019)

Neural networks are an instance of a general class of programs called differen-
tiable programs. As their name implies, differentiable programs are ones for which
we can compute derivatives, a property that is needed for standard techniques for
training neural networks. Recently, there have been interesting studies of what it
means for a program to be differentiable (Abadi and Plotkin, 2020; Sherman et al.,
2021). In the near future, it is likely that people will start using arbitrary differen-
tiable programs to deﬁne and train neural networks. Today, this is not the case,
most neural networks have one of a few prevalent architectures and operations.

19

Chapter 3

Correctness Properties

In this chapter, we will come up with a language for specifying properties of neu-
ral networks. The speciﬁcation language is a formulaic way of making statements
about the behavior of a neural network (or sometimes multiple neural networks).
Our concerns in this chapter are solely about specifying properties, not about au-
tomatically verifying them. So we will take liberty in specifying complex proper-
ties, ridiculous ones, and useless ones. In later parts of the book, we will constrain
the properties of interest to ﬁt certain veriﬁcation algorithms—for now, we have
fun.

3.1 Properties, Informally

Remember that a neural network deﬁnes a function f : Rn → Rm. The properties
we will consider here are of the form:

for any input x, the neural network produces an output that ...

In other words, properties dictate the input–output behavior of the network, but
not the internals of the network—how it comes up with the answer.

Sometimes, our properties will be more involved, talking about multiple in-

puts, and perhaps multiple networks:

for any inputs x, y, ... that ... the neural networks produce outputs
that ...

CHAPTER 3. CORRECTNESS PROPERTIES

20

Figure 3.1 Left: Handwritten 7 from mnist dataset. Middle: Same digit with increased
brightness. Right: Same digit but with a dot added in the top left.

The ﬁrst part of these properties, the one talking about inputs, is called the
precondition; the second part, talking about outputs, is called the postcondition. In
what follows, we will continue our informal introduction to properties using ex-
amples.

Image Recognition

Let’s say we have a neural network f that takes in an image and predicts a label
from dog, zebra, etc. An important property that we may be interested in ensuring
is robustness of such classiﬁer. A classiﬁer is robust if its prediction does not change
with small variations (or perturbations) of the input. For example, changing the
brightness slightly or damaging a few pixels should not change classiﬁcation.

Let’s ﬁx some image c that is classiﬁed as dog by f . To make sure that c is not
an adversarial image of a dog that is designed to fool the neural network, we will
check—prove or verify—the following property:

for any image x that is slightly brighter or darker than c, f (x) predicts
dog

Notice here that the precondition speciﬁes a set of images x that are brighter or
darker than c, and the postcondition speciﬁes that the classiﬁcation by f remains
unchanged.

CHAPTER 3. CORRECTNESS PROPERTIES

21

Robustness is a desirable property: you don’t want classiﬁcation to change
with a small movement in the brightness slider. But there are many other prop-
erties you desire—robustness to changes in contrast, rotations, Instagram ﬁlters,
white balance, and the list goes on. This hits at the crux of the speciﬁcation prob-
lem: we often cannot specify every possible thing that we desire, so we have to
choose some. (More on this later.)

For a concrete example, see Figure 3.1. The MNIST dataset (LeCun et al., 2010) is
a standard dataset for recognizing handwritten digits. The ﬁgure shows a hand-
written 7 along with two modiﬁed versions, one where brightness is increased
and one where a spurious dot is added—perhaps a drip of ink. We would like our
neural network to classify all three images as 7.

Natural-Language Processing

Suppose now that f takes an English sentence and decides whether it represents a
positive or negative sentiment. This problem arises, for example, in automatically
analyzing online reviews or tweets. We’re also interested in robustness in this
setting. For example, say we have ﬁxed a sentence c with positive sentiment, then
we might specify the following property:

for any sentence x that is c with a few spelling mistakes added, f (x)
should predict positive sentiment

For another example, instead of spelling mistakes, imagine replacing words

with synonyms:

for any sentence x that is c with some words replaced by synonyms,
then f (x) should predict positive sentiment

For instance, a neural network should classify both of these movie reviews as
positive reviews:

This movie is delightful
This movie is enjoyable

CHAPTER 3. CORRECTNESS PROPERTIES

22

We could also combine the two properties above to get a stronger property
specifying that prediction should not change in the presence of synonyms or spelling
mistakes.

Source Code

Say that our neural network f is a malware classiﬁer, taking a piece of code and
deciding whether it is malware or not. A malicious entity may try to modify a
malware to sneak it past the neural network by fooling it into thinking that it’s
a benign program. One trick the attacker may use is adding a piece of code that
does not change the malware’s operation but that fools the neural network. We
can state this property as follows: Say we have piece of malware c, then we can
state the following property:

for any program x that is equivalent to c and syntactically similar, then
f (x) predicts malware

Controllers

All of our examples so far have been robustness problems. Let’s now look at a
slightly different property. Say you have a controller deciding on the actions of a
robot. The controller looks at the state of the world and decides whether to move
left, right, forward, or backward. We, of course, do not want the robot to move
into an obstacle, whether it is a wall, a human, or another robot. As such, we
might specify the following property:

for any state x, if there is an obstacle to the right of the robot, then f (x)
should not predict right

We can state one such property per direction.

CHAPTER 3. CORRECTNESS PROPERTIES

23

3.2 A Speciﬁcation Language

Our speciﬁcations are going to look like this:

{ precondition }
r ← f (x)
{ postcondition }

The precondition is a Boolean function (predicate) that evaluates to true or false. The
precondition is deﬁned over a set of variables which will be used as inputs to the
neural networks we’re reasoning about. We will use xi to denote those variables.
The middle portion of the speciﬁcation is a number of calls to functions deﬁned by
neural networks; in this example, we only see one call to f , and the return value
is stored in a variable r. Generally, our speciﬁcation language allows a sequence
of such assignments, e.g.:

{ precondition }
r1 ← f (x1)
r2 ← g(x2)
...
{ postcondition }

Finally, the postcondition is a Boolean predicate over the variables appearing in
the precondition xi and the assigned variables rj.

The way to read a speciﬁcation, informally, is as follows:

for any values of x1, . . . , xn that make the precondition true, let r1 =
f (x1), r2 = g(x2), . . .. Then the postcondition is true.

If a correctness property is not true, i.e., the postcondition yields false, we will
also say that the property does not hold.

Example 3.A Recall our image brightness example from the previous section, and
say c is an actual grayscale image, where each element of c is the intensity of a
pixel, from 0 to 1 (black to white). For example, in our MNIST example in Fig-
ure 3.1, each digit is represented by 784 pixels (28 × 28), where each pixel is a
number between 0 and 1. Then, we can state the following speciﬁcation, which

CHAPTER 3. CORRECTNESS PROPERTIES

24

informally says that changing the brightness of c should not change the classiﬁca-
tion (recall the deﬁnition of class from Section 2.2):

{ |x − c| (cid:54) 0.1 }
r1 ← f (x)
r2 ← f (c)
{ class(r1) = class(r2) }

Let’s walk through this speciﬁcation:

Precondition Take any image x where each pixel is at most 0.1 away from its
counterpart in c. Here, both x and c are assumed to be the same size, and the (cid:54)
is deﬁned pointwise.1

Assignments Let r1 be the result of computing f (x) and r2 be the result of
computing f (c).

Postcondition Then, the predicted labels in vectors r1 and r2 are the same. Re-
call that in a classiﬁcation setting, each element of vector ri refers to the proba-
bility of a speciﬁc label. We use class as a shorthand to extract the index of the
largest element of the vector.

(cid:4)

Counterexamples

A counterexample to a property is a valuation of the variables in the precondition
(the xis) that falsiﬁes the postcondition. In Example 3.A, a counterexample would
be an image x whose classiﬁcation by f is different than that of image c and whose
distance from c, i.e., |x − c|, is less than 0.1.

1The pointwise operation | · | is known as the (cid:96)∞ norm, which we formally discuss in Chap-

ter 11 and compare it to other norms.

CHAPTER 3. CORRECTNESS PROPERTIES

25

Example 3.B Here’s a concrete example (not about image recognition, just a simple
function that adds 1 to the input):

{ x (cid:54) 0.1 }
r ← x + 1
{ r (cid:54) 1 }

This property does not hold. Consider replacing x with the value 0.1. Then, r ←
1 + 0.1 = 1.1. Therefore, the postcondition is falsiﬁed. So, setting x to 0.1 is a
(cid:4)
counterexample.

A Note on Hoare Logic

Our speciﬁcation language looks like speciﬁcations written in Hoare logic (Hoare,
1969). Speciﬁcations in Hoare logic are called Hoare triples, as they are composed
of three parts, just like our speciﬁcations. Hoare logic comes equipped with de-
duction rules that allows one to prove the validity of such speciﬁcations. For our
purposes in this book, we will not deﬁne the rules of Hoare logic, but many of
them will crop up implicitly throughout the book.

3.3 More Examples of Properties

We will now go through a bunch of example properties and write them in our
speciﬁcation language.

Equivalence of Neural Networks

Say you have a neural network f for image recognition and you want to replace
it with a new neural network g. Perhaps g is smaller and faster, and since you’re
interested in running the network on a stream of incoming images, efﬁciency is
very important. One thing you might want to prove is that f and g are equivalent;

CHAPTER 3. CORRECTNESS PROPERTIES

26

here’s how to write this property:

{ true }
r1 ← f (x)
r2 ← g(x)
{ class(r1) = class(r2) }

Notice that the precondition is true, meaning that for any image x, we want the
predicted labels of f and g to be the same. The true precondition indicates that the
inputs to the neural networks (x in this case) are unconstrained. This speciﬁcation
is very strong: the only way it can be true is if f and g agree on the classiﬁcation
on every possible input, which is highly unlikely in practice.

One possible alternative is to state that f and g return the same prediction on
some subset of images, plus or minus some brightness, as in our above example.
Say S is a ﬁnite set of images, then:

{ x1 ∈ S, |x1 − x3| (cid:54) 0.1, |x1 − x2| (cid:54) 0.1 }
r1 ← f (x2)
r2 ← g(x3)
{ class(r1) = class(r2) }

This says the following: Pick an image x1 and generate two variants, x2 and x3,
whose brightness differs a little bit from x1. Then, f and g should agree on the
classiﬁcation of the two images.

This is a more practical notion of equivalence than our ﬁrst attempt. Our ﬁrst
attempt forced f and g to agree on all possible inputs, but keep in mind that most
images (combinations of pixels) are meaningless noise, and therefore we don’t
care about their classiﬁcation. This speciﬁcation, instead, constrains equivalence
to an inﬁnite set of images that look like those in the set S.

Collision Avoidance

Our next example is one that has been a subject of study in the veriﬁcation liter-
ature, beginning with the pioneering work of Katz et al. (2017). Here we have a
collision avoidance system that runs on an autonomous aircraft. The system de-
tects intruder aircrafts and decides what to do. The reason the system is run on

CHAPTER 3. CORRECTNESS PROPERTIES

27

a neural network is due to its complexity: The trained neural network is much
smaller than a very large table of rules. In a sense, the neural network compresses
the rules into an efﬁciently executable program.

The inputs to the neural network are the following:

• vown: the aircraft’s velocity

• vint: the intruder aircraft’s velocity

• aint: the angle of the intruder with respect to the current ﬂying direction

• aown: the angle of the aircraft with respect to the intruder.

• d: the distance between the two aircrafts

• prev: the previous action taken.

Given the above values, the neural network decides how to steer:
left/right,
strong left/right, or nothing. Speciﬁcally, the neural network assigns a score to
every possible action, and the action with the lowest score is taken.

As you can imagine, many things can go wrong here, and if they do—disaster!
Katz et al. (2017) identify a number of properties that they verify. These properties
do not account for all possible scenarios, but they are important to check. Let’s
take a look at one that says if the intruder aircraft is far away, then the score for
doing nothing should be below some threshold.

{ d (cid:62) 55947, vown (cid:62) 1145, vint
r ← f (d, vown, vint, . . .)
{ score of nothing in r is below 1500 }

(cid:54) 60 }

Notice that the precondition speciﬁes that the distance between the two aircrafts is
more than 55947 feet, that the aircraft’s velocity is high, and the intruder’s velocity
is low. The postcondition speciﬁes that doing nothing should have a low score,
below some threshold. Intuitively, we should not panic if the two aircrafts are
quite far apart and have moving at very different velocities.

Katz et al. (2017) explore a number of such properties, and also consider ro-
bustness properties in the collision-avoidance setting. But how do we come up

CHAPTER 3. CORRECTNESS PROPERTIES

28

with such speciﬁc properties? It’s not straightforward. In this case, we really need
a domain expert who knows about collision-avoidance systems, and even then,
we might not cover all corner cases. A number of people in the veriﬁcation com-
munity, the author included, argue that speciﬁcation is harder than veriﬁcation—
that is, the hard part is asking the right questions!

Physics Modeling

Here is another example due to Qin et al. (2019). We want the neural network
to internalize some physical laws, such as the movement of a pendulum. At any
point in time, the state of the pendulum is a triple (v, h, w), its vertical position
v, its horizontal position h, and its angular velocity w. Given the state of the
pendulum, the neural network is to predict the state in the next time instance,
assuming that time is divided into discrete steps.

A natural property we may want to check is that the neural network’s un-
derstanding of how the pendulum moves adheres to the law of conservation of
energy. At any point in time, the energy of the pendulum is the sum of its po-
tential energy and its kinetic energy. (Were you paying attention in high school
physics?) As the pendulum goes up, its potential energy increases and kinetic en-
ergy decreases; as it goes down, the opposite happens. The sum of the kinetic and
potential energies should only decrease over time. We can state this property as
follows:

{ true }
v(cid:48), h(cid:48), w(cid:48) ← f (v, h, w)
{ E(h(cid:48), w(cid:48)) (cid:54) E(h, w) }

The expression E(h, w) is the energy of the pendulum, which is its potential
energy mgh, where m is the mass of the pendulum and g is the gravitational con-
stant, plus its kinetic energy 0.5ml2w2, where l is the length of the pendulum.

Natural-Language Processing

Let’s recall the natural language example from earlier in the chapter, where we
wanted to classify a sentence into whether it expresses a positive or negative sen-
timent. We decided that we want the classiﬁcation not to change if we replaced

CHAPTER 3. CORRECTNESS PROPERTIES

29

a word by a synonym. We can express this property in our language: Let c be a
ﬁxed sentence of length n. We assume that each element of vector c is a real num-
ber representing a word—called an embedding of the word. We also assume that
we have a thesaurus T, which given a word gives us a set of equivalent words.

{ 1 (cid:54) i (cid:54) n, w ∈ T(ci), x = c[i (cid:55)→ w] }
r1 ← f (x)
r2 ← f (c)
{ class(r1) = class(r2) }

The precondition speciﬁes that variable x is just like the sentence c, except that
some element i is replaced by a word w from the thesaurus. We use the notation
c[i (cid:55)→ w] to denote c with the ith element replaced with w and ci to denote the ith
element of c.

The above property allows a single word to be replaced by a synonym. We can

extend it to two words as follows (I know, it’s very ugly, but it works):

{ 1 (cid:54) i, j (cid:54) n , i (cid:54)= j , wi ∈ T(ci) , wj ∈ T(cj) , x = c[i (cid:55)→ wi, j (cid:55)→ wj] }
r1 ← f (x)
r2 ← f (c)
{ class(r1) = class(r2) }

Monotonicity

A standard mathematical property that we may desire of neural networks is mono-
tonicity (Sivaraman et al., 2020), meaning that larger inputs should lead to larger
outputs. For example, imagine you’re one of those websites that predict house
prices using machine learning. You’d expect the machine-learning model used is
monotonic with respect to square footage—if you increase the square footage of a
house, its price should not decrease, or perhaps increase. Or imagine a model that
estimates the risk of complications during surgery. You’d expect that increasing
the age of the patient should not decrease the risk. (I’m not a physician, but I like

CHAPTER 3. CORRECTNESS PROPERTIES

30

this example.) Here’s how you could encode monotonicity in our language:

{ x > x(cid:48) }
r ← f (x)
r(cid:48) ← f (x(cid:48))
{ r(cid:48) (cid:62) r(cid:48) }

In other words, pick any pair of inputs such that x > x(cid:48), we want f (x) (cid:62) f (x(cid:48)).
Of course, we can strengthen the property by making the postcondition a strict
inequality—that completely depends on the problem domain we’re working with.

Looking Ahead

We’re done with the ﬁrst part of the book. We have deﬁned neural networks and
how to specify their properties. In what follows, we will discuss different ways of
verifying properties automatically.

There has been an insane amount of work on robustness problems, particularly
for image recognition. Lack of robustness was ﬁrst observed by Szegedy et al.
(2014), and since then many approaches to discover and defend against robustness
violations (known as adversarial examples) have been proposed. We will survey
those later. The robustness properties for natural-language processing we have
deﬁned follow those of Ebrahimi et al. (2018) and Huang et al. (2019).

Part II

Constraint-Based Veriﬁcation

32

Chapter 4

Logics and Satisﬁability

In this part of the book, we will look into constraint-based techniques for veriﬁca-
tion. The idea is to take a correctness property and encode it as a set of constraints.
By solving the constraints, we can decide whether the correctness property holds
or not.

The constraints we will use are formulas in ﬁrst-order logic (FOL). FOL is a very
big and beautiful place, but neural networks only live in a small and cozy corner
of it—the corner that we will explore in this chapter.

4.1 Propositional Logic

We begin with the purest of all, propositional logic. A formula F in propositional
logic is over Boolean variables (traditionally given the names p, q, r, . . .) and de-
ﬁned using the following grammar:

F := true
false
var
Variable
| F ∧ F Conjunction (and)
| F ∨ F
Disjunction (or)
| ¬F
Negation (not)

Essentially, a formula in propositional logic deﬁnes a circuit with Boolean vari-
ables, AND gates (∧), OR gates (∨), and not gates (¬). Negation has the highest

CHAPTER 4. LOGICS AND SATISFIABILITY

33

operator precedence, followed by conjunction and then disjunction. At the end of
the day, all programs can be deﬁned as circuits, because everything is a bit on a
computer and there is a ﬁnite amount of memory, and therefore a ﬁnite number
of variables.

We will use fv(F) to denote the set of free variables appearing in the formula.
For our purposes, this is the set of all variables that are syntactically present in the
formula;

Example 4.A As an example, here is a formula

F (cid:44) (p ∧ q) ∨ ¬r
Observe the use of (cid:44); this is to denote that we’re syntactically deﬁning F to be
the formula on the right of (cid:44), as opposed to saying that the two formulas are
semantically equivalent (more on this in a bit). The set of free variables in F is
fv(F) = {p, q, r}.
(cid:4)

Interpretations

Let F be a formula over a set of variables fv(F). An interpretation I of F is a map
from variables fv(F) to true or false. Given an interpretation I of a formula F, we
will use I(F) to denote the formula where we have replaced each variable in fv(F)
with its interpretation in I.

Example 4.B Say we have the formula

F (cid:44) (p ∧ q) ∨ ¬r

and the interpretation

I = {p (cid:55)→ true, q (cid:55)→ true, r (cid:55)→ false}

Note that we represent I as a set of pairs of variables and their interpretations.
Applying I to F, we get

I(F) (cid:44) (true ∧ true) ∨ ¬false

(cid:4)

CHAPTER 4. LOGICS AND SATISFIABILITY

34

Evaluation Rules

We will deﬁne the following evaluation, or simpliﬁcation, rules for a formula. The
formula on the right of ≡ is an equivalent, but syntactically simpler, variant of the
one on the left:

Conjunction

true ∧ F ≡ F
F ∧ true ≡ F
false ∧ F ≡ false
F ∧ false ≡ false

false ∨ F ≡ F
F ∨ false ≡ F
true ∨ F ≡ true
F ∨ true ≡ true

¬true ≡ false
¬false ≡ true

Disjunction

Negation

If a given formula has no free variables, then by applying these rules repeat-
edly, you will get true or false. We will use eval(F) to denote the simplest form of
F we can get by repeatedly applying the above rules.

Satisﬁability

A formula F is satisﬁable (SAT) if there exists an interpretation I such that

in which case we will say that I is a model of F and denote it

eval(I(F)) = true

I |= F

We will also use I (cid:54)|= F to denote that I is not a model of F. It follows from our
deﬁnitions that I (cid:54)|= F iff I |= ¬F.

Equivalently, a formula F is unsatisﬁable (UNSAT) if for every interpretation I

we have eval(I(F)) = false.

Example 4.C Consider the formula F (cid:44) (p ∨ q) ∧ (¬p ∨ r). This formula is satisﬁ-
able; here is a model I = {p (cid:55)→ true, q (cid:55)→ false, r (cid:55)→ true}.
(cid:4)

CHAPTER 4. LOGICS AND SATISFIABILITY

35

Example 4.D Consider the formula F (cid:44) (p ∨ q) ∧ ¬p ∧ ¬q. This formula is unsat-
(cid:4)
isﬁable.

Validity and Equivalence

To prove properties of neural networks, we will be asking validity questions. A
formula F is valid if every possible interpretation I is a model of F. It follows that
a formula F is valid if and only if ¬F is unsatisﬁable.

Example 4.E Here is a valid formula F (cid:44) (¬p ∨ q) ∨ p. Pick any interpretation I
that you like; you will ﬁnd that I |= F.
(cid:4)

We will say that two formulas, A and B, are equivalent if and only if every
model I of A is a model of B, and vice versa. We will denote equivalence as A ≡ B.
There are many equivalences that are helpful when working with formulas. For
any formulas A, B, and C, we have commutativity of conjunction and disjunction,

A ∧ B ≡ B ∧ A
A ∨ B ≡ B ∨ A

We can push negation inwards:

¬(A ∧ B) ≡ ¬A ∨ ¬B
¬(A ∨ B) ≡ ¬A ∧ ¬B

Moreover, we have distributivity of conjunction over disjunction (DeMorgan’s laws),
and vice versa:

A ∨ (B ∧ C) ≡ (A ∨ B) ∧ (A ∨ C)
A ∧ (B ∨ C) ≡ (A ∧ B) ∨ (A ∧ C)

Implication and Bi-implication

We will often use an implication A ⇒ B to denote the formula

¬A ∨ B

Similarly, we will use a bi-implication A ⇔ B to denote the formula

(A ⇒ B) ∧ (B ⇒ A)

CHAPTER 4. LOGICS AND SATISFIABILITY

36

4.2 Arithmetic Theories

We can now extend propositional logic using theories. Each Boolean variable now
becomes a more complex Boolean expression over variables of different types.
For example, we can use the theory of linear real arithmetic (LRA), where a Boolean
expression is, for instance,

x + 3y + z (cid:54) 10

Alternatively, we can use the theory of arrays, and so an expression may look like:

a[10] = x

where a is an array indexed by integers. There are many other theories that people
have studied, including bitvectors (to model machine arithmetic) and strings (to
model string manipulation). The satisﬁabilty problem is now called satisﬁability
modulo theories (SMT), as we check satisﬁability with respect to interpretations of
the theory.

In this section, we will focus on the theory of linear real arithmetic (LRA), as it
is (1) decidable and (2) can represent a large class of neural-network operations,
as we will see in the next chapter.

Linear Real Arithmetic

In LRA, each propositional variable is replaced by a linear inequality of the form:

or

n
∑
i=1

n
∑
i=1

cixi + b (cid:54) 0

cixi + b < 0

where ci, b ∈ R and {xi}i is a ﬁxed set of variables. For example, we can have a
formula of the form:

(x + y (cid:54) 0 ∧ x − 2y < 10) ∨ x > 100

Note that > and (cid:62) can be rewritten into < and (cid:54). Also note that when a
coefﬁcient ci is 0, we simply drop the term cixi, as in the inequality x > 100 above,

CHAPTER 4. LOGICS AND SATISFIABILITY

37

which does not include the variable y. An equality x = 0 can be written as the
conjunction x (cid:62) 0 ∧ x (cid:54) 0. Similarly, a disequality x (cid:54)= 0 can be written as
x < 0 ∨ x > 0.

Models in LRA

As with propositional logic, the free variables fv(F) of a formula F in LRA is the
set of variables appearing in the formula.

An interpretation I of a formula F is an assignment of every free variable to a
real number. An interpretation I is a model of F, i.e., I |= F, iff eval(I(F)) = true.
Here, the extension of the simpliﬁcation rules to LRA formulas is straightforward:
all we need is to add standard rules for evaluating arithmetic inequalities, e.g.,
2 (cid:54) 0 ≡ false.

Example 4.F As an example, consider the following formula:

A model I for F is

F (cid:44) x − y > 0 ∧ x (cid:62) 0

{x (cid:55)→ 1, y (cid:55)→ 0}

Applying I to F, i.e., I(F), results in

1 − 0 > 0 ∧ 1 (cid:62) 0

Applying the evaluation rules, we get true.

(cid:4)

Real vs. Rational

In the literature, you might ﬁnd LRA being referred to as linear rational arithmetic.
There are two interrelated reasons for that: First, whenever we write formulas
in practice, the constants in those formulas are rational values—we can’t really
represent π, for instance, in computer memory. Second, let’s say that F contains
only rational coefﬁcients. Then, it follows that, if F is satisﬁable, there is a model
of F that assigns all free variables to rational values.

CHAPTER 4. LOGICS AND SATISFIABILITY

38

Example 4.G Let’s consider a simple formula like x < 10. While {x (cid:55)→ π} is
a model of x < 10, it also has satisfying assignments that assign x to a rational
constant, like {x (cid:55)→ 1/2}. This will always be the case: we cannot construct
formulas that only have irrational models, unless the formulas themselves contain
irrational constants, e.g., x = π.
(cid:4)

Non-Linear Arithmetic

Deciding satisﬁability of formulas in LRA is an NP-complete problem. If we extend
our theory to allow for polynomial inequalities, then the best known algorithms
are doubly exponential in the size of the formula in the worst case (Caviness and
Johnson, 2012). If we allow for transcedental functions—like exp, cos, log, etc.—
then satisﬁability becomes undecidable (Tarski, 1998). Thus, for all practical pur-
poses, we stick to LRA. Even though it is NP-complete (a term that sends shivers
down the spines of theoreticians), we have very efﬁcient algorithms that can scale
to large formulas.

Connections to MILP

Formulas in LRA, and the SMT problem for LRA, is equivalent to the mixed integer
linear programming (MILP) problem. Just as there are many SMT solvers, there are
many MILP solvers out there, too. So the natural question to ask is why don’t
we use MILP solvers? In short, we can, and maybe sometimes they will actually
be faster than SMT solvers. However, the SMT framework is quite general and
ﬂexible. So not only can we write formulas in LRA, but we can (1) write formulas
in different theories, as well as (2) formulas combining theories.

First, in practice, neural networks do not operate over real or rational arith-
metic. They run using ﬂoating point, ﬁxed point, or machine-integer arithmetic.
If we wish to be as precise as possible at analyzing neural networks, we can opt
for a bit-level encoding of its operations and use bitvector theories employed by
SMT solvers. (Machine arithmetic, surprisingly, is practically more expensive to
solve than linear real arithmetic, so most of the time we opt for a real-arithmetic
encoding of neural networks.)

CHAPTER 4. LOGICS AND SATISFIABILITY

39

Second, as we move forward and neural networks start showing up every-
where, we do not want to verify them in isolation, but in conjunction with other
pieces of code that the neural network interacts with. For example, think of a
piece of code that parses text and puts it in a form ready for the neural network to
consume. Analyzing such piece of code might require using string theories, which
allow us to use string concatenation and other string operations in formulas. SMT
solvers employ theorem-proving techniques for combining theories, and so we can
write formulas, for example, over strings and linear arithmetic.

These are the reasons why in this book we use SMT solvers as the target of our
constraint-based veriﬁcation: they give us many ﬁrst-order theories and allow us
to combine them. However, it is important to note that, at the time of writing this,
most research on constraint-based veriﬁcation focuses on linear real arithmetic
encodings.

Looking Ahead

In the next chapter, we will look at how to encode neural-network semantics, and
correctness properties, as formulas in LRA, thus enabling automated veriﬁcation
using SMT solvers. After that, we will spend some time studying the algorithms
underlying SMT solvers.

In veriﬁcation, we typically use fragments of ﬁrst-order logic to encode pro-
grams. FOL has a long and storied history. FOL is a very general logic, and its
satisﬁability is undecidable, thanks to a proof by Church (1936). SMT solvers,
which have been heavily studied over the past twenty years or so aim at solving
fragments of FOL, like LRA and other theories. I encourage the interested reader to
consult the Handbook of Satisﬁability for an in-depth exposition (Biere et al., 2009).

40

Chapter 5

Encodings of Neural Networks

Our goal in this chapter is to translate a neural network into a formula in lin-
ear real arithmetic (LRA). The idea is to have the formula precisely (or soundly)
capture the input–output relation of the neural network. Once we have such a
formula, we can use it to verify correctness properties using SMT solvers.

5.1 Encoding Nodes

We begin by characterizing a relational view of a neural network. This will help
us establish the correctness of our encoding.

Input-output Relations

Recall that a neural network is represented as a graph G that deﬁnes a function
fG : Rn → Rm. We deﬁne the input–output relation of fG as the binary relation RG
containing every possible input and its corresponding output after executing fG.
Formally, the input–output relation of fG is:

RG = {(a, b) | a ∈ Rn, b = fG(a)}

We will similarly use Rv to deﬁne the input–output relation of the function fv of a
single node v in G.

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

41

Example 5.A Consider the simple function fG(x) = x + 1. Its input–output relation
is

RG = {(a, a + 1) | a ∈ R}

(cid:4)

Encoding a Single Node, Illustrated

We begin by considering the case of a single node v and the associated function
fv : R → R. A node with a single input is illustrated as follows (recall that, by
deﬁnition, a node in our neural network can only produce a single real-valued
output):

v

Say fv(x) = x + 1. Then, we can construct the following formula in LRA to

model the relation Rv = {(a, a + 1) | a ∈ R}:

Fv (cid:44) vo = vin,1 + 1

where vo and vin,1 are real-valued variables. The symbol vo denotes the output of
node v and vin,1 denotes its ﬁrst input (it only has a single input).

Consider the models of Fv; they are all of the form:

{vin,1 (cid:55)→ a, vo (cid:55)→ a + 1}

for any real number a. We can see a clear one-to-one correspondence between
elements of Rv and models of Fv.

Let’s now take a look at a node v with two inputs; assume that fv(x) = x1 +

1.5x2.

v

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

42

The encoding Fv is as follows:

Fv (cid:44) vo = vin,1 + 1.5vin,2

Observe how the elements of the input vector, x1 and x2, correspond to the two
real-valued variables vin,1 and vin,2.

Encoding a Single Node, Formalized

Now that we have seen a couple of examples, let’s formalize the process of en-
coding the operation fv of some node v. We will assume that fv : Rnv → R is
piecewise-linear, i.e., of the form

f (x) =





∑j c1

∑j cl

j · xj + b1
...
j · xj + bl

if S1

if Sl

where j ranges from 1 to nv. We will additionally assume that each condition Si is
deﬁned as a formula in LRA over the elements of the input x. Now, the encoding
is as follows:

(cid:34)

l
(cid:94)

(cid:32)

Fv (cid:44)

Si ⇒

vo =

j · vin,j + bi
ci

i=1

j=1

nv∑

(cid:33)(cid:35)

The way to think of this encoding is as a combination of if statements: if Si is true,
then vo is equal to the ith inequality. The implication (⇒) gives us the ability to
model a conditional, where the left side of the implication is the condition and the
right side is the assignment. The big conjunction on the outside of the formula,
(cid:86)l
i=1, essentially combines if statements: “if S1 then ... AND if S2 then ... AND if S3

...”

Example 5.B The above encoding is way too general with too many superscripts
and subscripts. Here’s a simple and practical example, the ReLU function:

relu(x) =

(cid:40)

x

0

if x > 0
if x (cid:54) 0

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

A node v such that fv is ReLU would be encoded as follows:

Fv (cid:44) (vin,1 > 0
(cid:124) (cid:123)(cid:122) (cid:125)
x>0

⇒ vo = vin,1) ∧ (vin,1 (cid:54) 0
(cid:124) (cid:123)(cid:122) (cid:125)
x(cid:54)0

⇒ vo = 0)

43

(cid:4)

Soundness and Completeness

The above encoding precisely captures the semantics of a piecewise-linear node.
Let’s formally capture this fact: Fix some node v with a piecewise-linear function
fv. Let Fv be its encoding, as deﬁned above.

First, our encoding is sound: any execution of the node is captured by a model
of the formula Fv. Informally, soundness means that our encoding does not miss
any behavior of fv. Formally, let (a, b) ∈ Rv and let

I = {vin,1 (cid:55)→ a1, . . . , vin,n (cid:55)→ an, vo (cid:55)→ b}

Then, I |= Fv.

Second, our encoding is complete: any model of Fv maps to a behavior of fv.
Informally, completeness means that our encoding is tight, or does not introduce
new behaviors not exhibited by fv. Formally, let the following be a model of Fv:

I = {vin,1 (cid:55)→ a1, . . . , vin,n (cid:55)→ an, vo (cid:55)→ b}

Then, (a, b) ∈ Rv.

5.2 Encoding a Neural Network

We have shown how to encode a single node of a neural network. We’re now
ready to encode the full-blown graph. The encoding is broken up into two pieces:
(1) a formula encoding semantics of all nodes, and (2) a formula encoding the
connections between them, i.e., the edges.

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

44

Encoding the Nodes

Recall that a neural network is a graph G = (V, E), where the set of nodes V
contains input nodes Vin, which do not perform any operations. The following
formula combines the encodings of all non-input nodes in G:

FV (cid:44) (cid:94)

Fv

v∈V\Vin

Again, the big conjunction essentially says, “the output of v1 is ... AND the output of
node v2 is ... AND ...” This formula, however, is meaningless on its own: it simply
encodes the input–output relation of every node, but not the connections between
them!

Encoding the Edges

Let’s now encode the edges. We will do this for every node individually, encoding
all of its incoming edges. Fix some node v ∈ V \ Vin. Let (v1, v), . . . (vn, v) be
an ordered sequence of all edges whose target is v. Recall that in Section 2.5,
we assumed that there is a total ordering on edges. The reason for this ordering
is to be able to know which incoming edges feed into which inputs of a node.
Informally, the edge relation E gives us a bunch of wires to be plugged into node
v; the ordering tells us where to plug those wires—the ﬁrst wire in the ﬁrst socket,
the second wire in the second socket, and so on.
We can now deﬁne a formula for edges of v:

F◦→v (cid:44)

n
(cid:94)

i=1

vin,i = vo
i

Intuitively, for each edge (vi, v), we connect the output of node vi with the ith
input of v. We can now deﬁne FE as the conjunction of all incoming edges of all
non-input nodes:

FE (cid:44) (cid:94)

F◦→v

v∈V\Vin

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

45

Putting it all Together

Now that we have shown how to encode nodes and edges, there is nothing left to
encode! So let’s put things together. Given a graph G = (V, E), we will deﬁne its
encoding as follows:

FG

(cid:44) FV ∧ FE

Just as for the single-node encoding, we get soundness and completeness. Let
RG be the input–output relation of G. Soundness means that FG does not miss any
of the behaviors in RG. Completeness means that every model of FG maps to an
input–output behavior of G.

Note that the size of the encoding is linear in the size of the neural network
(number of nodes and edges). Simply, each node gets a formula and each edge
gets a formula. The formula of node v is of size linear in the size of the (piecewise)
linear function fv.

Correctness of the Encoding

Assume we that have the following ordered input nodes in Vin

and the following output nodes in Vo

v1, . . . , vn

vn+1, . . . , vn+m

Our encoding is sound and complete. First, let’s state soundness: Let (a, b) ∈

RG and let

I = {vo

1 (cid:55)→ a1, . . . , vo

n (cid:55)→ an} ∪ {vo

n+1 (cid:55)→ b1, . . . , vo

n+m (cid:55)→ bm}

Then, there exists I(cid:48) such that I ∪ I(cid:48) |= FG.

Notice that, unlike the single-node setting, the model of FG not only con-
tains assignments to inputs and outputs of the network, but also the intermediate
nodes. This is taken care of using I, which assigns values to the outputs of input
and output nodes, and I(cid:48), which assigns the inputs and outputs of all nodes and
therefore its domain does not overlap with I.

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

46

Similarly, completeness is stated as follows: Let the following be a model of

FG:

I = {vo

1 (cid:55)→ a1, . . . , vo

n (cid:55)→ an} ∪ {vo

n+1 (cid:55)→ b1, . . . , vo

n+m (cid:55)→ bm} ∪ I(cid:48)

Then, (a, b) ∈ RG.

An Example Network and its Encoding

Enough abstract mathematics. Let’s look at a concrete example neural network G.

v1

v2

v3

v4

Assume that fv3(x) = 2x1 + x2 and fv4(x) = relu(x).

We begin by constructing formulas for non-input nodes:

Fv3
Fv4

(cid:44) vo
(cid:44) (vin,1

3 + vin,2
3 = 2vin,1
4 > 0 =⇒ vo

3

4 = vin,1

4

) ∧ (vin,1

4

(cid:54) 0 =⇒ vo

4 = 0)

Next, we construct edge formulas:

F◦→v3
F◦→v4

(cid:44) (vin,1
(cid:44) vin,1

3 = vo
4 = vo

3

1 ) ∧ (vin,2

3 = vo
2 )

Finally, we conjoin all of the above formulas to arrive at the complete encoding of
G:

FG

(cid:44) Fv3 ∧ Fv4
(cid:123)(cid:122)
(cid:125)
FV

(cid:124)

∧ F◦→v3 ∧ F◦→v4
(cid:123)(cid:122)
(cid:125)
FE

(cid:124)

5.3 Handling Non-linear Activations

In the above, we have assumed that all of our nodes are associated with piecewise-
linear functions, allowing us to precisely capture their semantics in linear real
arithmetic. How can we handle non-piecewise-linear activations, like sigmoid

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

47

1

0.5

−4 −2

2

4

Figure 5.1 Sigmoid function

and tanh? One way to encode them is by overapproximating their behavior, which
gives us soundness but not completeness. As we will see, soundness means that
our encoding can ﬁnd proofs of correctness properties, and completeness means
that our encoding can ﬁnd counterexamples to correctness properties. So, by over-
approximating an activation function, we give up on counterexamples.

Handling Sigmoid

Let’s begin with the concrete example of the sigmoid activation:

σ(x) =

1
1 + exp(−x)

which is shown in Figure 5.1. The sigmoid function is (strictly) monotonically
increasing, so if we have two points a1 < a2, we know that σ(a1) < σ(a2). We can
as a result overapproximate the behavior of σ by saying: for any input between a1
and a2, the output of the function can be any value between σ(a1) and σ(a2).

Consider Figure 5.2. Here we picked three points on the sigmoid curve, shown
in red, with x coordinates −1, 0, and 1. The red rectangles deﬁne the lower and
upper bound on the output of the sigmoid function between two values of x.
For example, for inputs between 0 and 1, the output of the function is any value
between 0.5 and 0.73. For inputs more than 1, we know that the output must be
between 0.73 to 1 (the range of σ is upper bounded by 1).

Say for some node v, fv is a sigmoid activation. Then, one possible encoding,

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

48

following the approximation in Figure 5.2, is as follows:

Fv (cid:44) (vin,1 (cid:54) −1 =⇒ 0 < vo (cid:54) 0.26)

∧ (−1 < vin,1 (cid:54) 0 =⇒ 0.26 < vo (cid:54) 0.5)
∧ (0 < vin,1 (cid:54) 1 =⇒ 0.5 < vo (cid:54) 0.73)
∧ (vin,1 > 1 =⇒ 0.73 < vo < 1)

Each conjunct speciﬁes a range of inputs (left of implication) and the possible out-
puts in that range (right of implication). For example, the ﬁrst conjunct speciﬁes
that, for inputs (cid:54) −1, the output can be any value between 0 and 0.26.

Handling any Monotonic Function

We can generalize the above process to any monotonically (increasing or decreas-
ing) function fv.

Let’s assume that fv is monotonically increasing. We can pick a sequence of

real values c1 < · · · < cn. Then, we can construct the following encoding:

Fv (cid:44) (vin,1 (cid:54) c1 =⇒ lb < vo (cid:54) fv(c1))

∧ (c1 < vin,1 (cid:54) c2 =⇒ fv(c1) < vo (cid:54) fv(c2))
...
∧ (cn < vin,1 =⇒ fv(cn) < vo (cid:54) ub)

where lb and ub are the lower and upper bounds of the range of fv; for example,
for sigmoid, they are 0 and 1, respectively. If a function is unbounded, then we
can drop the constraints lb (cid:54) vo and vo (cid:54) ub.

The more points ci we choose and the closer they are to each other, the better
our approximation is. This encoding is sound but incomplete, because it captures
more behaviors than conceivable from the activation function.
In our sigmoid
example, for input (cid:62) 1, the encoding says that the output of the sigmoid is any
value between 0.73 and 1, as indicated by the right-most shaded area in Figure 5.2.

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

49

1

σ(x)

(1, 0.73)

(0, 0.5)

(−1, 0.26)

−10 −8 −6 −4 −2

2

4

6

8

10

Figure 5.2 Sigmoid function with overapproximation

x

5.4 Encoding Correctness Properties

Now that we have shown how to encode the semantics of neural networks as
logical constraints, we’re ready for the main dish: encoding entire correctness
properties.

Checking Robustness Example

We begin with a concrete example before seeing the general form. Say we have a
neural network G deﬁning a binary classiﬁer fG : Rn → R2. The neural network
fG takes a grayscale image as a vector of reals, between 0 and 1, describing the in-
tensity of each pixel (black to white), and predicts whether the image is of a cat or
a dog. Say we have an image c that is correctly classiﬁed as cat. We want to prove
that a small perturbation to the brightness of c does not change the prediction. We
formalize this as follows:

{ |x − c| (cid:54) 0.1 }
r ← fG(x)
{ r1 > r2 }

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

50

where the ﬁrst output, r1, is the probability of cat, while r2 is the probability of dog.
The high-level intuition for the encoding of this correctness property follows
how the property is written. The formula that we generate to check this statement,
called the veriﬁcation condition (VC), looks roughly like this:

(precondition ∧ neural network) =⇒ postcondition

If this formula is valid, then the correctness property holds.

Let’s assume for our example that the input nodes of the neural network are
{v1, . . . , vn} and the output nodes are {vn+1, vn+2}. Assume also that the formula
FG encodes the network, as described earlier in this chapter. We encode the cor-
rectness property as follows:

(cid:32) n
(cid:94)

i=1

(cid:124)

|xi − ci| (cid:54) 0.1

(cid:123)(cid:122)
precondition

(cid:33)

(cid:125)

∧

∧ FG
(cid:124)(cid:123)(cid:122)(cid:125)
network

(cid:32) n
(cid:94)

(cid:33)

xi = vo
i

(cid:124)

i=1

(cid:123)(cid:122)
network input

(cid:125)

=⇒ r1 > r2
(cid:124) (cid:123)(cid:122) (cid:125)
postcondition

Here’s the breakdown:

∧ (cid:0)r1 = vo

n+1 ∧ r2 = vo
(cid:123)(cid:122)
network output

n+2

(cid:1)

(cid:125)

(cid:124)

• The precondition is directly translated to an LRA formula. Since LRA for-
mulas don’t natively support vector operations, we decompose the vector
into its constituent scalars. Note that the absolute-value operation | · | is
not present natively in LRA, but, fear not, it is actually encodable: A lin-
ear inequality with absolute value, like |x| (cid:54) 5, can be written in LRA as
x (cid:54) 5 ∧ −x (cid:54) 5.

• The network is encoded as a formula FG, just as we saw earlier in Section 5.2.
The trick is that we now also need to connect the variables of FG with the
inputs x and output r. This is captured by the two subformulas labeled
“network input” and “network output”.

• The postcondition is encoded as is.

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

51

Encoding Correctness, Formalized

A correctness property is of the form

{ P }
r1 ← fG1
(x1)
r2 ← fG2(x2)
...
rl ← fGl
{ Q }

(xl)

While most examples we see in the book involve a single neural network fG, recall
from Chapter 3 that our properties allow us to consider a collection of networks.
Assume that the precondition and postcondition are encodable in LRA. We

then encode the veriﬁcation condition as follows:

(cid:32)

P ∧

l
(cid:94)

i=1

(cid:33)

Fi

=⇒ Q

where Fi is the encoding of the ith assignment ri ← fGi (xi). The assignment en-
coding Fi combines the encoding of the neural network FGi along with connections
with inputs and outputs, xi and ri, respectively:

Fi

(cid:44) FGi ∧





n
(cid:94)

j=1

xi,j = vo
i


 ∧





m
(cid:94)

j=1



ri,j = vo

n+j



Here we make two assumptions:

• The input and output variables of the encoding of Gi are v1, . . . , vn and

vn+1, . . . , vn+m, respectively.

• Each graph Gi has unique nodes and therefore input–output variables.

Informally, we can think of our encoding,

=⇒ Q, as saying the
following: “if the precondition is true AND we execute all l networks, then the postcon-
dition should be true”

i=1 Fi

(cid:16)

P ∧ (cid:86)l

(cid:17)

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

52

Soundness and Completeness

Say we have a correctness property that we have encoded as a formula F. Then,
we have the following soundness guarantee: If F is valid, then the correctness
property is true.

Completeness depends on whether all functions are encodable in LRA. Assum-
ing all functions are encodable in LRA, then, if F is invalid, we know that there is a
model I |= ¬F. This model is a counterexample to the correctness property. From
this model, we can read values for the input variables that result in outputs that
do not satisfy the postcondition. This is best seen through an example:

Example 5.C Take the following simple correctness property, where f (x) = x:

{ |x − 1| (cid:54) 0.1 }
r ← f (x)
{ r (cid:62) 1 }

This property is not true. Let x = 0.99; this satisﬁes the precondition. But,
f (0.99) = 0.99, which is less than 1. If we encode a formula F for this property,
then we will have a model I |= ¬F such that x is assigned 0.99.
(cid:4)

Looking Ahead

Ahh, this chapter was tiring! Thanks for sticking around. We have taken neural
networks, with all their glory, and translated them into formulas. In the coming
chapters, we will study algorithms for checking satisﬁability of these formulas.

To my knowledge, the ﬁrst encoding of neural networks as constraints for ver-
iﬁcation is due to Pulina and Tacchella (2010), predating the current explosion in
interest. Bastani et al. (2016) were the ﬁrst to view robustness veriﬁcation as a
constraint-solving problem.

Our encoding of sigmoid follows that of Ehlers (2017). A number of papers
have considered MILP encodings that are similar to the ones we give (Tjeng et al.,
2019a). In MILP, we don’t have disjunction, so we simulate disjunction with an

CHAPTER 5. ENCODINGS OF NEURAL NETWORKS

53

integer that can take the values {0, 1}. The main issue with LRA and MILP encod-
ings is disjunction; with no disjunction, the problem is polynomial-time solvable.
Disjunctions mostly arise due to ReLUs. We say that a ReLU is active if its out-
put is > 0 and inactive otherwise. If a ReLU is active or inactive for all possible
inputs to the neural network, as prescribed by the precondition, then we can get
rid of the disjunction, and treat it as the function f (x) = 0 (inactive) or f (x) = x
(active). With this idea in mind, there are two tricks to simplify veriﬁcation: (1)
We can come up with lightweight techniques to discover which ReLUs are ac-
tive or inactive. The abstraction-based veriﬁcation techniques discussed in Part
(2) Typically, when we train neural networks, we
III of the book can be used.
aim to maximize accuracy on some training data; we can additionally bias the
training towards neural networks where most of the ReLUs are always active/in-
active (Tjeng et al., 2019b).

In practice, neural networks are implemented using ﬁnite-precision arithmetic,
where real numbers are approximated as ﬂoating-point numbers, ﬁxed-point num-
bers, or even machine integers. Some papers carefully ensure that veriﬁcation re-
sults hold for a ﬂoating-point implementation of the network (Katz et al., 2017).
A recent paper has shown that veriﬁed neural networks in LRA may not really be
robust when one considers the bit-level behavior (Jia and Rinard, 2020b). A num-
ber of papers have also considered bit-level veriﬁcation of neural networks, using
propositional logic instead of LRA (Jia and Rinard, 2020a; Narodytska et al., 2018).

54

Chapter 6

DPLL Modulo Theories

In the previous chapter, we saw how to reduce the veriﬁcation problem to that
of checking satisﬁability of a logical formula. But how do we actually check sat-
isﬁability? In this chapter, we will meet the DPLL (Davis–Putnam–Logemann–
Loveland) algorithm, which was developed decades ago for checking satisﬁabil-
ity of Boolean formulas. Then we will see an extension of DPLL that can handle
ﬁrst-order formulas over theories.

These algorithms underlie modern SAT and SMT solvers. I’ll give a complete
description of DPLL in the sense that you can follow the chapter and implement a
working algorithm. But note that there are numerous data structures, implemen-
tation tricks, and heuristics that make DPLL really work in practice, and we will not
cover those here. (At the end of the chapter, I point you to additional resources.)

6.1 Conjunctive Normal Form (CNF)

The goal of DPLL is to take a Boolean formula F and decide whether it is SAT or
UNSAT. In case F is SAT, DPLL should also return a model of F. We begin by talking
about the shape of the formula DPLL expects as input.

DPLL expects formulas to be in conjunctive normal form (CNF). Luckily, all
Boolean formulas can be rewritten into equivalent formulas in CNF. (We will see
how later in this chapter.) A formula F in CNF is of the following form:

C1 ∧ · · · ∧ Cn

CHAPTER 6. DPLL MODULO THEORIES

55

where each subformula Ci is called a clause and is of the form

(cid:96)1 ∨ · · · ∨ (cid:96)mi

where each (cid:96)i is called a literal and is either a variable, e.g., p, or its negation, e.g.,
¬p.

Example 6.A The following is a CNF formula with two clauses, each of which
contains two literals:

(p ∨ ¬r) ∧ (¬p ∨ q)

The following formula is not in CNF:

(p ∧ q) ∨ (¬r)

(cid:4)

6.2 The DPLL Algorithm

The completely naïve way to decide satisﬁability is by trying every possible in-
terpretation and checking if it is a model of the formula. Of course, there are ex-
ponentially many interpretations in the number of variables. DPLL tries to avoid
doing a completely blind search, but, on a bad day, it will just devolve into an
exponential enumeration of all possible interpretations—after all, satisﬁability is
the canonical NP-complete problem.

DPLL alternates between two phases: deduction and search. Deduction tries
to simplify the formula using the laws of logic. Search simply searches for an
interpretation.

Deduction

The deduction part of DPLL is called Boolean constant propagation (BCP). Imagine
that you have the following formula in CNF:

((cid:96)) ∧ C2 ∧ · · · Cn

CHAPTER 6. DPLL MODULO THEORIES

56

Notice that the ﬁrst clause consists of a single literal—we call it a unit clause.
Clearly, any model of this formula must assign (cid:96) the value true: Speciﬁcally, if
(cid:96) is a variable p, then p must be assigned true; if (cid:96) is a negation ¬p, then p must be
assigned false.

The BCP phase of DPLL will simply look for all unit clauses and replace their
literals with true. BCP is enabled by the fact that formulas are in CNF, and it can be
quite effective at proving SAT or UNSAT.

Example 6.B Consider the following formula

F (cid:44) (p) ∧ (¬p ∨ r) ∧ (¬r ∨ q)

BCP ﬁrst ﬁnds the unit clause (p) and assigns p the value true. This results in the
following formula:

(true) ∧ (¬true ∨ r) ∧ (¬r ∨ q)

≡ (r) ∧ (¬r ∨ q)

Clearly BCP’s job is not done: the simpliﬁcation has produced another unit clause,
(r). BCP sets r to true, resulting in the following formula:

(true) ∧ (¬true ∨ q)

≡ (q)

Finally, we’re left with a single clause, the unit clause (q). Therefore BCP assigns
q the value true, resulting in the ﬁnal formula true. This means that F is SAT, and
{p (cid:55)→ true, q (cid:55)→ true, r (cid:55)→ true} is a model.
(cid:4)

In the above example, BCP managed to show satisﬁability of F. Note that BCP
can also prove unsatisﬁability, by simplifying the formula to false. But BCP might
get stuck if it cannot ﬁnd unit clauses. This is where search is employed by DPLL.

Deduction + Search

Algorithm 1 shows the entire DPLL algorithm. The algorithm takes a formula F in
CNF.

CHAPTER 6. DPLL MODULO THEORIES

57

Algorithm 1: DPLL

Data: A formula F in CNF form
Result: I |= F or UNSAT

(cid:46) Boolean constant propagation (BCP)
while there is a unit clause ((cid:96)) in F do

Let F be F[(cid:96) (cid:55)→ true]

if F is true then return SAT

(cid:46) Search
for every variable p in F do

If DPLL(F[p (cid:55)→ true]) is SAT then return SAT
If DPLL(F[p (cid:55)→ false]) is SAT then return SAT

return UNSAT

(cid:46) The model I that is returned by DPLL when the result is SAT is
maintained implicitly in the sequence of assignments to variables (of the
form [l (cid:55)→ ·] and [p (cid:55)→ ·])

The ﬁrst part performs BCP: the algorithm keeps simplifying the formula until
no more unit clauses exist. We use the notation F[(cid:96) (cid:55)→ true] to mean replace all
occurrences of (cid:96) in F with true and simplify the resulting formula. Speciﬁcally, if
(cid:96) is a variable p, then all occurrences of p are replaced with true; if (cid:96) is a negation
¬p, then all occurrences of p are replaced with false.

After BCP is done, the algorithm checks if the formula is true, which means that

BCP has proven that the formula is SAT.

If BCP is unsuccessful in proving SAT, then DPLL moves to the search phase:
it iteratively chooses variables and tries to replace them with true or false, calling
DPLL recursively on the resulting formula. The order in which variables are cho-
sen in search is critical to DPLL’s performance. There is a lot of research on variable
selection. One of the popular heuristics maintains a continuously updated score
sheet, where variables with higher scores are chosen ﬁrst.

Algorithm 1, as presented, returns SAT when the input formula is satisﬁable,
but does not return a model. The model I that is returned by DPLL when the result

CHAPTER 6. DPLL MODULO THEORIES

58

is SAT is maintained implicitly in the sequence of assignments to variables (of the
form [l (cid:55)→ ·] and [p (cid:55)→ ·]) made by BCP and search that led to SAT being returned.
The algorithm returns UNSAT when it has exhausted all possible satisfying assign-
ments.

Example 6.C Consider the following formula given to DPLL

F (cid:44) (p ∨ r) ∧ (¬p ∨ q) ∧ (¬q ∨ ¬r)

First level of recursion DPLL begins by attempting BCP, which cannot ﬁnd any
unit clauses. Then, it proceeds to search. Suppose that search chooses variable
p, setting it to true by invoking DPLL recursively on

F1 = F[p (cid:55)→ true] = q ∧ (¬q ∨ ¬r)

Second level of recursion Next, DPLL attempts BCP on F1. First, it sets q to true,
resulting in the formula

F2 = F1[q (cid:55)→ true] = (¬r)

Then, it sets r to false, resulting in

F3 = F2[r (cid:55)→ false] = true

Since F3 is true, DPLL returns SAT. Implicitly, DPLL has built up a model of F:

{p (cid:55)→ true, q (cid:55)→ true, r (cid:55)→ false}

(cid:4)

Partial Models

Note that DPLL may terminate with SAT but without assigning every variable in
the formula. We call the resulting model a partial model. You can take a partial
model and extend it by assigning the remaining variables in any way you like, and
you’ll still have a model of the formula.

CHAPTER 6. DPLL MODULO THEORIES

59

Example 6.D Consider this simple formula:

F (cid:44) p ∧ (q ∨ p ∨ ¬r) ∧ (p ∨ ¬q)

The ﬁrst thing DPLL will do is apply BCP, which will set the unit clause p to true.
The rest of the formula then simpliﬁes to true. This means that q and r are useless
variables—give them any interpretation and you’ll end up with a model, as long
as p is assigned true. Therefore, we call I = {p → true} a partial model of F.
Formally, eval(I(F)) = true.
(cid:4)

6.3 DPLL Modulo Theories

We have seen how DPLL can decide satisﬁability of Boolean formulas. We now
present DPLL modulo theories, or DPLLT, an extension of DPLL that can handle for-
mulas over, for example, arithmetic theories like LRA. The key idea of DPLLT is to
start by treating a formula as if it is completely Boolean, and then incrementally
add more and more theory information until we can conclusively say that a for-
mula is SAT or UNSAT. We begin by deﬁning the notion of Boolean abstraction of
a formula.

Boolean Abstraction

For illustration, we assume that we’re dealing with formulas in LRA, as with the
previous chapters. Say we have the following formula in LRA:

F (cid:44) (x (cid:54) 0 ∨ x (cid:54) 10) ∧ (¬x (cid:54) 0)

The Boolean abstraction of F, denoted FB, is the formula where every unique linear
inequality in the formula is replaced with a special Boolean variable, as follows:

FB (cid:44) (p ∨ q) ∧ (¬p)

The inequality x (cid:54) 0 is abstracted as p and x (cid:54) 10 is abstracted as q. Note that
both occurrences of x (cid:54) 0 are replaced by the same Boolean variable, though this

CHAPTER 6. DPLL MODULO THEORIES

60

Algorithm 2: DPLLT

Data: A formula F in CNF form over theory T
Result: I |= F or UNSAT

Let FB be the abstraction of F
while true do

If DPLL(FB) is UNSAT then return UNSAT
Let I be the model returned by DPLL(FB)
Assume I is represented as a formula
if IT is satisﬁable (using a theory solver) then

return SAT and the model returned by theory solver

else

Let FB be FB ∧ ¬I

need not be the case for the correctness of our exposition. We will also use the
superscript T to map Boolean formulas back to theory formulas, e.g., (FB)T is F.

We call this process abstraction because constraints are lost in the process; namely,

the relation between different inequalities is obliterated. Formally speaking, if FB
is UNSAT, then F is UNSAT. But the converse does not hold: if FB is SAT, it does
not mean that F is SAT.

Example 6.E Consider the formula F (cid:44) x (cid:54) 0 ∧ x (cid:62) 10. This formula is clearly
UNSAT. However, its abstraction, p ∧ q, is SAT.
(cid:4)

Lazy DPLL Modulo Theories

The DPLLT algorithm takes a formula F, over some theory like LRA, and decides
satisﬁability. DPLLT assumes access to a theory solver. The theory solver takes
a conjunction of, for example, linear inequalities, and checks their satisﬁability.
In a sense, the theory solver takes care of conjunctions and the DPLL algorithm
takes care of disjunctions. In the case of LRA, the theory solver can be the Simplex
algorithm, which we will see in the next chapter.

DPLLT, shown in Algorithm 2, works as follows: First, using vanilla DPLL, it
checks if the abstraction FB is UNSAT, in which case it can declare that F is UNSAT,

CHAPTER 6. DPLL MODULO THEORIES

61

following the properties of abstraction discussed above. The tricky part comes
when dealing with the case where FB is SAT, because that does not necessarily
mean that F is SAT. This is where the theory solver comes into play. We take
the model I returned by DPLL(FB) and map it to a formula IT in the theory; for
example, if the theory we’re working with is LRA, IT is a conjunction of linear
If the theory solver deems IT satisﬁable, then we know that F is
inequalities.
satisﬁable and we’re done. Otherwise, DPLLT learns the fact that I is not a model.
So it negates I and conjoins it to FB. In a sense, the DPLLT lazily learns more and
more facts, reﬁning the abstraction, until it can decide SAT or UNSAT.

Example 6.F Consider the following LRA formula F:

and its abstraction FB:

x (cid:62) 10 ∧ (x < 0 ∨ y (cid:62) 0)

p ∧ (q ∨ r)

where p denotes x (cid:62) 10, q denotes x < 0, and r denotes y (cid:62) 0.

First iteration DPLLT begins by invoking DPLL on FB. Suppose DPLL returns
the partial model

I1 = {p (cid:55)→ true, q (cid:55)→ true}

We will represent I1 as a formula

p ∧ q

Next, we check if I1 is indeed a model of F. We do so by invoking the theory
solver on IT

1 , which is

x (cid:62) 10
(cid:124) (cid:123)(cid:122) (cid:125)
p

∧ x < 0
(cid:124) (cid:123)(cid:122) (cid:125)
q

1 is UNSAT, because x cannot be (cid:62) 10 and < 0.
The theory solver will say that IT
Therefore, DPLLT blocks this model by conjoining ¬I1 to FB. This makes FB the
following formula, which is still in CNF, because ¬I1 is a clause:

p ∧ (q ∨ r) ∧ (¬p ∨ ¬q)
(cid:125)

(cid:124)

(cid:123)(cid:122)
¬I1

CHAPTER 6. DPLL MODULO THEORIES

62

In other words, we’re saying that we cannot have a model that sets both x (cid:62) 10
and x < 0 to true.

Second iteration In the second iteration, DPLLT invokes DPLL on the updated
FB. DPLL cannot give us the same model I1. So it gives us another one, say
I2 = p ∧ ¬q ∧ r. The theory solver checks IT
2 , which is satisﬁable, and returns
its own theory-speciﬁc model, e.g., {x (cid:55)→ 10, y (cid:55)→ 0}. We’re now done, and we
return the model.

(cid:4)

6.4 Tseitin’s Transformation

We’ve so far assumed that formulas are in CNF. Indeed, we can take any formula
and turn it into an equivalent formula in CNF. We can do this by applying De-
Morgan’s laws (see Chapter 4), by distributing disjunction over conjunction. For
example, r ∨ (p ∧ q) can be rewritten into the equivalent (r ∨ p) ∧ (r ∨ q). This
transformation, unfortunately, can lead to an exponential explosion in the size
It turns out that there’s a simple technique, known as Tseitin’s
of the formula.
transformation, that produces a formula of size linear in the size of the non-CNF
formula.

Tseitin’s transformation takes a formula F and produces a CNF formula F(cid:48). The
set of variables of F is a subset of the variables in F(cid:48); i.e., Tseitin’s transformation
creates new variables. Tseitin’s transformation guarantees the following proper-
ties:

1. Any model of F(cid:48) is also a model of F, if we disregard the interpretations of

newly added variables.

2. If F(cid:48) is UNSAT, then F is UNSAT.

Therefore, given a non-CNF formula F, to check its satisﬁability, we can simply
invoke DPLL on F(cid:48).

CHAPTER 6. DPLL MODULO THEORIES

63

Intuition

Tseitin’s transformation is pretty much the same as rewriting a complex arithmetic
expression in a program into a sequence of instructions where every instruction is
an application of a single unary or binary operator. (This is roughly what a com-
piler does when compiling a high-level program to assembly or an intermediate
representation.) For example, consider the following function (in Python syntax):

def f(x,y,z):

return x + (2*y + 3)

The return expression can be rewritten into a sequence of operations, each oper-
ating on one or two variables, as follows, where t1, t2, and t3 are temporary
variables:

def f(x,y,z):
t1 = 2 * y
t2 = t1 + 3
t3 = x + t2
return t3

Intuitively, each subexpression is computed and stored in a temporary variable:
t1 contains the expression 2*y, t2 contains the expression 2*y + 3, and t3 con-
tains the entire expression x + (2*y + 3).

Tseitin Step 1: NNF

The ﬁrst thing that Tseitin’s transformation does is to push negation inwards so
that ¬ only appears next to variables. E.g., ¬(p ∧ r) is rewritten into ¬p ∨ ¬r. This
is known as negation normal form (NNF). Any formula can be easily translated to
NNF by repeatedly applying the following rewrite rules until we can’t rewrite the
formula any further:

¬(F1 ∧ F2) → ¬F1 ∨ ¬F2
¬(F1 ∨ F2) → ¬F1 ∧ ¬F2

¬¬F → F

CHAPTER 6. DPLL MODULO THEORIES

64

In other words, (sub)formulas that match the patterns on the left of → are trans-
formed into the patterns on the right. In what follows we assume formulas are in
NNF.

Tseitin Step 2: Subformula Rewriting

We deﬁne a subformula of F to be any subformula that contains a disjunction or
conjunction—i.e., we don’t consider subformulas at the level of literals.

Example 6.G The following formula F is decomposed into 4 subformulas:

F (cid:44) (p ∧ q)
(cid:124) (cid:123)(cid:122) (cid:125)
F1

∨ (q ∧ ¬r
(cid:124) (cid:123)(cid:122) (cid:125)
F2

(cid:124)

(cid:123)(cid:122)
F3

(cid:124)

(cid:123)(cid:122)
F4

∧s)

(cid:125)

(cid:125)

F1 and F2 are in the deepest level of nesting, while F4 is in the shallowest level.
(cid:4)
Notice that F2 is a subformula of F3, and all of Fi are subformulas of F4.

Now for the transformation steps, we assume that F has n subformulas:

1. For every subformula Fi of F, create a fresh variable ti. These variables are
analogous to the temporary variables ti we introduced to our Python program above.

2. Next, starting with those most-deeply nested subformula, for every subfor-
i, where ◦
i may be the

mula Fi, create the following formula: Let Fi be of the form (cid:96)i ◦ (cid:96)(cid:48)
is ∧ or ∨ and (cid:96)i, (cid:96)(cid:48)
new variable tj denoting a subformula Fj of Fi. Create the formula

i are literals. Note that one or both of (cid:96)i and (cid:96)(cid:48)

F(cid:48)
i

(cid:44) ti ⇔ ((cid:96)i ◦ (cid:96)(cid:48)
i)

These formulas are analogous to the assignments to the temporary variables in our
Python program above, where ⇔ is the logical analogue of variable assignment (=).

Example 6.H Continuing our running example, for subformula F1, we create the
formula

F(cid:48)
1

(cid:44) t1 ⇔ (p ∧ q)

CHAPTER 6. DPLL MODULO THEORIES

65

For subformula F2, we create

For subformula F3, we create

F(cid:48)
2

(cid:44) t2 ⇔ (q ∧ ¬r)

F(cid:48)
3

(cid:44) t3 ⇔ (t2 ∧ s)

(Notice that q ∧ ¬r is replaced by the variable t2.) Finally, for subformula F4, we
create

F(cid:48)
4

(cid:44) t4 ⇔ (t1 ∨ t3)

(cid:4)

Notice that each F(cid:48)

i can be written in CNF. This is because, following the deﬁ-

nition of ⇔ and DeMorgan’s laws, we have:

(cid:96)1 ⇔ ((cid:96)2 ∨ (cid:96)3) ≡ (¬(cid:96)1 ∨ (cid:96)2 ∨ (cid:96)3) ∧ ((cid:96)1 ∨ ¬(cid:96)2) ∧ ((cid:96)1 ∨ ¬(cid:96)3)

and

(cid:96)1 ⇔ ((cid:96)2 ∧ (cid:96)3) ≡ (¬(cid:96)1 ∨ (cid:96)2) ∧ (¬(cid:96)1 ∨ (cid:96)3) ∧ ((cid:96)1 ∨ ¬(cid:96)2 ∨ ¬(cid:96)3)

Finally, we construct the following CNF formula:

F(cid:48) (cid:44) tn ∧ (cid:94)

F(cid:48)
i

i

By construction, in any model of F(cid:48), each ti is assigned true if and only if the
subformula Fi evaluates to true. Therefore, the constraint tn in F’ says that F must
be true. You can think of tn as the return statement in our transformed Python program.

Example 6.I Continuing our running example, we ﬁnally construct the CNF for-
mula

F(cid:48) (cid:44) t4 ∧ F(cid:48)

1 ∧ F(cid:48)

2 ∧ F(cid:48)

3 ∧ F(cid:48)

4

Since all of the formulas F(cid:48)
(cid:4)

i can be written in CNF, as described above, F(cid:48) is in CNF.

At this point, we’re done. Given a formula in some theory and a theory solver,
we ﬁrst rewrite the formula in CNF, using Tseitin’s tranformation, and invoke
DPLLT.

CHAPTER 6. DPLL MODULO THEORIES

66

Looking Ahead

I gave a trimmed down version of DPLLT. A key idea in modern SAT and SMT
solvers is conﬂict-driven clause learning, a graph data structure that helps us cut
the search space by identifying sets of interpretations that do not make satisfying
assignments. I encourage the interested reader to consult Biere et al. (2009) for a
detailed exposition of clause learning and other ideas.

I also encourage you to play with popular SAT and SMT solvers. For example,
MiniSAT (Een, 2005), as the name suggests, has a small and readable codebase.
For SMT solvers, I recommend checking out Z3 (de Moura and Bjørner, 2008) and
CVC4 (Barrett et al., 2011). One of the interesting ideas underlying SMT solvers is
theory combination (Nelson and Oppen, 1979), where you can solve formulas com-
bining different theories. This is useful when doing general program veriﬁcation,
where programs may manipulate strings, arrays, integers, etc.
In the future, I
strongly suspect that theory combination will be needed for neural-network veri-
ﬁcation, because we will start looking at neural networks as components of bigger
pieces of software.

67

Chapter 7

Neural Theory Solvers

In the previous chapter, we discussed DPLLT for solving formulas in FOL. In this
chapter, we study the Simplex algorithm for solving conjunctions of literals in linear
real arithmetic. Then, we extend the solver to natively handle rectiﬁed linear units
(ReLUs), which would normally be encoded as disjunctions and thus dealt with
using the SAT solver.

7.1 Theory Solving and Normal Forms

The Problem

The theory solver for LRA receives a formula F as a conjunction of linear inequal-
ities:

(cid:32) m
∑
j=1

n
(cid:94)

i=1

(cid:33)

cij · xj

(cid:62) bi

where cij, bi ∈ R. The goal is to check satisﬁability of F, and, if satisﬁable, discover
a model I |= F.

Notice that our formulas do not have strict inequalities (>). The approach
we present here can be easily generalized to handle strict inequalities, but for
simplicity, we stick with inequalities.1

1See Dutertre and De Moura (2006) for how to handle strict inequalities. In most instances
of verifying properties of neural networks, we do not need strict inequalities to encode network
semantics or properties.

CHAPTER 7. NEURAL THEORY SOLVERS

68

Simplex Form

The Simplex algorithm, discussed in the next section, expects formulas to be in a
certain form (just like how DPLL expected propositional formulas to be in CNF).
Speciﬁcally, Simplex expects formulas to be conjunctions of equalities of the form

and bounds of the form

ci · xi = 0

∑
i

(cid:54) xi

(cid:54) ui

li

where ui, li ∈ R ∪ {∞, −∞}. We use ∞ (resp. −∞) to indicate that a variable has
no upper (resp. lower) bound.

Therefore, given a formula F, we need to translate it into an equivalent formula
of the above form, which we will call the Simplex form (also known as slack form).
It turns out that translating a formula into Simplex form is pretty simple. Suppose
that

F (cid:44)

(cid:32) m
∑
j=1

n
(cid:94)

i=1

(cid:33)

cij · xj

(cid:62) bi

Then, we take every inequality and translate it into two conjuncts, an equality and
a bound. From the ith inequality,

we generate the equality

and the bound

m
∑
j=1

cij · xj

(cid:62) bi

si =

m
∑
j=1

cij · xj

si

(cid:62) bi

where si is a new variable, called a slack variable. Slack variables are analogous to
the temporary variables introduced by Tseitin’s transformation (Chapter 6).

CHAPTER 7. NEURAL THEORY SOLVERS

69

Example 7.A Consider the formula F, which we will use as our running example:

x + y (cid:62) 0
−2x + y (cid:62) 2
−10x + y (cid:62) −5

For clarity, we will drop the conjunction operator and simply list the inequalities.
We convert F into a formula Fs in Simplex form:

s1 = x + y
s2 = −2x + y
s3 = −10x + y
s1
s2
s3

(cid:62) 0
(cid:62) 2
(cid:62) −5

This transformation is a simple rewriting of the original formula that maintains
satisﬁability. Let Fs be the Simplex form of some formula F. Then, we have the
following guarantees (again, the analogue of Tseitin’s transformation for non-CNF
formulas):

1. Any model of Fs is a model of F, disregarding assignments to slack variables.

(cid:4)

2. If Fs is UNSAT, then F is UNSAT.

7.2 The Simplex Algorithm

We’re now ready to present the Simplex algorithm. This is a very old idea due
to George Dantizg, who developed it in 1947 (see Dantzig’s recollection of the
origins of Simplex (Dantzig, 1990)). The goal of the algorithm is to ﬁnd a satisfying
assignment that maximizes some objective function. Our interest in veriﬁcation is
typically to ﬁnd any satisfying assignment, and so the algorithm we will present
is a subset of Simplex.

CHAPTER 7. NEURAL THEORY SOLVERS

70

Intuition

One can think of the Simplex algorithm as a procedure that simultaneously looks
for a model and a proof of unsatisﬁability. It starts with some interpretation and
continues to update it in every iteration, until it ﬁnds a model or discovers a proof
of unsatisﬁability. We start from the interpretation I that sets all variables to 0.
In
This assignment satisﬁes all the equalities, but may not satisfy the bounds.
every iteration of Simplex, we pick a bound that is not satisﬁed and we modify I
to satisfy it, or we discover that the formula is unsatisﬁable. Let’s see this process
pictorially on a satisﬁable example before we dig into the math.

x + y (cid:62) 0

I1

-3
-3

-2
-2

-1
-1

−2x + y (cid:62) 2

−10x + y (cid:62) −5

0
0

2
2

3
3

x

y

I0

4
4

3
3

2
2
I2

-1
-1

-2
-2

Figure 7.1 Simplex example

Example 7.B Recall the formula F from our running example, illustrated in Fig-
ure 7.1, where the satisfying assignments are the shaded region. Each inequality
deﬁnes halfspace, i.e., splits R2 in half. Taking all of the inequalities, we get the
shaded region—the intersection of the all halfspaces.

x + y (cid:62) 0
−2x + y (cid:62) 2
−10x + y (cid:62) −5

CHAPTER 7. NEURAL THEORY SOLVERS

71

Simplex begins with the initial interpretation

I0 = {x (cid:55)→ 0, y (cid:55)→ 0}

shown in Figure 7.1, which is not a model of the formula.

Simplex notices that

I0 (cid:54)|= −2x + y (cid:62) 2

and so it decreases the interpretation of x to −1, resulting in I1. Then, Simplex
notices that

I1 (cid:54)|= x + y (cid:62) 0

and so it increases the interpretation of y from 0 to 2/3, resulting in the satisfying
assignment I2. (Notice that x also changes in I2; we will see why shortly.) In a
sense, Simplex plays Whac-A-Mole, trying to satisfy one inequality only to break
another, until it arrives at an assignment that satisﬁes all inequalities. Luckily, the
(cid:4)
algorithm actually terminates.

Basic and Non-basic Variables

Recall that Simplex expects an input formula to be in Simplex form. The set of
variables in the formula are broken into two subsets:

Basic variables are those that appear on the left hand side of an equality; ini-
tially, basic variables are the slack variables.

Non-basic variables are all other variables.

As Simplex progresses, it will rewrite the formula, thus some basic variables will
become non-basic and vice versa.

Example 7.C In our running example, initially the set of basic variables is {s1, s2, s3}
and non-basic variables is {x, y}.
(cid:4)

To ensure termination of Simplex, we will ﬁx a total ordering on the set of all
(basic and non-basic) variables. So, when we say “the ﬁrst variable that...”, we’re

CHAPTER 7. NEURAL THEORY SOLVERS

72

referring to the ﬁrst variable per our ordering. To easily refer to variables, we will
assume they are of the form x1, . . . , xn. Given a basic variable xi and a non-basic
variable xj, we will use cij to denote the coefﬁcient of xj in the equality

xi = . . . + cij · xj + . . .

For a variable xi, we will use li and ui to denote its lower bound and upper bound,
respectively. If a variable does not have an upper bound (resp. lower bound), its
upper bound is ∞ (resp. −∞). Note that non-slack variables have no bounds.

Simplex in Detail

We’re now equipped to present the Simplex algorithm, shown in Algorithm 3.
The algorithm maintains the following two invariants:

1. The interpretation I always satisﬁes the equalities, so only the bounds may

be violated. This is initially true, as I assigns all variables to 0.

2. The bounds of non-basic variables are all satisﬁed. This is initially true, as

non-basic variables have no bounds.

In every iteration of the while loop, Simplex looks for a basic variable whose
bounds are not satisﬁed by the current interpretation, and attempts to ﬁx the in-
terpretation. There are two symmetric cases, encoded as two branches of the if
statement, xi < li or xi > ui.

Let’s consider the ﬁrst case, xi < li. Since xi is less than li, we need to increase
its assignment in I. We do this indirectly by modifying the assignment of a non-
basic variable xj. But which xj should we pick? In principle, we can pick any xj
such that the coefﬁcient cij (cid:54)= 0, and adjust the interpretation of xj accordingly. If
you look at the algorithm, there are a few extra conditions. If we cannot ﬁnd an
xj that satisﬁes these conditions, then the problem is UNSAT. We will discuss the
unsatisﬁability conditions shortly. For now, assume we have found an xj. We can
increase its current interpretation by li−I(xi)
; this makes the interpretation of xi
cij
increase by li − I(xi), thus barely satisfying the lower bound, i.e., I(xi) = li. Note
that the interpretations of basic variables are assumed to change automatically

CHAPTER 7. NEURAL THEORY SOLVERS

73

when we change the interpretation of non-basic variables. This maintains the ﬁrst
invariant of the algorithm.2

After we have updated the interpretation of xj, there is a chance that we have
violated one of the bounds of xj. Therefore, we rewrite the formulas such that xj
becomes a basic variable and xi a non-basic variable. This is known as the pivot
operation, and it is mechanically done as follows: Take the following equality,
where N is the set of indices of non-basic variables:

xi = ∑
k∈N

cikxk

and rewrite it by moving xj to the left-hand side:

xi
cij

xj = −

(cid:124)

+ ∑

k∈N\{j}
(cid:123)(cid:122)
replace xj with this

cik
cij

xk

(cid:125)

Now, replace xj in all other equalities with the expression above. This operation
results in a set of equalities where xj only appears once, on the left-hand side. And
so, after pivoting, xj becomes a basic variable and xi a non-basic one.

Example 7.D Let’s now work through our running example in detail. Recall that
our formula is:

s1 = x + y
s2 = −2x + y
s3 = −10x + y
s1
s2
s3

(cid:62) 0
(cid:62) 2
(cid:62) −5

Say the variables are ordered as follows:

x, y, s1, s2, s3

2Basic variables are sometimes called dependent variables and non-basic variables independent
variables, indicating that the assignments of basic variables depend on those of non-basic vari-
ables.

CHAPTER 7. NEURAL THEORY SOLVERS

74

Algorithm 3: Simplex

Data: A formula F in Simplex form
Result: I |= F or UNSAT

Let I be the interpretation that sets all variables fv(F) to 0
while true do

if I |= F then return I
Let xi be the ﬁrst basic variable s.t. I(xi) < li or I(xi) > ui
if I(xi) < li then

Let xj be the ﬁrst non-basic variable s.t.

(I(xj) < uj and cij > 0) or (I(xj) > lj and cij < 0)

if If no such xj exists then return UNSAT
I(xj) ← I(xj) + li−I(xi)

cij

else

Let xj be the ﬁrst non-basic variable s.t.

(I(xj) > lj and cij > 0) or (I(xj) < uj and cij < 0)

if If no such xj exists then return UNSAT
I(xj) ← I(xj) + ui−I(xi)

cij

Pivot xi and xj

Initially, the bounds of s1 and s3 are satisﬁed, but s2 is violated, because s2
I0(s2) = 0, as all variables are assigned 0 initially.

(cid:62) 2 but

First iteration In the ﬁrst iteration, we pick the variable x to ﬁx the bounds
of s2, as it is the ﬁrst one in our ordering. Note that x is unbounded (i.e., its
bounds are −∞ and ∞), so it easily satisﬁes the conditions. To increase the
interpretation of s2 to 2, and satisfy its lower bound, we can decrease I0(x) to
−1, resulting in the following satisfying assignment:

I1 = {x (cid:55)→ −1, y (cid:55)→ 0, s1 (cid:55)→ −1, s2 (cid:55)→ 2, s3 (cid:55)→ 10}

(Revisit Figure 7.1 for an illustration.) We now pivot s2 and x, producing the

CHAPTER 7. NEURAL THEORY SOLVERS

75

following set of equalities (the bounds always remain the same):

x = 0.5y − 0.5s2
s1 = 1.5y − 0.5s2
s3 = −4y + 5s2

Second iteration The only basic variable not satisfying its bounds is now s1,
since I1(s1) = −1 < 0. The ﬁrst non-basic variable that we can tweak is y.
We can increase the value of I(y) by 1/1.5 = 2/3, resulting in the following
interpretation:

I2 = {x (cid:55)→ −2/3, y (cid:55)→ 2/3, s1 (cid:55)→ 0, s2 (cid:55)→ 2, s3 (cid:55)→ 7/3}

At this point, we pivot y with s1.

Third iteration Simplex terminates since I2 |= F.

(cid:4)

Why is Simplex Correct?

First, you may wonder, why does Simplex terminate? The answer is due to the
fact that we order variables and always look for the ﬁrst variable violating bounds.
This is known as Bland’s rule (Bland, 1977). Bland’s rule ensures that we never
revisit the same set of basic and non-basic variables.

Second, you may wonder, is Simplex actually correct? If Simplex returns an
interpretation I, it is easy to see that I |= F, since Simplex checks that condition
before it terminates. But what about the case when it says UNSAT? To illustrate
correctness in this setting, we will look at an example.

CHAPTER 7. NEURAL THEORY SOLVERS

76

Example 7.E Consider the following formula in Simplex form:

s1 = x + y
s2 = −x − 2y
s3 = −x + y
s1
s2
s3

(cid:62) 0
(cid:62) 2
(cid:62) 1

This formula is UNSAT—use your favorite SMT solver to check this. Imagine an
execution of Simplex that performs the following two pivot operations: (1) s1 with
x and (2) s2 with y.

The ﬁrst pivot results in the following formula:

x = s1 − y
s2 = −s1 − y
s3 = −s1 + 2y

The second pivot results in the following formula:

x = 2s1 + s2
y = −s2 − s1
s3 = −3s1 − 2ss

The algorithm maintains the invariant that all non-basic variables satisfy their
bounds. So we have s1

(cid:62) 2. Say s3 violates its bound, i.e.,

(cid:62) 0 and s2

−3s1 − 2s2 < 1

The only way to ﬁx this is by decreasing the interpretations of s1 and s2. But even
if we assign s1 and s2 the values 0 and 2 (their lower bounds), respectively, we
(cid:62) 1. Contradiction! So Simplex ﬁgures out the formula is UNSAT.
cannot make s3
The conditions for choosing variable xj in Algorithm 3 encode this argument. (cid:4)

As the above example illustrates, we can think of Simplex as constructing a

proof by contradiction to prove that a set of linear inequalities is unsatisﬁable.

CHAPTER 7. NEURAL THEORY SOLVERS

77

7.3 The Reluplex Algorithm

Using the Simplex algorithm as the theory solver within DPLLT allows us to solve
formulas in LRA. So, at this point in our development, we know how to algo-
rithmically reason about neural networks with piecewise-linear activations, like
ReLUs. Unfortunately, this approach has been shown to not scale to large net-
works. One of the reasons is that ReLUs are encoded as disjunctions, as we saw in
Chapter 5. This means that the SAT-solving part of DPLLT will handle the disjunc-
tions, and may end up considering every possible case of the disjunction—ReLU
being active (output = input) or inactive (output = 0)—leading to many calls to
Simplex, exponential in the number of ReLUs.

To ﬁx those issues, the work of Katz et al. (2017) developed an extension of
Simplex, called Reluplex, that natively handles ReLU constraints in addition to
linear inequalities. The key idea is to try to delay case splitting on ReLUs. In the
worst case, Reluplex may end up with an exponential explosion, just like DPLLT
with Simplex, but empirically it has been shown to be a promising approach for
scaling SMT solving to larger neural networks. In what follows, we present the
Reluplex algorithm.

Reluplex Form

Just like with Simplex, Reluplex expects formulas to be in a certain form. We
will call this form Reluplex form, where formulas contain (1) equalities (same as
Simplex), (2) bounds (same as Simplex), and (3) ReLU constraints of the form

xi = relu(xj)

Given a conjunction of inequalities and ReLU constraints, we can translate them
into Reluplex form by translating the inequalities into Simplex form. Additionally,
(cid:62) 0, which is
for each ReLU constraint xi = relu(xj), we can add the bound xi
implied by the deﬁnition of a ReLU.

CHAPTER 7. NEURAL THEORY SOLVERS

78

Example 7.F Consider the following formula:

x + y (cid:62) 2
y = relu(x)

We translate it into the following Reluplex form:

s1 = x + y
y = relu(x)

(cid:62) 2
s1
y (cid:62) 0

(cid:4)

Reluplex in Detail

We now present the Reluplex algorithm. The original presentation by Katz et al.
(2017) is a set of rules that can be applied non-deterministically to arrive at an
answer. Here, we present a speciﬁc schedule of the Reluplex algorithm.

The key idea of Reluplex is to call Simplex on equalities and bounds, and then
try to massage the interpretation returned by Simplex to satisfy all ReLU con-
straints. Reluplex is shown in Algorithm 4.

Initially, Simplex is invoked on the formula F(cid:48), which is the original formula F
but without the ReLU constraints. If Simplex returns UNSAT, then we know that F
is UNSAT—this is because F ⇒ F(cid:48) is valid. Otherwise, if Simplex returns a model
I |= F(cid:48), it may not be the case that I |= F, since F(cid:48) is a weaker (less constrained)
formula.

If I (cid:54)|= F, then we know that one of the ReLU constraints is not satisﬁed. We
pick one of the violated ReLU constraints xi = relu(xj) and modify I to make sure
it is not violated. Note that if any of xi and xj is a basic variable, we pivot it with
a non-basic variable. This is because we want to modify the interpretation of one
of xi or xj, which may affect the interpretation of the other variable if it is a basic

CHAPTER 7. NEURAL THEORY SOLVERS

79

Algorithm 4: Reluplex

Data: A formula F in Reluplex form
Result: I |= F or UNSAT

Let I be the interpretation that sets all variables fv(F) to 0
Let F(cid:48) be the non-ReLU constraints of F
while true do

(cid:46) Calling Simplex (note that we supply Simplex with a reference to the
initial interpretation and it can modify it)
r ← Simplex(F(cid:48), I)
If r is UNSAT then return UNSAT
r is an interpretation I
if I |= F then return I

(cid:46) Handle violated ReLU constraint
Let ReLU constraint xi = relu(xj) be s.t. I(xi) (cid:54)= relu(I(xj))
if xi is basic then

pivot xi with non-basic variable xk, where k (cid:54)= j and cik (cid:54)= 0

if xj is basic then

pivot xj with non-basic variable xk, where k (cid:54)= i and cjk (cid:54)= 0

Perform one of the following operations:

I(xi) ← relu(I(xj)) or

I(xj) ← I(xi)

(cid:46) Case splitting (ensures termination)
if uj > 0, lj < 0, and xi = relu(xj) considered more than τ times then
(cid:62) 0 ∧ xi = xj)
(cid:54) 0 ∧ xi = 0)

r1 ← Reluplex(F ∧ xj
r2 ← Reluplex(F ∧ xj
if r1 = r2 = UNSAT then return UNSAT
if r1 (cid:54)= UNSAT then return r1
return r2

(cid:54)= 0.3 Finally, we modify the interpretation of xi or xj, ensuring
variable and cij
that I |= xi = relu(xj). Note that the choice of xi or xj is up to the implementation.
The problem is that ﬁxing a ReLU constraint may end up violating a bound,

3These conditions are not explicit in Katz et al. (2017), but their absence may lead to wasted

iterations (or Update rules in Katz et al. (2017)) that do not ﬁx violations of ReLU constraints.

CHAPTER 7. NEURAL THEORY SOLVERS

80

and so Simplex need be invoked again. We assume that the interpretation I in
Reluplex is the same one that is modiﬁed by invocations of Simplex.

Case Splitting

If we simply apply Reluplex without the last piece of the algorithm—case splitting—
it may not terminate. Speciﬁcally, it may get into a loop where Simplex satisﬁes
all bounds but violates a ReLU, and then satisfying the ReLU causes a bound to
be violated, and on and on.

The last piece of Reluplex checks if we are getting into an inﬁnite loop, by
ensuring that we do not attempt to ﬁx a ReLU constraint more than τ times,
some ﬁxed threshold.
If this threshold is exceeded, then the ReLU constraint
xi = relu(xj) is split into its two cases:

and

F1

(cid:44) xj

(cid:62) 0 ∧ xi = xj

F2

(cid:44) xj

(cid:54) 0 ∧ xi = 0

Reluplex is invoked recursively on two instances of the problem, F ∧ F1 and F ∧ F2.
If both instances are UNSAT, then the formula F is UNSAT. If any of the instances
is SAT, then F is SAT. This is due to the fact that

F ≡ (F ∧ F1) ∨ (F ∧ F2)

Looking Ahead

We’re done with constraint-based veriﬁcation. In the next part of the book, we
will look at different approaches that are more efﬁcient at the expense of failing to
provide proofs in some cases.

There are a number of interesting problems that we didn’t cover. A critical one
is soundness with respect to machine arithmetic. Our discussion has assumed
real-valued variables, but, of course, that’s not the case in the real world—we use
machine arithmetic. A recent paper has shown that veriﬁed neural networks in

CHAPTER 7. NEURAL THEORY SOLVERS

81

LRA may not really be robust when one considers the bit-level behavior (Jia and
Rinard, 2020b).

Another issue is scalability of the analysis. Using arbitrary-precision rational
numbers can be very expensive, as the size of the numerators and denominators
can blow up due to pivot operations. Reluplex (Katz et al., 2017) ends up us-
ing ﬂoating-point approximations, and carefully ensures the results are sound by
keeping track of round-off errors.

At the time of writing, constraint-based veriﬁcation approaches have only
managed to scale to neural networks with around a hundred thousand ReLUs (Tjeng
et al., 2019a), which is small compared to state-of-the-art neural networks. This is
still a triumph, as the veriﬁcation problem is NP-hard (Katz et al., 2017). It is, how-
ever, unclear how much further we can push this technology. Scaling constraint-
based veriﬁcation further requires progress along two fronts: (1) developing and
training neural networks that are friendlier to veriﬁcation (less ReLUs is better),
and (2) advancing the algorithms underlying SMT solvers and MILP solvers.

Part III

Abstraction-Based Veriﬁcation

83

Chapter 8

Neural Interval Abstraction

In the previous part of the book, we described how to precisely capture the seman-
tics of a neural network by encoding it, along with a correctness property, as a for-
mula in ﬁrst-order logic. Typically, this means that we’re solving an NP-complete
problem, like satisﬁability modulo linear real arithmetic (equivalently, mixed in-
teger linear programming). While we have fantastic algorithms and tools that
surprisingly work well for such hard problems, scaling to large neural networks
remains an issue.

In this part of the book, we will look at approximate techniques for neural-
network veriﬁcation. By approximate, we mean that they overapproximate—or
abstract—the semantics of a neural-network, and therefore can produce proofs of
correctness, but when they fail, we do not know whether a correctness property
holds or not. The approach we use is based on abstract interpretation (Cousot and
Cousot, 1977), a well-studied framework for deﬁning program analyses. Abstract
interpretation is a very rich theory, and the math can easily make you want to
quit computer science and live a monastic life in the woods, away from anything
that can be considered technology. But fear not, it is a very simple idea, and we
will take a pragmatic approach here in deﬁning it and using it for neural-network
veriﬁcation.

CHAPTER 8. NEURAL INTERVAL ABSTRACTION

84

8.1 Set Semantics and Veriﬁcation

Let’s focus on the following correctness property, deﬁning robustness of a neural
network f : Rn → Rm on an input grayscale image c whose classiﬁcation label is
1.

{ |x − c| (cid:54) 0.1 }
r ← f (x)
{ class(r) = 1 }

Concretely, this property makes the following statement: Pick any image x that
is like c but is slightly brighter or darker by at most 0.1 per pixel, assuming each
pixel is some real number encoding its brightness. Now, execute the network on
x. The network must predict that x is of class 1.

The issue in checking such statement is that there are inﬁnitely many possible
images x. Even if there are ﬁnitely many images—because, at the end of the day,
we’re using bits—the number is still enormous, and we cannot conceivably run
all those images through the network and ensure that each and every one of them
is assigned class 1. But let’s just, for the sake of argument, imagine that we can lift
the function f to work over sets of images. That is, we will deﬁne a version of f of
the form:

where P (S) is the powerset of set S. Speciﬁcally,

f s : P (Rn) → P (Rm)

f s(X) = {y | x ∈ X, y = f (x)}

Armed with f s, we can run it with the following input set:

X = {x | |x − c| (cid:54) 0.1}

which is the set of all images x deﬁned above in the precondition of our correct-
ness property. By computing f s(X), we get the predictions of the neural network
f for all images in X. To verify our property, we simply check that

f s(X) ⊆ {y | class(y) = 1}

In other words, all runs of f on every image x ∈ X result in the network predicting
class 1.

CHAPTER 8. NEURAL INTERVAL ABSTRACTION

85

The above discussion may sound like crazy talk: we cannot simply take a neu-
ral network f and generate a version f s that takes an inﬁnite set of images. In this
chapter, we will see that we actually can, but we will often have to lose precision:
we will deﬁne an abstract version of our theoretical f s that may return more an-
swers. The trick is to deﬁne inﬁnite sets of inputs using data structures that we
can manipulate, called abstract domains.

In this chapter, we will meet the interval abstract domain. We will focus our
attention on the problem of executing the neural network on an inﬁnite set. Later,
in Chapter 11, we come back to the veriﬁcation problem.

8.2 The Interval Domain

Let’s begin by considering a very simple function

f (x) = x + 1

I would like to evaluate this function on a set of inputs X; that is, I would like to
somehow evaluate

f s(X) = {x + 1 | x ∈ X}

We call f s the concrete transformer of f .

Abstract interpretation simpliﬁes this problem by only considering sets X that
have a nice form. Speciﬁcally, the interval abstract domain considers an interval of
real numbers written as [l, u], where l, u ∈ R and l (cid:54) u. An interval [l, u] denotes
the potentially inﬁnite set

{x | l (cid:54) x (cid:54) u}

So we can now deﬁne a version of our function f s that operates over an interval,
as follows:

f a([l, u]) = [l + 1, u + 1]

We call f a an abstract transformer of f . In other words, f a takes a set of real numbers
and returns a set of real numbers, but the sets are restricted to those that can be
deﬁned as intervals. Observe how we can mechanically evaluate this abstract
transformer on an arbitrary interval [l, u]: add 1 to l and add 1 to u, arriving at

CHAPTER 8. NEURAL INTERVAL ABSTRACTION

86

l

u

l + 1

u + 1

Figure 8.1 Illustration of an abstract transformer of f (x) = x + 1.

the interval [l + 1, u + 1]. Geometrically, if we have an interval on the number line
from l to u, and we add 1 to every point in this interval, then the whole interval
shifts to the right by 1. This is illustrated in Figure 8.1. Note that the interval
[l, u] is an inﬁnite set (assuming l < u), and so f a adds 1 to an inﬁnite set of real
numbers!

Example 8.A Continuing our example,

f a([0, 10]) = [1, 11]

If we pass a singleton interval, e.g., [1, 1], we get f a([1, 1]) = [2, 2]—exactly the
(cid:4)
behavior of f .

Generally, we will use the notation ([l1, u1], . . . , [ln, un]) to denote an n-dimensional

interval, or a hyperrectangular region in Rn, i.e., the set of all n-ary vectors

{x ∈ Rn | li

(cid:54) xi

(cid:54) ui}

Soundness

Whenever we design an abstract transformer f a, we need to ensure that it is a
sound approximation of f s. This means that its output is a superset of that of the
concrete transformer, f s. The reason is that we will be using f a for veriﬁcation,
so to declare that a property holds, we cannot afford to miss any behavior of the
neural network.

Formally, we deﬁne soundness as follows: For any interval [l, u], we have

f s([l, u]) ⊆ f a([l, u])

CHAPTER 8. NEURAL INTERVAL ABSTRACTION

87

Equivalently, we can say that for any x ∈ [l, u], we have

f (x) ∈ f a([l, u])

In practice, we will often ﬁnd that

f s([l, u]) ⊂ f a([l, u])

for many functions and intervals of interest. This is expected, as our goal is to
design abstract transformers that are easy to evaluate, and so we will often lose
precision, meaning overapproximate the results of f s. We will see some simple
examples shortly.

The Interval Domain is Non-relational

The interval domain is non-relational, meaning that it cannot capture the relations
between different dimensions. We illustrate this fact with an example.

Example 8.B Consider the set

X = {(x, x) | 0 (cid:54) x (cid:54) 1}

We cannot represent this set precisely in the interval domain. The best we can do
is the square between (0, 0) and (1, 1), denoted as the 2-dimensional interval

and illustrated as the gray region below:

([0, 1], [0, 1])

The set X deﬁnes points where higher values of the x coordinate associate with
higher values of the y coordinate. But our abstract domain can only represent
rectangles whose faces are parallel to the axes. This means that we can’t capture
the relation between the two dimensions: we simply say that any value of x in
[0, 1] can associate with any value of y in [0, 1].
(cid:4)

CHAPTER 8. NEURAL INTERVAL ABSTRACTION

88

8.3 Basic Abstract Transformers

We now look at examples of abstract transformers for basic arithmetic operations.

Addition

Consider the binary function:
P (R2) → P (R) is deﬁned as follows:

f (x, y) = x + y. The concrete transformer f s

:

f s(X) = {x + y | (x, y) ∈ X}

We deﬁne f a as a function that takes two intervals, i.e., a rectangle, one represent-
ing the range of values of x1 and the other of x2:

f a([l, u], [l(cid:48), u(cid:48)]) = [l + l(cid:48), u + u(cid:48)]

The deﬁnition looks very much like f , except that we perform addition on the
lower bounds and the upper bounds of the two input intervals.

Example 8.C Consider

f a([1, 5], [100, 200]) = [101, 205]

The lower bound, 101, results from adding the lower bounds of x and y (1 + 100);
the upper bound, 205, results from adding the upper bounds of x and y (5 + 200).
(cid:4)

It is simple to prove soundness of our abstract transformer f a. Take any

(x, y) ∈ ([l, u], [l(cid:48), u(cid:48)])

By deﬁnition, l (cid:54) x (cid:54) u and l(cid:48) (cid:54) y (cid:54) u(cid:48). So we have

l + l(cid:48) (cid:54) x + y (cid:54) u + u(cid:48)

By deﬁnition of an interval, we have

x + y ∈ [l + l(cid:48), u + u(cid:48)]

CHAPTER 8. NEURAL INTERVAL ABSTRACTION

89

Multiplication

Multiplication is a bit trickier. The reason is that the signs might ﬂip, making the
lower bound an upper bound. So we have to be a bit more careful.

Let f (x, y) = x ∗ y. If we only consider positive inputs, then we can deﬁne f a

just like we did for addition:

But consider

f a([l, u], [l(cid:48), u(cid:48)]) = [l ∗ l(cid:48), u ∗ u(cid:48)]

f a([−1, 1], [−3, −2]) = [3, −2]

We’re in trouble: [3, −2] is not even an interval as per our deﬁnition—the upper
bound is less than the lower bound!

To ﬁx this issue, we need to consider every possible combination of lower and

upper bounds as follows:

f a([l, u], [l(cid:48), u(cid:48)]) = [min(B), max(B)]

where

B = {l ∗ l(cid:48), l ∗ u(cid:48), u ∗ l(cid:48), u ∗ u(cid:48)}

Example 8.D Consider the following abstract multiplication of two intervals:

f a([−1, 1], [−3, −2]) = [min(B), max(B)]

= [−3, 3]

where B = {3, 2, −3, −2}.

(cid:4)

8.4 General Abstract Transformers

We will now deﬁne general abstract transformers for classes of operations that
commonly appear in neural networks.

CHAPTER 8. NEURAL INTERVAL ABSTRACTION

90

Aﬃne Functions

For an afﬁne function

f (x1, . . . , xn) = ∑

cixi

i

where ci ∈ R, we can deﬁne the abstract transformer as follows:

f a([l1, u1], . . . , [ln, un]) =

(cid:35)

(cid:34)

∑
i

i, ∑
l(cid:48)

u(cid:48)
i

i

where l(cid:48)

i = min(cili, ciui) and u(cid:48)

i = max(cili, ciui).

Notice that the deﬁnition looks pretty much like addition: sum up the lower
bounds and the upper bounds. The difference is that we also have to consider the
coefﬁcients, ci, which may result in ﬂipping an interval’s bounds when ci < 0.

Example 8.E Consider f (x, y) = 3x + 2y. Then,

f ([5, 10], [20, 30]) = [3 · 5 + 2 · 20, 3 · 10 + 2 · 30]

= [55, 90]

(cid:4)

Monotonic Functions Most activation functions used in neural networks are mono-
tonically increasing, e.g., ReLU and sigmoid. It turns out that it’s easy to deﬁne
an abstract transformer for any monotonically increasing function f : R → R, as
follows:

Simply, we apply f to the lower and upper bounds.

f a([l, u]) = [ f (l), f (u)]

Example 8.F Figure 8.2 illustrates how to apply ReLU to an interval [3, 5]. The
shaded region shows that any value y in the interval [3, 5] results in a value

relu(3) (cid:54) relu(y) (cid:54) relu(5)

that is, a value in the interval [relu(3), relu(5)].

(cid:4)

CHAPTER 8. NEURAL INTERVAL ABSTRACTION

91

relu(x)

relu(u) = 5
relu(l) = 3

x

l = 3 u = 5

Figure 8.2 ReLU function over an interval of inputs [l, u]

Composing Abstract Transformers

Say we have a function composition f ◦ g—this notation means ( f ◦ g)(x) =
f (g(x)). We don’t have to deﬁne an abstract transformer for the composition:
we can simply compose the two abstract transformers of f and g, as f a ◦ ga, and
this will be a sound abstract transformer of f ◦ g.

Composition is very important, as neural networks are a composition of many

operations.

Example 8.G Let

g(x) = 3x
f (x) = relu(x)
h(x) = f (g(x))

The function h represents a very simple neural network, one that applies an

afﬁne function followed by a ReLU on an input in R.

We deﬁne

ha([l, u]) = f a(ga([l, u]))
where f a and ga are as deﬁned earlier for monotonic functions and afﬁne func-
tions, respectively. For example, on the input interval [2, 3], we have

ha([2, 3]) = f a(ga([2, 3]))

= f a([6, 9])
= [6, 9]

CHAPTER 8. NEURAL INTERVAL ABSTRACTION

92

(cid:4)

8.5 Abstractly Interpreting Neural Networks

We have seen how to construct abstract transformers for a range of functions and
how to compose abstract transformers. We now direct our attention to construct-
ing an abstract transformer for a neural network.

Recall that a neural network is deﬁned as a graph G = (V, E), giving rise to
a function fG : Rn → Rm, where n = |Vin| and m = |Vo|. Recall that Vin are
input nodes and Vo are output nodes of G. We would like to construct an abstract
transformer f a

G that takes n intervals and outputs m intervals.
G([l1, u1], . . . , [ln, un]) as follows:

We deﬁne f a

• First, for every input node vi, we deﬁne

outa(vi) = [li, ui]

Recall that we assume a ﬁxed ordering of nodes.

• Second, for every non-input node v, we deﬁne

outa(v) = f a

v (outa(v1), . . . , outa(vk))

where f a
(v1, v), . . . , (vk, v),

v is the abstract transformer of fv, and v has the incoming edges

• Finally, the output of f a

G is the set of intervals outa(v1), . . . , outa(vm), where

v1, . . . , vm are the output nodes.

Example 8.H Consider the following simple neural network G:

v1

v2

v3

v4

CHAPTER 8. NEURAL INTERVAL ABSTRACTION

93

Assume that fv3(x) = 2x1 + x2 and fv4(x) = relu(x).

Say we want to evaluate f a

G([0, 1], [2, 3]). We can do this as follows, where f a
v3
v4 follow the deﬁnitions we discussed above for afﬁne and monotonically

and f a
increasing functions, respectively.

outa(v1) = [0, 1]
outa(v2) = [2, 3]
outa(v3) = [2 ∗ 0 + 2, 2 ∗ 1 + 3] = [2, 5]
outa(v4) = [relu(2), relu(5)] = [2, 5]

It’s nice to see the outputs of every node written on the edges of the graph as

follows:

v1

v2

[0, 1]

[2, 3]

[2, 5]

v3

v4

[2, 5]

(cid:4)

Limitations of the Interval Domain

The interval domain, as described, seems infallible. We will now see how it can,
and often does, overshoot: compute wildly overapproximating solutions. The
primary reason for this is that the interval domain is non-relational, meaning it
cannot keep track of relations between different values, e.g., the inputs and out-
puts of a function.

Example 8.I Consider the following, admittedly bizarre, neural network:

[0, 1]

[−1, 0]

v2

v1

[0, 1]

v3

[−1, 1]

CHAPTER 8. NEURAL INTERVAL ABSTRACTION

94

where

fv2(x) = −x
fv3(x) = x1 + x2

Clearly, for any input x, fG(x) = 0. Therefore, ideally, we can deﬁne our abstract
transformer simply as f a

G([l, u]) = [0, 0] for any interval [l, u].
Unfortunately, if we follow the recipe above, we get a much bigger interval
than [0, 0]. For example, on the input [0, 1], f a
G returns [−1, 1], as illustrated on
the graph above. The reason this happens is because the output node, v3, receives
two intervals as input, not knowing that one is the negation of the other. In other
words, it doesn’t know the relation between, or provenance of, the two intervals.
(cid:4)

Example 8.J Here’s another simple network, G, where fv2 and fv3 are ReLUs.
Therefore, fG(x) = (x, x) for any positive input x.

v1

[0, 1]

v2

[0, 1]

[0, 1]

v3

[0, 1]

Following our recipe, we have f a

G([0, 1]) = ([0, 1], [0, 1]). In other words, the
abstract transformer tells us that, for inputs between 0 and 1, the neural network
can output any pair (x, y) where 0 (cid:54) x, y (cid:54) 1. But that’s too loose an approx-
imation: we should expect to see only outputs (x, x) where 0 (cid:54) x (cid:54) 1. Again,
we have lost the relation between the two output nodes. They both should return
the same number, but the interval domain, and our abstract transformers, are not
(cid:4)
strong enough to capture that fact.

CHAPTER 8. NEURAL INTERVAL ABSTRACTION

95

Looking Ahead

We’ve seen how interval arithmetic can be used to efﬁciently evaluate a neural
network on a set of inputs, paying the price of efﬁciency with precision. Next, we
will see more precise abstract domains.

The abstract interpretation framework was introduced by Cousot and Cousot
(1977) in their seminal paper. Abstract interpretation is a general framework,
based on lattice theory, for deﬁning and reasoning about program analyses. In our
exposition, we avoided the use of lattices, because we do not aim for generality—
we just want to analyze neural networks. Nonetheless, the lattice-based formal-
ization allows us to easily construct the most-precise abstract transformers for any
operation.

Interval arithmetic is an old idea that predates program analysis, even com-
puter science:
it is a standard tool in the natural sciences for measuring accu-
mulated measurement errors. For neural-network veriﬁcation, interval arithmetic
ﬁrst appeared in a number of papers starting in 2018 (Gehr et al., 2018; Gowal
et al., 2018; Wang et al., 2018). To implement interval arithmetic for real neural
networks efﬁciently, one needs to employ parallel matrix operations (e.g., using a
GPU). Intuitively, an operation like matrix addition can be implemented with two
matrix additions for interval arithmetic, one for upper bounds and one for lower
bounds.

There are also powerful techniques that employ the interval domain (or any
means to bound the output of various nodes of the network) with search. We did
not cover this combination here but I would encourage you to check out FastLin
approach (Weng et al., 2018) and its successor, CROWN (Zhang et al., 2018a). (Both
are nicely summarized by Li et al. (2019))

One interesting application of the interval domain is as a quick-and-dirty way
for speeding up constraint-based veriﬁcation. Tjeng et al. (2019b) propose using
something like the interval domain to bound the interval of values taken by a
ReLU for a range of inputs to the neural network. If the interval of inputs of a
ReLU is above or below 0, then we can replace the ReLU with a linear function,
f (x) = x or f (x) = 0, respectively. This simpliﬁes the constraints for constraint-
based veriﬁcation, as there’s no longer a disjunction.

96

Chapter 9

Neural Zonotope Abstraction

In the previous chapter, we deﬁned the interval abstract domain, which allows us
to succinctly capture inﬁnite sets in Rn by deﬁning lower and upper bounds per
dimension. In R2, an interval deﬁnes a rectangle; in R3, an interval deﬁnes a box;
in higher dimensions, it deﬁnes hyperrectangles.

The issue with the interval domain is that it does not relate the values of var-
ious dimensions—it just bounds each dimension. For example, in R2, we cannot
capture the set of points where x = y and 0 (cid:54) x (cid:54) 1. The best we can do is
the square region ([0, 1], [0, 1]). Syntactically speaking, an abstract element in the
interval domain is captured by constraints of the form:

(cid:54) xi

(cid:54) ui

li

(cid:94)

i

where every inequality involves a single variable, and therefore no relationships
between variables are captured. So the interval domain is called non-relational. In
this chapter, we will look at a relational abstract domain, the zonotope domain, and
discuss its application to neural networks.

Figure 9.1 Examples of zonotopes in R2

CHAPTER 9. NEURAL ZONOTOPE ABSTRACTION

97

9.1 What the Heck is a Zonotope?

Let’s begin with deﬁning a 1-dimensional zonotope. We assume we have a set of
m real-valued generator variables, denoted (cid:101)1, . . . , (cid:101)m. A 1-dimensional zonotope
is the set of all points

(cid:40)

c0 +

m
∑
i=1

ci · (cid:101)i

(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

(cid:101)i ∈ [−1, 1]

where ci ∈ R.

If you work out a few examples of the above deﬁnition, you’ll notice that a
1-dimensional zonotope is just a convoluted way of deﬁning an interval. For ex-
ample, if we have one generator variable, (cid:101), then a zonotope is the set

{c0 + c1(cid:101) | (cid:101) ∈ [−1, 1]}

which is the interval [c0 − c1, c0 + c1], assuming c1
of the interval.

(cid:62) 0. Note that c0 is the center

Zonotopes start being more expressive than intervals in R2 and beyond. In

n-dimensions, a zonotope with m generators is the set of all points












c10 +

c1i · (cid:101)i

m
∑
i=1
(cid:123)(cid:122)
ﬁrst dimension

(cid:124)

. . . , cn0 +

cni · (cid:101)i

m
∑
i=1
(cid:123)(cid:122)
nth dimension

(cid:125)

(cid:125)

(cid:124)








(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:101)i ∈ [−1, 1]





This is best illustrated through a series of examples in R2.1

Example 9.A Consider the following two-dimensional zonotope with two genera-
tors.

(1 + (cid:101)1, 2 + (cid:101)2)

where we drop the set notation for clarity. Notice that in the ﬁrst dimension the
coefﬁcient of (cid:101)2 is 0, and in the second dimension the coefﬁcient of (cid:101)1 is 0. Since the
two dimensions do not share generators, we get the following box shape whose
center is (1, 2).

1In the VR edition of the book, I take the reader on a guided 3D journey of zonotopes; since

you cheaped out and just downloaded the free pdf, we’ll have to do with R2.

CHAPTER 9. NEURAL ZONOTOPE ABSTRACTION

98

2
1

1 2

Observe that the center of the zonotope is the vector of constant coefﬁcients of

the two dimensions, (1, 2), as illustrated below:

( 1
(cid:124)(cid:123)(cid:122)(cid:125)

+ (cid:101)1, 2

(cid:124)(cid:123)(cid:122)(cid:125)

+ (cid:101)2)

(cid:4)

Example 9.B Now consider the following zonotope with 1 generator:

(2 + (cid:101)1, 2 + (cid:101)1)

Since the two dimensions share the same expression, this means that two dimen-
sions are equal, and so we get we get a line shape centered at (2, 2):

3
2
1

1 2 3

The reason (cid:101)1 is called a generator is because we can think of it as a constructor
of a zonotope. In this example, starting from the center point (2,2), the generator
(cid:101)1 stretches the point (2,2) to (3,3), by adding (1,1) (the two coefﬁcients of (cid:101)1) and
stretches the center to (1,1) by subtracting (1,1). See the following illustration:

3
2
1

1 2 3

CHAPTER 9. NEURAL ZONOTOPE ABSTRACTION

99

(cid:4)

Example 9.C Now consider the following zonotope with 2 generators,

which is visualized as follows, with the center point (2,3) in red.

(2 + (cid:101)1, 3 + (cid:101)1 + (cid:101)2)

5
4
3
2
1

1

2

3

4

Let’s see how this zonotope is generated in two steps, by considering one gen-
erator at a time. The coefﬁcients of (cid:101)1 are (1,1), so it stretches the center point (2,3)
along the (1,1) vector, generating a line:

5
4
3
2
1

1

2

3

4

Next, the coefﬁcients of (cid:101)2 are (0,1), so it stretches all points along the (0, 1)

vector, resulting in the zonotope we plotted earlier:

5
4
3
2
1

1

2

3

4

CHAPTER 9. NEURAL ZONOTOPE ABSTRACTION

100

(cid:4)

You may have deduced by now that adding more generators adds more faces
to the zonotope. For example, the right-most zonotope in Figure 9.1 uses three
generators to produce the three pairs of parallel faces.

A Compact Notation

Going forward, we will use a compact notation to describe an n-dimensional
zonotope with m generator variables:

(cid:40)(cid:32)

c10 +

m
∑
i=1

c1i · (cid:101)i, . . . , cn0 +

m
∑
i=1

cni · (cid:101)i

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

(cid:101)i ∈ [−1, 1]

Speciﬁcally, we will deﬁne it as a tuple of vectors of coefﬁcients:

((cid:104)c10, . . . , c1m(cid:105), . . . , (cid:104)cn0, . . . , cnm(cid:105))

For an even more compact presentation, will also use

((cid:104)c1i(cid:105)i, . . . , (cid:104)cni(cid:105)i)

where i ranges from 0 to m, the number of generators; we drop the index i when
it’s clear from context.

We can compute the upper bound of the zonotope (the largest possible value)

in the j dimension by solving the following optimization problem:

max cj0 +

m
∑
i=1

cji(cid:101)i

s.t. (cid:101)i ∈ [−1, 1]

This can be easily solved by setting (cid:101)i to 1 if cji > 0 and −1 otherwise.

Similarly, we can compute the lower bound of the zonotope in the jth dimen-
sion by minimizing instead of maximizing, and solving the optimization problem
by setting (cid:101)i to −1 if cji > 0 and 1 otherwise.

CHAPTER 9. NEURAL ZONOTOPE ABSTRACTION

101

Example 9.D Recall our parallelogram from Example 9.C:

(2 + (cid:101)1, 3 + (cid:101)1 + (cid:101)2)

In our compact notation, we write this as

((cid:104)2, 1, 0(cid:105), (cid:104)3, 1, 1(cid:105))

The upper bound in the vertical dimension, 3 + (cid:101)1 + (cid:101)2, is

3 + 1 + 1 = 5

where (cid:101)1 and (cid:101)2 are set to 1.

(cid:4)

9.2 Basic Abstract Transformers

Now that we have seen zonotopes, let’s deﬁne some abstract transformers over
zonotopes.

Addition

For addition, f (x, y) = x + y, we will deﬁne the abstract transformer f a that takes
a two-dimensional zonotope deﬁning a set of values of (x, y). We will assume a
ﬁxed number of generators m. So, for addition, its abstract transformer is of the
form

f a((cid:104)c10, . . . , c1m(cid:105), (cid:104)c20, . . . , c2m(cid:105))

Compare this to the interval domain, where f a([l1, u1], [l2, u2])

It turns out that addition over zonotopes is straightforward: we just sum up

the coefﬁcients:

f a((cid:104)c10, . . . , c1m(cid:105), (cid:104)c20, . . . , c2m(cid:105)) = (cid:104)c10 + c20, . . . , c1m + c2m(cid:105)

Example 9.E Consider the simple zonotope (0 + (cid:101)1, 1 + (cid:101)2). This represents the
following box:

CHAPTER 9. NEURAL ZONOTOPE ABSTRACTION

102

2
1

−1

1

The set of possible values we can get by adding the x and y dimensions in
this box is the interval between −1 and 3. Following the deﬁnition of the abstract
transformer for addition:

f a((cid:104)0, 1, 0(cid:105), (cid:104)1, 0, 1(cid:105)) = (cid:104)1, 1, 1(cid:105)

That is the output zonotope is the set

{1 + (cid:101)1 + (cid:101)2 | (cid:101)1, (cid:101)2 ∈ [−1, 1]}

which is the interval [−1, 3].

Aﬃne Functions

For an afﬁne function

f (x1, . . . , xn) = ∑

ajxj

j

where aj ∈ R, we can deﬁne the abstract transformer as follows:

f a((cid:104)c1i(cid:105), . . . (cid:104)cni(cid:105)) =

(cid:42)

∑
j

ajcj0, . . . , ∑

ajcjm

j

(cid:43)

Intuitively, we apply f to the center point and coefﬁcients of (cid:101)1, (cid:101)2, etc.

Example 9.F Consider f (x, y) = 3x + 2y. Then,

f a((cid:104)1, 2, 3(cid:105), (cid:104)0, 1, 1(cid:105)) = (cid:104) f (1, 0), f (2, 1), f (3, 1)(cid:105)

= (cid:104)3, 8, 11(cid:105)

(cid:4)

(cid:4)

CHAPTER 9. NEURAL ZONOTOPE ABSTRACTION

103

9.3 Abstract Transformers of Activation Functions

We now discuss how to construct an abstract transformer for the ReLU activation
function.

Limitations of the Interval Domain

Let’s ﬁrst recall the interval abstract transformer of ReLU:

relua([l, u]) = [relu(l), relu(u)]

The issue with the interval domain is we don’t know how points in the output
interval relua([l, u]) relate to the input interval [l, u]—i.e., which inputs are re-
sponsible for which outputs.

Geometrically, we think of the interval domain as approximating the ReLU

function with a box as follows:

relu(u)

relu(u)

relu(l)

l

u

l

u

The ﬁgure on the left shows the case where the lower bound is negative and the
upper bound is positive; the right ﬁgure shows the case where the lower bound is
positive.

A Zonotope Transformer for ReLU

Let’s slowly build the ReLU abstract transformer for zonotopes. We’re given a
1-dimensional zonotope (cid:104)ci(cid:105)i as input. We will use u to denote the upper bound
of the zonotope and l the lower bound.

relua((cid:104)ci(cid:105)i) =






(cid:104)ci(cid:105)i
(cid:104)0(cid:105)i
?

for l (cid:62) 0
for u (cid:54) 0
otherwise

CHAPTER 9. NEURAL ZONOTOPE ABSTRACTION

104

If l (cid:62) 0, then we simply return the input zonotope back; if u (cid:54) 0, then the
answer is 0; when the zonotope has both negative and positive values, there are
many ways to deﬁne the output, and so I’ve left it as a question mark. The easy
approach is to simply return the interval [l, u] encoded as a zonotope. But it turns
out that we can do better: since zonotopes allow us to relate inputs and outputs,
we can shear a box into a parallelogram that ﬁts the shape of ReLU more tightly,
as follows:

relu(u)

relu(u)

l

u

l

u

The approximation on the right has a smaller area than the approximation af-
forded by the interval domain on the left. The idea is that a smaller area results in
a better approximation, albeit an incomparable one, as the parallelogram returns
negative values, while the box doesn’t. Let’s try to describe this parallelogram as
a zonotope.

The bottom face of the zonotope is the line

y = λx

for some slope λ. It follows that the top face must be

y = λx + u(1 − λ)

If we set λ = 0, we get two horizontal faces, i.e., the interval approximation shown
above. The higher we crank up λ, the tighter the parallelogram gets. But, we can’t
increase λ past u/(u − l); this ensures that the parallelogram covers the ReLU
along the input range [l, u]. So, we will set

λ =

u
u − l

CHAPTER 9. NEURAL ZONOTOPE ABSTRACTION

105

It follows that the distance between the top and bottom faces of the parallelo-
gram is u(1 − λ),. Therefore, the center of the zonotope (in the vertical axis) must
be the point

η =

u(1 − λ)
2

With this information, we can complete the deﬁnition of relua as follows:

relua((cid:104)c1, . . . , cm(cid:105)) =





for l (cid:62) 0
(cid:104)ci(cid:105)i,
for u (cid:54) 0
(cid:104)0(cid:105)i,
(cid:104)λc1, . . . , λcm, 0(cid:105) + (cid:104)η, 0, 0, . . . , η(cid:105) otherwise

There are two non-trivial things we do here:

• First, we add a new generator, (cid:101)m+1, in order to stretch the parallelogram in
the vertical axis; its coefﬁcient is η, which is half the hight of the parallelo-
gram.

• Second, we add the input zonotope scaled by λ with coefﬁcient 0 for the new
generator; this ensures that we capture the relation between the input and
output.

Let’s look at an example for clarity:

Example 9.G Say we invoke relua with the interval between l = −1 and u = 1, i.e.,

relua((cid:104)0, 1(cid:105))

Here, λ = 0.5 and η = 0.25. So the result of relua is the following zonotope:

(cid:104)0, 0.5, 0(cid:105) + (cid:104)0.25, 0, 0.25(cid:105) = (cid:104)0.25, 0.5, 0.25(cid:105)

The 2-dimensional zonotope composed of the input and output zonotopes of relua
is

((cid:104)0, 1, 0(cid:105), (cid:104)0.25, 0.5, 0.25(cid:105))

or, explicitly,

(0 + (cid:101)1, 0.25 + 0.5(cid:101)1 + 0.25(cid:101)2)
This zonotope, centered at (0, 0.25), is illustrated below:

CHAPTER 9. NEURAL ZONOTOPE ABSTRACTION

106

1

η = 0.25

−1

1

(cid:4)

Other Abstract Transformers

We saw how to design an abstract transformer for ReLU. We can follow a similar
approach to design abstract transformers for sigmoid. It is indeed a good exercise
to spend some time designing a zonotope transformer for sigmoid or tanh—and
don’t look at the literature (Singh et al., 2018)!

It is interesting to note that as the abstract domain gets richer—allowing cra-
zier and crazier shapes—the more incomparable abstract transformers you can
derive (Sharma et al., 2014). With the interval abstract domain, which is the sim-
plest you can go without being trivial, the best you can do is a box to approximate
a ReLU or a sigmoid. But with zonotopes, there are inﬁnitely many shapes that
you can come up with. So designing abstract transformers becomes an art, and
it’s hard to predict which transformers will do well in practice.

9.4 Abstractly Interpreting Neural Networks with Zonotopes

We can now use our zonotope abstract transformers to abstractly interpret an en-
tire neural network in precisely the same way we did intervals. We review the
process here for completeness.

Recall that a neural network is deﬁned as a graph G = (V, E), giving rise to
a function fG : Rn → Rm, where n = |Vin| and m = |Vo|. We would like to
construct an abstract transformer f a
G that takes an n-dimensional zonotope and
outputs an m-dimensional zonotope.

CHAPTER 9. NEURAL ZONOTOPE ABSTRACTION

107

We deﬁne f a

G((cid:104)c1j(cid:105), . . . (cid:104)cnj(cid:105)) as follows:

• First, for every input node vi, we deﬁne

outa(vi) = (cid:104)cij(cid:105)j

Recall that we assume a ﬁxed ordering of nodes.

• Second, for every non-input node v, we deﬁne

outa(v) = f a

v (outa(v1), . . . , outa(vk))

where f a
(v1, v), . . . , (vk, v),

v is the abstract transformer of fv, and v has the incoming edges

• Finally, the output of f a

G is the m-dimensional zonotope

(outa(v1), . . . , outa(vm))

where v1, . . . , vm are the output nodes.

One thing to note is that some abstract transformers (for activation functions)
add new generators. We can assume that all of these generators are already in the
input zonotope but with coefﬁcients set to 0, and they only get non-zero coefﬁ-
cients in the outputs of activation function nodes.

Example 9.H Consider the following neural network, which we saw in the last
chapter,

where

v2

v1

v3

fv2(x) = −x
fv3(x) = x1 + x2

CHAPTER 9. NEURAL ZONOTOPE ABSTRACTION

108

Clearly, for any input x, fG(x) = 0. Consider any input zonotope (cid:104)ci(cid:105). The output
node, v3, receives the two-dimensional zonotope

((cid:104)−ci(cid:105), (cid:104)ci(cid:105))

The two dimensions cancel each other out, resulting in the zonotope (cid:104)0(cid:105), which is
the singleton set {0}.

In contrast, with the interval domain, given input interval [0, 1], you get the
(cid:4)

output interval [−1, 1].

Looking Ahead

We’ve seen the zonotope domain, an efﬁcient extension beyond simple interval
arithmetic. Next, we will look at full-blown polyhedra.

To my knowledge, the zonotope domain was ﬁrst introduced by Girard (2005)
in the context of hybrid-system model checking. In the context of neural-network
veriﬁcation, Gehr et al. (2018) were the ﬁrst to use zonotopes, and introduced
precise abstract transformers (Singh et al., 2018), one of which we covered here.
In practice, we try to limit the number of generators to keep veriﬁcation fast. This
can be done by occasionally projecting out some of the generators heuristically as
we’re abstractly interpreting the neural network.

A standard optimization in program analysis is to combine program opera-
tions and construct more precise abstract transformers for the combination. This
allows us to extract more relational information. In the context of neural networks,
this amounts to combining activation functions in a layer of the network. Singh
et al. (2019a) showed how to elegantly do this for zonotopes.

109

Chapter 10

Neural Polyhedron Abstraction

In the previous chapter, we saw the zonotope abstract domain, which is more
expressive than the interval domain. Speciﬁcally, instead of approximating func-
tions using a hyperrectangle, the zonotope domain allows us to approximate func-
tions using a zonotope, e.g., a parallelogram, capturing relations between differ-
ent dimensions.

In this section, we look at an even more expressive abstract domain, the poly-
hedron domain. Unlike the zonotope domain, the polyhedron domain allows us
to approximate functions using arbitrary convex polyhedra. A polyhedron in Rn
is a region made of straight (as opposed to curved) faces; a convex shape is one
where the line between any two points in the shape is completely contained in the
shape. Convex polyhedra can be speciﬁed as a set of linear inequalities. Using
convex polyhedra, we approximate a ReLU as follows:

relu(u)

l

u

This is the smallest convex polyhedron that approximates ReLU. You can vi-
sually check that it is convex. This approximation is clearly more precise than
that afforded by the interval and zonotope domains, as it is fully contained in the
approximations of ReLU in those domains:

CHAPTER 10. NEURAL POLYHEDRON ABSTRACTION

110

relu(u)

relu(u)

l

u

l

u

10.1 Convex Polyhedra

We will deﬁne a polyhedron in a manner analogous to a zonotope, using a set
of m generator variables, (cid:101)1, . . . , (cid:101)m. With zonotopes the generators are bounded
in the interval [−1, 1]; with polyhedra, generators are bounded by a set of linear
inequalities.

Let’s ﬁrst revisit and generalize the deﬁnition of a zonotope. A zonotope in Rn

is a set of points deﬁned as follows:

(cid:40)(cid:32)

c10 +

m
∑
i=1

c1i · (cid:101)i, . . . , cn0 +

m
∑
i=1

cni · (cid:101)i

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

F((cid:101)1, . . . , (cid:101)m)

where F is a Boolean function that evaluates to true iff all of its arguments are
between −1 and 1.

With polyhedra, we will deﬁne F as a set (conjunction) of linear inequalities

over the generator variables, e.g.,

0 (cid:54) (cid:101)1

(cid:54) 5 ∧ (cid:101)1 = (cid:101)2

(equalities are deﬁned as two inequalities). We will always assume that F deﬁnes
a bounded polyhedron, i.e., gives a lower and upper bound for each generator;
(cid:54) 0 is not allowed, because it does not enforce a lower bound on (cid:101)1.
e.g., (cid:101)1
In the 1-dimensional case, a polyhedron is simply an interval. Let’s look at

higher dimensional examples:

Example 10.A Consider the following 2-dimensional polyhedron:

{((cid:101)1, (cid:101)2) | F((cid:101)1, (cid:101)2)}

CHAPTER 10. NEURAL POLYHEDRON ABSTRACTION

111

where

F ≡ 0 (cid:54) (cid:101)1

(cid:54) 1 ∧ (cid:101)2

(cid:54) (cid:101)1 ∧ (cid:101)2

(cid:62) 0

This polyhedron is illustrated as follows:

Clearly, this shape is not a zonotope, because its faces are not parallel.

(cid:4)

Example 10.B In 3 dimensions, a polyhedron may look something like this1

One can add more faces by adding more linear inequalities to F.

(cid:4)

From now on, given a polyhedron

(cid:40)(cid:32)

c10 +

m
∑
i=1

c1i · (cid:101)i, . . . , cn0 +

m
∑
i=1

cni · (cid:101)i

(cid:33)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)

(cid:41)

F((cid:101)1, . . . , (cid:101)m)

we will abbreviate it as the tuple:

1Adapted from Westburg (2017).

((cid:104)c1i(cid:105)i, . . . (cid:104)cni(cid:105)i, F)

CHAPTER 10. NEURAL POLYHEDRON ABSTRACTION

112

10.2 Computing Upper and Lower Bounds

Given a polyhedron ((cid:104)c1i(cid:105)i, . . . (cid:104)cni(cid:105)i, F), we will often want to compute the lower
and upper bounds of one of the dimensions. Unlike with the interval and zono-
tope domains, this process is not straightforward. Speciﬁcally, it involves solving
a linear program, which takes polynomial time in the number of variables and
constraints.

To compute the lower bound of the jth dimension, we solve the following lin-

ear programming problem:

m
∑
i=1

cji(cid:101)i

min cj0 +

s.t. F

Similarly, we compute the upper bound of the jth dimension by maximizing in-
stead of minimizing.

Example 10.C Take our triangle shape from Example 10.A, deﬁned using two gen-
erators:

((cid:104)0, 1, 0(cid:105), (cid:104)0, 0, 1(cid:105), F)

where

F ≡ 0 (cid:54) (cid:101)1

(cid:54) 1 ∧ (cid:101)2

(cid:54) (cid:101)1 ∧ (cid:101)2

(cid:62) 0

To compute the upper bound of ﬁrst dimension, we solve

max (cid:101)1
s.t. F

The answer here is 1, which is obvious from the constraints.

(cid:4)

10.3 Abstract Transformers for Polyhedra

We’re now ready to go over some abstract transformers for polyhedra.

CHAPTER 10. NEURAL POLYHEDRON ABSTRACTION

113

Aﬃne Functions

For afﬁne functions, it is really the same transformer as the one for the zonotope
domain, except that we carry around the set of linear inequalities F—for the zono-
tope domain, F is ﬁxed throughout.
Speciﬁcally, for an afﬁne function

f (x1, . . . , xn) = ∑

ajxj

j

where aj ∈ R, we can deﬁne the abstract transformer as follows:

f a((cid:104)c1i(cid:105), . . . (cid:104)cni(cid:105), F) =

(cid:32)(cid:42)

∑
j

ajcj0, . . . , ∑

ajcjm

j

(cid:43)

(cid:33)

, F

Notice that the set of linear inequalities does not change between the input and

output of the function—i.e., there are no new constraints added.

Example 10.D Consider f (x, y) = 3x + 2y . Then,

f a((cid:104)1, 2, 3(cid:105), (cid:104)0, 1, 1(cid:105), F) = ((cid:104)3, 8, 11(cid:105), F)

(cid:4)

Rectiﬁed Linear Unit

Let’s now look at the abstract transformer for ReLU, which we illustrated earlier
in the chapter:

relu(u)

l

u

CHAPTER 10. NEURAL POLYHEDRON ABSTRACTION

114

This is the tightest convex polyhedron we can use to approximate the ReLU
function. We can visually verify that tightening the shape any further will either
make it not an approximation or not convex—e.g., by bending the top face down-
wards, we get a better approximation but lose convexity.

Let’s see how to formally deﬁne relua. The key point is that the top face is the

line

y =

u(x − l)
u − l

This is easy to check using vanilla geometry. Now, our goal is to deﬁne the shaded
region, which is bounded by y = 0 from below, y = x from the right, and y =
u(x−l)
u−l

from above.

We therefore deﬁne relua as follows:

relua((cid:104)ci(cid:105)i, F) = ((cid:104)0, 0, . . . , 0
(cid:125)

(cid:124)

, 1(cid:105), F(cid:48))

(cid:123)(cid:122)
m

where

F(cid:48) ≡ F ∧ (cid:101)m+1

(cid:54) u((cid:104)ci(cid:105) − l)
(u − l)

∧ (cid:101)m+1
∧ (cid:101)m+1

(cid:62) 0
(cid:62) (cid:104)ci(cid:105)

There are a number of things to note here:

• l and u are the lower and upper bounds of the input polyhedron, which can

be computed using linear programming.

• (cid:104)ci(cid:105)i is used to denote the full term c0 + ∑m

i=1 ci(cid:101)i.

• Observe that we’ve added a new generator, (cid:101)m+1. The new set of constraints
F(cid:48) relate this new generator to the input, effectively deﬁning the shaded re-
gion.

Example 10.E Consider the 1-dimensional polyhedron

((cid:104)0, 1(cid:105), −1 (cid:54) (cid:101)1

(cid:54) 1)

CHAPTER 10. NEURAL POLYHEDRON ABSTRACTION

115

which is the interval between −1 and 1. Invoking

relua((cid:104)0, 1(cid:105), −1 (cid:54) (cid:101)1

(cid:54) 1)

results in ((cid:104)0, 0, 1(cid:105), F(cid:48)), where

F(cid:48) ≡ − 1 (cid:54) (cid:101)1

(cid:54) 1
(cid:54) (cid:101)1 + 1
2

∧ (cid:101)2

∧ (cid:101)2
∧ (cid:101)2

(cid:62) 0
(cid:62) (cid:101)1

If we plot the region deﬁned by F(cid:48), using (cid:101)1 as the x-axis and (cid:101)2 as the y-axis, we
get the shaded region

1

−1

1

(cid:4)

Other Activation Functions

For ReLU, the transformer we presented is the most precise. For other activation
functions, like sigmoid, there are many ways to deﬁne abstract transformers for
the polyhedron domain. Intuitively, one can keep adding more and more faces to
the polyhedron to get a more precise approximation of the sigmoid curve.

CHAPTER 10. NEURAL POLYHEDRON ABSTRACTION

116

10.4 Abstractly Interpreting Neural Networks with Polyhedra

We can now use our abstract transformers to abstractly interpret an entire neural
network, in precisely the same way we did for zonotopes, except that we’re now
carrying around a set of constraints. We review the process here for completeness.
Recall that a neural network is deﬁned as a graph G = (V, E), giving rise to
a function fG : Rn → Rm, where n = |Vin| and m = |Vo|. We would like to
construct an abstract transformer f a
G that takes an n-dimensional polyhedron and
outputs an m-dimensional polyhedron.

We deﬁne f a

G((cid:104)c1j(cid:105), . . . (cid:104)cnj(cid:105), F) as follows:

• First, for every input node vi, we deﬁne

outa(vi) = ((cid:104)cij(cid:105)j, F)

Recall that we assume a ﬁxed ordering of nodes.

• Second, for every non-input node v, we deﬁne

(cid:32)

outa(v) = f a
v

p1, . . . , pk,

(cid:33)

k
(cid:94)

i=1

Fk

v is the abstract transformer of fv, v has the incoming edges (v1, v), . . . , (vk, v),

where f a
and

outa(vi) = (pi, Fi)

Observe what is happening here: we’re combining (with ∧) the constraints
from the incoming edges. This ensures that we capture the relations between
incoming values.

• Finally, the output of f a

G is the m-dimensional polyhedron

(cid:32)

p1, . . . , pm,

(cid:33)

m
(cid:94)

i=1

Fi

where v1, . . . , vm are the output nodes and outa(vi) = (pi, Fi)

CHAPTER 10. NEURAL POLYHEDRON ABSTRACTION

117

Some abstract transformers (for activation functions) add new generators. We
can assume that all of these generators are already in the input polyhedron but
with coefﬁcients set to 0, and they only get non-zero coefﬁcients in the outputs of
activation function nodes.

Looking Ahead

We looked at the polyhedron abstract domain, which was ﬁrst introduced by
Cousot and Halbwachs (1978). To minimize the size of the constraints, Singh
et al. (2019b) use a specialized polyhedron restriction that limits the number of
constraints, and apply it to neural-network veriﬁcation. Another representation
of polyhedra, with specialized abstract transformers for convolutional neural net-
works, is ImageStars (Tran et al., 2020b). For a good description of efﬁcient poly-
hedron domain operations and representations, for general programs, please con-
sult Singh et al. (2017).

118

Chapter 11

Verifying with Abstract Interpretation

We have seen a number of abstract domains that allow us to evaluate a neural
network on an inﬁnite set of inputs. We will now see how to use this idea for
veriﬁcation of speciﬁc properties. While abstract interpretation can be used, in
principle, to verify any property in our language of correctness properties, much
of the work in the literature is restricted to speciﬁc properties of the form:

{ precondition }
r ← f (x)
{ postcondition }

where the precondition deﬁnes a set of possible values for x, the inputs of the neu-
ral network, and the postcondition deﬁnes a set of possible correct values of r, the
outputs of the neural network. To verify such properties with abstract interpreta-
tion, we need to perform three tasks:

1. Soundly represent the set of values of x in the abstract domain.

2. Abstractly interpret the neural network f on all values of x, resulting in an

overapproximation of values of r.

3. Check that all values of r satisfy the postcondition.

We’ve seen how to do (2), abstractly interpreting the neural network. We will now
see how to do (1) and (3) for speciﬁc correctness properties from the literature.

CHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION

119

11.1 Robustness in Image Recognition

In image recognition, we’re often interested in ensuring that all images similar to
some image c have the same prediction as the label of c. Let’s say that the label of
c is y. Then we can deﬁne robustness using the following property:

{ (cid:107)x − c(cid:107)p

(cid:54) (cid:101) }

r ← f (x)
{ class(r) = y }

where (cid:107)x(cid:107)p is the (cid:96)p norm of a vector and (cid:101) > 0. Typically we use the (cid:96)2 (Eu-
clidean) or the (cid:96)∞ norm as the distance metric between two images:

(cid:107)z(cid:107)2 =

(cid:114)∑

|zi|2

i

(cid:107)z(cid:107)∞ = max

|zi|

i
Intuitively, the (cid:96)2 norm is the length of the straight-line between two images in
Rn, while (cid:96)∞ is the largest discrepancy between two images. For example, if each
element of an image’s vector represents one pixel, then the (cid:96)∞ norm tells us the
biggest difference between two corresponding pixels.

Example 11.A

(cid:107)(1, 2) − (2, 4)(cid:107)2 = (cid:107)(−1, −2)(cid:107)2

√

3

=

(cid:107)(1, 2) − (2, 4)(cid:107)∞ = (cid:107)(−1, −2)(cid:107)∞

= 2

(cid:4)

Example 11.B Consider an image c where every element of c represents the bright-
ness of a grayscale pixel, from black to white, say from 0 to 1. If we want to repre-
sent the set of all images that are like c but where each pixel differs by a brightness

CHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION

120

amount of 0.2, then we can use the (cid:96)∞ norm in the precondition, i.e., the set of im-
ages x where

(cid:107)x − c(cid:107)∞ (cid:54) 0.2

This is because the (cid:96)∞ norm captures the maximum discrepancy a pixel in c can
withstand. As an example, consider the handwritten 7 digit on the left and a
version of it on the right where each pixel’s brightness was changed by up to 0.2
randomly:

Now consider the case where we want to represent all images that are like c
but where a small region has a very different brightness. For example, on the left
we see the handwritten 7 and on the right we see the same handwritten digit but
with a small bright dot:

To characterize a set of images that have such noise, like the dot above, we
shouldn’t use (cid:96)∞ norm, because (cid:96)∞ bounds the brightness difference for all pixels,
but not some pixels, and here the brightness difference that results in the white dot

CHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION

121

is extreme—from 0 (black) to white (1). Instead, we can use the (cid:96)2 norm. For the
above pair of images, their (cid:96)∞-norm distance is 1; their (cid:96)2-norm distance is also 1,
but the precondition

{(cid:107)x − c(cid:107)∞ (cid:54) 1}

includes the images that are all black or all white, which are clearly not the digit
7. The precondition

{(cid:107)x − c(cid:107)2

(cid:54) 1}

on the other hand, only allows a small number of pixels to signiﬁcantly differ in
(cid:4)
brightness.

For veriﬁcation, we will start by focusing on the (cid:96)∞-norm case and the interval

domain.

Abstracting the Precondition

Our ﬁrst goal is to represent the precondition in the interval domain. The precon-
dition is the set of the following images:

{x | (cid:107)x − c(cid:107)∞ (cid:54) (cid:101)}

Example 11.C Say c = (0, 0) and (cid:101) = 1. Then the above set is the following region:

1

−1

1

As the illustration above hints, it turns out that we can represent the set {x |

(cid:107)x − c(cid:107)∞ (cid:54) (cid:101)} precisely in the interval domain as

I = ([c1 − (cid:101), c1 + (cid:101)], . . . , [cn − (cid:101), cn + (cid:101)])

Informally, this is because the (cid:96)∞ norm allows us to take any element of c and
change it by (cid:101) independently of other dimensions.

(cid:4)

CHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION

122

Checking the Postcondition

Now that we have represented the set of values that x can take in the interval do-
main as I, we can go ahead and evaluate the abstract transformer f a(I), resulting
in an output of the form

I(cid:48) = ([l1, u1], . . . , [lm, um])

representing all possible values of r, and potentially more.

The postcondition speciﬁes that class(r) = y. Recall that class(r) is the index of
the largest element of r. To prove the property, we have to show that for all r ∈ I(cid:48),
class(r) = y. We make the observation that

if ly > ui for all i (cid:54)= y,
then for all r ∈ I(cid:48), class(r) = y

In other words, if the yth interval is larger than all others, then we know that
the classiﬁcation is always y. Notice that this is a one-sided check: if ly (cid:54) ui
for some i (cid:54)= y, then we can’t disprove the property. This is because the set I(cid:48)
overapproximates the set of possible predictions of the neural network on the
precondition. So I(cid:48) may include spurious predictions.

Example 11.D Suppose that

f a(I) = I(cid:48) = ([0.1, 0.2], [0.3, 0.4])

Then, class(r) = 2 for all r ∈ I(cid:48). This is because the second interval is strictly larger
than the ﬁrst interval.
Now suppose that

I(cid:48) = ([0.1, 0.2], [0.15, 0.4])

These two intervals overlap in the region 0.15 to 0.2. This means that we cannot
conclusively say that class(r) = 2 for all r ∈ I(cid:48), and so veriﬁcation fails.
(cid:4)

CHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION

123

Verifying Robustness with Zonotopes

Let’s think of how to check the (cid:96)∞-robustness property using the zonotope do-
main. Since the precondition is a hyperrectangular set, we can precisely represent
it as a zonotope Z. Then, we evaluate the abstract transformer f a(Z), resulting in
a zonotope Z(cid:48).

The fun bit is checking the postcondition. We want to make sure that di-
mension y is greater than all others. The problem boils down to checking if a
1-dimensional zonotope is always > 0. Consider the zonotope

Z(cid:48) = ((cid:104)c1i(cid:105), . . . (cid:104)cmi(cid:105))

To check that dimension y is greater than dimension j, we check if the lower bound
of the 1-dimensional zonotope

(cid:104)cyi(cid:105) − (cid:104)cji(cid:105)

is > 0.

Example 11.E Suppose that

which is visualized as follows, with the center point (2,4) in red:

Z(cid:48) = (2 + (cid:101)1, 4 + (cid:101)1 + (cid:101)2)

5
4
3
2
1

1

2

3

4

Clearly, for any point (x, y) in this region, we have y > x. To check that y > x
mechanically, we subtract the x dimension from the y dimension:

(4 + (cid:101)1 + (cid:101)2) − (2 + (cid:101)1) = 2 + (cid:101)2

The resulting 1-dimensional zonotope (2 + (cid:101)2) denotes the interval [1, 3], which is
(cid:4)
greater than zero.

CHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION

124

Verifying Robustness with Polyhedra

With the polyhedron domain, the story is analogous to zonotopes but requires in-
voking a linear-program solver. We represent the precondition as a hyperrectan-
gular polyhedron Y. Then, we evaluate the abstract transformer, f a(Y), resulting
in the polyhedron

Y(cid:48) = ((cid:104)c1i(cid:105), . . . (cid:104)cmi(cid:105), F)

To check if dimension y is greater than dimension j, we ask a linear-program
solver if the following constraints are satisﬁable

F ∧ (cid:104)cyi(cid:105) > (cid:104)cji(cid:105)

Robustness in (cid:96)2 Norm

Let’s now consider the precondition with the set of images within an (cid:96)2 norm of
c:

{x | (cid:107)x − c(cid:107)2

(cid:54) (cid:101)}

Example 11.F Say c = (0, 0) and (cid:101) = 1. Then the above set is the following circular
region:

1

−1

1

This set cannot be represented precisely in the interval domain. To ensure that
we can verify the property, we need to overapproximate the circle with a box. The
best we can do is using the tightest box around the circle, i.e., ([−1, 1], [−1, 1]),
shown below in red:

(cid:4)

CHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION

125

1

−1

1

The zonotope and polyhedron domains also cannot represent the circular set
precisely. However, there isn’t a tightest zonotope or polyhedron that overap-
proximates the circle. For example, with polyhedra, one can keep adding more
and more faces, getting a better and better approximation, as illustrated below:

In practice, there is, of course, a precision–scalability tradeoff: more faces mean

more complex constraints and therefore slower veriﬁcation.

11.2 Robustness in Natural-Language Processing

We will now take a look at another robustness property from natural-language
processing. The goal is to show that replacing words with synonyms does not
change the prediction of the neural network. For instance, a common task is sen-
timent analysis, where the neural network predicts whether, say, a movie review
is positive or negative. Replacing “amazing” with “outstanding” should not fool
the neural network into thinking a positive review is a negative one.

We assume that the input to the neural network is a vector where element i is
a numerical representation of the ith word in the sentence, and that each word w
has a ﬁnite set of possible synonyms Sw, where we assume w ∈ Sw. Just as with
images, we assume a ﬁxed sentence c with label y for which we want to show

CHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION

126

robustness. We therefore deﬁne the correctness property as follows:

{ xi ∈ Sci for all i }
r ← f (x)
{ class(r) = y }

Intuitively, the precondition deﬁnes all vectors x that are like c but where some
words are replaced by synonyms.

The set of possible vectors x is ﬁnite, but it is exponentially large in the length
of the input sentence. So it is not wise to verify the property by evaluating the
neural network on every possible x. We can, however, represent an overapproxi-
mation of the set of possible sentences in the interval domain. The idea is to take
interval between the largest and smallest possible numerical representations of
the synonyms of every word, as follows:

([min Sc1, max Sc1], . . . , [min Scn, max Scn])

This set contains all the values of x, and more, but it is easy to construct, since
we need only go through every set of synonyms Sci individually, avoiding an
exponential explosion.

The rest of the veriﬁcation process follows that of image robustness. In prac-
tice, similar words tend to have close numerical representations, thanks to the
power of word embeddings (Mikolov et al., 2013). This ensures that the interval
is pretty tight. If words received arbitrary numerical representations, then our
abstraction can be arbitrarily bad.

Looking Ahead

We saw examples of how to verify properties via abstract interpretation. The an-
noying thing is that for every abstract domain and every property of interest, we
may need custom operations. Most works that use abstract interpretation so far
have focused on the properties I covered in this chapter. Other properties from
earlier in the book can also be veriﬁed via the numerical domains we’ve seen. For

CHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION

127

example, the aircraft controller from Chapter 3 has properties of the form:

{ d (cid:62) 55947, vown (cid:62) 1145, vint
r ← f (d, vown, vint, . . .)
{ score of nothing in r is below 1500 }

(cid:54) 60 }

Note that the precondition can be captured precisely in the interval domain.

At the time of writing, abstraction-based techniques have been applied suc-
cessfully to relatively large neural networks, with up to a million neurons along
more than thirty layers (Müller et al., 2020; Tran et al., 2020a). Achieving such
results requires performant implementations, particularly for more complicated
domains like the zonotope and polyhedron domain. For instance, Müller et al.
(2020) come up with data-parallel implementations of polyhedron abstract trans-
formers that run on a GPU. Further, there are heuristics that can be employed to
minimize the number of generators in the zonotope domain—limiting the number
of generators reduces precision while improving efﬁciency. It is also important to
note that thus far most of the action in the abstraction-based veriﬁcation space,
and veriﬁcation of neural networks at large, has been focused on (cid:96)p-robustness
properties for images. (We’re also starting to see evidence that the ideas can ap-
ply to natural-language robustness (Zhang et al., 2021).) So it’s unclear whether
veriﬁcation will work for more complex perceptual notions of robustness—e.g.,
rotating an image or changing the virtual background on a video—or other more
complex properties and domains, e.g., malware detection.

The robustness properties we discussed check if a ﬁxed region of inputs sur-
rounding a point lead to the same prediction. Alternatively, we can ask, how big
is the region around a point that leads to the same prediction? Naïvely, we can do this
by repeatedly performing veriﬁcation with larger and larger (cid:96)2 or (cid:96)∞ bounds un-
til veriﬁcation fails. Some techniques exploit the geometric structure of a neural
network—induced by ReLUs—to grow a robust region around a point (Zhang
et al., 2018b; Fromherz et al., 2021).

As we discussed throughout this part of the book, abstract-interpretation tech-
niques can make stupid mistakes due to severe overapproximations. However,
abstract interpretation works well in practice for veriﬁcation. Why? Two recent
papers shed light on this question from a theoretical perspective (Baader et al.,

CHAPTER 11. VERIFYING WITH ABSTRACT INTERPRETATION

128

2020; Wang et al., 2020). The papers generalize the universal approximation prop-
erty of neural networks (Section 2.1) to veriﬁcation with the interval domain or
any domain that is more precise. Speciﬁcally, imagine that we have a neural net-
work that is robust as per the (cid:96)∞ norm; i.e., the following property is true for a
bunch of inputs of interest:

{ (cid:107)x − c(cid:107)∞ (cid:54) (cid:101) }
r ← f (x)
{ class(r) = y }

But suppose that abstract interpretation using the interval domain fails to prove
robustness for most (or all) inputs of interest. It turns out that we can always con-
struct a neural network f (cid:48), using any realistic activation function (ReLU, sigmoid,
etc.), that is very similar to f —as similar as we like—and for which we can prove
robustness using abstract interpretation. The bad news, as per Wang et al. (2020),
is that the construction of f (cid:48) is likely exponential in the size of the domain.

129

Chapter 12

Abstract Training of Neural Networks

You have reached the ﬁnal chapter of this glorious journey. So far on our jour-
ney, we have assumed that we’re given a neural network that we want to verify.
These neural networks are, almost always, constructed by learning from data. In
this chapter, we will see how to train a neural network that is more amenable to
veriﬁcation via abstract interpretation for a property of interest.

12.1 Training Neural Networks

We begin by describing neural network training from a data set. Speciﬁcally, we
will focus throughout this chapter on a classiﬁcation setting.

Optimization Objective

A dataset is of the form

{(x1, y1), . . . , (xm, ym)}
where each xi ∈ Rn is an input to the neural network, e.g., an image or a sentence,
and yi ∈ {0, 1} is a binary label, e.g., indicating if a given image is that of a cat
or if a sentence has a positive or negative sentiment. Each item in the dataset is
typically assumed to be sampled independently from a probability distribution,
e.g., the distribution of all images of animals.

Given a dataset, we would like to construct a function in Rn → R that makes
the right prediction on most of the points in the dataset. Speciﬁcally, we assume

CHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS

130

that we have a family of functions represented as a parameterized function fθ,
where θ is a vector of weights. We would like to ﬁnd the best function by searching
the space of θ values. For example, we can have the family of afﬁne functions

fθ(x) = θ1 + θ2x1 + θ3x2

To ﬁnd the best function in the function family, we effectively need to solve an

optimization problem like this one:

argmin
θ

1
m

m
∑
i=1

1[ fθ(xi) = yi]

where 1[b] is 1 if b is true and 0 otherwise. Intuitively, we want the function that
makes the smallest number of prediction mistakes on our dataset {(x1, y1), . . . , (xm, ym)}.

Practically, this optimization objective is quite challenging to solve, since the
objective is non-differentiable—because of the Boolean 1[·] operation, which isn’t
smooth. Instead, we often solve a relaxed optimization objective like mean squared
error (MSE), which minimizes how far fθ’s prediction is from each yi. MSE looks like
this:

argmin
θ

1
m

m
∑
i=1

( fθ(xi) − yi)2

Once we’ve ﬁgured out the best values of θ, we can predict the label of an input x
by computing fθ(x) and declaring label 1 iff fθ(x) (cid:62) 0.5.

We typically use a general form to describe the optimization objective. We
assume that we’re given a loss function L(θ, x, y) which measures how bad is the
prediction fθ(x) is compared to the label y. Formally, we solve

argmin
θ

1
m

m
∑
i=1

L(θ, xi, yi)

(12.1)

Squared error is one example loss function, but there are others, like cross-
entropy loss. For our purposes here, we’re not interested in what loss function is
used.

Loss Function as a Neural Network

The family of functions fθ is represented as a neural network graph Gθ, where
every node v’s function fv may be parameterized by θ. It turns out that we can we

CHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS

131

represent the loss function L also as a neural network; speciﬁcally, we represent L
as an extension of the graph Gθ by adding a node at the very end that computes,
for example, the squared difference between fθ(x) and y. By viewing the loss
function L as a neural network, we can abstractly interpret it, as we shall see later
in the chapter.

Suppose that fθ : Rn → R has a graph of the form

v1

...

vn

vo

where the dotted arrows indicate potentially intermediate nodes. We can con-
struct the graph of a loss function L(θ, x, y) by adding an input node vy for the
label y and creating a new output node vL that compares the output of fθ (the
node vo) with y.

v1

...

vn

vy

vo

vL

Here, input node vy takes in the label y and fvL encodes the loss function, e.g.,
mean squared error ( f (x) − y)2.

Gradient Descent

How do we ﬁnd values of θ that minimize the loss? Generally, this is a hard
problem, so we just settle for a good enough set of values. The simplest thing to
do is to randomly sample different values of θ and return the best one after some
number of samples. But this is a severely inefﬁcient approach.

Typically, neural-network training employs a form of gradient descent. Gradient
It works by

descent is a very old algorithm, due to Cauchy in the mid 1800s.

CHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS

132

starting with a random value of θ and iteratively nudging it towards better values
by following the gradient of the optimization objective. The idea is that starting
from some point x0, if we want to minimize g(x0), then our best bet is to move in
the direction of the negative gradient at x0.

The gradient of a function g(θ) with respect to inputs θ, denoted ∇g, is the

vector of partial derivatives1

(cid:18) ∂g
∂θ1

, . . . ,

(cid:19)

∂g
∂θn

The gradient at a speciﬁc value θ0, denoted (∇g)(θ0), is

(cid:18) ∂g
∂θ1

(θ0), . . . ,

(cid:19)

(θ0)

∂g
∂θn

If you haven’t played with partial derivatives in a while, I recommend Deisenroth
et al. (2020) for a machine-learning-speciﬁc refresher.

Gradient descent can be stated as follows:

1. Start with j = 0 and a random value of θ, called θ0.

2. Set θ j+1 to θ j − η((∇g)(θi)).

3. Set j to j + 1 and repeat.

Here η > 0 is the learning rate, which constrains the size of the change of θ: too
small a value and you’ll make baby steps towards a good solution; too large a
value and you’ll bounce wildly around unable to catch a good region of solu-
tions for θ, potentially even diverging. The choice of η is typically determined
empirically by monitoring the progress of the algorithm for a few iterations. The
algorithm is usually terminated when the loss has been sufﬁciently minimized or
when it starts making tiny steps, asymptotically converging to a solution.

In our setting, our optimization objective is

1
m

m
∑
i=1

L(θ, xi, yi)

1The gradient is typically a column vector, but for simplicity of presentation we treat it as a

row vector here.

CHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS

133

Following the beautiful properties of derivatives, the gradient of this function is

1
m

m
∑
i=1

∇L(θ, xi, yi)

It follows that the second step of gradient descent can be rewritten as

Set θ j+1 to θ j − η
m

∑m

i=1 ∇L(θ j, xi, yi).

In other words, we compute the gradient for every point in the dataset indepen-
dently and take the average.

Stochastic Gradient Descent

In practice, gradient descent is incredibly slow. So people typically use stochastic
gradient descent (SGD). The idea is that, instead of computing the average gradient
in every iteration for the entire dataset, we use a random subset of the dataset to
approximate the gradient. SGD is also known as mini-batch gradient descent. Speciﬁ-
cally, here’s how SGD looks:

1. Start with j = 0 and a random value of θ, called θ0.

2. Divide the dataset into a random set of k batches, B1, . . . , Bk.

3. For i from 1 to k,

Set θ j+1 to θ j −

η
m

∑
(x,y)∈Bi

∇L(θ j, x, y)

Set j to j + 1

4. Go to step 2.

In practice, the number of batches k (equivalently size of the batch) is typically a
function of how much data you can cram into the GPU at any one point.2

2To readers from the future: In the year 2021, graphics cards and some specialized accelera-
tors used to be the best thing around for matrix multiplication. What have you folks settled on,
quantum or DNA computers?

CHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS

134

12.2 Adversarial Training with Abstraction

The standard optimization objective for minimizing loss (Equation (12.1)) is only
interested in, well, minimizing the average loss for the dataset, i.e., getting as
many predictions right. So there is no explicit goal of generating robust neural
networks, for any deﬁnition of robustness. As expected, this translates to neural
networks that are generally not very robust to perturbations in the input. Fur-
thermore, even if the trained network is robust on some inputs, veriﬁcation with
abstract interpretation often fails to produce a proof. This is due to the over-
approximate nature of abstract interpretation. One can always rewrite a neural
network—or any program for that matter—into one that fools abstract interpreta-
tion, causing it to loose a lot of precision and therefore fail to verify properties of
interest. Therefore, we’d like to train neural networks that are friendly for abstract
interpretation.

We will now see how to change the optimization objective to produce robust
networks and how to use abstract interpretation within SGD to solve this opti-
mization objective.

Robust Optimization Objective

Let’s consider the image-recognition-robustness property from the previous chap-
ter: For every (x, y) in our dataset, we want the neural network to predict y on all
images z such that (cid:107)x − z(cid:107)∞ (cid:54) (cid:101). We can characterize this set as

R(x) = {z | (cid:107)x − z(cid:107)∞ (cid:54) (cid:101)}

Using this set, we will rewrite our optimization objective as follows:
m
∑
i=1

argmin
θ

L(θ, z, yi)

max
z∈R(xi)

1
m

(12.2)

Intuitively, instead of minimizing the loss for (xi, yi), we minimize the loss for
the worst-case perturbation of xi from the set R(xi). This is known as a robust-
optimization problem (Ben-Tal et al., 2009). Training the neural network using such
objective is known as adversarial training—think of an adversary (represented
using the max) that’s always trying to mess with your dataset to maximize the
loss as you are performing the training (Madry et al., 2018).

CHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS

135

Solving Robust Optimization via Abstract Interpretation

We will now see how to solve the robust-optimization problem using SGD and
abstract interpretation!

Let’s use the interval domain. The set R(x) can be deﬁned in the interval do-
main precisely, as we saw in the last chapter, since it deﬁnes a set of images within
an (cid:96)∞-norm bound. Therefore, we can overapproximate the inner maximization
by abstractly interpreting L on the entire set R(xi). (Remember that L, as far as
we’re concerned, is just a neural network.) Speciﬁcally, by virtue of soundness of
the abstract transformer La, we know that

where

(cid:18)

max
z∈R(xi)

L(θ, z, yi)

(cid:19)

(cid:54) u

La(θ, R(xi), yi) = [l, u]

In other words, we can overapproximate the inner maximization by abstractly
interpreting the loss function on the set R(xi) and taking the upper bound.
We can now rewrite our robust-optimization objective as follows:

argmin
θ

1
m

m
∑
i=1

upper bound of La(θ, R(xi), yi)

(12.3)

Instead of thinking of La as an abstract transformer in the interval domain, we can
think of it as a function that takes a vector of inputs, denoting lower and upper
bounds of R(x), and returns the pair of lower and upper bounds. We call this idea
ﬂattening the abstract transformer; we illustrate ﬂattening with a simple example:

Example 12.A Consider the ReLU function relu(x) = max(0, x). The interval ab-
stract transformer is

relua([l, u]) = [max(0, l), max(0, u)]

We can ﬂatten it into a function reluaf : R2 → R2 as follows:

reluaf(l, u) = (max(0, l), max(0, u))

CHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS

Notice that reluaf returns a pair in R2 as opposed to an interval.

136

(cid:4)

With this insight, we can ﬂatten the abstract loss function La into Laf. Then, we

just invoke SGD on the following optimization problem,

argmin
θ

1
m

m
∑
i=1

Laf
u (θ, li1, ui1, . . . , lin, uin, yi)

(12.4)

where Laf
lower bound (remember Equation (12.3)), and R(xi) = ([li1, ui1], . . . , [lin, uin]).

u is only the upper bound of the output of Laf, i.e., we throw away the

SGD can optimize such objective because all of the abstract transformers of the
interval domain that are of interest for neural networks are differentiable (almost
everywhere). The same idea can be adapted to the zonotope domain, but it’s a tad
bit uglier.

Example 12.B Given a function f : R → R, its zonotope abstract transformer f a
is one that takes as input a 1-dimensional input zonotope with m generator vari-
ables, (cid:104)c0, . . . , cm(cid:105), and outputs a 1-dimensional zonotope also with m generators,
(cid:104)c(cid:48)

m(cid:105). We can ﬂatten f a by treating it as a function in

0, . . . , c(cid:48)

f af : Rm+1 → Rm+1

where the m + 1 arguments and outputs are the coefﬁcients of the m generator
(cid:4)
variables and the center point.

Flattening does not work for the polyhedron domain, because it invokes a
black-box linear-programming solver for activation functions, which is not dif-
ferentiable.

Looking Ahead

We saw how to use abstract interpretation to train (empirically) more robust neu-
ral networks. It has been shown that neural networks trained with abstract inter-
pretation tend to be (1) more robust to perturbation attacks and (2) are veriﬁably

CHAPTER 12. ABSTRACT TRAINING OF NEURAL NETWORKS

137

robust using abstract interpretation. The second point is subtle: You could have a
neural network that satisﬁes a correctness property of interest, but that does not
mean that an abstract domain will succeed at verifying that the neural network
satisﬁes the property. By incorporating abstract interpretation into training, we
guide SGD towards neural networks that are amenable to veriﬁcation.

The ﬁrst use of abstract interpretation within the training loop came in 2018 (Mir-

man et al., 2018; Gowal et al., 2018). Since then, many approaches have used ab-
stract interpretation to train robust image-recognition as well as natural-language-
processing models (Zhang et al., 2020, 2021; Jia et al., 2019; Xu et al., 2020; Huang
et al., 2019). Robust optimization is a rich ﬁeld (Ben-Tal et al., 2009); to my knowl-
edge, Madry et al. (2018) were the ﬁrst to pose training of (cid:96)p-robust neural net-
works as a robust-optimization problem.

There are numerous techniques for producing neural networks that are amenable

to veriﬁcation. For instance, Sivaraman et al. (2020) use constraint-based veriﬁca-
tion to verify that a neural network is monotone. Since constraint-based tech-
niques can be complete, they can produce counterexamples, which are then used
to retrain the neural network, steering it towards monotonicity. Another interest-
ing direction in the constraint-based world is to train neural networks towards
ReLUs whose inputs are always positive or always negative (Xiao et al., 2019).
This ensures that the generated constraints have as few disjunctions as possible,
because the encoding of the ReLU will be linear (i.e., no disjunction).

(cid:96)p-robustness properties are closely related to the notion of Lipschitz continuity.

For instance, a network f : Rn → Rm is K-Lipschitz under the (cid:96)2 norm if

(cid:107) f (x) − f (y)(cid:107)2

(cid:54) K (cid:107)x − y(cid:107)2

The smallest K satisfying the above is called the Lipschitz constant of f . If we
can bound K, then we can prove (cid:96)2-robustness of f . A number of works aim to
construct networks with constrained Lipschitz constants, e.g., by adding special
layers to the network architecture or modifying the training procedure (Trockman
and Kolter, 2021; Leino et al., 2021; Li et al., 2019)

138

Chapter 13

The Challenges Ahead

My goal with this book is to give an introduction to two salient neural-network
veriﬁcation approaches. But, as you may expect, there are many interesting ideas,
issues, and prospects that we did not discuss.

Correctness Properties

In Part I of the book, we saw a general language of correctness properties, and
saw a number of interesting examples across many domains. One of the hardest
problems in the ﬁeld veriﬁcation—and the one that is least discussed—is how to
actually come up with such properties (also known as speciﬁcations). For instance,
we saw forms of the robustness property many times throughout the book. Ro-
bustness, at a high level, is very desirable. You expect an intelligent system to be
robust in the face of silly transformations to its input. But how exactly do we
deﬁne robustness? Much of the literature focuses on (cid:96)p norms, which we saw in
Chapter 11. But one can easily perform transformations that lie outside (cid:96)p norms,
e.g., rotations to an image, or work in domains where (cid:96)p norms don’t make much
sense, e.g., natural language, source code, or other structured data.

Therefore, coming up with the right properties to verify and enforce is a chal-

lenging, domain-dependent problem requiring a lot of careful thought.

Veriﬁcation Scalability

Every year, state-of-the-art neural networks blow up in size, gaining more and
more parameters. We’re talking about billions of parameters. There is no clear

CHAPTER 13. THE CHALLENGES AHEAD

139

end in sight. This poses incredible challenges for veriﬁcation. Constraint-based
approaches are already not very scalable, and abstraction-based approaches tend
to lose precision with more and more operations. So we need creative ways to
make sure that veriﬁcation technology keeps up with the parameter arms race.

Veriﬁcation Across the Stack

Veriﬁcation research has focused on checking properties of neural networks in
isolation. But neural networks are, almost always, a part of a bigger more com-
plex system. For instance, a neural network in a self-driving car receives a video
stream from multiple cameras and makes decisions on how to steer, speed up,
or brake. These video streams run through layers of encoding, and the decisions
made by the neural network go through actuators with their own control software
and sensors. So, if one wants to claim any serious correctness property of a neural-
network-driven car, one needs to look at all of the software components together
as a system. This makes the veriﬁcation problem challenging for two reasons: (1)
The size of the entire stack is clearly bigger than just the neural network, so scala-
bility can be an issue. (2) Different components may require different veriﬁcation
techniques, e.g., abstract domains.

Another issue with veriﬁcation approaches is the lack of emphasis on the train-
ing algorithms that produce neural networks. For example, training algorithms
may themselves not be robust: a small corruption to the data may create vastly
different neural networks. For instance, a number of papers have shown that poi-
soning the dataset through minimal manipulation can cause a neural network to
pick up on spurious correlations that can be exploited by an attacker. Imagine a
neural network that detects whether a piece of code is malware. This network can
be trained using a dataset of malware and non-malware. By adding silly lines of
code to some of the non-malware code in the dataset, like print("LOL"), we can
force the neural network to learn a correlation between the existence of this print
statement and the fact that a piece of code is not malware (Ramakrishnan and Al-
barghouthi, 2020). This can then be exploited by an attacker. This idea is known
as installing a backdoor in the neural network.

So it’s important to prove that our training algorithm is not susceptible to small
perturbations in the input data. This is a challenging problem, but researchers

CHAPTER 13. THE CHALLENGES AHEAD

140

have started to look at it for simple models (Drews et al., 2020; Rosenfeld et al.,
2020).

Veriﬁcation in Dynamic Environments

Often, neural networks are deployed in a dynamic setting, where the neural net-
work interacts with the environment, e.g., a self-driving car. Proving correctness
in this setting is rather challenging. First, one has to understand the interaction
between the neural network and the environment—the dynamics. This is typically
hard to pin down precisely, as real-world physics may not be as clean as textbook
formulas. Further, the world can be uncertain, e.g., we have to somehow reason
about other crazy drivers on the road. Second, in such settings, one needs to verify
that a neural-network-based controller maintains the system in a safe state (e.g.,
on the road, no crash, etc.). This requires an inductive proof, as one has to reason
about arbitrarily many time steps of control. Third, sometimes the neural network
is learning on-the-go, using reinforcement learning, where the neural network tries
things to see how the environment responds, like a toddler stumbling around.
So we have to ensure that the neural network does not do stupid things as it is
learning.

Recently, there have been a number of approaches attempting to verify prop-
erties of neural networks in dynamic and reinforcement-learning settings (Bastani
et al., 2018; Zhu et al., 2019; Ivanov et al., 2019; Anderson et al., 2020).

Probabilistic Approaches

The veriﬁcation problems we covered are hard, yes-or-no problems. A recent ap-
proach, called randomized smoothing (Cohen et al., 2019; Lécuyer et al., 2019), has
shown that one can get probabilistic guarantees, at least for some robustness prop-
erties (Ye et al., 2020; Bojchevski et al., 2020). Instead of saying a neural network
is robust or not around some input, we say it is robust with a high probability.

141

Bibliography

Martín Abadi and Gordon D. Plotkin. A simple differentiable programming lan-
guage. Proc. ACM Program. Lang., 4(POPL):38:1–38:28, 2020. doi: 10.1145/
3371106. URL https://doi.org/10.1145/3371106.

Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jef-
frey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gor-
don Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete War-
den, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensorﬂow: A sys-
tem for large-scale machine learning.
In Kimberly Keeton and Timothy
Roscoe, editors, 12th USENIX Symposium on Operating Systems Design and Im-
plementation, OSDI 2016, Savannah, GA, USA, November 2-4, 2016, pages 265–
283. USENIX Association, 2016. URL https://www.usenix.org/conference/
osdi16/technical-sessions/presentation/abadi.

Turing Alan. On checking a large routine. In Report of a Conference on 11i9h Speed

Automatic Calculating Machines, pages 67–69, 1949.

Greg Anderson, Abhinav Verma, Isil Dillig, and Swarat Chaudhuri. Neurosym-
bolic reinforcement learning with formally veriﬁed exploration.
In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and
Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/
paper/2020/hash/448d5eda79895153938a8431919f4c9f-Abstract.html.

BIBLIOGRAPHY

142

Maximilian Baader, Matthew Mirman, and Martin T. Vechev. Universal approxi-
mation with certiﬁed networks. In 8th International Conference on Learning Repre-
sentations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,
2020. URL https://openreview.net/forum?id=B1gX8kBtPr.

Clark W. Barrett, Christopher L. Conway, Morgan Deters, Liana Hadarean, Dejan
Jovanovic, Tim King, Andrew Reynolds, and Cesare Tinelli. CVC4. In Ganesh
Gopalakrishnan and Shaz Qadeer, editors, Computer Aided Veriﬁcation - 23rd In-
ternational Conference, CAV 2011, Snowbird, UT, USA, July 14-20, 2011. Proceed-
ings, volume 6806 of Lecture Notes in Computer Science, pages 171–177. Springer,
2011. doi: 10.1007/978-3-642-22110-1\_14. URL https://doi.org/10.1007/
978-3-642-22110-1_14.

Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis,
Aditya V. Nori, and Antonio Criminisi. Measuring neural net robust-
ness with constraints.
In Daniel D. Lee, Masashi Sugiyama, Ulrike von
Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neu-
ral Information Processing Systems 29: Annual Conference on Neural Informa-
tion Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pages
2613–2621, 2016. URL https://proceedings.neurips.cc/paper/2016/hash/
980ecd059122ce2e50136bda65c25e07-Abstract.html.

Osbert Bastani, Yewen Pu, and Armando Solar-Lezama. Veriﬁable reinforce-
ment learning via policy extraction. In Samy Bengio, Hanna M. Wallach, Hugo
Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, edi-
tors, Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
Montréal, Canada, pages 2499–2509, 2018. URL https://proceedings.neurips.
cc/paper/2018/hash/e6d8545daa42d5ced125a4bf747b3688-Abstract.html.

Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust Optimization,
volume 28 of Princeton Series in Applied Mathematics. Princeton University Press,
2009. ISBN 978-1-4008-3105-0. doi: 10.1515/9781400831050. URL https://doi.
org/10.1515/9781400831050.

BIBLIOGRAPHY

143

Armin Biere, Marijn Heule, Hans van Maaren, and Toby Walsh, editors. Handbook
of Satisﬁability, volume 185 of Frontiers in Artiﬁcial Intelligence and Applications.
IOS Press, 2009. ISBN 978-1-58603-929-5.

Robert G. Bland. New ﬁnite pivoting rules for the simplex method. Math. Oper.
Res., 2(2):103–107, 1977. doi: 10.1287/moor.2.2.103. URL https://doi.org/10.
1287/moor.2.2.103.

Aleksandar Bojchevski, Johannes Klicpera, and Stephan Günnemann. Efﬁcient
robustness certiﬁcates for discrete data: Sparsity-aware randomized smoothing
for graphs, images and more. In Proceedings of the 37th International Conference
on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of
Proceedings of Machine Learning Research, pages 1003–1013. PMLR, 2020. URL
http://proceedings.mlr.press/v119/bojchevski20a.html.

Bob F Caviness and Jeremy R Johnson. Quantiﬁer elimination and cylindrical alge-

braic decomposition. Springer Science & Business Media, 2012.

Alonzo Church. A note on the entscheidungsproblem. J. Symb. Log., 1(1):40–41,

1936. doi: 10.2307/2269326. URL https://doi.org/10.2307/2269326.

Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certiﬁed adversarial ro-
bustness via randomized smoothing.
In Kamalika Chaudhuri and Ruslan
Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine
Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of
Proceedings of Machine Learning Research, pages 1310–1320. PMLR, 2019. URL
http://proceedings.mlr.press/v97/cohen19c.html.

Patrick Cousot and Radhia Cousot. Abstract interpretation: A uniﬁed lattice
model for static analysis of programs by construction or approximation of ﬁx-
In Robert M. Graham, Michael A. Harrison, and Ravi Sethi, editors,
points.
Conference Record of the Fourth ACM Symposium on Principles of Programming Lan-
guages, Los Angeles, California, USA, January 1977, pages 238–252. ACM, 1977.
doi: 10.1145/512950.512973. URL https://doi.org/10.1145/512950.512973.

Patrick Cousot and Nicolas Halbwachs. Automatic discovery of linear restraints
In Alfred V. Aho, Stephen N. Zilles, and

among variables of a program.

BIBLIOGRAPHY

144

Thomas G. Szymanski, editors, Conference Record of the Fifth Annual ACM Sym-
posium on Principles of Programming Languages, Tucson, Arizona, USA, January
1978, pages 84–96. ACM Press, 1978. doi: 10.1145/512760.512770. URL https:
//doi.org/10.1145/512760.512770.

George B Dantzig. Origins of the simplex method. In A history of scientiﬁc comput-

ing, pages 141–151. 1990.

Leonardo Mendonça de Moura and Nikolaj Bjørner. Z3: an efﬁcient SMT solver.
In C. R. Ramakrishnan and Jakob Rehof, editors, Tools and Algorithms for the
Construction and Analysis of Systems, 14th International Conference, TACAS 2008,
Held as Part of the Joint European Conferences on Theory and Practice of Software,
ETAPS 2008, Budapest, Hungary, March 29-April 6, 2008. Proceedings, volume 4963
of Lecture Notes in Computer Science, pages 337–340. Springer, 2008. doi: 10.1007/
978-3-540-78800-3\_24. URL https://doi.org/10.1007/978-3-540-78800-3_
24.

Marc Peter Deisenroth, A Aldo Faisal, and Cheng Soon Ong. Mathematics for ma-

chine learning. Cambridge University Press, 2020.

Samuel Drews, Aws Albarghouthi, and Loris D’Antoni. Proving data-poisoning
robustness in decision trees. In Alastair F. Donaldson and Emina Torlak, editors,
Proceedings of the 41st ACM SIGPLAN International Conference on Programming
Language Design and Implementation, PLDI 2020, London, UK, June 15-20, 2020,
pages 1083–1097. ACM, 2020. doi: 10.1145/3385412.3385975. URL https://
doi.org/10.1145/3385412.3385975.

Bruno Dutertre and Leonardo De Moura. Integrating simplex with dpll (t). Com-

puter Science Laboratory, SRI International, Tech. Rep. SRI-CSL-06-01, 2006.

Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. Hotﬂip: White-box
adversarial examples for text classiﬁcation.
In Iryna Gurevych and Yusuke
Miyao, editors, Proceedings of the 56th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2:
Short Papers, pages 31–36. Association for Computational Linguistics, 2018. doi:
10.18653/v1/P18-2006. URL https://www.aclweb.org/anthology/P18-2006/.

BIBLIOGRAPHY

145

Niklas Een. Minisat: A sat solver with conﬂict-clause minimization. In Proc. SAT-
05: 8th Int. Conf. on Theory and Applications of Satisﬁability Testing, pages 502–518,
2005.

Rüdiger Ehlers. Formal veriﬁcation of piece-wise linear feed-forward neural net-
works. In Deepak D’Souza and K. Narayan Kumar, editors, Automated Technol-
ogy for Veriﬁcation and Analysis - 15th International Symposium, ATVA 2017, Pune,
India, October 3-6, 2017, Proceedings, volume 10482 of Lecture Notes in Computer
Science, pages 269–286. Springer, 2017. doi: 10.1007/978-3-319-68167-2\_19.
URL https://doi.org/10.1007/978-3-319-68167-2_19.

Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei
Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world
attacks on deep learning visual classiﬁcation. In 2018 IEEE Conference on Com-
puter Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-
22, 2018, pages 1625–1634. IEEE Computer Society, 2018. doi: 10.1109/CVPR.
2018.00175. URL http://openaccess.thecvf.com/content_cvpr_2018/html/
Eykholt_Robust_Physical-World_Attacks_CVPR_2018_paper.html.

Aymeric Fromherz, Klas Leino, Matt Fredrikson, Bryan Parno, and Corina S.
Pasareanu. Fast geometric projections for local robustness certiﬁcation.
In
9th International Conference on Learning Representations, ICLR 2021, Virtual Event,
Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/
forum?id=zWy1uxjDdZJ.

Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin T. Vechev. AI2: safety and robustness certiﬁcation of
neural networks with abstract interpretation. In 2018 IEEE Symposium on Secu-
rity and Privacy, SP 2018, Proceedings, 21-23 May 2018, San Francisco, California,
USA, pages 3–18. IEEE Computer Society, 2018. doi: 10.1109/SP.2018.00058.
URL https://doi.org/10.1109/SP.2018.00058.

Antoine Girard. Reachability of uncertain linear systems using zonotopes.

In
Manfred Morari and Lothar Thiele, editors, Hybrid Systems: Computation and
Control, 8th International Workshop, HSCC 2005, Zurich, Switzerland, March 9-11,

BIBLIOGRAPHY

146

2005, Proceedings, volume 3414 of Lecture Notes in Computer Science, pages 291–
305. Springer, 2005. doi: 10.1007/978-3-540-31954-2\_19. URL https://doi.
org/10.1007/978-3-540-31954-2_19.

Ian J. Goodfellow, Yoshua Bengio, and Aaron C. Courville. Deep Learning. Adap-
tive computation and machine learning. MIT Press, 2016. ISBN 978-0-262-03561-
3. URL http://www.deeplearningbook.org/.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli
Qin, Jonathan Uesato, Relja Arandjelovic, Timothy A. Mann, and Pushmeet
Kohli. On the effectiveness of interval bound propagation for training veriﬁ-
ably robust models. CoRR, abs/1810.12715, 2018. URL http://arxiv.org/abs/
1810.12715.

C. A. R. Hoare. An axiomatic basis for computer programming. Commun. ACM,
12(10):576–580, 1969. doi: 10.1145/363235.363259. URL https://doi.org/10.
1145/363235.363259.

Kurt Hornik, Maxwell B. Stinchcombe, and Halbert White. Multilayer feedfor-
ward networks are universal approximators. Neural Networks, 2(5):359–366,
1989. doi: 10.1016/0893-6080(89)90020-8. URL https://doi.org/10.1016/
0893-6080(89)90020-8.

Po-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama,
Sven Gowal, Krishnamurthy Dvijotham, and Pushmeet Kohli. Achieving ver-
iﬁed robustness to symbol substitutions via interval bound propagation.
In
Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language Processing, EMNLP-
IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 4081–4091. Associ-
ation for Computational Linguistics, 2019. doi: 10.18653/v1/D19-1419. URL
https://doi.org/10.18653/v1/D19-1419.

Radoslav Ivanov, James Weimer, Rajeev Alur, George J. Pappas, and Insup Lee.
Verisig: verifying safety properties of hybrid systems with neural network
In Necmiye Ozay and Pavithra Prabhakar, editors, Proceedings of
controllers.

BIBLIOGRAPHY

147

the 22nd ACM International Conference on Hybrid Systems: Computation and Con-
trol, HSCC 2019, Montreal, QC, Canada, April 16-18, 2019, pages 169–178. ACM,
2019. doi: 10.1145/3302504.3311806. URL https://doi.org/10.1145/3302504.
3311806.

Kai Jia and Martin Rinard.

Efﬁcient exact veriﬁcation of binarized neural
networks, 2020a. URL https://proceedings.neurips.cc/paper/2020/hash/
1385974ed5904a438616ff7bdb3f7439-Abstract.html.

Kai Jia and Martin Rinard. Exploiting veriﬁed neural networks via ﬂoating point
numerical error. CoRR, abs/2003.03021, 2020b. URL https://arxiv.org/abs/
2003.03021.

Robin Jia, Aditi Raghunathan, Kerem Göksel, and Percy Liang. Certiﬁed robust-
ness to adversarial word substitutions. In Kentaro Inui, Jing Jiang, Vincent Ng,
and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on Natu-
ral Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7,
2019, pages 4127–4140. Association for Computational Linguistics, 2019. doi:
10.18653/v1/D19-1423. URL https://doi.org/10.18653/v1/D19-1423.

Guy Katz, Clark W. Barrett, David L. Dill, Kyle Julian, and Mykel J. Kochen-
derfer. Reluplex: An efﬁcient SMT solver for verifying deep neural net-
works. In Rupak Majumdar and Viktor Kuncak, editors, Computer Aided Ver-
iﬁcation - 29th International Conference, CAV 2017, Heidelberg, Germany, July 24-
28, 2017, Proceedings, Part I, volume 10426 of Lecture Notes in Computer Sci-
ence, pages 97–117. Springer, 2017. doi: 10.1007/978-3-319-63387-9\_5. URL
https://doi.org/10.1007/978-3-319-63387-9_5.

Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database.

ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist, 2, 2010.

Mathias Lécuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman
Jana. Certiﬁed robustness to adversarial examples with differential privacy. In
2019 IEEE Symposium on Security and Privacy, SP 2019, San Francisco, CA, USA,

BIBLIOGRAPHY

148

May 19-23, 2019, pages 656–672. IEEE, 2019. doi: 10.1109/SP.2019.00044. URL
https://doi.org/10.1109/SP.2019.00044.

Klas Leino, Zifan Wang, and Matt Fredrikson. Globally-robust neural networks.
In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International
Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, vol-
ume 139 of Proceedings of Machine Learning Research, pages 6212–6222. PMLR,
2021. URL http://proceedings.mlr.press/v139/leino21a.html.

Moshe Leshno, Vladimir Ya. Lin, Allan Pinkus, and Shimon Schocken. Multi-
layer feedforward networks with a nonpolynomial activation function can ap-
proximate any function. Neural Networks, 6(6):861–867, 1993. doi: 10.1016/
URL https://doi.org/10.1016/S0893-6080(05)
S0893-6080(05)80131-5.
80131-5.

Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger B. Grosse, and Jörn-
Henrik Jacobsen. Preventing gradient attenuation in lipschitz constrained con-
volutional networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelz-
imer, Florence d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances
in Neural Information Processing Systems 32: Annual Conference on Neural Infor-
mation Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver,
BC, Canada, pages 15364–15376, 2019. URL https://proceedings.neurips.cc/
paper/2019/hash/1ce3e6e3f452828e23a0c94572bef9d9-Abstract.html.

Changliu Liu, Tomer Arnon, Christopher Lazarus, Christopher A. Strong,
Clark W. Barrett, and Mykel J. Kochenderfer. Algorithms for verifying deep
neural networks. Found. Trends Optim., 4(3-4):244–404, 2021. doi: 10.1561/
2400000035. URL https://doi.org/10.1561/2400000035.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. Towards deep learning models resistant to adversarial attacks.
In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
2018. URL https://openreview.net/forum?id=rJzIBfZAb.

BIBLIOGRAPHY

149

Tomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estima-
tion of word representations in vector space. In Yoshua Bengio and Yann Le-
Cun, editors, 1st International Conference on Learning Representations, ICLR 2013,
Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings, 2013. URL
http://arxiv.org/abs/1301.3781.

Matthew Mirman, Timon Gehr, and Martin T. Vechev. Differentiable abstract inter-
pretation for provably robust neural networks. In Jennifer G. Dy and Andreas
Krause, editors, Proceedings of the 35th International Conference on Machine Learn-
ing, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80
of Proceedings of Machine Learning Research, pages 3575–3583. PMLR, 2018. URL
http://proceedings.mlr.press/v80/mirman18b.html.

Christoph Müller, Gagandeep Singh, Markus Püschel, and Martin T. Vechev. Neu-
ral network robustness veriﬁcation on gpus. CoRR, abs/2007.10868, 2020. URL
https://arxiv.org/abs/2007.10868.

Vinod Nair and Geoffrey E. Hinton. Rectiﬁed linear units improve restricted
boltzmann machines. In Johannes Fürnkranz and Thorsten Joachims, editors,
Proceedings of the 27th International Conference on Machine Learning (ICML-10),
June 21-24, 2010, Haifa, Israel, pages 807–814. Omnipress, 2010. URL https:
//icml.cc/Conferences/2010/papers/432.pdf.

Nina Narodytska, Shiva Prasad Kasiviswanathan, Leonid Ryzhyk, Mooly Sagiv,
and Toby Walsh. Verifying properties of binarized deep neural networks. In
Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-
Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative
Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on
Educational Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana,
USA, February 2-7, 2018, pages 6615–6624. AAAI Press, 2018. URL https://
www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16898.

Greg Nelson and Derek C. Oppen. Simpliﬁcation by cooperating decision proce-
dures. ACM Trans. Program. Lang. Syst., 1(2):245–257, 1979. doi: 10.1145/357073.
357079. URL https://doi.org/10.1145/357073.357079.

BIBLIOGRAPHY

150

Michael A. Nielsen. Neural Networks and Deep Learning. Determination Press, 2018.

URL http://neuralnetworksanddeeplearning.com/.

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gre-
gory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Al-
ban Desmaison, Andreas Köpf, Edward Yang, Zachary DeVito, Martin Raison,
Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
Soumith Chintala. Pytorch: An imperative style, high-performance deep learn-
ing library. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
d’Alché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural
Information Processing Systems 32: Annual Conference on Neural Information Pro-
cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
pages 8024–8035, 2019. URL https://proceedings.neurips.cc/paper/2019/
hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html.

Luca Pulina and Armando Tacchella. An abstraction-reﬁnement approach to
In Tayssir Touili, Byron Cook, and
veriﬁcation of artiﬁcial neural networks.
Paul B. Jackson, editors, Computer Aided Veriﬁcation, 22nd International Confer-
ence, CAV 2010, Edinburgh, UK, July 15-19, 2010. Proceedings, volume 6174 of
Lecture Notes in Computer Science, pages 243–257. Springer, 2010. doi: 10.1007/
978-3-642-14295-6\_24. URL https://doi.org/10.1007/978-3-642-14295-6_
24.

Chongli Qin, Krishnamurthy (Dj) Dvijotham, Brendan O’Donoghue, Rudy Bunel,
Robert Stanforth, Sven Gowal, Jonathan Uesato, Grzegorz Swirszcz, and Push-
meet Kohli. Veriﬁcation of non-linear speciﬁcations for neural networks. In 7th
International Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/
forum?id=HyeFAsRctQ.

Goutham Ramakrishnan and Aws Albarghouthi. Backdoors in neural models of
source code. CoRR, abs/2006.06841, 2020. URL https://arxiv.org/abs/2006.
06841.

Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, and J. Zico Kolter. Certi-
In Pro-

ﬁed robustness to label-ﬂipping attacks via randomized smoothing.

BIBLIOGRAPHY

151

ceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-
18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Re-
search, pages 8230–8241. PMLR, 2020. URL http://proceedings.mlr.press/
v119/rosenfeld20b.html.

Rahul Sharma, Aditya V. Nori, and Alex Aiken. Bias-variance tradeoffs in pro-
In Suresh Jagannathan and Peter Sewell, editors, The 41st
gram analysis.
Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Lan-
guages, POPL ’14, San Diego, CA, USA, January 20-21, 2014, pages 127–138. ACM,
2014. doi: 10.1145/2535838.2535853. URL https://doi.org/10.1145/2535838.
2535853.

Benjamin Sherman, Jesse Michel, and Michael Carbin. λs: computable seman-
tics for differentiable programming with higher-order functions and datatypes.
Proc. ACM Program. Lang., 5(POPL):1–31, 2021. doi: 10.1145/3434284. URL
https://doi.org/10.1145/3434284.

Gagandeep Singh, Markus Püschel, and Martin T. Vechev. Fast polyhedra abstract
In Giuseppe Castagna and Andrew D. Gordon, editors, Proceedings
domain.
of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages,
POPL 2017, Paris, France, January 18-20, 2017, pages 46–59. ACM, 2017. doi:
10.1145/3009837.3009885. URL https://doi.org/10.1145/3009837.3009885.

Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, and Mar-
tin T. Vechev.
In Samy Ben-
Fast and effective robustness certiﬁcation.
gio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-
Bianchi, and Roman Garnett, editors, Advances in Neural Information Pro-
cessing Systems 31: Annual Conference on Neural Information Processing Sys-
tems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages
URL https://proceedings.neurips.cc/paper/2018/
10825–10836, 2018.
hash/f2f446980d8e971ef3da97af089481c3-Abstract.html.

Gagandeep Singh, Rupanshu Ganvir, Markus Püschel, and Martin T. Vechev. Be-
yond the single neuron convex barrier for neural network certiﬁcation.
In
Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alché-
Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Informa-

BIBLIOGRAPHY

152

tion Processing Systems 32: Annual Conference on Neural Information Process-
ing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada,
pages 15072–15083, 2019a. URL https://proceedings.neurips.cc/paper/
2019/hash/0a9fdbb17feb6ccb7ec405cfb85222c4-Abstract.html.

Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin T. Vechev. An ab-
stract domain for certifying neural networks. Proc. ACM Program. Lang., 3
(POPL):41:1–41:30, 2019b. doi: 10.1145/3290354. URL https://doi.org/10.
1145/3290354.

Aishwarya Sivaraman, Golnoosh Farnadi, Todd D. Millstein, and Guy Van den
Broeck. Counterexample-guided learning of monotonic neural networks.
In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems
33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS
2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.
cc/paper/2020/hash/8ab70731b1553f17c11a3bbc87e0b605-Abstract.html.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In
Yoshua Bengio and Yann LeCun, editors, 2nd International Conference on Learning
Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
Proceedings, 2014. URL http://arxiv.org/abs/1312.6199.

Alfred Tarski. A decision method for elementary algebra and geometry. In Quanti-
ﬁer elimination and cylindrical algebraic decomposition, pages 24–84. Springer, 1998.

Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural
networks with mixed integer programming. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. Open-
Review.net, 2019a. URL https://openreview.net/forum?id=HyGIdiRqtm.

Vincent Tjeng, Kai Yuanqing Xiao, and Russ Tedrake. Evaluating robustness of
In 7th International Con-
neural networks with mixed integer programming.
ference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-

BIBLIOGRAPHY

153

9, 2019. OpenReview.net, 2019b. URL https://openreview.net/forum?id=
HyGIdiRqtm.

Hoang-Dung Tran, Stanley Bak, Weiming Xiang, and Taylor T. Johnson. Veriﬁca-
tion of deep convolutional neural networks using imagestars. In Shuvendu K.
Lahiri and Chao Wang, editors, Computer Aided Veriﬁcation - 32nd International
Conference, CAV 2020, Los Angeles, CA, USA, July 21-24, 2020, Proceedings, Part
I, volume 12224 of Lecture Notes in Computer Science, pages 18–42. Springer,
2020a. doi: 10.1007/978-3-030-53288-8\_2. URL https://doi.org/10.1007/
978-3-030-53288-8_2.

Hoang-Dung Tran, Stanley Bak, Weiming Xiang, and Taylor T. Johnson. Veriﬁca-
tion of deep convolutional neural networks using imagestars. In Shuvendu K.
Lahiri and Chao Wang, editors, Computer Aided Veriﬁcation - 32nd International
Conference, CAV 2020, Los Angeles, CA, USA, July 21-24, 2020, Proceedings, Part
I, volume 12224 of Lecture Notes in Computer Science, pages 18–42. Springer,
2020b. doi: 10.1007/978-3-030-53288-8\_2. URL https://doi.org/10.1007/
978-3-030-53288-8_2.

Asher Trockman and J. Zico Kolter. Orthogonalizing convolutional layers with
the cayley transform. In 9th International Conference on Learning Representations,
ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL
https://openreview.net/forum?id=Pbj8H_jEHYv.

Alan Turing. Intelligent machinery. 1948. The Essential Turing, page 395, 1969.

Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. For-
mal security analysis of neural networks using symbolic intervals. In William
Enck and Adrienne Porter Felt, editors, 27th USENIX Security Symposium,
USENIX Security 2018, Baltimore, MD, USA, August 15-17, 2018, pages 1599–
1614. USENIX Association, 2018. URL https://www.usenix.org/conference/
usenixsecurity18/presentation/wang-shiqi.

Zi Wang, Aws Albarghouthi, Gautam Prakriya, and Somesh Jha.

Interval uni-
versal approximation for neural networks. CoRR, abs/2007.06093, 2020. URL
https://arxiv.org/abs/2007.06093.

BIBLIOGRAPHY

154

Tsui-Wei Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca
Daniel, Duane S. Boning, and Inderjit S. Dhillon. Towards fast computa-
tion of certiﬁed robustness for relu networks. In Jennifer G. Dy and Andreas
Krause, editors, Proceedings of the 35th International Conference on Machine Learn-
ing, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80
of Proceedings of Machine Learning Research, pages 5273–5282. PMLR, 2018. URL
http://proceedings.mlr.press/v80/weng18a.html.

Jimmy Westburg.

https://tex.stackexchange.com/questions/356121/

how-to-draw-these-polyhedrons, 2017.

Kai Y. Xiao, Vincent Tjeng, Nur Muhammad (Mahi) Shaﬁullah, and Aleksander
Madry. Training for faster adversarial robustness veriﬁcation via inducing relu
stability. In 7th International Conference on Learning Representations, ICLR 2019,
New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://
openreview.net/forum?id=BJfIVjAcKm.

Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie
Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturba-
In H. Larochelle,
tion analysis for scalable certiﬁed robustness and beyond.
M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neu-
ral Information Processing Systems, volume 33, pages 1129–1141. Curran Asso-
ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/
0cbc5671ae26f67871cb914d81ef8fc1-Paper.pdf.

Mao Ye, Chengyue Gong, and Qiang Liu. SAFER: A structure-free approach for
certiﬁed robustness to adversarial word substitutions. In Dan Jurafsky, Joyce
Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th An-
nual Meeting of the Association for Computational Linguistics, ACL 2020, Online,
July 5-10, 2020, pages 3465–3475. Association for Computational Linguistics,
2020. doi: 10.18653/v1/2020.acl-main.317. URL https://doi.org/10.18653/
v1/2020.acl-main.317.

Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel.
Efﬁcient neural network robustness certiﬁcation with general activation func-
In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grau-
tions.

BIBLIOGRAPHY

155

man, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Pro-
cessing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages
4944–4953, 2018a. URL https://proceedings.neurips.cc/paper/2018/hash/
d04863f100d59b3eb688a11f95b0ae60-Abstract.html.

Xin Zhang, Armando Solar-Lezama, and Rishabh Singh.

Interpreting neu-
ral network judgments via minimal, stable, and symbolic corrections.
In
Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò
Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information
Processing Systems 31: Annual Conference on Neural Information Processing
Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages
4879–4890, 2018b. URL https://proceedings.neurips.cc/paper/2018/hash/
300891a62162b960cf02ce3827bb363c-Abstract.html.

Yuhao Zhang, Aws Albarghouthi, and Loris D’Antoni. Robustness to pro-
grammable string transformations via augmented abstract training. In Proceed-
ings of the 37th International Conference on Machine Learning, ICML 2020, 13-18
July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research,
pages 11023–11032. PMLR, 2020. URL http://proceedings.mlr.press/v119/
zhang20b.html.

Yuhao Zhang, Aws Albarghouthi, and Loris D’Antoni. Certiﬁed robustness to
programmable transformations in LSTMs. CoRR, abs/2102.07818, 2021. URL
https://arxiv.org/abs/2102.07818.

He Zhu, Zikang Xiong, Stephen Magill, and Suresh Jagannathan. An induc-
tive synthesis framework for veriﬁable reinforcement learning. In Kathryn S.
McKinley and Kathleen Fisher, editors, Proceedings of the 40th ACM SIGPLAN
Conference on Programming Language Design and Implementation, PLDI 2019,
Phoenix, AZ, USA, June 22-26, 2019, pages 686–701. ACM, 2019. doi: 10.1145/
3314221.3314638. URL https://doi.org/10.1145/3314221.3314638.

